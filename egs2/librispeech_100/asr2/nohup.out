2024-10-12T23:26:52 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_noctc_1gpu.yaml --inference_config conf/decode_ctc0.3.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-10-12T23:26:52 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-10-12T23:26:52 (asr2.sh:1420:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-10-12T23:26:52 (asr2.sh:1463:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-10-12T23:26:52 (asr2.sh:1467:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-10-12 23:26:52,590 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_noctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-10-12 23:26:52,606 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 358, in main
    process.wait(0.5)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
KeyboardInterrupt
2024-10-12T23:27:08 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_noctc_1gpu.yaml --inference_config conf/decode_ctc0.3.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-10-12T23:27:08 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-10-12T23:27:08 (asr2.sh:1420:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-10-12T23:27:08 (asr2.sh:1463:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-10-12T23:27:08 (asr2.sh:1467:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-10-12 23:27:08,888 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_noctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-10-12 23:27:08,902 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
2024-10-13T07:04:40 (asr2.sh:1782:main) Successfully finished. [elapsed=27452s]
2024-10-13T13:31:47 (asr2.sh:288:main) ./asr2.sh --stage 14 --stop_stage 14 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_noctc_1gpu.yaml --inference_config conf/decode_ctc0.0.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-10-13T13:31:47 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-10-13T13:31:47 (asr2.sh:1542:main) Stage 14: Decoding: training_dir=exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp
2024-10-13T13:31:47 (asr2.sh:1570:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc0.0_asr_model_valid.acc.ave/run.sh'. You can resume the process from stage 14 using this script
2024-10-13T13:31:47 (asr2.sh:1599:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc0.0_asr_model_valid.acc.ave/test_clean/logdir/asr_inference.*.log'
2024-10-13T14:04:26 (asr2.sh:1599:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc0.0_asr_model_valid.acc.ave/test_other/logdir/asr_inference.*.log'
2024-10-13T14:35:57 (asr2.sh:1599:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc0.0_asr_model_valid.acc.ave/dev_clean/logdir/asr_inference.*.log'
2024-10-13T15:16:15 (asr2.sh:1599:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc0.0_asr_model_valid.acc.ave/dev_other/logdir/asr_inference.*.log'
2024-10-13T16:09:44 (asr2.sh:1782:main) Successfully finished. [elapsed=9477s]
2024-11-17T13:03:23 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-17T13:03:23 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-17T13:03:23 (asr2.sh:1420:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-11-17T13:03:23 (asr2.sh:1463:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-11-17T13:03:23 (asr2.sh:1467:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-11-17 13:03:23,907 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-11-17 13:03:23,921 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
2024-11-17T13:13:05 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-17T13:13:05 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-17T13:13:05 (asr2.sh:1420:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-11-17T13:13:05 (asr2.sh:1463:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-11-17T13:13:05 (asr2.sh:1467:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-11-17 13:13:05,904 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-11-17 13:13:05,917 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
2024-11-17T15:51:18 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --gpu_kmeans false --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large_myst/21 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 4 --src_lang wavlm_large_myst_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-17T15:51:18 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-17T15:51:18 (asr2.sh:1421:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-11-17T15:51:18 (asr2.sh:1464:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-11-17T15:51:18 (asr2.sh:1468:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-11-17 15:51:18,244 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-11-17 15:51:18,257 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
2024-11-17T19:24:24 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-17T19:24:24 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-17T19:24:24 (asr2.sh:1421:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-11-17T19:24:24 (asr2.sh:1464:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-11-17T19:24:24 (asr2.sh:1468:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-11-17 19:24:24,862 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-11-17 19:24:24,876 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
2024-11-17T19:24:35 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --gpu_kmeans false --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large_myst/21 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 4 --src_lang wavlm_large_myst_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-17T19:24:35 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-17T19:24:35 (asr2.sh:1421:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-11-17T19:24:35 (asr2.sh:1464:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-11-17T19:24:35 (asr2.sh:1468:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-11-17 19:24:35,426 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-11-17 19:24:35,440 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
2024-11-18T00:39:19 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-18T00:39:19 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-18T00:39:19 (asr2.sh:1421:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-11-18T00:39:19 (asr2.sh:1464:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-11-18T00:39:19 (asr2.sh:1468:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-11-18 00:39:19,406 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-11-18 00:39:19,420 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
2024-11-18T00:39:23 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --gpu_kmeans false --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large_myst/21 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 4 --src_lang wavlm_large_myst_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-18T00:39:23 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-18T00:39:23 (asr2.sh:1421:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-11-18T00:39:23 (asr2.sh:1464:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-11-18T00:39:23 (asr2.sh:1468:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-11-18 00:39:23,407 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-11-18 00:39:23,420 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
2024-11-18T03:26:27 (asr2.sh:1783:main) Successfully finished. [elapsed=10028s]
2024-11-18T06:49:59 (asr2.sh:1783:main) Successfully finished. [elapsed=22236s]
2024-11-18T20:00:25 (asr2.sh:288:main) ./asr2.sh --stage 14 --stop_stage 14 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-18T20:00:25 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-18T20:00:25 (asr2.sh:1543:main) Stage 14: Decoding: training_dir=exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp
2024-11-18T20:00:25 (asr2.sh:1571:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/run.sh'. You can resume the process from stage 14 using this script
2024-11-18T20:00:25 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/asr_inference.*.log'
run.pl: 2 / 2 failed, log is in exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/asr_inference.*.log
# python3 -m espnet2.bin.mt_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/test_clean/text.rm.wavlm_large_21_km2000,src_text,text --key_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/keys.1.scp --mt_train_config exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml --mt_model_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/output.1 --config conf/decode_ctc1.0_greedy.yaml 
# Started at Mon Nov 18 20:00:25 PST 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/mt_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/test_clean/text.rm.wavlm_large_21_km2000,src_text,text --key_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/keys.1.scp --mt_train_config exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml --mt_model_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/output.1 --config conf/decode_ctc1.0_greedy.yaml
2024-11-18 20:00:31,969 (abs_task:2300) INFO: config file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml
2024-11-18 20:00:32,356 (mt:347) INFO: Vocabulary size: 5000
2024-11-18 20:00:32,356 (mt:361) INFO: Source vocabulary size: 6000
2024-11-18 20:00:32,577 (discrete_asr_espnet_model:96) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 561, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 557, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 348, in inference
    text2text = Text2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 269, in from_pretrained
    return Text2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 67, in __init__
    mt_model, mt_train_args = MTTask.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth'
# Accounting: time=8 threads=1
# Ended (code 1) at Mon Nov 18 20:00:33 PST 2024, elapsed time 8 seconds
# python3 -m espnet2.bin.mt_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/test_clean/text.rm.wavlm_large_21_km2000,src_text,text --key_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/keys.2.scp --mt_train_config exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml --mt_model_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/output.2 --config conf/decode_ctc1.0_greedy.yaml 
# Started at Mon Nov 18 20:00:25 PST 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/mt_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/test_clean/text.rm.wavlm_large_21_km2000,src_text,text --key_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/keys.2.scp --mt_train_config exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml --mt_model_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/output.2 --config conf/decode_ctc1.0_greedy.yaml
2024-11-18 20:00:31,692 (abs_task:2300) INFO: config file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml
2024-11-18 20:00:32,080 (mt:347) INFO: Vocabulary size: 5000
2024-11-18 20:00:32,080 (mt:361) INFO: Source vocabulary size: 6000
2024-11-18 20:00:32,279 (discrete_asr_espnet_model:96) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 561, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 557, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 348, in inference
    text2text = Text2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 269, in from_pretrained
    return Text2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 67, in __init__
    mt_model, mt_train_args = MTTask.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth'
# Accounting: time=7 threads=1
# Ended (code 1) at Mon Nov 18 20:00:32 PST 2024, elapsed time 7 seconds
2024-11-18T20:00:48 (asr2.sh:288:main) ./asr2.sh --stage 14 --stop_stage 14 --gpu_kmeans false --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large_myst/21 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --src_lang wavlm_large_myst_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-18T20:00:48 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-18T20:00:48 (asr2.sh:1543:main) Stage 14: Decoding: training_dir=exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp
2024-11-18T20:00:49 (asr2.sh:1571:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/run.sh'. You can resume the process from stage 14 using this script
2024-11-18T20:00:49 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/asr_inference.*.log'
run.pl: 2 / 2 failed, log is in exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/asr_inference.*.log
# python3 -m espnet2.bin.mt_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/test_clean/text.rm.wavlm_large_myst_21_km2000,src_text,text --key_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/keys.1.scp --mt_train_config exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml --mt_model_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/output.1 --config conf/decode_ctc1.0_greedy.yaml 
# Started at Mon Nov 18 20:00:49 PST 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/mt_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/test_clean/text.rm.wavlm_large_myst_21_km2000,src_text,text --key_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/keys.1.scp --mt_train_config exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml --mt_model_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/output.1 --config conf/decode_ctc1.0_greedy.yaml
2024-11-18 20:00:54,035 (abs_task:2300) INFO: config file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml
2024-11-18 20:00:54,389 (mt:347) INFO: Vocabulary size: 5000
2024-11-18 20:00:54,389 (mt:361) INFO: Source vocabulary size: 6000
2024-11-18 20:00:54,557 (discrete_asr_espnet_model:96) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 561, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 557, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 348, in inference
    text2text = Text2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 269, in from_pretrained
    return Text2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 67, in __init__
    mt_model, mt_train_args = MTTask.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth'
# Accounting: time=6 threads=1
# Ended (code 1) at Mon Nov 18 20:00:55 PST 2024, elapsed time 6 seconds
# python3 -m espnet2.bin.mt_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/test_clean/text.rm.wavlm_large_myst_21_km2000,src_text,text --key_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/keys.2.scp --mt_train_config exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml --mt_model_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/output.2 --config conf/decode_ctc1.0_greedy.yaml 
# Started at Mon Nov 18 20:00:49 PST 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/mt_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/test_clean/text.rm.wavlm_large_myst_21_km2000,src_text,text --key_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/keys.2.scp --mt_train_config exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml --mt_model_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/output.2 --config conf/decode_ctc1.0_greedy.yaml
2024-11-18 20:00:54,032 (abs_task:2300) INFO: config file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml
2024-11-18 20:00:54,396 (mt:347) INFO: Vocabulary size: 5000
2024-11-18 20:00:54,396 (mt:361) INFO: Source vocabulary size: 6000
2024-11-18 20:00:54,566 (discrete_asr_espnet_model:96) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 561, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 557, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 348, in inference
    text2text = Text2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 269, in from_pretrained
    return Text2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 67, in __init__
    mt_model, mt_train_args = MTTask.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth'
# Accounting: time=6 threads=1
# Ended (code 1) at Mon Nov 18 20:00:55 PST 2024, elapsed time 6 seconds
2024-11-18T20:03:46 (asr2.sh:288:main) ./asr2.sh --stage 14 --stop_stage 14 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-18T20:03:46 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-18T20:03:47 (asr2.sh:1543:main) Stage 14: Decoding: training_dir=exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp
2024-11-18T20:03:47 (asr2.sh:1571:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 14 using this script
2024-11-18T20:03:47 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/test_clean/logdir/asr_inference.*.log'
2024-11-18T20:03:48 (asr2.sh:288:main) ./asr2.sh --stage 14 --stop_stage 14 --gpu_kmeans false --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large_myst/21 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_myst_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-18T20:03:48 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-18T20:03:48 (asr2.sh:1543:main) Stage 14: Decoding: training_dir=exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp
2024-11-18T20:03:48 (asr2.sh:1571:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 14 using this script
2024-11-18T20:03:48 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/test_clean/logdir/asr_inference.*.log'
2024-11-18T20:15:14 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/test_other/logdir/asr_inference.*.log'
2024-11-18T20:15:20 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/test_other/logdir/asr_inference.*.log'
2024-11-18T20:25:41 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_clean/logdir/asr_inference.*.log'
2024-11-18T20:26:12 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_clean/logdir/asr_inference.*.log'
2024-11-18T20:37:29 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_other/logdir/asr_inference.*.log'
2024-11-18T20:38:06 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_other/logdir/asr_inference.*.log'
2024-11-18T20:48:08 (asr2.sh:1783:main) Successfully finished. [elapsed=2662s]
2024-11-18T20:49:06 (asr2.sh:1783:main) Successfully finished. [elapsed=2718s]
2024-11-23T23:34:57 (asr2_hf.sh:288:main) ./asr2_hf.sh --stage 13 --stop_stage 13 --gpu_kmeans false --kmeans_opts --batch_bins 1 --nj 4 --kmeans_feature wavlm_large_myst_finetune/24 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_myst_finetune_24_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_finetune_24_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-23T23:34:57 (asr2_hf.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-23T23:34:57 (asr2_hf.sh:1424:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-11-23T23:34:57 (asr2_hf.sh:1467:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-11-23T23:34:57 (asr2_hf.sh:1471:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-11-23 23:34:57,282 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-11-23 23:34:57,297 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
                                                                                   2024-11-25T15:16:39 (asr2_hf.sh:288:main) ./asr2_hf.sh --stage 14 --stop_stage 14 --gpu_kmeans false --kmeans_opts --batch_bins 1 --nj 4 --kmeans_feature wavlm_large_myst_finetune/24 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_myst_finetune_24_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_finetune_24_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-25T15:16:39 (asr2_hf.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-25T15:16:39 (asr2_hf.sh:1546:main) Stage 14: Decoding: training_dir=exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp
2024-11-25T15:16:39 (asr2_hf.sh:1574:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 14 using this script
2024-11-25T15:16:39 (asr2_hf.sh:1605:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/test_clean/logdir/asr_inference.*.log'
2024-11-25T15:29:56 (asr2_hf.sh:1605:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/test_other/logdir/asr_inference.*.log'
2024-11-25T15:41:30 (asr2_hf.sh:1605:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_clean/logdir/asr_inference.*.log'
2024-11-25T21:26:27 (asr2_hf.sh:288:main) ./asr2_hf.sh --stage 14 --stop_stage 14 --gpu_kmeans false --kmeans_opts --batch_bins 1 --nj 4 --kmeans_feature wavlm_large_myst_finetune/24 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_myst_finetune_24_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_finetune_24_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-25T21:26:27 (asr2_hf.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-25T21:26:27 (asr2_hf.sh:1546:main) Stage 14: Decoding: training_dir=exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp
2024-11-25T21:26:27 (asr2_hf.sh:1574:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 14 using this script
2024-11-25T21:26:27 (asr2_hf.sh:1605:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_clean/logdir/asr_inference.*.log'
2024-11-25T21:39:58 (asr2_hf.sh:1605:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_other/logdir/asr_inference.*.log'
2024-11-25T21:51:49 (asr2_hf.sh:1790:main) Successfully finished. [elapsed=1522s]
run_onlyctc_myst_ft_kmeans.sh: line 62: unexpected EOF while looking for matching `"'
