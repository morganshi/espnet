2024-10-12T23:26:52 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_noctc_1gpu.yaml --inference_config conf/decode_ctc0.3.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-10-12T23:26:52 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-10-12T23:26:52 (asr2.sh:1420:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-10-12T23:26:52 (asr2.sh:1463:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-10-12T23:26:52 (asr2.sh:1467:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-10-12 23:26:52,590 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_noctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-10-12 23:26:52,606 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 358, in main
    process.wait(0.5)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
KeyboardInterrupt
2024-10-12T23:27:08 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_noctc_1gpu.yaml --inference_config conf/decode_ctc0.3.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-10-12T23:27:08 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-10-12T23:27:08 (asr2.sh:1420:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-10-12T23:27:08 (asr2.sh:1463:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-10-12T23:27:08 (asr2.sh:1467:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-10-12 23:27:08,888 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_noctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-10-12 23:27:08,902 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
2024-10-13T07:04:40 (asr2.sh:1782:main) Successfully finished. [elapsed=27452s]
2024-10-13T13:31:47 (asr2.sh:288:main) ./asr2.sh --stage 14 --stop_stage 14 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_noctc_1gpu.yaml --inference_config conf/decode_ctc0.0.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-10-13T13:31:47 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-10-13T13:31:47 (asr2.sh:1542:main) Stage 14: Decoding: training_dir=exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp
2024-10-13T13:31:47 (asr2.sh:1570:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc0.0_asr_model_valid.acc.ave/run.sh'. You can resume the process from stage 14 using this script
2024-10-13T13:31:47 (asr2.sh:1599:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc0.0_asr_model_valid.acc.ave/test_clean/logdir/asr_inference.*.log'
2024-10-13T14:04:26 (asr2.sh:1599:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc0.0_asr_model_valid.acc.ave/test_other/logdir/asr_inference.*.log'
2024-10-13T14:35:57 (asr2.sh:1599:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc0.0_asr_model_valid.acc.ave/dev_clean/logdir/asr_inference.*.log'
2024-10-13T15:16:15 (asr2.sh:1599:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_noctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc0.0_asr_model_valid.acc.ave/dev_other/logdir/asr_inference.*.log'
2024-10-13T16:09:44 (asr2.sh:1782:main) Successfully finished. [elapsed=9477s]
2024-11-17T13:03:23 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-17T13:03:23 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-17T13:03:23 (asr2.sh:1420:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-11-17T13:03:23 (asr2.sh:1463:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-11-17T13:03:23 (asr2.sh:1467:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-11-17 13:03:23,907 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-11-17 13:03:23,921 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
2024-11-17T13:13:05 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-17T13:13:05 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-17T13:13:05 (asr2.sh:1420:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-11-17T13:13:05 (asr2.sh:1463:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-11-17T13:13:05 (asr2.sh:1467:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-11-17 13:13:05,904 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-11-17 13:13:05,917 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
2024-11-17T15:51:18 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --gpu_kmeans false --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large_myst/21 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 4 --src_lang wavlm_large_myst_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-17T15:51:18 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-17T15:51:18 (asr2.sh:1421:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-11-17T15:51:18 (asr2.sh:1464:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-11-17T15:51:18 (asr2.sh:1468:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-11-17 15:51:18,244 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-11-17 15:51:18,257 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
2024-11-17T19:24:24 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-17T19:24:24 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-17T19:24:24 (asr2.sh:1421:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-11-17T19:24:24 (asr2.sh:1464:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-11-17T19:24:24 (asr2.sh:1468:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-11-17 19:24:24,862 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-11-17 19:24:24,876 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
2024-11-17T19:24:35 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --gpu_kmeans false --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large_myst/21 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 4 --src_lang wavlm_large_myst_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-17T19:24:35 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-17T19:24:35 (asr2.sh:1421:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-11-17T19:24:35 (asr2.sh:1464:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-11-17T19:24:35 (asr2.sh:1468:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-11-17 19:24:35,426 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-11-17 19:24:35,440 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
2024-11-18T00:39:19 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-18T00:39:19 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-18T00:39:19 (asr2.sh:1421:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-11-18T00:39:19 (asr2.sh:1464:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-11-18T00:39:19 (asr2.sh:1468:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-11-18 00:39:19,406 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-11-18 00:39:19,420 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
2024-11-18T00:39:23 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --gpu_kmeans false --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large_myst/21 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 4 --src_lang wavlm_large_myst_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-18T00:39:23 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-18T00:39:23 (asr2.sh:1421:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-11-18T00:39:23 (asr2.sh:1464:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-11-18T00:39:23 (asr2.sh:1468:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-11-18 00:39:23,407 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-11-18 00:39:23,420 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
2024-11-18T03:26:27 (asr2.sh:1783:main) Successfully finished. [elapsed=10028s]
2024-11-18T06:49:59 (asr2.sh:1783:main) Successfully finished. [elapsed=22236s]
2024-11-18T20:00:25 (asr2.sh:288:main) ./asr2.sh --stage 14 --stop_stage 14 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-18T20:00:25 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-18T20:00:25 (asr2.sh:1543:main) Stage 14: Decoding: training_dir=exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp
2024-11-18T20:00:25 (asr2.sh:1571:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/run.sh'. You can resume the process from stage 14 using this script
2024-11-18T20:00:25 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/asr_inference.*.log'
run.pl: 2 / 2 failed, log is in exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/asr_inference.*.log
# python3 -m espnet2.bin.mt_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/test_clean/text.rm.wavlm_large_21_km2000,src_text,text --key_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/keys.1.scp --mt_train_config exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml --mt_model_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/output.1 --config conf/decode_ctc1.0_greedy.yaml 
# Started at Mon Nov 18 20:00:25 PST 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/mt_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/test_clean/text.rm.wavlm_large_21_km2000,src_text,text --key_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/keys.1.scp --mt_train_config exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml --mt_model_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/output.1 --config conf/decode_ctc1.0_greedy.yaml
2024-11-18 20:00:31,969 (abs_task:2300) INFO: config file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml
2024-11-18 20:00:32,356 (mt:347) INFO: Vocabulary size: 5000
2024-11-18 20:00:32,356 (mt:361) INFO: Source vocabulary size: 6000
2024-11-18 20:00:32,577 (discrete_asr_espnet_model:96) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 561, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 557, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 348, in inference
    text2text = Text2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 269, in from_pretrained
    return Text2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 67, in __init__
    mt_model, mt_train_args = MTTask.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth'
# Accounting: time=8 threads=1
# Ended (code 1) at Mon Nov 18 20:00:33 PST 2024, elapsed time 8 seconds
# python3 -m espnet2.bin.mt_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/test_clean/text.rm.wavlm_large_21_km2000,src_text,text --key_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/keys.2.scp --mt_train_config exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml --mt_model_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/output.2 --config conf/decode_ctc1.0_greedy.yaml 
# Started at Mon Nov 18 20:00:25 PST 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/mt_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/test_clean/text.rm.wavlm_large_21_km2000,src_text,text --key_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/keys.2.scp --mt_train_config exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml --mt_model_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/output.2 --config conf/decode_ctc1.0_greedy.yaml
2024-11-18 20:00:31,692 (abs_task:2300) INFO: config file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml
2024-11-18 20:00:32,080 (mt:347) INFO: Vocabulary size: 5000
2024-11-18 20:00:32,080 (mt:361) INFO: Source vocabulary size: 6000
2024-11-18 20:00:32,279 (discrete_asr_espnet_model:96) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 561, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 557, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 348, in inference
    text2text = Text2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 269, in from_pretrained
    return Text2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 67, in __init__
    mt_model, mt_train_args = MTTask.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth'
# Accounting: time=7 threads=1
# Ended (code 1) at Mon Nov 18 20:00:32 PST 2024, elapsed time 7 seconds
2024-11-18T20:00:48 (asr2.sh:288:main) ./asr2.sh --stage 14 --stop_stage 14 --gpu_kmeans false --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large_myst/21 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --src_lang wavlm_large_myst_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-18T20:00:48 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-18T20:00:48 (asr2.sh:1543:main) Stage 14: Decoding: training_dir=exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp
2024-11-18T20:00:49 (asr2.sh:1571:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/run.sh'. You can resume the process from stage 14 using this script
2024-11-18T20:00:49 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/asr_inference.*.log'
run.pl: 2 / 2 failed, log is in exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/asr_inference.*.log
# python3 -m espnet2.bin.mt_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/test_clean/text.rm.wavlm_large_myst_21_km2000,src_text,text --key_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/keys.1.scp --mt_train_config exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml --mt_model_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/output.1 --config conf/decode_ctc1.0_greedy.yaml 
# Started at Mon Nov 18 20:00:49 PST 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/mt_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/test_clean/text.rm.wavlm_large_myst_21_km2000,src_text,text --key_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/keys.1.scp --mt_train_config exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml --mt_model_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/output.1 --config conf/decode_ctc1.0_greedy.yaml
2024-11-18 20:00:54,035 (abs_task:2300) INFO: config file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml
2024-11-18 20:00:54,389 (mt:347) INFO: Vocabulary size: 5000
2024-11-18 20:00:54,389 (mt:361) INFO: Source vocabulary size: 6000
2024-11-18 20:00:54,557 (discrete_asr_espnet_model:96) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 561, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 557, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 348, in inference
    text2text = Text2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 269, in from_pretrained
    return Text2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 67, in __init__
    mt_model, mt_train_args = MTTask.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth'
# Accounting: time=6 threads=1
# Ended (code 1) at Mon Nov 18 20:00:55 PST 2024, elapsed time 6 seconds
# python3 -m espnet2.bin.mt_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/test_clean/text.rm.wavlm_large_myst_21_km2000,src_text,text --key_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/keys.2.scp --mt_train_config exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml --mt_model_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/output.2 --config conf/decode_ctc1.0_greedy.yaml 
# Started at Mon Nov 18 20:00:49 PST 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/mt_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/test_clean/text.rm.wavlm_large_myst_21_km2000,src_text,text --key_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/keys.2.scp --mt_train_config exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml --mt_model_file exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.acc.ave/test_clean/logdir/output.2 --config conf/decode_ctc1.0_greedy.yaml
2024-11-18 20:00:54,032 (abs_task:2300) INFO: config file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/config.yaml
2024-11-18 20:00:54,396 (mt:347) INFO: Vocabulary size: 5000
2024-11-18 20:00:54,396 (mt:361) INFO: Source vocabulary size: 6000
2024-11-18 20:00:54,566 (discrete_asr_espnet_model:96) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 561, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 557, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 348, in inference
    text2text = Text2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 269, in from_pretrained
    return Text2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_inference.py", line 67, in __init__
    mt_model, mt_train_args = MTTask.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/valid.acc.ave.pth'
# Accounting: time=6 threads=1
# Ended (code 1) at Mon Nov 18 20:00:55 PST 2024, elapsed time 6 seconds
2024-11-18T20:03:46 (asr2.sh:288:main) ./asr2.sh --stage 14 --stop_stage 14 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-18T20:03:46 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-18T20:03:47 (asr2.sh:1543:main) Stage 14: Decoding: training_dir=exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp
2024-11-18T20:03:47 (asr2.sh:1571:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 14 using this script
2024-11-18T20:03:47 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/test_clean/logdir/asr_inference.*.log'
2024-11-18T20:03:48 (asr2.sh:288:main) ./asr2.sh --stage 14 --stop_stage 14 --gpu_kmeans false --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large_myst/21 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_myst_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-18T20:03:48 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-18T20:03:48 (asr2.sh:1543:main) Stage 14: Decoding: training_dir=exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp
2024-11-18T20:03:48 (asr2.sh:1571:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 14 using this script
2024-11-18T20:03:48 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/test_clean/logdir/asr_inference.*.log'
2024-11-18T20:15:14 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/test_other/logdir/asr_inference.*.log'
2024-11-18T20:15:20 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/test_other/logdir/asr_inference.*.log'
2024-11-18T20:25:41 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_clean/logdir/asr_inference.*.log'
2024-11-18T20:26:12 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_clean/logdir/asr_inference.*.log'
2024-11-18T20:37:29 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_other/logdir/asr_inference.*.log'
2024-11-18T20:38:06 (asr2.sh:1600:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_other/logdir/asr_inference.*.log'
2024-11-18T20:48:08 (asr2.sh:1783:main) Successfully finished. [elapsed=2662s]
2024-11-18T20:49:06 (asr2.sh:1783:main) Successfully finished. [elapsed=2718s]
2024-11-23T23:34:57 (asr2_hf.sh:288:main) ./asr2_hf.sh --stage 13 --stop_stage 13 --gpu_kmeans false --kmeans_opts --batch_bins 1 --nj 4 --kmeans_feature wavlm_large_myst_finetune/24 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_myst_finetune_24_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_finetune_24_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-23T23:34:57 (asr2_hf.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-23T23:34:57 (asr2_hf.sh:1424:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2024-11-23T23:34:57 (asr2_hf.sh:1467:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/run.sh'. You can resume the process from stage 13 using this script
2024-11-23T23:34:57 (asr2_hf.sh:1471:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/train.log'
2024-11-23 23:34:57,282 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000_sp/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000_sp/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000_sp/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000_sp/train/text_shape.bpe
2024-11-23 23:34:57,297 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/train.log
                                                                                   2024-11-25T15:16:39 (asr2_hf.sh:288:main) ./asr2_hf.sh --stage 14 --stop_stage 14 --gpu_kmeans false --kmeans_opts --batch_bins 1 --nj 4 --kmeans_feature wavlm_large_myst_finetune/24 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_myst_finetune_24_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_finetune_24_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-25T15:16:39 (asr2_hf.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-25T15:16:39 (asr2_hf.sh:1546:main) Stage 14: Decoding: training_dir=exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp
2024-11-25T15:16:39 (asr2_hf.sh:1574:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 14 using this script
2024-11-25T15:16:39 (asr2_hf.sh:1605:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/test_clean/logdir/asr_inference.*.log'
2024-11-25T15:29:56 (asr2_hf.sh:1605:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/test_other/logdir/asr_inference.*.log'
2024-11-25T15:41:30 (asr2_hf.sh:1605:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_clean/logdir/asr_inference.*.log'
2024-11-25T21:26:27 (asr2_hf.sh:288:main) ./asr2_hf.sh --stage 14 --stop_stage 14 --gpu_kmeans false --kmeans_opts --batch_bins 1 --nj 4 --kmeans_feature wavlm_large_myst_finetune/24 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_myst_finetune_24_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --speed_perturb_factors 0.9 1.0 1.1 --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_finetune_24_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-11-25T21:26:27 (asr2_hf.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-11-25T21:26:27 (asr2_hf.sh:1546:main) Stage 14: Decoding: training_dir=exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp
2024-11-25T21:26:27 (asr2_hf.sh:1574:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 14 using this script
2024-11-25T21:26:27 (asr2_hf.sh:1605:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_clean/logdir/asr_inference.*.log'
2024-11-25T21:39:58 (asr2_hf.sh:1605:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000_sp/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_other/logdir/asr_inference.*.log'
2024-11-25T21:51:49 (asr2_hf.sh:1790:main) Successfully finished. [elapsed=1522s]
run_onlyctc_myst_ft_kmeans.sh: line 62: unexpected EOF while looking for matching `"'
2024-12-12T18:29:28 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-12-12T18:29:28 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-12-12T18:29:28 (asr2.sh:1423:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100, valid_set=dump/raw/dev
2024-12-12T18:29:28 (asr2.sh:1466:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000/run.sh'. You can resume the process from stage 13 using this script
2024-12-12T18:29:28 (asr2.sh:1470:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000/train.log'
2024-12-12 18:29:28,789 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000 --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100/text.rm.wavlm_large_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_21_km2000_bpe6000_bpe5000/train/text_shape.bpe
2024-12-12 18:29:28,802 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000/train.log
./asr2.sh: line 1510: --fold_length: command not found
2024-12-12T21:23:08 (asr2_hf.sh:288:main) ./asr2_hf.sh --stage 13 --stop_stage 13 --gpu_kmeans false --kmeans_opts --batch_bins 1 --nj 4 --kmeans_feature wavlm_large_myst_finetune/24 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_myst_finetune_24_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_finetune_24_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-12-12T21:23:09 (asr2_hf.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-12-12T21:23:09 (asr2_hf.sh:1424:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100, valid_set=dump/raw/dev
2024-12-12T21:23:09 (asr2_hf.sh:1467:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/run.sh'. You can resume the process from stage 13 using this script
2024-12-12T21:23:09 (asr2_hf.sh:1471:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log'
2024-12-12 21:23:09,165 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000 --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/text_shape.bpe
2024-12-12 21:23:09,178 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log
2024-12-12T21:23:18 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --gpu_kmeans false --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large_myst/21 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_myst_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-12-12T21:23:18 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-12-12T21:23:18 (asr2.sh:1423:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100, valid_set=dump/raw/dev
2024-12-12T21:23:18 (asr2.sh:1466:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/run.sh'. You can resume the process from stage 13 using this script
2024-12-12T21:23:18 (asr2.sh:1470:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/train.log'
2024-12-12 21:23:18,577 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000 --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100/text.rm.wavlm_large_myst_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/train/text_shape.bpe
2024-12-12 21:23:18,591 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/train.log
run.pl: job failed, log is in exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/train.log', '--gpu', '1', 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/train.log', 'python3', '-m', 'espnet2.bin.mt_train', '--use_preprocessor', 'true', '--bpemodel', 'data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model', '--token_type', 'bpe', '--token_list', 'data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt', '--src_bpemodel', 'data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/bpe.model', '--src_token_type', 'bpe', '--src_token_list', 'data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text.ts.en,text,text', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text.rm.wavlm_large_myst_21_km2000,src_text,text', '--valid_shape_file', 'exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/valid/text_shape.bpe', '--valid_shape_file', 'exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/valid/src_text_shape.bpe', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '150', '--fold_length', '150', '--output_dir', 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000', '--config', 'conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml', '--train_data_path_and_name_and_type', 'dump/raw/train_clean_100/text.rm.wavlm_large_myst_21_km2000,src_text,text', '--train_data_path_and_name_and_type', 'dump/raw/train_clean_100/text.ts.en,text,text', '--train_shape_file', 'exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/train/src_text_shape.bpe', '--train_shape_file', 'exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/train/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/train.log ###################
# python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000 --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100/text.rm.wavlm_large_myst_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/train/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Thu Dec 12 21:23:18 PST 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/mt_train.py --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000 --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100/text.rm.wavlm_large_myst_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/train/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[lambda-Lambda-Vector] 2024-12-12 21:23:23,406 (mt:347) INFO: Vocabulary size: 5000
[lambda-Lambda-Vector] 2024-12-12 21:23:23,406 (mt:361) INFO: Source vocabulary size: 6000
[lambda-Lambda-Vector] 2024-12-12 21:23:23,570 (discrete_asr_espnet_model:96) WARNING: Set decoder to none as ctc_weight==1.0
[lambda-Lambda-Vector] 2024-12-12 21:23:24,089 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[lambda-Lambda-Vector] 2024-12-12 21:23:24,094 (abs_task:1388) INFO: Model structure:
ESPnetDiscreteASRModel(
  (frontend): Embedding(
    (embed): Sequential(
      (0): Embedding(6000, 512)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (encoder): EBranchformerEncoder(
    (embed): Conv1dSubsampling2(
      (conv): Sequential(
        (0): Conv1d(512, 256, kernel_size=(3,), stride=(1,))
        (1): ReLU()
        (2): Conv1d(256, 256, kernel_size=(3,), stride=(2,))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_mt): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (specaug): SpecAug(
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=10, axis=time)
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetDiscreteASRModel
    Total Number of model parameters: 28.32 M
    Number of trainable parameters: 28.32 M (100.0%)
    Size: 113.3 MB
    Type: torch.float32
[lambda-Lambda-Vector] 2024-12-12 21:23:24,094 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 1.3333333333333336e-07
    maximize: False
    weight_decay: 1e-06
)
[lambda-Lambda-Vector] 2024-12-12 21:23:24,094 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[lambda-Lambda-Vector] 2024-12-12 21:23:24,094 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/config.yaml
[lambda-Lambda-Vector] 2024-12-12 21:23:24,482 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  src_text: {"path": "dump/raw/train_clean_100/text.rm.wavlm_large_myst_21_km2000", "type": "text"}
  text: {"path": "dump/raw/train_clean_100/text.ts.en", "type": "text"}
  preprocess: <espnet2.train.preprocessor.MutliTokenizerCommonPreprocessor object at 0x7dbf2be1c700>)
[lambda-Lambda-Vector] 2024-12-12 21:23:24,482 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=769, batch_bins=64000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2024-12-12 21:23:24,482 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=769, mean=37.1, min=19, max=184
[lambda-Lambda-Vector] 2024-12-12 21:23:24,510 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  text: {"path": "dump/raw/dev/text.ts.en", "type": "text"}
  src_text: {"path": "dump/raw/dev/text.rm.wavlm_large_myst_21_km2000", "type": "text"}
  preprocess: <espnet2.train.preprocessor.MutliTokenizerCommonPreprocessor object at 0x7dbf2dd808b0>)
[lambda-Lambda-Vector] 2024-12-12 21:23:24,510 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=100, batch_bins=64000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2024-12-12 21:23:24,510 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=100, mean=55.5, min=1, max=148
[lambda-Lambda-Vector] 2024-12-12 21:23:24,512 (trainer:311) INFO: 1/70epoch started
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
[lambda-Lambda-Vector] 2024-12-12 21:23:28,718 (trainer:779) INFO: 1epoch:train:1-38batch: iter_time=0.002, forward_time=0.042, loss_ctc=832.482, loss=832.482, backward_time=0.040, grad_norm=1.249e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.006, optim0_lr0=2.733e-06, train_time=0.111
[lambda-Lambda-Vector] 2024-12-12 21:23:32,274 (trainer:779) INFO: 1epoch:train:39-76batch: iter_time=5.366e-05, forward_time=0.033, loss_ctc=542.621, loss=542.621, backward_time=0.036, grad_norm=1.420e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.004, optim0_lr0=7.800e-06, train_time=0.093
[lambda-Lambda-Vector] 2024-12-12 21:23:35,844 (trainer:779) INFO: 1epoch:train:77-114batch: iter_time=5.383e-05, forward_time=0.033, loss_ctc=408.865, loss=408.865, backward_time=0.036, grad_norm=498.610, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.004, optim0_lr0=1.287e-05, train_time=0.094
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_train.py", line 19, in main
    MTTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1210, in main
    cls.main_worker(args)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 677, in train_one_epoch
    scaler.scale(loss).backward()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

# Accounting: time=21 threads=1
# Ended (code 1) at Thu Dec 12 21:23:39 PST 2024, elapsed time 21 seconds

run.pl: job failed, log is in exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log', '--gpu', '1', 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log', 'python3', '-m', 'espnet2.bin.mt_train', '--use_preprocessor', 'true', '--bpemodel', 'data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model', '--token_type', 'bpe', '--token_list', 'data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt', '--src_bpemodel', 'data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/bpe.model', '--src_token_type', 'bpe', '--src_token_list', 'data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text.ts.en,text,text', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text', '--valid_shape_file', 'exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/text_shape.bpe', '--valid_shape_file', 'exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/src_text_shape.bpe', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '150', '--fold_length', '150', '--output_dir', 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000', '--config', 'conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml', '--train_data_path_and_name_and_type', 'dump/raw/train_clean_100/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text', '--train_data_path_and_name_and_type', 'dump/raw/train_clean_100/text.ts.en,text,text', '--train_shape_file', 'exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/src_text_shape.bpe', '--train_shape_file', 'exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log ###################
# python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000 --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Thu Dec 12 21:23:09 PST 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/mt_train.py --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000 --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[lambda-Lambda-Vector] 2024-12-12 21:23:13,916 (mt:347) INFO: Vocabulary size: 5000
[lambda-Lambda-Vector] 2024-12-12 21:23:13,916 (mt:361) INFO: Source vocabulary size: 6000
[lambda-Lambda-Vector] 2024-12-12 21:23:14,081 (discrete_asr_espnet_model:96) WARNING: Set decoder to none as ctc_weight==1.0
[lambda-Lambda-Vector] 2024-12-12 21:23:14,530 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[lambda-Lambda-Vector] 2024-12-12 21:23:14,534 (abs_task:1388) INFO: Model structure:
ESPnetDiscreteASRModel(
  (frontend): Embedding(
    (embed): Sequential(
      (0): Embedding(6000, 512)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (encoder): EBranchformerEncoder(
    (embed): Conv1dSubsampling2(
      (conv): Sequential(
        (0): Conv1d(512, 256, kernel_size=(3,), stride=(1,))
        (1): ReLU()
        (2): Conv1d(256, 256, kernel_size=(3,), stride=(2,))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_mt): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (specaug): SpecAug(
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=10, axis=time)
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetDiscreteASRModel
    Total Number of model parameters: 28.32 M
    Number of trainable parameters: 28.32 M (100.0%)
    Size: 113.3 MB
    Type: torch.float32
[lambda-Lambda-Vector] 2024-12-12 21:23:14,534 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 1.3333333333333336e-07
    maximize: False
    weight_decay: 1e-06
)
[lambda-Lambda-Vector] 2024-12-12 21:23:14,534 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[lambda-Lambda-Vector] 2024-12-12 21:23:14,534 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/config.yaml
[lambda-Lambda-Vector] 2024-12-12 21:23:14,925 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  src_text: {"path": "dump/raw/train_clean_100/text.rm.wavlm_large_myst_finetune_24_km2000", "type": "text"}
  text: {"path": "dump/raw/train_clean_100/text.ts.en", "type": "text"}
  preprocess: <espnet2.train.preprocessor.MutliTokenizerCommonPreprocessor object at 0x7500727e8700>)
[lambda-Lambda-Vector] 2024-12-12 21:23:14,925 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=950, batch_bins=64000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2024-12-12 21:23:14,926 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=950, mean=30.0, min=16, max=161
[lambda-Lambda-Vector] 2024-12-12 21:23:14,954 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  text: {"path": "dump/raw/dev/text.ts.en", "type": "text"}
  src_text: {"path": "dump/raw/dev/text.rm.wavlm_large_myst_finetune_24_km2000", "type": "text"}
  preprocess: <espnet2.train.preprocessor.MutliTokenizerCommonPreprocessor object at 0x7500744572e0>)
[lambda-Lambda-Vector] 2024-12-12 21:23:14,954 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=116, batch_bins=64000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2024-12-12 21:23:14,954 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=116, mean=47.9, min=4, max=148
[lambda-Lambda-Vector] 2024-12-12 21:23:14,957 (trainer:311) INFO: 1/70epoch started
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
[lambda-Lambda-Vector] 2024-12-12 21:23:20,184 (trainer:779) INFO: 1epoch:train:1-47batch: iter_time=0.001, forward_time=0.042, loss_ctc=1.214e+03, loss=1.214e+03, backward_time=0.041, grad_norm=1.976e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.006, optim0_lr0=3.333e-06, train_time=0.111
[lambda-Lambda-Vector] 2024-12-12 21:23:24,816 (trainer:779) INFO: 1epoch:train:48-94batch: iter_time=5.349e-05, forward_time=0.035, loss_ctc=527.204, loss=527.204, backward_time=0.040, grad_norm=1.455e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.005, optim0_lr0=9.600e-06, train_time=0.098
[lambda-Lambda-Vector] 2024-12-12 21:23:29,411 (trainer:779) INFO: 1epoch:train:95-141batch: iter_time=5.806e-05, forward_time=0.034, loss_ctc=409.371, loss=409.371, backward_time=0.038, grad_norm=582.860, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.005, optim0_lr0=1.587e-05, train_time=0.098
[lambda-Lambda-Vector] 2024-12-12 21:23:33,934 (trainer:779) INFO: 1epoch:train:142-188batch: iter_time=5.637e-05, forward_time=0.033, loss_ctc=335.967, loss=335.967, backward_time=0.037, grad_norm=313.664, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.005, optim0_lr0=2.213e-05, train_time=0.096
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[lambda-Lambda-Vector] 2024-12-12 21:23:38,472 (trainer:779) INFO: 1epoch:train:189-235batch: iter_time=5.745e-05, forward_time=0.033, loss_ctc=312.783, loss=312.783, backward_time=0.037, grad_norm=164.981, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.005, optim0_lr0=2.840e-05, train_time=0.096
[lambda-Lambda-Vector] 2024-12-12 21:23:42,939 (trainer:779) INFO: 1epoch:train:236-282batch: iter_time=5.633e-05, forward_time=0.032, loss_ctc=290.340, loss=290.340, backward_time=0.036, grad_norm=90.594, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.005, optim0_lr0=3.467e-05, train_time=0.095
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_train.py", line 19, in main
    MTTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1210, in main
    cls.main_worker(args)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 614, in train_one_epoch
    retval = model(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/discrete_asr_espnet_model.py", line 162, in forward
    loss_ctc, cer_ctc = self._calc_ctc_loss(
  File "/data/mohan/workdir/espnet/espnet2/asr/discrete_asr_espnet_model.py", line 316, in _calc_ctc_loss
    loss_ctc = self.ctc(encoder_out, encoder_out_lens, ys_pad, ys_pad_lens)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/ctc.py", line 180, in forward
    loss = self.loss_fn(ys_hat, ys_true, hlens, ys_lens).to(
  File "/data/mohan/workdir/espnet/espnet2/asr/ctc.py", line 77, in loss_fn
    th_pred = th_pred.log_softmax(2).float()
RuntimeError: CUDA error: unknown error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

# Accounting: time=36 threads=1
# Ended (code 1) at Thu Dec 12 21:23:45 PST 2024, elapsed time 36 seconds

2024-12-12T21:25:22 (asr2_hf.sh:288:main) ./asr2_hf.sh --stage 13 --stop_stage 13 --gpu_kmeans false --kmeans_opts --batch_bins 1 --nj 4 --kmeans_feature wavlm_large_myst_finetune/24 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_myst_finetune_24_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_finetune_24_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-12-12T21:25:22 (asr2_hf.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-12-12T21:25:22 (asr2_hf.sh:1424:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100, valid_set=dump/raw/dev
2024-12-12T21:25:22 (asr2_hf.sh:1467:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/run.sh'. You can resume the process from stage 13 using this script
2024-12-12T21:25:22 (asr2_hf.sh:1471:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log'
2024-12-12 21:25:22,767 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000 --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/text_shape.bpe
2024-12-12 21:25:22,781 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log
run.pl: job failed, log is in exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log', '--gpu', '1', 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log', 'python3', '-m', 'espnet2.bin.mt_train', '--use_preprocessor', 'true', '--bpemodel', 'data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model', '--token_type', 'bpe', '--token_list', 'data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt', '--src_bpemodel', 'data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/bpe.model', '--src_token_type', 'bpe', '--src_token_list', 'data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text.ts.en,text,text', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text', '--valid_shape_file', 'exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/text_shape.bpe', '--valid_shape_file', 'exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/src_text_shape.bpe', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '150', '--fold_length', '150', '--output_dir', 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000', '--config', 'conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml', '--train_data_path_and_name_and_type', 'dump/raw/train_clean_100/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text', '--train_data_path_and_name_and_type', 'dump/raw/train_clean_100/text.ts.en,text,text', '--train_shape_file', 'exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/src_text_shape.bpe', '--train_shape_file', 'exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log ###################
# python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000 --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Thu Dec 12 21:25:22 PST 2024
#
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/mt_train.py --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000 --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[lambda-Lambda-Vector] 2024-12-12 21:25:27,300 (mt:347) INFO: Vocabulary size: 5000
[lambda-Lambda-Vector] 2024-12-12 21:25:27,301 (mt:361) INFO: Source vocabulary size: 6000
[lambda-Lambda-Vector] 2024-12-12 21:25:27,466 (discrete_asr_espnet_model:96) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_train.py", line 19, in main
    MTTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1210, in main
    cls.main_worker(args)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1352, in main_worker
    model = model.to(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1173, in to
    return self._apply(convert)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 779, in _apply
    module._apply(fn)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 779, in _apply
    module._apply(fn)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 779, in _apply
    module._apply(fn)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 804, in _apply
    param_applied = fn(param)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1159, in convert
    return t.to(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/cuda/__init__.py", line 293, in _lazy_init
    torch._C._cuda_init()
RuntimeError: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.
# Accounting: time=5 threads=1
# Ended (code 1) at Thu Dec 12 21:25:27 PST 2024, elapsed time 5 seconds

2024-12-12T21:26:03 (asr2_hf.sh:288:main) ./asr2_hf.sh --stage 13 --stop_stage 13 --gpu_kmeans false --kmeans_opts --batch_bins 1 --nj 4 --kmeans_feature wavlm_large_myst_finetune/24 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_myst_finetune_24_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_finetune_24_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-12-12T21:26:03 (asr2_hf.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-12-12T21:26:03 (asr2_hf.sh:1424:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100, valid_set=dump/raw/dev
2024-12-12T21:26:03 (asr2_hf.sh:1467:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/run.sh'. You can resume the process from stage 13 using this script
2024-12-12T21:26:03 (asr2_hf.sh:1471:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log'
2024-12-12 21:26:03,810 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000 --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/text_shape.bpe
2024-12-12 21:26:03,823 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log
run.pl: job failed, log is in exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log', '--gpu', '1', 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log', 'python3', '-m', 'espnet2.bin.mt_train', '--use_preprocessor', 'true', '--bpemodel', 'data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model', '--token_type', 'bpe', '--token_list', 'data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt', '--src_bpemodel', 'data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/bpe.model', '--src_token_type', 'bpe', '--src_token_list', 'data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text.ts.en,text,text', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text', '--valid_shape_file', 'exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/text_shape.bpe', '--valid_shape_file', 'exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/src_text_shape.bpe', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '150', '--fold_length', '150', '--output_dir', 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000', '--config', 'conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml', '--train_data_path_and_name_and_type', 'dump/raw/train_clean_100/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text', '--train_data_path_and_name_and_type', 'dump/raw/train_clean_100/text.ts.en,text,text', '--train_shape_file', 'exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/src_text_shape.bpe', '--train_shape_file', 'exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log ###################
# python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000 --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Thu Dec 12 21:26:03 PST 2024
#
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/mt_train.py --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000 --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[lambda-Lambda-Vector] 2024-12-12 21:26:08,352 (mt:347) INFO: Vocabulary size: 5000
[lambda-Lambda-Vector] 2024-12-12 21:26:08,353 (mt:361) INFO: Source vocabulary size: 6000
[lambda-Lambda-Vector] 2024-12-12 21:26:08,518 (discrete_asr_espnet_model:96) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/mt_train.py", line 19, in main
    MTTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1210, in main
    cls.main_worker(args)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1352, in main_worker
    model = model.to(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1173, in to
    return self._apply(convert)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 779, in _apply
    module._apply(fn)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 779, in _apply
    module._apply(fn)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 779, in _apply
    module._apply(fn)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 804, in _apply
    param_applied = fn(param)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1159, in convert
    return t.to(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/cuda/__init__.py", line 293, in _lazy_init
    torch._C._cuda_init()
RuntimeError: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.
# Accounting: time=5 threads=1
# Ended (code 1) at Thu Dec 12 21:26:08 PST 2024, elapsed time 5 seconds

2024-12-12T21:31:21 (asr2.sh:288:main) ./asr2.sh --stage 14 --stop_stage 14 --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large/21 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-12-12T21:31:21 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-12-12T21:31:21 (asr2.sh:1545:main) Stage 14: Decoding: training_dir=exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000
2024-12-12T21:31:21 (asr2.sh:1573:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 14 using this script
2024-12-12T21:31:21 (asr2.sh:1602:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/test_clean/logdir/asr_inference.*.log'
2024-12-12T21:50:34 (asr2.sh:1602:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/test_other/logdir/asr_inference.*.log'
2024-12-12T22:06:56 (asr2.sh:1602:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_clean/logdir/asr_inference.*.log'
2024-12-12T22:26:24 (asr2.sh:1602:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_21_km2000_bpe_rm6000_bpe_ts5000/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_other/logdir/asr_inference.*.log'
2024-12-12T22:43:19 (asr2.sh:1785:main) Successfully finished. [elapsed=4318s]
2024-12-13T13:56:20 (asr2_hf.sh:288:main) ./asr2_hf.sh --stage 13 --stop_stage 13 --gpu_kmeans false --kmeans_opts --batch_bins 1 --nj 4 --kmeans_feature wavlm_large_myst_finetune/24 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_myst_finetune_24_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_finetune_24_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-12-13T13:56:20 (asr2_hf.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-12-13T13:56:20 (asr2_hf.sh:1424:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100, valid_set=dump/raw/dev
2024-12-13T13:56:20 (asr2_hf.sh:1467:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/run.sh'. You can resume the process from stage 13 using this script
2024-12-13T13:56:20 (asr2_hf.sh:1471:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log'
2024-12-13 13:56:20,735 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_finetune_24_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000 --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100/text.rm.wavlm_large_myst_finetune_24_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_finetune_24_km2000_bpe6000_bpe5000/train/text_shape.bpe
2024-12-13 13:56:20,748 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/train.log
2024-12-13T13:56:44 (asr2.sh:288:main) ./asr2.sh --stage 13 --stop_stage 13 --gpu_kmeans false --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large_myst/21 --nclusters 2000 --ngpu 1 --nj 4 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_myst_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-12-13T13:56:45 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-12-13T13:56:45 (asr2.sh:1423:main) Stage 13: ASR Training: train_set=dump/raw/train_clean_100, valid_set=dump/raw/dev
2024-12-13T13:56:45 (asr2.sh:1466:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/run.sh'. You can resume the process from stage 13 using this script
2024-12-13T13:56:45 (asr2.sh:1470:main) ASR training started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/train.log'
2024-12-13 13:56:45,160 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/train.log' --log exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/.dimt_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.mt_train --use_preprocessor true --bpemodel data/token_list/tgt_bpe_unigram5000_ts_en/bpe.model --token_type bpe --token_list data/token_list/tgt_bpe_unigram5000_ts_en/tokens.txt --src_bpemodel data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/bpe.model --src_token_type bpe --src_token_list data/token_list/src_bpe_unigram6000_rm_wavlm_large_myst_21_km2000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/text.ts.en,text,text --valid_data_path_and_name_and_type dump/raw/dev/text.rm.wavlm_large_myst_21_km2000,src_text,text --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/valid/text_shape.bpe --valid_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/valid/src_text_shape.bpe --resume true --ignore_init_mismatch false --fold_length 150 --fold_length 150 --output_dir exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000 --config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --train_data_path_and_name_and_type dump/raw/train_clean_100/text.rm.wavlm_large_myst_21_km2000,src_text,text --train_data_path_and_name_and_type dump/raw/train_clean_100/text.ts.en,text,text --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/train/src_text_shape.bpe --train_shape_file exp/asr_stats_raw_rm_wavlm_large_myst_21_km2000_bpe6000_bpe5000/train/text_shape.bpe
2024-12-13 13:56:45,173 (launch:348) INFO: log file: exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/train.log
2024-12-13T17:03:35 (asr2.sh:1785:main) Successfully finished. [elapsed=11211s]
2024-12-13T17:23:48 (asr2_hf.sh:1790:main) Successfully finished. [elapsed=12448s]
2024-12-13T17:54:18 (asr2_hf.sh:288:main) ./asr2_hf.sh --stage 14 --stop_stage 14 --gpu_kmeans false --kmeans_opts --batch_bins 1 --nj 4 --kmeans_feature wavlm_large_myst_finetune/24 --nclusters 2000 --ngpu 1 --nj 2 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_myst_finetune_24_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_finetune_24_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-12-13T17:54:19 (asr2_hf.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-12-13T17:54:19 (asr2_hf.sh:1546:main) Stage 14: Decoding: training_dir=exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000
2024-12-13T17:54:19 (asr2_hf.sh:1574:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 14 using this script
2024-12-13T17:54:19 (asr2_hf.sh:1605:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/test_clean/logdir/asr_inference.*.log'
2024-12-13T17:54:31 (asr2.sh:288:main) ./asr2.sh --stage 14 --stop_stage 14 --gpu_kmeans false --kmeans_opts --batch_bins 600000 --nj 4 --kmeans_feature wavlm_large_myst/21 --nclusters 2000 --ngpu 1 --nj 2 --inference_nj 2 --inference_asr_model valid.cer_ctc.best.pth --src_lang wavlm_large_myst_21_km2000 --tgt_lang en --src_token_type bpe --src_nbpe 6000 --tgt_token_type bpe --tgt_nbpe 5000 --src_case rm --tgt_case ts --use_lm false --asr_config conf/train_discrete_asr_e_branchformer1_onlyctc_1gpu.yaml --inference_config conf/decode_ctc1.0_greedy.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other dev_clean dev_other --src_bpe_train_text dump/raw/train_clean_100_sp/text.rm.wavlm_large_myst_21_km2000 --tgt_bpe_train_text dump/raw/train_clean_100_sp/text.ts.en --lm_train_text dump/raw/train_clean_100_sp/text.ts.en
2024-12-13T17:54:31 (asr2.sh:625:main) Skipped stages:  8 9 10 11 16 17 
2024-12-13T17:54:31 (asr2.sh:1545:main) Stage 14: Decoding: training_dir=exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000
2024-12-13T17:54:31 (asr2.sh:1573:main) Generate 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 14 using this script
2024-12-13T17:54:31 (asr2.sh:1602:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/test_clean/logdir/asr_inference.*.log'
2024-12-13T18:05:04 (asr2.sh:1602:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/test_other/logdir/asr_inference.*.log'
2024-12-13T18:07:54 (asr2_hf.sh:1605:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/test_other/logdir/asr_inference.*.log'
2024-12-13T18:14:56 (asr2.sh:1602:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_clean/logdir/asr_inference.*.log'
2024-12-13T18:19:51 (asr2_hf.sh:1605:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_clean/logdir/asr_inference.*.log'
2024-12-13T18:25:42 (asr2.sh:1602:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_21_km2000_bpe_rm6000_bpe_ts5000/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_other/logdir/asr_inference.*.log'
2024-12-13T18:33:45 (asr2_hf.sh:1605:main) Decoding started... log: 'exp/asr_train_discrete_asr_e_branchformer1_onlyctc_1gpu_raw_wavlm_large_myst_finetune_24_km2000_bpe_rm6000_bpe_ts5000/decode_ctc1.0_greedy_asr_model_valid.cer_ctc.best/dev_other/logdir/asr_inference.*.log'
2024-12-13T18:35:50 (asr2.sh:1785:main) Successfully finished. [elapsed=2479s]
2024-12-13T18:45:42 (asr2_hf.sh:1790:main) Successfully finished. [elapsed=3084s]
