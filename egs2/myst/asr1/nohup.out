2024-10-16T02:39:47 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-16T02:39:47 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-16T02:39:47 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-16T02:39:47 (asr.sh:1407:main) Generate 'exp/asr_train_asr_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-16T02:39:47 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_raw_en_bpe5000/train.log'
2024-10-16 02:39:48,300 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_raw_en_bpe5000/train.log' --log exp/asr_train_asr_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-16 02:39:48,315 (launch:237) INFO: single-node with 2gpu on distributed mode
2024-10-16 02:39:48,316 (launch:348) INFO: log file: exp/asr_train_asr_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_raw_en_bpe5000/train.log', '--gpu', '2', 'exp/asr_train_asr_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_raw_en_bpe5000', '--config', 'conf/train_asr.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '2', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_raw_en_bpe5000/train.log ###################
# Started at Wed Oct 16 02:39:48 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed True
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[dl:0/2] 2024-10-16 02:40:16,148 (asr:523) INFO: Vocabulary size: 5000
[dl:0/2] 2024-10-16 02:40:17,873 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl:0/2] 2024-10-16 02:40:17,879 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5000, bias=True)
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 38.47 M
    Number of trainable parameters: 38.47 M (100.0%)
    Size: 153.89 MB
    Type: torch.float32
[dl:0/2] 2024-10-16 02:40:17,879 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 1.3333333333333336e-07
    maximize: False
    weight_decay: 1e-06
)
[dl:0/2] 2024-10-16 02:40:17,879 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[dl:0/2] 2024-10-16 02:40:17,879 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_raw_en_bpe5000/config.yaml
[dl:0/2] 2024-10-16 02:40:18,099 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/2] 2024-10-16 02:40:18,509 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7bb1700439a0>)
[dl:0/2] 2024-10-16 02:40:18,509 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=911, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/2] 2024-10-16 02:40:18,510 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=911, mean=61.1, min=16, max=362
[dl:0/2] 2024-10-16 02:40:18,522 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/2] 2024-10-16 02:40:18,557 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7bb15f5624d0>)
[dl:0/2] 2024-10-16 02:40:18,557 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=144, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/2] 2024-10-16 02:40:18,557 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=144, mean=62.8, min=18, max=309
[dl:0/2] 2024-10-16 02:40:18,567 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/2] 2024-10-16 02:40:18,575 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7bb197078d60>)
[dl:0/2] 2024-10-16 02:40:18,575 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl:0/2] 2024-10-16 02:40:18,575 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
dl:1672199:1672199 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1672199:1672199 [0] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1672199:1672199 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1672199:1672199 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.20.5+cuda12.4
dl:1672200:1672200 [1] NCCL INFO cudaDriverVersion 12050
dl:1672200:1672200 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1672200:1672200 [1] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1672200:1672200 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARNProcess SpawnProcess-2:
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 256, in run
    dp_model = torch.nn.parallel.DistributedDataParallel(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 798, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/utils.py", line 269, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1970, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'
 Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'
dl:1672200:1672270 [1] NCCL INFO init.cc:1475 -> 1
dl:1672200:1672270 [1] NCCL INFO group.cc:64 -> 1 [Async thread]
dl:1672200:1672200 [1] NCCL INFO group.cc:418 -> 1
dl:1672200:1672200 [1] NCCL INFO group.cc:95 -> 1
dl:1672200:1672277 [0] NCCL INFO comm 0xd913600 rank 0 nranks 0 cudaDev 0 busId 0 - Abort COMPLETE
W1016 02:40:20.461000 140242749486912 torch/multiprocessing/spawn.py:145] Terminating process 1672199 via signal SIGTERM
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1273, in main
    while not ProcessContext(processes, error_files).join():
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 177, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 1 terminated with exit code 1
# Accounting: time=33 threads=1
# Ended (code 1) at Wed Oct 16 02:40:21 EDT 2024, elapsed time 33 seconds

2024-10-16T02:41:47 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-16T02:41:47 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-16T02:41:47 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-16T02:41:47 (asr.sh:1407:main) Generate 'exp/asr_train_asr_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-16T02:41:47 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_raw_en_bpe5000/train.log'
2024-10-16 02:41:47,274 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_raw_en_bpe5000/train.log' --log exp/asr_train_asr_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-16 02:41:47,287 (launch:237) INFO: single-node with 2gpu on distributed mode
2024-10-16 02:41:47,288 (launch:348) INFO: log file: exp/asr_train_asr_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_raw_en_bpe5000/train.log', '--gpu', '2', 'exp/asr_train_asr_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_raw_en_bpe5000', '--config', 'conf/train_asr.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '2', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_raw_en_bpe5000/train.log ###################
# Started at Wed Oct 16 02:41:47 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed True
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[dl:0/2] 2024-10-16 02:42:00,295 (asr:523) INFO: Vocabulary size: 5000
[dl:0/2] 2024-10-16 02:42:01,138 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl:0/2] 2024-10-16 02:42:01,144 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5000, bias=True)
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 38.47 M
    Number of trainable parameters: 38.47 M (100.0%)
    Size: 153.89 MB
    Type: torch.float32
[dl:0/2] 2024-10-16 02:42:01,145 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 1.3333333333333336e-07
    maximize: False
    weight_decay: 1e-06
)
[dl:0/2] 2024-10-16 02:42:01,145 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[dl:0/2] 2024-10-16 02:42:01,145 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_raw_en_bpe5000/config.yaml
[dl:0/2] 2024-10-16 02:42:01,290 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/2] 2024-10-16 02:42:01,700 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x70fb9056b9a0>)
[dl:0/2] 2024-10-16 02:42:01,700 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=1737, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/2] 2024-10-16 02:42:01,701 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=1737, mean=32.1, min=6, max=201
[dl:0/2] 2024-10-16 02:42:01,713 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/2] 2024-10-16 02:42:01,746 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x70fb9038a4d0>)
[dl:0/2] 2024-10-16 02:42:01,746 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=275, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/2] 2024-10-16 02:42:01,746 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=275, mean=32.9, min=5, max=156
[dl:0/2] 2024-10-16 02:42:01,756 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/2] 2024-10-16 02:42:01,764 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x70fbb7e84d60>)
[dl:0/2] 2024-10-16 02:42:01,764 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl:0/2] 2024-10-16 02:42:01,764 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
dl:1672665:1672665 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1672665:1672665 [0] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1672665:1672665 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1672665:1672665 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.20.5+cuda12.4
dl:1672666:1672666 [1] NCCL INFO cudaDriverVersion 12050
dl:1672666:1672666 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1672666:1672666 [1] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1672666:1672666 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARNProcess SpawnProcess-2:
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 256, in run
    dp_model = torch.nn.parallel.DistributedDataParallel(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 798, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/utils.py", line 269, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1970, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'
 Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'
dl:1672666:1672694 [1] NCCL INFO init.cc:1475 -> 1
dl:1672666:1672694 [1] NCCL INFO group.cc:64 -> 1 [Async thread]
dl:1672666:1672666 [1] NCCL INFO group.cc:418 -> 1
dl:1672666:1672666 [1] NCCL INFO group.cc:95 -> 1
dl:1672666:1672695 [0] NCCL INFO comm 0xcff8130 rank 0 nranks 0 cudaDev 0 busId 0 - Abort COMPLETE
W1016 02:42:03.391000 129485385557824 torch/multiprocessing/spawn.py:145] Terminating process 1672665 via signal SIGTERM
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1273, in main
    while not ProcessContext(processes, error_files).join():
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 177, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 1 terminated with exit code 1
# Accounting: time=17 threads=1
# Ended (code 1) at Wed Oct 16 02:42:04 EDT 2024, elapsed time 17 seconds

2024-10-16T07:04:33 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-16T07:04:33 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-16T07:04:33 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-16T07:04:33 (asr.sh:1407:main) Generate 'exp/asr_train_asr_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-16T07:04:33 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_raw_en_bpe5000/train.log'
2024-10-16 07:04:34,064 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_raw_en_bpe5000/train.log' --log exp/asr_train_asr_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-16 07:04:34,080 (launch:237) INFO: single-node with 2gpu on distributed mode
2024-10-16 07:04:34,081 (launch:348) INFO: log file: exp/asr_train_asr_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_raw_en_bpe5000/train.log', '--gpu', '2', 'exp/asr_train_asr_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_raw_en_bpe5000', '--config', 'conf/train_asr.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '2', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed True 
# Started at Wed Oct 16 07:04:34 UTC 2024
#
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /root/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
[nltk_data] Downloading package cmudict to /root/nltk_data...
[nltk_data]   Unzipping corpora/cmudict.zip.
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed True
Process SpawnProcess-2:
Process SpawnProcess-1:
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1317, in main_worker
    distributed_option.init_torch_distributed()
  File "/data/mohan/workdir/espnet/espnet2/train/distributed_utils.py", line 97, in init_torch_distributed
    torch.distributed.init_process_group(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
    return func(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 89, in wrapper
    func_return = func(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1312, in init_process_group
    default_pg, _ = _new_process_group_helper(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1533, in _new_process_group_helper
    backend_class = ProcessGroupNCCL(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1317, in main_worker
    distributed_option.init_torch_distributed()
ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!
  File "/data/mohan/workdir/espnet/espnet2/train/distributed_utils.py", line 97, in init_torch_distributed
    torch.distributed.init_process_group(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
    return func(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 89, in wrapper
    func_return = func(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1312, in init_process_group
    default_pg, _ = _new_process_group_helper(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1533, in _new_process_group_helper
    backend_class = ProcessGroupNCCL(
ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!
W1016 07:04:50.021000 140589378172736 torch/multiprocessing/spawn.py:145] Terminating process 13414 via signal SIGTERM
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1273, in main
    while not ProcessContext(processes, error_files).join():
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 177, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 1 terminated with exit code 1
# Accounting: time=16 threads=1
# Ended (code 1) at Wed Oct 16 07:04:50 UTC 2024, elapsed time 16 seconds

2024-10-16T03:05:54 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-16T03:05:54 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-16T03:05:54 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-16T03:05:54 (asr.sh:1407:main) Generate 'exp/asr_train_asr_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-16T03:05:54 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_raw_en_bpe5000/train.log'
2024-10-16 03:05:54,566 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_raw_en_bpe5000/train.log' --log exp/asr_train_asr_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed false -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-16 03:05:54,579 (launch:245) INFO: single-node with 2gpu using DataParallel
2024-10-16 03:05:54,580 (launch:348) INFO: log file: exp/asr_train_asr_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_raw_en_bpe5000/train.log', '--gpu', '2', 'exp/asr_train_asr_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_raw_en_bpe5000', '--config', 'conf/train_asr.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '2', '--multiprocessing_distributed', 'False']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed False 
# Started at Wed Oct 16 03:05:54 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed False
[dl] 2024-10-16 03:06:00,840 (asr:523) INFO: Vocabulary size: 5000
[dl] 2024-10-16 03:06:01,657 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl] 2024-10-16 03:06:01,663 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5000, bias=True)
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 38.47 M
    Number of trainable parameters: 38.47 M (100.0%)
    Size: 153.89 MB
    Type: torch.float32
[dl] 2024-10-16 03:06:01,663 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 1.3333333333333336e-07
    maximize: False
    weight_decay: 1e-06
)
[dl] 2024-10-16 03:06:01,663 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[dl] 2024-10-16 03:06:01,663 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_raw_en_bpe5000/config.yaml
[dl] 2024-10-16 03:06:01,812 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 03:06:02,292 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7232a7f74a00>)
[dl] 2024-10-16 03:06:02,292 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=1737, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-16 03:06:02,293 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=1737, mean=32.1, min=6, max=201
[dl] 2024-10-16 03:06:02,304 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 03:06:02,341 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7232a7d7b250>)
[dl] 2024-10-16 03:06:02,341 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=275, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-16 03:06:02,341 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=275, mean=32.9, min=5, max=156
[dl] 2024-10-16 03:06:02,352 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 03:06:02,359 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7232cefdcf10>)
[dl] 2024-10-16 03:06:02,359 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl] 2024-10-16 03:06:02,359 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[dl] 2024-10-16 03:06:02,362 (trainer:311) INFO: 1/70epoch started
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1210, in main
    cls.main_worker(args)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 614, in train_one_epoch
    retval = model(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 184, in forward
    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 189, in replicate
    return replicate(module, device_ids, not torch.is_grad_enabled())
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/replicate.py", line 110, in replicate
    param_copies = _broadcast_coalesced_reshape(params, devices, detach)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/replicate.py", line 83, in _broadcast_coalesced_reshape
    tensor_copies = Broadcast.apply(devices, *tensors)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 23, in forward
    outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 57, in broadcast_coalesced
    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU  has a total capacity of 15.63 GiB of which 2.62 MiB is free. Process 1167699 has 15.24 GiB memory in use. Including non-PyTorch memory, this process has 386.00 MiB memory in use. Of the allocated memory 104.53 MiB is allocated by PyTorch, and 13.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
# Accounting: time=13 threads=1
# Ended (code 1) at Wed Oct 16 03:06:07 EDT 2024, elapsed time 13 seconds

2024-10-16T03:07:15 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-16T03:07:16 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-16T03:07:16 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-16T03:07:16 (asr.sh:1407:main) Generate 'exp/asr_train_asr_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-16T03:07:16 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_raw_en_bpe5000/train.log'
2024-10-16 03:07:16,173 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_raw_en_bpe5000/train.log' --log exp/asr_train_asr_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed false -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-16 03:07:16,187 (launch:245) INFO: single-node with 2gpu using DataParallel
2024-10-16 03:07:16,188 (launch:348) INFO: log file: exp/asr_train_asr_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_raw_en_bpe5000/train.log', '--gpu', '2', 'exp/asr_train_asr_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_raw_en_bpe5000', '--config', 'conf/train_asr.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '2', '--multiprocessing_distributed', 'False']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed False 
# Started at Wed Oct 16 03:07:16 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed False
[dl] 2024-10-16 03:07:22,873 (asr:523) INFO: Vocabulary size: 5000
[dl] 2024-10-16 03:07:23,694 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl] 2024-10-16 03:07:23,700 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5000, bias=True)
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 38.47 M
    Number of trainable parameters: 38.47 M (100.0%)
    Size: 153.89 MB
    Type: torch.float32
[dl] 2024-10-16 03:07:23,700 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 1.3333333333333336e-07
    maximize: False
    weight_decay: 1e-06
)
[dl] 2024-10-16 03:07:23,700 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[dl] 2024-10-16 03:07:23,700 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_raw_en_bpe5000/config.yaml
[dl] 2024-10-16 03:07:23,845 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 03:07:24,234 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7375d25809d0>)
[dl] 2024-10-16 03:07:24,235 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=3286, batch_bins=4000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-16 03:07:24,235 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=3286, mean=17.0, min=5, max=111
[dl] 2024-10-16 03:07:24,246 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 03:07:24,280 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7375d2387220>)
[dl] 2024-10-16 03:07:24,281 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=514, batch_bins=4000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-16 03:07:24,281 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=514, mean=17.6, min=5, max=99
[dl] 2024-10-16 03:07:24,291 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 03:07:24,299 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7375f95dcf10>)
[dl] 2024-10-16 03:07:24,299 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl] 2024-10-16 03:07:24,299 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[dl] 2024-10-16 03:07:24,301 (trainer:311) INFO: 1/70epoch started
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1210, in main
    cls.main_worker(args)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 614, in train_one_epoch
    retval = model(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 184, in forward
    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 189, in replicate
    return replicate(module, device_ids, not torch.is_grad_enabled())
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/replicate.py", line 110, in replicate
    param_copies = _broadcast_coalesced_reshape(params, devices, detach)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/replicate.py", line 83, in _broadcast_coalesced_reshape
    tensor_copies = Broadcast.apply(devices, *tensors)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 23, in forward
    outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 57, in broadcast_coalesced
    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU  has a total capacity of 15.63 GiB of which 2.62 MiB is free. Process 1167699 has 15.24 GiB memory in use. Including non-PyTorch memory, this process has 386.00 MiB memory in use. Of the allocated memory 99.10 MiB is allocated by PyTorch, and 18.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
# Accounting: time=10 threads=1
# Ended (code 1) at Wed Oct 16 03:07:26 EDT 2024, elapsed time 10 seconds

2024-10-16T03:08:34 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-16T03:08:34 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-16T03:08:34 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-16T03:08:34 (asr.sh:1407:main) Generate 'exp/asr_train_asr_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-16T03:08:34 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_raw_en_bpe5000/train.log'
2024-10-16 03:08:34,976 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_raw_en_bpe5000/train.log' --log exp/asr_train_asr_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed false -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-16 03:08:34,991 (launch:245) INFO: single-node with 2gpu using DataParallel
2024-10-16 03:08:34,992 (launch:348) INFO: log file: exp/asr_train_asr_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_raw_en_bpe5000/train.log', '--gpu', '2', 'exp/asr_train_asr_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_raw_en_bpe5000', '--config', 'conf/train_asr.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '2', '--multiprocessing_distributed', 'False']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed False 
# Started at Wed Oct 16 03:08:35 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed False
[dl] 2024-10-16 03:08:41,180 (asr:523) INFO: Vocabulary size: 5000
[dl] 2024-10-16 03:08:41,983 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl] 2024-10-16 03:08:41,990 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5000, bias=True)
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 38.47 M
    Number of trainable parameters: 38.47 M (100.0%)
    Size: 153.89 MB
    Type: torch.float32
[dl] 2024-10-16 03:08:41,990 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 1.3333333333333336e-07
    maximize: False
    weight_decay: 1e-06
)
[dl] 2024-10-16 03:08:41,990 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[dl] 2024-10-16 03:08:41,990 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_raw_en_bpe5000/config.yaml
[dl] 2024-10-16 03:08:42,135 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 03:08:42,522 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7656958749a0>)
[dl] 2024-10-16 03:08:42,522 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=6106, batch_bins=2000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-16 03:08:42,523 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=6106, mean=9.1, min=1, max=57
[dl] 2024-10-16 03:08:42,533 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 03:08:42,568 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x76569567b190>)
[dl] 2024-10-16 03:08:42,568 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=972, batch_bins=2000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-16 03:08:42,568 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=972, mean=9.3, min=1, max=55
[dl] 2024-10-16 03:08:42,578 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 03:08:42,586 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x765695679f90>)
[dl] 2024-10-16 03:08:42,586 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl] 2024-10-16 03:08:42,586 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[dl] 2024-10-16 03:08:42,588 (trainer:311) INFO: 1/70epoch started
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1210, in main
    cls.main_worker(args)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 614, in train_one_epoch
    retval = model(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 184, in forward
    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 189, in replicate
    return replicate(module, device_ids, not torch.is_grad_enabled())
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/replicate.py", line 110, in replicate
    param_copies = _broadcast_coalesced_reshape(params, devices, detach)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/replicate.py", line 83, in _broadcast_coalesced_reshape
    tensor_copies = Broadcast.apply(devices, *tensors)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 23, in forward
    outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 57, in broadcast_coalesced
    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU  has a total capacity of 15.63 GiB of which 2.62 MiB is free. Process 1167699 has 15.24 GiB memory in use. Including non-PyTorch memory, this process has 386.00 MiB memory in use. Of the allocated memory 97.40 MiB is allocated by PyTorch, and 20.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
# Accounting: time=10 threads=1
# Ended (code 1) at Wed Oct 16 03:08:45 EDT 2024, elapsed time 10 seconds

2024-10-16T03:10:44 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-16T03:10:44 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-16T03:10:44 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-16T03:10:44 (asr.sh:1407:main) Generate 'exp/asr_train_asr_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-16T03:10:44 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_raw_en_bpe5000/train.log'
2024-10-16 03:10:44,528 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_raw_en_bpe5000/train.log' --log exp/asr_train_asr_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed false -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-16 03:10:44,543 (launch:348) INFO: log file: exp/asr_train_asr_raw_en_bpe5000/train.log
2024-10-16T15:54:05 (asr.sh:1808:main) Successfully finished. [elapsed=45801s]
2024-10-16T23:32:04 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_lr1e-3_warm_10000.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-16T23:32:04 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-16T23:32:04 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-16T23:32:04 (asr.sh:1407:main) Generate 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-16T23:32:04 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log'
2024-10-16 23:32:06,020 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log' --log exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-16 23:32:06,065 (launch:348) INFO: log file: exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
bash: line 1: 1875122 Killed                  ( python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True ) 2>> exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log >> exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log', '--gpu', '1', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000', '--config', 'conf/train_asr_lr1e-3_warm_10000.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Wed Oct 16 23:32:06 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[dl] 2024-10-16 23:32:24,115 (asr:523) INFO: Vocabulary size: 5000
[dl] 2024-10-16 23:32:25,797 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl] 2024-10-16 23:32:25,808 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5000, bias=True)
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 38.47 M
    Number of trainable parameters: 38.47 M (100.0%)
    Size: 153.89 MB
    Type: torch.float32
[dl] 2024-10-16 23:32:25,808 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 1e-07
    maximize: False
    weight_decay: 1e-06
)
[dl] 2024-10-16 23:32:25,808 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=10000)
[dl] 2024-10-16 23:32:25,808 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/config.yaml
[dl] 2024-10-16 23:32:26,008 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 23:32:26,445 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x79f7c9b609d0>)
[dl] 2024-10-16 23:32:26,445 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=1737, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-16 23:32:26,466 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=1737, mean=32.1, min=6, max=201
[dl] 2024-10-16 23:32:26,580 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 23:32:26,639 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x79f7c9967220>)
[dl] 2024-10-16 23:32:26,639 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=275, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-16 23:32:26,639 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=275, mean=32.9, min=5, max=156
[dl] 2024-10-16 23:32:26,650 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 23:32:26,657 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x79f7f0b74f40>)
[dl] 2024-10-16 23:32:26,657 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl] 2024-10-16 23:32:26,657 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[dl] 2024-10-16 23:32:26,659 (trainer:311) INFO: 1/70epoch started
[dl] 2024-10-16 23:32:56,112 (trainer:779) INFO: 1epoch:train:1-86batch: iter_time=0.003, forward_time=0.144, loss_ctc=1.647e+03, loss_att=153.798, acc=1.675e-04, loss=75.209, backward_time=0.130, grad_norm=2.396e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.031, optim0_lr0=6.500e-07, train_time=2.765
[dl] 2024-10-16 23:33:21,642 (trainer:779) INFO: 1epoch:train:87-172batch: iter_time=1.733e-04, forward_time=0.116, loss_ctc=1.394e+03, loss_att=141.067, acc=3.851e-05, loss=64.614, backward_time=0.116, grad_norm=4.016e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.700e-06, train_time=2.381
[dl] 2024-10-16 23:33:47,636 (trainer:779) INFO: 1epoch:train:173-258batch: iter_time=1.594e-04, forward_time=0.117, loss_ctc=1.184e+03, loss_att=151.364, acc=1.088e-04, loss=57.659, backward_time=0.119, grad_norm=4.166e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=2.800e-06, train_time=2.416
[dl] 2024-10-16 23:34:14,990 (trainer:779) INFO: 1epoch:train:259-344batch: iter_time=1.515e-04, forward_time=0.122, loss_ctc=906.107, loss_att=170.828, acc=1.541e-04, loss=48.926, backward_time=0.127, grad_norm=3.169e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.900e-06, train_time=2.537
[dl] 2024-10-16 23:34:42,600 (trainer:779) INFO: 1epoch:train:345-430batch: iter_time=1.545e-04, forward_time=0.124, loss_ctc=558.501, loss_att=172.980, acc=2.009e-04, loss=36.080, backward_time=0.128, grad_norm=1.800e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=4.950e-06, train_time=2.572
[dl] 2024-10-16 23:35:08,919 (trainer:779) INFO: 1epoch:train:431-516batch: iter_time=1.507e-04, forward_time=0.119, loss_ctc=265.769, loss_att=131.986, acc=1.290e-04, loss=21.515, backward_time=0.121, grad_norm=868.968, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=6.000e-06, train_time=2.472
[dl] 2024-10-16 23:35:36,284 (trainer:779) INFO: 1epoch:train:517-602batch: iter_time=1.544e-04, forward_time=0.122, loss_ctc=272.382, loss_att=165.181, acc=2.193e-04, loss=24.668, backward_time=0.128, grad_norm=610.046, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=7.100e-06, train_time=2.536
[dl] 2024-10-16 23:36:03,045 (trainer:779) INFO: 1epoch:train:603-688batch: iter_time=1.428e-04, forward_time=0.119, loss_ctc=230.793, loss_att=153.116, acc=2.565e-04, loss=22.052, backward_time=0.125, grad_norm=507.787, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=8.200e-06, train_time=2.476
[dl] 2024-10-16 23:36:28,644 (trainer:779) INFO: 1epoch:train:689-774batch: iter_time=1.600e-04, forward_time=0.116, loss_ctc=187.175, loss_att=129.722, acc=8.384e-04, loss=18.370, backward_time=0.119, grad_norm=405.132, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=9.250e-06, train_time=2.398
[dl] 2024-10-16 23:36:54,968 (trainer:779) INFO: 1epoch:train:775-860batch: iter_time=1.587e-04, forward_time=0.119, loss_ctc=199.159, loss_att=141.266, acc=0.002, loss=19.829, backward_time=0.121, grad_norm=379.917, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.030e-05, train_time=2.429
[dl] 2024-10-16 23:37:21,012 (trainer:779) INFO: 1epoch:train:861-946batch: iter_time=1.600e-04, forward_time=0.117, loss_ctc=213.723, loss_att=154.327, acc=0.005, loss=21.518, backward_time=0.120, grad_norm=358.332, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.140e-05, train_time=2.422
[dl] 2024-10-16 23:37:47,255 (trainer:779) INFO: 1epoch:train:947-1032batch: iter_time=1.606e-04, forward_time=0.118, loss_ctc=199.389, loss_att=147.073, acc=0.018, loss=20.346, backward_time=0.121, grad_norm=345.832, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.250e-05, train_time=2.442
[dl] 2024-10-16 23:38:13,086 (trainer:779) INFO: 1epoch:train:1033-1118batch: iter_time=1.525e-04, forward_time=0.117, loss_ctc=179.414, loss_att=134.468, acc=0.036, loss=18.494, backward_time=0.119, grad_norm=283.663, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.355e-05, train_time=2.396
[dl] 2024-10-16 23:38:39,674 (trainer:779) INFO: 1epoch:train:1119-1204batch: iter_time=1.510e-04, forward_time=0.120, loss_ctc=188.178, loss_att=142.456, acc=0.050, loss=19.522, backward_time=0.123, grad_norm=285.190, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.460e-05, train_time=2.473
[dl] 2024-10-16 23:39:06,006 (trainer:779) INFO: 1epoch:train:1205-1290batch: iter_time=1.703e-04, forward_time=0.119, loss_ctc=181.679, loss_att=138.242, acc=0.063, loss=18.909, backward_time=0.122, grad_norm=253.307, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.570e-05, train_time=2.463
[dl] 2024-10-16 23:39:32,079 (trainer:779) INFO: 1epoch:train:1291-1376batch: iter_time=1.536e-04, forward_time=0.117, loss_ctc=167.627, loss_att=128.496, acc=0.070, loss=17.529, backward_time=0.121, grad_norm=207.311, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.680e-05, train_time=2.414
[dl] 2024-10-16 23:39:58,214 (trainer:779) INFO: 1epoch:train:1377-1462batch: iter_time=1.460e-04, forward_time=0.118, loss_ctc=163.232, loss_att=125.359, acc=0.064, loss=17.090, backward_time=0.122, grad_norm=196.826, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.785e-05, train_time=2.442
[dl] 2024-10-16 23:40:25,272 (trainer:779) INFO: 1epoch:train:1463-1548batch: iter_time=1.499e-04, forward_time=0.122, loss_ctc=161.289, loss_att=123.045, acc=0.053, loss=16.815, backward_time=0.126, grad_norm=180.047, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.890e-05, train_time=2.497
[dl] 2024-10-16 23:40:51,528 (trainer:779) INFO: 1epoch:train:1549-1634batch: iter_time=1.500e-04, forward_time=0.119, loss_ctc=167.747, loss_att=127.206, acc=0.057, loss=17.421, backward_time=0.121, grad_norm=151.258, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=2.000e-05, train_time=2.452
[dl] 2024-10-16 23:41:17,932 (trainer:779) INFO: 1epoch:train:1635-1720batch: iter_time=1.583e-04, forward_time=0.119, loss_ctc=149.537, loss_att=113.334, acc=0.056, loss=15.524, backward_time=0.121, grad_norm=124.987, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=2.110e-05, train_time=2.453
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
[dl] 2024-10-16 23:42:56,686 (trainer:365) INFO: 1epoch results: [train] iter_time=3.188e-04, forward_time=0.120, loss_ctc=424.608, loss_att=141.492, acc=0.024, loss=28.303, backward_time=0.123, grad_norm=1.027e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.100e-05, train_time=2.472, time=8 minutes and 56.77 seconds, total_count=1737, gpu_max_cached_mem_GB=10.412, [valid] loss_ctc=147.135, cer_ctc=1.000, loss_att=111.570, acc=0.066, cer=0.694, wer=1.000, loss=122.239, time=50.89 seconds, total_count=275, gpu_max_cached_mem_GB=15.139, [att_plot] time=42.37 seconds, total_count=0, gpu_max_cached_mem_GB=15.139
[dl] 2024-10-16 23:42:58,889 (trainer:433) INFO: The best model has been updated: valid.acc
[dl] 2024-10-16 23:42:58,889 (trainer:299) INFO: 2/70epoch started. Estimated time to finish: 12 hours, 7 minutes and 3.85 seconds
[dl] 2024-10-16 23:43:24,704 (trainer:779) INFO: 2epoch:train:1-86batch: iter_time=0.003, forward_time=0.113, loss_ctc=148.422, loss_att=111.730, acc=0.061, loss=15.342, backward_time=0.115, grad_norm=102.115, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=2.235e-05, train_time=2.385
[dl] 2024-10-16 23:43:51,978 (trainer:779) INFO: 2epoch:train:87-172batch: iter_time=1.591e-04, forward_time=0.121, loss_ctc=152.119, loss_att=114.984, acc=0.058, loss=15.766, backward_time=0.124, grad_norm=96.252, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=2.340e-05, train_time=2.550
[dl] 2024-10-16 23:44:18,671 (trainer:779) INFO: 2epoch:train:173-258batch: iter_time=1.399e-04, forward_time=0.119, loss_ctc=135.612, loss_att=102.989, acc=0.056, loss=14.097, backward_time=0.120, grad_norm=74.082, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=2.450e-05, train_time=2.476
[dl] 2024-10-16 23:44:46,299 (trainer:779) INFO: 2epoch:train:259-344batch: iter_time=1.363e-04, forward_time=0.121, loss_ctc=165.808, loss_att=126.123, acc=0.064, loss=17.254, backward_time=0.125, grad_norm=70.871, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=2.560e-05, train_time=2.569
[dl] 2024-10-16 23:45:12,049 (trainer:779) INFO: 2epoch:train:345-430batch: iter_time=1.468e-04, forward_time=0.115, loss_ctc=129.215, loss_att=99.824, acc=0.058, loss=13.580, backward_time=0.117, grad_norm=50.489, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=2.665e-05, train_time=2.405
[dl] 2024-10-16 23:45:37,661 (trainer:779) INFO: 2epoch:train:431-516batch: iter_time=1.376e-04, forward_time=0.114, loss_ctc=120.046, loss_att=93.827, acc=0.061, loss=12.712, backward_time=0.115, grad_norm=40.142, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=2.770e-05, train_time=2.378
[dl] 2024-10-16 23:46:03,574 (trainer:779) INFO: 2epoch:train:517-602batch: iter_time=1.502e-04, forward_time=0.115, loss_ctc=128.196, loss_att=100.706, acc=0.076, loss=13.619, backward_time=0.118, grad_norm=37.910, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=2.880e-05, train_time=2.407
[dl] 2024-10-16 23:46:30,043 (trainer:779) INFO: 2epoch:train:603-688batch: iter_time=1.422e-04, forward_time=0.117, loss_ctc=129.789, loss_att=103.262, acc=0.080, loss=13.903, backward_time=0.120, grad_norm=34.636, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=2.990e-05, train_time=2.460
[dl] 2024-10-16 23:46:55,963 (trainer:779) INFO: 2epoch:train:689-774batch: iter_time=1.510e-04, forward_time=0.116, loss_ctc=117.389, loss_att=94.573, acc=0.082, loss=12.677, backward_time=0.117, grad_norm=26.641, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.095e-05, train_time=2.394
[dl] 2024-10-16 23:47:22,715 (trainer:779) INFO: 2epoch:train:775-860batch: iter_time=1.372e-04, forward_time=0.118, loss_ctc=140.922, loss_att=114.155, acc=0.088, loss=15.273, backward_time=0.122, grad_norm=29.012, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.200e-05, train_time=2.493
[dl] 2024-10-16 23:47:49,439 (trainer:779) INFO: 2epoch:train:861-946batch: iter_time=1.431e-04, forward_time=0.118, loss_ctc=123.298, loss_att=100.792, acc=0.087, loss=13.443, backward_time=0.121, grad_norm=22.678, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.310e-05, train_time=2.502
[dl] 2024-10-16 23:48:15,637 (trainer:779) INFO: 2epoch:train:947-1032batch: iter_time=1.439e-04, forward_time=0.116, loss_ctc=112.332, loss_att=92.548, acc=0.086, loss=12.310, backward_time=0.118, grad_norm=18.389, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.420e-05, train_time=2.427
[dl] 2024-10-16 23:48:41,651 (trainer:779) INFO: 2epoch:train:1033-1118batch: iter_time=1.546e-04, forward_time=0.116, loss_ctc=116.905, loss_att=96.625, acc=0.093, loss=12.839, backward_time=0.118, grad_norm=15.270, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.525e-05, train_time=2.404
[dl] 2024-10-16 23:49:07,542 (trainer:779) INFO: 2epoch:train:1119-1204batch: iter_time=1.589e-04, forward_time=0.115, loss_ctc=127.707, loss_att=105.397, acc=0.099, loss=14.011, backward_time=0.116, grad_norm=16.685, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.630e-05, train_time=2.434
[dl] 2024-10-16 23:49:33,924 (trainer:779) INFO: 2epoch:train:1205-1290batch: iter_time=1.535e-04, forward_time=0.117, loss_ctc=116.839, loss_att=96.298, acc=0.115, loss=12.808, backward_time=0.118, grad_norm=13.628, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.740e-05, train_time=2.442
[dl] 2024-10-16 23:49:59,187 (trainer:779) INFO: 2epoch:train:1291-1376batch: iter_time=1.382e-04, forward_time=0.113, loss_ctc=94.407, loss_att=78.110, acc=0.113, loss=10.375, backward_time=0.113, grad_norm=10.510, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.850e-05, train_time=2.352
[dl] 2024-10-16 23:50:24,923 (trainer:779) INFO: 2epoch:train:1377-1462batch: iter_time=1.544e-04, forward_time=0.115, loss_ctc=112.842, loss_att=92.647, acc=0.121, loss=12.338, backward_time=0.116, grad_norm=10.497, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.955e-05, train_time=2.392
[dl] 2024-10-16 23:50:51,134 (trainer:779) INFO: 2epoch:train:1463-1548batch: iter_time=1.623e-04, forward_time=0.117, loss_ctc=110.090, loss_att=90.014, acc=0.122, loss=12.005, backward_time=0.119, grad_norm=9.539, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=4.060e-05, train_time=2.451
[dl] 2024-10-16 23:51:17,396 (trainer:779) INFO: 2epoch:train:1549-1634batch: iter_time=1.429e-04, forward_time=0.117, loss_ctc=122.327, loss_att=99.629, acc=0.119, loss=13.305, backward_time=0.119, grad_norm=16.621, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=4.170e-05, train_time=2.423
[dl] 2024-10-16 23:51:43,685 (trainer:779) INFO: 2epoch:train:1635-1720batch: iter_time=1.418e-04, forward_time=0.116, loss_ctc=116.994, loss_att=94.753, acc=0.124, loss=12.678, backward_time=0.119, grad_norm=12.002, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=4.280e-05, train_time=2.451
[dl] 2024-10-16 23:53:15,438 (trainer:365) INFO: 2epoch results: [train] iter_time=2.672e-04, forward_time=0.116, loss_ctc=125.054, loss_att=99.683, acc=0.089, loss=13.412, backward_time=0.119, grad_norm=35.056, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.270e-05, train_time=2.441, time=8 minutes and 50.01 seconds, total_count=3474, gpu_max_cached_mem_GB=15.139, [valid] loss_ctc=109.617, cer_ctc=1.000, loss_att=88.279, acc=0.130, cer=0.713, wer=1.000, loss=94.680, time=50.05 seconds, total_count=550, gpu_max_cached_mem_GB=15.139, [att_plot] time=36.48 seconds, total_count=0, gpu_max_cached_mem_GB=15.139
[dl] 2024-10-16 23:53:19,893 (trainer:433) INFO: The best model has been updated: valid.acc
[dl] 2024-10-16 23:53:19,893 (trainer:299) INFO: 3/70epoch started. Estimated time to finish: 11 hours, 50 minutes and 9.94 seconds
[dl] 2024-10-16 23:53:46,947 (trainer:779) INFO: 3epoch:train:1-86batch: iter_time=0.003, forward_time=0.119, loss_ctc=117.684, loss_att=94.982, acc=0.129, loss=12.724, backward_time=0.121, grad_norm=11.917, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.012, optim0_lr0=4.405e-05, train_time=2.528
[dl] 2024-10-16 23:54:13,168 (trainer:779) INFO: 3epoch:train:87-172batch: iter_time=1.528e-04, forward_time=0.117, loss_ctc=104.824, loss_att=84.494, acc=0.129, loss=11.324, backward_time=0.118, grad_norm=12.918, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=4.510e-05, train_time=2.444
[dl] 2024-10-16 23:54:39,812 (trainer:779) INFO: 3epoch:train:173-258batch: iter_time=1.363e-04, forward_time=0.118, loss_ctc=109.418, loss_att=87.847, acc=0.132, loss=11.790, backward_time=0.120, grad_norm=10.162, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=4.620e-05, train_time=2.465
[dl] 2024-10-16 23:55:05,703 (trainer:779) INFO: 3epoch:train:259-344batch: iter_time=1.526e-04, forward_time=0.115, loss_ctc=110.929, loss_att=88.591, acc=0.134, loss=11.912, backward_time=0.117, grad_norm=13.313, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=4.730e-05, train_time=2.411
[dl] 2024-10-16 23:55:31,999 (trainer:779) INFO: 3epoch:train:345-430batch: iter_time=1.456e-04, forward_time=0.116, loss_ctc=127.658, loss_att=101.478, acc=0.134, loss=13.666, backward_time=0.120, grad_norm=9.051, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=4.835e-05, train_time=2.459
[dl] 2024-10-16 23:55:58,957 (trainer:779) INFO: 3epoch:train:431-516batch: iter_time=1.275e-04, forward_time=0.119, loss_ctc=129.494, loss_att=102.825, acc=0.131, loss=13.853, backward_time=0.122, grad_norm=11.013, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=4.940e-05, train_time=2.490
[dl] 2024-10-16 23:56:25,088 (trainer:779) INFO: 3epoch:train:517-602batch: iter_time=1.364e-04, forward_time=0.116, loss_ctc=121.496, loss_att=96.078, acc=0.135, loss=12.963, backward_time=0.118, grad_norm=12.070, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.050e-05, train_time=2.442
[dl] 2024-10-16 23:56:51,224 (trainer:779) INFO: 3epoch:train:603-688batch: iter_time=1.526e-04, forward_time=0.116, loss_ctc=111.712, loss_att=88.327, acc=0.139, loss=11.918, backward_time=0.119, grad_norm=12.215, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.160e-05, train_time=2.425
[dl] 2024-10-16 23:57:17,922 (trainer:779) INFO: 3epoch:train:689-774batch: iter_time=1.472e-04, forward_time=0.119, loss_ctc=110.216, loss_att=86.619, acc=0.141, loss=11.712, backward_time=0.121, grad_norm=11.528, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.265e-05, train_time=2.497
[dl] 2024-10-16 23:57:44,943 (trainer:779) INFO: 3epoch:train:775-860batch: iter_time=1.535e-04, forward_time=0.119, loss_ctc=115.510, loss_att=91.125, acc=0.142, loss=12.305, backward_time=0.123, grad_norm=13.604, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.370e-05, train_time=2.496
[dl] 2024-10-16 23:58:10,348 (trainer:779) INFO: 3epoch:train:861-946batch: iter_time=1.520e-04, forward_time=0.113, loss_ctc=105.998, loss_att=83.261, acc=0.140, loss=11.260, backward_time=0.115, grad_norm=10.350, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.480e-05, train_time=2.387
[dl] 2024-10-16 23:58:36,626 (trainer:779) INFO: 3epoch:train:947-1032batch: iter_time=1.570e-04, forward_time=0.116, loss_ctc=121.068, loss_att=94.337, acc=0.151, loss=12.795, backward_time=0.119, grad_norm=14.415, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.590e-05, train_time=2.428
[dl] 2024-10-16 23:59:03,121 (trainer:779) INFO: 3epoch:train:1033-1118batch: iter_time=1.407e-04, forward_time=0.117, loss_ctc=104.291, loss_att=81.287, acc=0.146, loss=11.024, backward_time=0.119, grad_norm=15.973, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=5.695e-05, train_time=2.480
[dl] 2024-10-16 23:59:28,976 (trainer:779) INFO: 3epoch:train:1119-1204batch: iter_time=1.455e-04, forward_time=0.115, loss_ctc=111.426, loss_att=86.489, acc=0.151, loss=11.746, backward_time=0.116, grad_norm=13.922, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.800e-05, train_time=2.407
[dl] 2024-10-16 23:59:54,681 (trainer:779) INFO: 3epoch:train:1205-1290batch: iter_time=1.364e-04, forward_time=0.115, loss_ctc=120.496, loss_att=93.361, acc=0.151, loss=12.688, backward_time=0.115, grad_norm=15.299, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.910e-05, train_time=2.379
[dl] 2024-10-17 00:00:20,554 (trainer:779) INFO: 3epoch:train:1291-1376batch: iter_time=1.464e-04, forward_time=0.116, loss_ctc=96.907, loss_att=75.204, acc=0.152, loss=10.214, backward_time=0.115, grad_norm=11.440, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=6.020e-05, train_time=2.406
[dl] 2024-10-17 00:00:46,299 (trainer:779) INFO: 3epoch:train:1377-1462batch: iter_time=1.420e-04, forward_time=0.116, loss_ctc=109.348, loss_att=84.579, acc=0.154, loss=11.501, backward_time=0.115, grad_norm=8.704, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=6.125e-05, train_time=2.418
[dl] 2024-10-17 00:01:12,313 (trainer:779) INFO: 3epoch:train:1463-1548batch: iter_time=1.464e-04, forward_time=0.117, loss_ctc=106.748, loss_att=82.511, acc=0.158, loss=11.223, backward_time=0.116, grad_norm=9.988, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=6.230e-05, train_time=2.410
[dl] 2024-10-17 00:01:37,853 (trainer:779) INFO: 3epoch:train:1549-1634batch: iter_time=1.591e-04, forward_time=0.115, loss_ctc=100.552, loss_att=77.415, acc=0.159, loss=10.544, backward_time=0.114, grad_norm=12.439, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=6.340e-05, train_time=2.375
[dl] 2024-10-17 00:02:04,868 (trainer:779) INFO: 3epoch:train:1635-1720batch: iter_time=1.487e-04, forward_time=0.120, loss_ctc=121.131, loss_att=92.747, acc=0.166, loss=12.658, backward_time=0.121, grad_norm=16.010, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=6.450e-05, train_time=2.500
[dl] 2024-10-17 00:03:32,172 (trainer:365) INFO: 3epoch results: [train] iter_time=2.805e-04, forward_time=0.117, loss_ctc=111.858, loss_att=87.874, acc=0.144, loss=11.884, backward_time=0.118, grad_norm=12.324, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.440e-05, train_time=2.440, time=8 minutes and 49.89 seconds, total_count=5211, gpu_max_cached_mem_GB=15.139, [valid] loss_ctc=107.303, cer_ctc=0.996, loss_att=81.672, acc=0.169, cer=0.705, wer=1.000, loss=89.361, time=50.12 seconds, total_count=825, gpu_max_cached_mem_GB=15.139, [att_plot] time=32.27 seconds, total_count=0, gpu_max_cached_mem_GB=15.139
[dl] 2024-10-17 00:03:36,435 (trainer:433) INFO: The best model has been updated: valid.acc
[dl] 2024-10-17 00:03:36,435 (trainer:299) INFO: 4/70epoch started. Estimated time to finish: 11 hours, 35 minutes and 58.32 seconds
[dl] 2024-10-17 00:04:02,302 (trainer:779) INFO: 4epoch:train:1-86batch: iter_time=0.003, forward_time=0.113, loss_ctc=94.738, loss_att=72.695, acc=0.169, loss=9.913, backward_time=0.112, grad_norm=12.533, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.011, optim0_lr0=6.575e-05, train_time=2.393
[dl] 2024-10-17 00:04:28,964 (trainer:779) INFO: 4epoch:train:87-172batch: iter_time=1.499e-04, forward_time=0.119, loss_ctc=112.012, loss_att=85.561, acc=0.172, loss=11.687, backward_time=0.119, grad_norm=13.796, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=6.680e-05, train_time=2.472
[dl] 2024-10-17 00:04:55,332 (trainer:779) INFO: 4epoch:train:173-258batch: iter_time=1.557e-04, forward_time=0.118, loss_ctc=94.212, loss_att=72.073, acc=0.173, loss=9.839, backward_time=0.117, grad_norm=12.756, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=6.790e-05, train_time=2.475
[dl] 2024-10-17 00:05:22,587 (trainer:779) INFO: 4epoch:train:259-344batch: iter_time=1.424e-04, forward_time=0.121, loss_ctc=120.605, loss_att=91.828, acc=0.165, loss=12.558, backward_time=0.122, grad_norm=20.593, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=6.900e-05, train_time=2.526
[dl] 2024-10-17 00:05:48,299 (trainer:779) INFO: 4epoch:train:345-430batch: iter_time=1.567e-04, forward_time=0.115, loss_ctc=107.675, loss_att=82.009, acc=0.171, loss=11.214, backward_time=0.115, grad_norm=11.903, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=7.005e-05, train_time=2.420
[dl] 2024-10-17 00:06:14,460 (trainer:779) INFO: 4epoch:train:431-516batch: iter_time=1.501e-04, forward_time=0.116, loss_ctc=111.586, loss_att=84.844, acc=0.174, loss=11.608, backward_time=0.117, grad_norm=23.402, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=7.110e-05, train_time=2.410
[dl] 2024-10-17 00:06:40,278 (trainer:779) INFO: 4epoch:train:517-602batch: iter_time=1.476e-04, forward_time=0.115, loss_ctc=111.535, loss_att=84.549, acc=0.179, loss=11.581, backward_time=0.116, grad_norm=19.013, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=7.220e-05, train_time=2.393
[dl] 2024-10-17 00:07:06,587 (trainer:779) INFO: 4epoch:train:603-688batch: iter_time=1.699e-04, forward_time=0.118, loss_ctc=106.794, loss_att=80.808, acc=0.173, loss=11.075, backward_time=0.117, grad_norm=12.782, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=7.330e-05, train_time=2.451
[dl] 2024-10-17 00:07:35,005 (trainer:779) INFO: 4epoch:train:689-774batch: iter_time=0.018, forward_time=0.121, loss_ctc=120.427, loss_att=91.364, acc=0.177, loss=12.510, backward_time=0.123, grad_norm=11.969, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=7.435e-05, train_time=2.603
[dl] 2024-10-17 00:08:58,221 (trainer:779) INFO: 4epoch:train:775-860batch: iter_time=0.049, forward_time=0.463, loss_ctc=114.179, loss_att=86.336, acc=0.175, loss=11.836, backward_time=0.278, grad_norm=15.649, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.011, optim0_lr0=7.540e-05, train_time=7.511
[dl] 2024-10-17 00:10:04,570 (trainer:779) INFO: 4epoch:train:861-946batch: iter_time=1.412e-04, forward_time=0.378, loss_ctc=103.366, loss_att=78.256, acc=0.184, loss=10.724, backward_time=0.242, grad_norm=12.605, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=7.650e-05, train_time=6.214
[dl] 2024-10-17 00:10:32,146 (trainer:779) INFO: 4epoch:train:947-1032batch: iter_time=0.014, forward_time=0.118, loss_ctc=112.772, loss_att=85.004, acc=0.186, loss=11.667, backward_time=0.120, grad_norm=20.018, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=7.760e-05, train_time=2.595
[dl] 2024-10-17 00:11:02,408 (trainer:779) INFO: 4epoch:train:1033-1118batch: iter_time=0.063, forward_time=0.114, loss_ctc=104.685, loss_att=79.027, acc=0.184, loss=10.841, backward_time=0.114, grad_norm=16.177, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=7.865e-05, train_time=2.812
[dl] 2024-10-17 00:11:34,349 (trainer:779) INFO: 4epoch:train:1119-1204batch: iter_time=0.071, forward_time=0.118, loss_ctc=113.798, loss_att=85.639, acc=0.196, loss=11.761, backward_time=0.119, grad_norm=20.388, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=7.970e-05, train_time=2.926
[dl] 2024-10-17 00:12:04,475 (trainer:779) INFO: 4epoch:train:1205-1290batch: iter_time=0.047, forward_time=0.119, loss_ctc=120.350, loss_att=90.596, acc=0.188, loss=12.440, backward_time=0.120, grad_norm=19.813, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=8.080e-05, train_time=2.849
[dl] 2024-10-17 00:12:42,463 (trainer:779) INFO: 4epoch:train:1291-1376batch: iter_time=0.148, forward_time=0.117, loss_ctc=105.144, loss_att=78.928, acc=0.212, loss=10.849, backward_time=0.117, grad_norm=15.644, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=8.190e-05, train_time=3.515
[dl] 2024-10-17 00:13:21,709 (trainer:779) INFO: 4epoch:train:1377-1462batch: iter_time=0.162, forward_time=0.118, loss_ctc=103.314, loss_att=77.373, acc=0.207, loss=10.644, backward_time=0.118, grad_norm=24.726, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=8.295e-05, train_time=3.645
[dl] 2024-10-17 00:13:59,988 (trainer:779) INFO: 4epoch:train:1463-1548batch: iter_time=0.144, forward_time=0.128, loss_ctc=97.613, loss_att=73.482, acc=0.202, loss=10.090, backward_time=0.120, grad_norm=14.835, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.011, optim0_lr0=8.400e-05, train_time=3.471
[dl] 2024-10-17 00:14:41,067 (trainer:779) INFO: 4epoch:train:1549-1634batch: iter_time=0.125, forward_time=0.148, loss_ctc=129.779, loss_att=97.291, acc=0.193, loss=13.380, backward_time=0.135, grad_norm=21.331, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=8.510e-05, train_time=3.940
[dl] 2024-10-17 00:15:17,088 (trainer:779) INFO: 4epoch:train:1635-1720batch: iter_time=0.128, forward_time=0.116, loss_ctc=121.269, loss_att=90.738, acc=0.194, loss=12.487, backward_time=0.117, grad_norm=14.429, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=8.620e-05, train_time=3.332
[dl] 2024-10-17 00:17:29,498 (trainer:365) INFO: 4epoch results: [train] iter_time=0.049, forward_time=0.149, loss_ctc=109.676, loss_att=82.966, acc=0.184, loss=11.372, backward_time=0.133, grad_norm=16.725, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=7.610e-05, train_time=3.258, time=11 minutes and 47.51 seconds, total_count=6948, gpu_max_cached_mem_GB=15.139, [valid] loss_ctc=105.084, cer_ctc=0.973, loss_att=77.956, acc=0.218, cer=0.759, wer=1.000, loss=86.094, time=1 minute and 34.09 seconds, total_count=1100, gpu_max_cached_mem_GB=15.139, [att_plot] time=31.46 seconds, total_count=0, gpu_max_cached_mem_GB=15.139
[dl] 2024-10-17 00:17:35,535 (trainer:433) INFO: The best model has been updated: valid.acc
[dl] 2024-10-17 00:17:35,536 (trainer:299) INFO: 5/70epoch started. Estimated time to finish: 12 hours, 24 minutes and 56.46 seconds
[dl] 2024-10-17 00:18:07,595 (trainer:779) INFO: 5epoch:train:1-86batch: iter_time=0.068, forward_time=0.118, loss_ctc=106.458, loss_att=79.399, acc=0.217, loss=10.940, backward_time=0.118, grad_norm=20.454, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.011, optim0_lr0=8.745e-05, train_time=3.059
[dl] 2024-10-17 00:18:34,770 (trainer:779) INFO: 5epoch:train:87-172batch: iter_time=0.005, forward_time=0.120, loss_ctc=108.772, loss_att=80.976, acc=0.211, loss=11.164, backward_time=0.121, grad_norm=23.758, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=8.850e-05, train_time=2.492
[dl] 2024-10-17 00:19:02,082 (trainer:779) INFO: 5epoch:train:173-258batch: iter_time=1.470e-04, forward_time=0.121, loss_ctc=109.576, loss_att=81.466, acc=0.213, loss=11.237, backward_time=0.122, grad_norm=21.802, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=8.960e-05, train_time=2.539
[dl] 2024-10-17 00:19:34,322 (trainer:779) INFO: 5epoch:train:259-344batch: iter_time=0.075, forward_time=0.117, loss_ctc=107.317, loss_att=80.130, acc=0.209, loss=11.036, backward_time=0.117, grad_norm=15.614, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=9.070e-05, train_time=2.986
[dl] 2024-10-17 00:20:08,735 (trainer:779) INFO: 5epoch:train:345-430batch: iter_time=0.100, forward_time=0.119, loss_ctc=114.469, loss_att=84.945, acc=0.219, loss=11.725, backward_time=0.119, grad_norm=23.305, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.011, optim0_lr0=9.175e-05, train_time=3.274
[dl] 2024-10-17 00:20:34,533 (trainer:779) INFO: 5epoch:train:431-516batch: iter_time=1.619e-04, forward_time=0.114, loss_ctc=100.913, loss_att=75.347, acc=0.222, loss=10.377, backward_time=0.115, grad_norm=22.099, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=9.280e-05, train_time=2.396
[dl] 2024-10-17 00:21:00,128 (trainer:779) INFO: 5epoch:train:517-602batch: iter_time=1.422e-04, forward_time=0.114, loss_ctc=105.680, loss_att=78.385, acc=0.219, loss=10.822, backward_time=0.115, grad_norm=11.645, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=9.390e-05, train_time=2.368
[dl] 2024-10-17 00:21:26,630 (trainer:779) INFO: 5epoch:train:603-688batch: iter_time=1.385e-04, forward_time=0.117, loss_ctc=113.287, loss_att=83.896, acc=0.220, loss=11.589, backward_time=0.119, grad_norm=17.318, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=9.500e-05, train_time=2.470
[dl] 2024-10-17 00:21:52,365 (trainer:779) INFO: 5epoch:train:689-774batch: iter_time=1.341e-04, forward_time=0.114, loss_ctc=108.758, loss_att=80.871, acc=0.209, loss=11.155, backward_time=0.116, grad_norm=15.900, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=9.605e-05, train_time=2.384
[dl] 2024-10-17 00:22:17,377 (trainer:779) INFO: 5epoch:train:775-860batch: iter_time=1.516e-04, forward_time=0.112, loss_ctc=93.349, loss_att=69.214, acc=0.226, loss=9.557, backward_time=0.111, grad_norm=11.252, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=9.710e-05, train_time=2.347
[dl] 2024-10-17 00:22:43,872 (trainer:779) INFO: 5epoch:train:861-946batch: iter_time=1.403e-04, forward_time=0.117, loss_ctc=118.864, loss_att=88.113, acc=0.215, loss=12.167, backward_time=0.119, grad_norm=17.787, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=9.820e-05, train_time=2.463
[dl] 2024-10-17 00:23:11,614 (trainer:779) INFO: 5epoch:train:947-1032batch: iter_time=0.028, forward_time=0.114, loss_ctc=105.161, loss_att=77.836, acc=0.214, loss=10.754, backward_time=0.114, grad_norm=25.324, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=9.930e-05, train_time=2.567
[dl] 2024-10-17 00:23:40,044 (trainer:779) INFO: 5epoch:train:1033-1118batch: iter_time=0.031, forward_time=0.116, loss_ctc=111.585, loss_att=82.293, acc=0.228, loss=11.385, backward_time=0.118, grad_norm=17.215, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.003e-04, train_time=2.680
[dl] 2024-10-17 00:24:07,147 (trainer:779) INFO: 5epoch:train:1119-1204batch: iter_time=0.026, forward_time=0.113, loss_ctc=99.548, loss_att=73.467, acc=0.229, loss=10.161, backward_time=0.114, grad_norm=14.441, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.014e-04, train_time=2.515
[dl] 2024-10-17 00:24:33,363 (trainer:779) INFO: 5epoch:train:1205-1290batch: iter_time=0.005, forward_time=0.116, loss_ctc=105.189, loss_att=77.663, acc=0.216, loss=10.740, backward_time=0.116, grad_norm=17.618, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.025e-04, train_time=2.427
[dl] 2024-10-17 00:25:00,270 (trainer:779) INFO: 5epoch:train:1291-1376batch: iter_time=1.448e-04, forward_time=0.120, loss_ctc=114.009, loss_att=84.305, acc=0.217, loss=11.652, backward_time=0.121, grad_norm=12.481, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.036e-04, train_time=2.499
[dl] 2024-10-17 00:25:28,306 (trainer:779) INFO: 5epoch:train:1377-1462batch: iter_time=0.025, forward_time=0.116, loss_ctc=98.766, loss_att=72.880, acc=0.232, loss=10.081, backward_time=0.116, grad_norm=14.430, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.046e-04, train_time=2.636
[dl] 2024-10-17 00:25:55,094 (trainer:779) INFO: 5epoch:train:1463-1548batch: iter_time=0.003, forward_time=0.118, loss_ctc=113.841, loss_att=83.650, acc=0.219, loss=11.588, backward_time=0.119, grad_norm=27.992, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.057e-04, train_time=2.484
[dl] 2024-10-17 00:26:21,502 (trainer:779) INFO: 5epoch:train:1549-1634batch: iter_time=1.407e-04, forward_time=0.117, loss_ctc=112.370, loss_att=82.356, acc=0.215, loss=11.420, backward_time=0.119, grad_norm=24.890, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.068e-04, train_time=2.457
[dl] 2024-10-17 00:26:50,443 (trainer:779) INFO: 5epoch:train:1635-1720batch: iter_time=0.020, forward_time=0.123, loss_ctc=112.132, loss_att=82.229, acc=0.232, loss=11.400, backward_time=0.122, grad_norm=17.182, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.079e-04, train_time=2.678
[dl] 2024-10-17 00:28:16,417 (trainer:365) INFO: 5epoch results: [train] iter_time=0.019, forward_time=0.117, loss_ctc=107.466, loss_att=79.579, acc=0.219, loss=10.993, backward_time=0.117, grad_norm=18.555, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=9.780e-05, train_time=2.578, time=9 minutes and 19.99 seconds, total_count=8685, gpu_max_cached_mem_GB=15.139, [valid] loss_ctc=103.334, cer_ctc=0.963, loss_att=75.181, acc=0.243, cer=0.750, wer=1.000, loss=83.626, time=50.04 seconds, total_count=1375, gpu_max_cached_mem_GB=15.139, [att_plot] time=30.85 seconds, total_count=0, gpu_max_cached_mem_GB=15.139
[dl] 2024-10-17 00:28:20,927 (trainer:433) INFO: The best model has been updated: valid.acc
[dl] 2024-10-17 00:28:20,928 (trainer:299) INFO: 6/70epoch started. Estimated time to finish: 12 hours, 6 minutes and 45.49 seconds
[dl] 2024-10-17 00:28:48,229 (trainer:779) INFO: 6epoch:train:1-86batch: iter_time=0.003, forward_time=0.119, loss_ctc=118.075, loss_att=86.642, acc=0.224, loss=12.009, backward_time=0.121, grad_norm=13.589, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.091e-04, train_time=2.534
[dl] 2024-10-17 00:29:14,412 (trainer:779) INFO: 6epoch:train:87-172batch: iter_time=1.511e-04, forward_time=0.116, loss_ctc=108.383, loss_att=79.117, acc=0.231, loss=10.987, backward_time=0.119, grad_norm=15.329, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.102e-04, train_time=2.451
[dl] 2024-10-17 00:29:41,182 (trainer:779) INFO: 6epoch:train:173-258batch: iter_time=1.543e-04, forward_time=0.118, loss_ctc=108.007, loss_att=79.003, acc=0.228, loss=10.963, backward_time=0.120, grad_norm=18.264, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.113e-04, train_time=2.497
[dl] 2024-10-17 00:30:08,016 (trainer:779) INFO: 6epoch:train:259-344batch: iter_time=1.483e-04, forward_time=0.119, loss_ctc=107.421, loss_att=78.366, acc=0.252, loss=10.885, backward_time=0.120, grad_norm=14.476, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.124e-04, train_time=2.483
[dl] 2024-10-17 00:30:34,424 (trainer:779) INFO: 6epoch:train:345-430batch: iter_time=1.477e-04, forward_time=0.118, loss_ctc=107.993, loss_att=78.763, acc=0.248, loss=10.942, backward_time=0.119, grad_norm=12.667, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.135e-04, train_time=2.477
[dl] 2024-10-17 00:31:00,563 (trainer:779) INFO: 6epoch:train:431-516batch: iter_time=1.502e-04, forward_time=0.116, loss_ctc=98.345, loss_att=71.727, acc=0.237, loss=9.964, backward_time=0.117, grad_norm=18.593, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.145e-04, train_time=2.414
[dl] 2024-10-17 00:31:26,734 (trainer:779) INFO: 6epoch:train:517-602batch: iter_time=1.494e-04, forward_time=0.116, loss_ctc=103.854, loss_att=75.594, acc=0.243, loss=10.509, backward_time=0.117, grad_norm=19.787, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.156e-04, train_time=2.437
[dl] 2024-10-17 00:31:53,447 (trainer:779) INFO: 6epoch:train:603-688batch: iter_time=1.542e-04, forward_time=0.119, loss_ctc=108.268, loss_att=78.823, acc=0.230, loss=10.957, backward_time=0.120, grad_norm=21.815, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.167e-04, train_time=2.482
[dl] 2024-10-17 00:32:19,288 (trainer:779) INFO: 6epoch:train:689-774batch: iter_time=1.471e-04, forward_time=0.115, loss_ctc=103.887, loss_att=75.502, acc=0.236, loss=10.502, backward_time=0.117, grad_norm=13.514, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.177e-04, train_time=2.402
[dl] 2024-10-17 00:32:45,805 (trainer:779) INFO: 6epoch:train:775-860batch: iter_time=1.801e-04, forward_time=0.118, loss_ctc=112.737, loss_att=81.981, acc=0.230, loss=11.401, backward_time=0.119, grad_norm=13.778, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.188e-04, train_time=2.463
[dl] 2024-10-17 00:33:24,287 (trainer:779) INFO: 6epoch:train:861-946batch: iter_time=0.151, forward_time=0.121, loss_ctc=107.668, loss_att=78.378, acc=0.242, loss=10.896, backward_time=0.120, grad_norm=22.586, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.011, optim0_lr0=1.199e-04, train_time=3.551
[dl] 2024-10-17 00:34:08,417 (trainer:779) INFO: 6epoch:train:947-1032batch: iter_time=0.221, forward_time=0.120, loss_ctc=103.496, loss_att=75.244, acc=0.236, loss=10.465, backward_time=0.119, grad_norm=15.212, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.210e-04, train_time=4.072
[dl] 2024-10-17 00:34:54,850 (trainer:779) INFO: 6epoch:train:1033-1118batch: iter_time=0.249, forward_time=0.119, loss_ctc=100.413, loss_att=72.910, acc=0.250, loss=10.145, backward_time=0.117, grad_norm=14.192, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.220e-04, train_time=4.292
[dl] 2024-10-17 00:35:45,825 (trainer:779) INFO: 6epoch:train:1119-1204batch: iter_time=0.300, forward_time=0.119, loss_ctc=101.458, loss_att=73.424, acc=0.254, loss=10.229, backward_time=0.119, grad_norm=13.640, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.231e-04, train_time=4.717
[dl] 2024-10-17 00:36:37,721 (trainer:779) INFO: 6epoch:train:1205-1290batch: iter_time=0.321, forward_time=0.116, loss_ctc=98.403, loss_att=71.162, acc=0.251, loss=9.917, backward_time=0.116, grad_norm=16.225, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.242e-04, train_time=4.910
[dl] 2024-10-17 00:37:23,419 (trainer:779) INFO: 6epoch:train:1291-1376batch: iter_time=0.242, forward_time=0.118, loss_ctc=109.378, loss_att=79.350, acc=0.232, loss=11.045, backward_time=0.119, grad_norm=13.865, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.253e-04, train_time=4.199
[dl] 2024-10-17 00:38:21,341 (trainer:779) INFO: 6epoch:train:1377-1462batch: iter_time=0.399, forward_time=0.115, loss_ctc=90.153, loss_att=65.015, acc=0.253, loss=9.070, backward_time=0.112, grad_norm=18.113, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.264e-04, train_time=5.585
[dl] 2024-10-17 00:39:11,104 (trainer:779) INFO: 6epoch:train:1463-1548batch: iter_time=0.282, forward_time=0.122, loss_ctc=121.683, loss_att=87.936, acc=0.231, loss=12.258, backward_time=0.122, grad_norm=19.512, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.011, optim0_lr0=1.274e-04, train_time=4.469
[dl] 2024-10-17 00:40:00,954 (trainer:779) INFO: 6epoch:train:1549-1634batch: iter_time=0.285, forward_time=0.121, loss_ctc=108.160, loss_att=78.379, acc=0.235, loss=10.914, backward_time=0.120, grad_norm=20.356, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.011, optim0_lr0=1.285e-04, train_time=4.599
[dl] 2024-10-17 00:40:48,562 (trainer:779) INFO: 6epoch:train:1635-1720batch: iter_time=0.262, forward_time=0.120, loss_ctc=104.457, loss_att=75.320, acc=0.246, loss=10.508, backward_time=0.119, grad_norm=22.035, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.296e-04, train_time=4.504
# Accounting: time=5207 threads=1
# Ended (code 137) at Thu Oct 17 00:58:54 EDT 2024, elapsed time 5207 seconds

2024-10-17T01:20:31 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_lr1e-3_warm_10000.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-17T01:20:31 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-17T01:20:31 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-17T01:20:31 (asr.sh:1407:main) Generate 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-17T01:20:32 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log'
2024-10-17 01:20:32,193 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log' --log exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-17 01:20:32,209 (launch:348) INFO: log file: exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
bash: line 1: 1911186 Killed                  ( python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True ) 2>> exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log >> exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log', '--gpu', '1', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000', '--config', 'conf/train_asr_lr1e-3_warm_10000.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Thu Oct 17 01:20:32 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[dl] 2024-10-17 01:20:39,864 (asr:523) INFO: Vocabulary size: 5000
[dl] 2024-10-17 01:20:45,024 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl] 2024-10-17 01:20:45,031 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5000, bias=True)
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 38.47 M
    Number of trainable parameters: 38.47 M (100.0%)
    Size: 153.89 MB
    Type: torch.float32
[dl] 2024-10-17 01:20:45,031 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 1e-07
    maximize: False
    weight_decay: 1e-06
)
[dl] 2024-10-17 01:20:45,031 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=10000)
[dl] 2024-10-17 01:20:45,031 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/config.yaml
[dl] 2024-10-17 01:20:45,233 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-17 01:20:46,779 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7df63ed55840>)
[dl] 2024-10-17 01:20:46,779 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=1737, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-17 01:20:46,780 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=1737, mean=32.1, min=6, max=201
[dl] 2024-10-17 01:20:47,100 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-17 01:20:47,252 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7df63eb62fb0>)
[dl] 2024-10-17 01:20:47,252 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=275, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-17 01:20:47,252 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=275, mean=32.9, min=5, max=156
[dl] 2024-10-17 01:20:47,263 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-17 01:20:47,270 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7df63eb62dd0>)
[dl] 2024-10-17 01:20:47,270 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl] 2024-10-17 01:20:47,270 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
# Accounting: time=55 threads=1
# Ended (code 137) at Thu Oct 17 01:21:27 EDT 2024, elapsed time 55 seconds

2024-10-17T03:50:11 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_lr1e-3_warm_10000.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-17T03:50:11 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-17T03:50:11 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-17T03:50:11 (asr.sh:1407:main) Generate 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-17T03:50:11 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log'
2024-10-17 03:50:11,309 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log' --log exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-17 03:50:11,323 (launch:348) INFO: log file: exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
bash: line 1: 1939269 Killed                  ( python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True ) 2>> exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log >> exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log', '--gpu', '1', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000', '--config', 'conf/train_asr_lr1e-3_warm_10000.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Thu Oct 17 03:50:11 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[dl] 2024-10-17 03:50:18,858 (asr:523) INFO: Vocabulary size: 5000
[dl] 2024-10-17 03:50:19,675 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl] 2024-10-17 03:50:19,681 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5000, bias=True)
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 38.47 M
    Number of trainable parameters: 38.47 M (100.0%)
    Size: 153.89 MB
    Type: torch.float32
[dl] 2024-10-17 03:50:19,681 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 1e-07
    maximize: False
    weight_decay: 1e-06
)
[dl] 2024-10-17 03:50:19,681 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=10000)
[dl] 2024-10-17 03:50:19,681 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/config.yaml
[dl] 2024-10-17 03:50:19,824 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-17 03:50:20,326 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7bd29b26d8a0>)
[dl] 2024-10-17 03:50:20,326 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=1737, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-17 03:50:20,327 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=1737, mean=32.1, min=6, max=201
[dl] 2024-10-17 03:50:20,338 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-17 03:50:20,387 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7bd29b07b010>)
[dl] 2024-10-17 03:50:20,387 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=275, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-17 03:50:20,387 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=275, mean=32.9, min=5, max=156
[dl] 2024-10-17 03:50:20,397 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-17 03:50:20,405 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7bd29b0b2ef0>)
[dl] 2024-10-17 03:50:20,405 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl] 2024-10-17 03:50:20,405 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[dl] 2024-10-17 03:50:24,771 (trainer:174) INFO: The training was resumed using exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/checkpoint.pth
[dl] 2024-10-17 03:50:24,774 (trainer:311) INFO: 6/70epoch started
[dl] 2024-10-17 03:50:56,047 (trainer:779) INFO: 6epoch:train:1-86batch: iter_time=0.004, forward_time=0.151, loss_ctc=118.148, loss_att=86.612, acc=0.226, loss=12.009, backward_time=0.138, grad_norm=14.542, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.045, optim0_lr0=1.091e-04, train_time=2.935
# Accounting: time=63 threads=1
# Ended (code 137) at Thu Oct 17 03:51:14 EDT 2024, elapsed time 63 seconds

2024-10-17T03:52:52 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_lr1e-3_warm_10000.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-17T03:52:52 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-17T03:52:52 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-17T03:52:52 (asr.sh:1407:main) Generate 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-17T03:52:52 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log'
2024-10-17 03:52:53,104 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log' --log exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-17 03:52:53,118 (launch:348) INFO: log file: exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
bash: line 1: 1940291 Killed                  ( python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True ) 2>> exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log >> exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log', '--gpu', '1', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000', '--config', 'conf/train_asr_lr1e-3_warm_10000.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Thu Oct 17 03:52:53 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[dl] 2024-10-17 03:52:59,232 (asr:523) INFO: Vocabulary size: 5000
[dl] 2024-10-17 03:53:00,031 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl] 2024-10-17 03:53:00,037 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5000, bias=True)
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 38.47 M
    Number of trainable parameters: 38.47 M (100.0%)
    Size: 153.89 MB
    Type: torch.float32
[dl] 2024-10-17 03:53:00,037 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 1e-07
    maximize: False
    weight_decay: 1e-06
)
[dl] 2024-10-17 03:53:00,037 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=10000)
[dl] 2024-10-17 03:53:00,037 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/config.yaml
[dl] 2024-10-17 03:53:00,183 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-17 03:53:00,563 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x707a00b79870>)
[dl] 2024-10-17 03:53:00,563 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=1737, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-17 03:53:00,564 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=1737, mean=32.1, min=6, max=201
[dl] 2024-10-17 03:53:00,574 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-17 03:53:00,608 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x707a00986fe0>)
[dl] 2024-10-17 03:53:00,608 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=275, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-17 03:53:00,609 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=275, mean=32.9, min=5, max=156
[dl] 2024-10-17 03:53:00,619 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-17 03:53:00,626 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x707a009baec0>)
[dl] 2024-10-17 03:53:00,626 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl] 2024-10-17 03:53:00,626 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[dl] 2024-10-17 03:53:01,083 (trainer:174) INFO: The training was resumed using exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/checkpoint.pth
[dl] 2024-10-17 03:53:01,085 (trainer:311) INFO: 6/70epoch started
[dl] 2024-10-17 03:53:29,158 (trainer:779) INFO: 6epoch:train:1-86batch: iter_time=0.003, forward_time=0.134, loss_ctc=118.149, loss_att=86.612, acc=0.226, loss=12.009, backward_time=0.122, grad_norm=14.566, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.026, optim0_lr0=1.091e-04, train_time=2.618
# Accounting: time=61 threads=1
# Ended (code 137) at Thu Oct 17 03:53:54 EDT 2024, elapsed time 61 seconds

2024-10-17T03:54:29 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_lr1e-3_warm_10000.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-17T03:54:29 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-17T03:54:29 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-17T03:54:29 (asr.sh:1407:main) Generate 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-17T03:54:29 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log'
2024-10-17 03:54:29,277 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log' --log exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-17 03:54:29,290 (launch:348) INFO: log file: exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
2024-10-18T14:36:39 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-18T14:36:39 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-18T14:36:39 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-18T14:36:39 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-18T14:36:39 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_ebranchformer_raw_en_bpe5000/train.log'
2024-10-18 14:36:39,884 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_ebranchformer_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_ebranchformer_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_ebranchformer_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ebranchformer_raw_en_bpe5000 --config conf/train_asr_wavlm_ebranchformer.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-18 14:36:39,902 (launch:237) INFO: single-node with 2gpu on distributed mode
2024-10-18 14:36:39,903 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_ebranchformer_raw_en_bpe5000/train.log
2024-10-25T23:34:37 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference false --inference_nj 8 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-25T23:34:37 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-25T23:34:37 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-25T23:34:37 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-25T23:34:37 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log'
2024-10-25 23:34:37,674 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-25 23:34:37,689 (launch:237) INFO: single-node with 2gpu on distributed mode
2024-10-25 23:34:37,690 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log', '--gpu', '2', 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_ebranchformer_onlyctc.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '2', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log ###################
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,030 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,030 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,030 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,030 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,030 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,030 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,030 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,030 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.grep_a.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.grep_a.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.grep_a.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.grep_a.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.grep_a.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.grep_a.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.grep_a.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.grep_a.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.grep_a.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,404 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,413 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): S3prlFrontend(
    (upstream): S3PRLUpstream(
      (upstream): UpstreamExpert(
        (model): WavLM(
          (feature_extractor): ConvFeatureExtractionModel(
            (conv_layers): ModuleList(
              (0): Sequential(
                (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
                (1): Dropout(p=0.0, inplace=False)
                (2): Sequential(
                  (0): TransposeLast()
                  (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (2): TransposeLast()
                )
                (3): GELU(approximate='none')
              )
              (1-4): 4 x Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
                (1): Dropout(p=0.0, inplace=False)
                (2): Sequential(
                  (0): TransposeLast()
                  (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (2): TransposeLast()
                )
                (3): GELU(approximate='none')
              )
              (5-6): 2 x Sequential(
                (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
                (1): Dropout(p=0.0, inplace=False)
                (2): Sequential(
                  (0): TransposeLast()
                  (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (2): TransposeLast()
                )
                (3): GELU(approximate='none')
              )
            )
          )
          (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)
          (dropout_input): Dropout(p=0.0, inplace=False)
          (dropout_features): Dropout(p=0.0, inplace=False)
          (encoder): TransformerEncoder(
            (pos_conv): Sequential(
              (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
              (1): SamePad()
              (2): GELU(approximate='none')
            )
            (layers): ModuleList(
              (0): TransformerSentenceEncoderLayer(
                (self_attn): MultiheadAttention(
                  (dropout_module): Dropout(p=0.0, inplace=False)
                  (relative_attention_bias): Embedding(320, 16)
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (grep_linear): Linear(in_features=64, out_features=8, bias=True)
                )
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
                (dropout3): Dropout(p=0.0, inplace=False)
                (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (1-23): 23 x TransformerSentenceEncoderLayer(
                (self_attn): MultiheadAttention(
                  (dropout_module): Dropout(p=0.0, inplace=False)
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (grep_linear): Linear(in_features=64, out_features=8, bias=True)
                )
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
                (dropout3): Dropout(p=0.0, inplace=False)
                (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (featurizer): Featurizer()
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (preencoder): LinearProjection(
    (linear_out): Linear(in_features=1024, out_features=128, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=7936, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 342.80 M
    Number of trainable parameters: 27.35 M (8.0%)
    Size: 109.41 MB
    Type: torch.float32
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,413 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 1.3333333333333336e-07
    maximize: False
    weight_decay: 1e-06
)
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,413 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,413 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,579 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,982 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7853b7a3d450>)
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,982 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=3286, batch_bins=4000000, sort_in_batch=descending, sort_batch=descending)
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,983 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=3286, mean=17.0, min=5, max=111
[SPAPL-Psychic:0/2] 2024-10-25 23:35:07,003 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[SPAPL-Psychic:0/2] 2024-10-25 23:35:07,186 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7853b784aad0>)
[SPAPL-Psychic:0/2] 2024-10-25 23:35:07,186 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=514, batch_bins=4000000, sort_in_batch=descending, sort_batch=descending)
[SPAPL-Psychic:0/2] 2024-10-25 23:35:07,186 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=514, mean=17.6, min=5, max=99
[SPAPL-Psychic:0/2] 2024-10-25 23:35:07,198 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[SPAPL-Psychic:0/2] 2024-10-25 23:35:07,207 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7853be7a84c0>)
[SPAPL-Psychic:0/2] 2024-10-25 23:35:07,207 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[SPAPL-Psychic:0/2] 2024-10-25 23:35:07,207 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
SPAPL-Psychic:2552223:2552223 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
SPAPL-Psychic:2552223:2552223 [0] NCCL INFO Bootstrap : Using enp179s0f0:164.67.196.69<0>
SPAPL-Psychic:2552223:2552223 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
SPAPL-Psychic:2552223:2552223 [0] NCCL INFO cudaDriverVersion 12060
NCCL version 2.20.5+cuda12.4
Process SpawnProcess-2:
Process SpawnProcess-1:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 256, in run
    dp_model = torch.nn.parallel.DistributedDataParallel(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 798, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/utils.py", line 269, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 256, in run
    dp_model = torch.nn.parallel.DistributedDataParallel(
torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1970, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
nvmlDeviceGetHandleByIndex(0) failed: Unknown Error
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 798, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/utils.py", line 269, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1970, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
nvmlDeviceGetHandleByIndex(0) failed: Unknown Error
SPAPL-Psychic:2552224:2552224 [1] NCCL INFO cudaDriverVersion 12060
SPAPL-Psychic:2552224:2552224 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
SPAPL-Psychic:2552224:2552224 [1] NCCL INFO Bootstrap : Using enp179s0f0:164.67.196.69<0>
SPAPL-Psychic:2552224:2552224 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO NET/IB : No device found.
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO NET/Socket : Using [0]enp179s0f0:164.67.196.69<0>
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO Using non-device net plugin version 0
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO Using network Socket

SPAPL-Psychic:2552224:2552406 [1] misc/nvmlwrap.cc:127 NCCL WARN nvmlDeviceGetHandleByIndex(0) failed: Unknown Error
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO misc/nvmlwrap.cc:185 -> 2
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO init.cc:338 -> 2
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO init.cc:1492 -> 2
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO group.cc:64 -> 2 [Async thread]
SPAPL-Psychic:2552224:2552224 [1] NCCL INFO group.cc:418 -> 2
SPAPL-Psychic:2552224:2552224 [1] NCCL INFO group.cc:95 -> 2
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO NET/IB : No device found.
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO NET/Socket : Using [0]enp179s0f0:164.67.196.69<0>
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO Using non-device net plugin version 0
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO Using network Socket

SPAPL-Psychic:2552223:2552405 [0] misc/nvmlwrap.cc:127 NCCL WARN nvmlDeviceGetHandleByIndex(0) failed: Unknown Error
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO misc/nvmlwrap.cc:185 -> 2
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO init.cc:338 -> 2
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO init.cc:1492 -> 2
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO group.cc:64 -> 2 [Async thread]
SPAPL-Psychic:2552223:2552223 [0] NCCL INFO group.cc:418 -> 2
SPAPL-Psychic:2552223:2552223 [0] NCCL INFO group.cc:95 -> 2
SPAPL-Psychic:2552223:2552408 [0] NCCL INFO comm 0xc6b3a70 rank 0 nranks 2 cudaDev 0 busId 1a000 - Abort COMPLETE
SPAPL-Psychic:2552224:2552409 [0] NCCL INFO comm 0x597e67d0 rank 1 nranks 2 cudaDev 1 busId 67000 - Abort COMPLETE
W1025 23:35:08.599000 130328679135040 torch/multiprocessing/spawn.py:145] Terminating process 2552224 via signal SIGTERM
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1273, in main
    while not ProcessContext(processes, error_files).join():
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 177, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with exit code 1
# Accounting: time=32 threads=1
# Ended (code 1) at Fri Oct 25 23:35:09 PDT 2024, elapsed time 32 seconds

2024-10-26T03:25:50 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 8 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-26T03:25:50 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-26T03:25:50 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-26T03:25:50 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-26T03:25:50 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log'
2024-10-26 03:25:51,956 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-26 03:25:51,972 (launch:237) INFO: single-node with 4gpu on distributed mode
2024-10-26 03:25:51,992 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log', '--gpu', '4', 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_ebranchformer_onlyctc.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '4', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log ###################
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.grep_linear.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.grep_linear.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn_layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn_layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.fc1.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.fc1.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.fc2.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.fc2.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.final_layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.final_layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.grep_a.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.k_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.k_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.v_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.v_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.q_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.q_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.out_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.out_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.grep_linear.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.grep_linear.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn_layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn_layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.fc1.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.fc1.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.fc2.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.fc2.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.final_layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.final_layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.grep_a.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.k_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.k_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.v_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.v_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.q_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.q_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.out_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.out_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.grep_linear.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.grep_linear.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn_layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn_layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.fc1.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.fc1.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.fc2.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.fc2.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.final_layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.final_layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.grep_a.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.k_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.k_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.v_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.v_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.q_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.q_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.out_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.out_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.grep_linear.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.grep_linear.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn_layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn_layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.fc1.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.fc1.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.fc2.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.fc2.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.final_layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.final_layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,964 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,964 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,964 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,964 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:37,871 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl:0/4] 2024-10-26 03:26:37,886 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): S3prlFrontend(
    (upstream): S3PRLUpstream(
      (upstream): UpstreamExpert(
        (model): WavLM(
          (feature_extractor): ConvFeatureExtractionModel(
            (conv_layers): ModuleList(
              (0): Sequential(
                (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
                (1): Dropout(p=0.0, inplace=False)
                (2): Sequential(
                  (0): TransposeLast()
                  (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (2): TransposeLast()
                )
                (3): GELU(approximate='none')
              )
              (1-4): 4 x Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
                (1): Dropout(p=0.0, inplace=False)
                (2): Sequential(
                  (0): TransposeLast()
                  (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (2): TransposeLast()
                )
                (3): GELU(approximate='none')
              )
              (5-6): 2 x Sequential(
                (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
                (1): Dropout(p=0.0, inplace=False)
                (2): Sequential(
                  (0): TransposeLast()
                  (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (2): TransposeLast()
                )
                (3): GELU(approximate='none')
              )
            )
          )
          (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)
          (dropout_input): Dropout(p=0.0, inplace=False)
          (dropout_features): Dropout(p=0.0, inplace=False)
          (encoder): TransformerEncoder(
            (pos_conv): Sequential(
              (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
              (1): SamePad()
              (2): GELU(approximate='none')
            )
            (layers): ModuleList(
              (0): TransformerSentenceEncoderLayer(
                (self_attn): MultiheadAttention(
                  (dropout_module): Dropout(p=0.0, inplace=False)
                  (relative_attention_bias): Embedding(320, 16)
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (grep_linear): Linear(in_features=64, out_features=8, bias=True)
                )
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
                (dropout3): Dropout(p=0.0, inplace=False)
                (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (1-23): 23 x TransformerSentenceEncoderLayer(
                (self_attn): MultiheadAttention(
                  (dropout_module): Dropout(p=0.0, inplace=False)
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (grep_linear): Linear(in_features=64, out_features=8, bias=True)
                )
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
                (dropout3): Dropout(p=0.0, inplace=False)
                (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (featurizer): Featurizer()
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (preencoder): LinearProjection(
    (linear_out): Linear(in_features=1024, out_features=128, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=7936, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 342.80 M
    Number of trainable parameters: 27.35 M (8.0%)
    Size: 109.41 MB
    Type: torch.float32
[dl:0/4] 2024-10-26 03:26:37,886 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 1.3333333333333336e-07
    maximize: False
    weight_decay: 1e-06
)
[dl:0/4] 2024-10-26 03:26:37,886 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[dl:0/4] 2024-10-26 03:26:37,926 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
[dl:0/4] 2024-10-26 03:26:38,104 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/4] 2024-10-26 03:26:38,629 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x747130a7e710>)
[dl:0/4] 2024-10-26 03:26:38,629 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=3286, batch_bins=4000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/4] 2024-10-26 03:26:38,629 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=3286, mean=17.0, min=5, max=111
[dl:0/4] 2024-10-26 03:26:38,685 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/4] 2024-10-26 03:26:38,891 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7471308723e0>)
[dl:0/4] 2024-10-26 03:26:38,891 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=514, batch_bins=4000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/4] 2024-10-26 03:26:38,891 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=514, mean=17.6, min=5, max=99
[dl:0/4] 2024-10-26 03:26:38,902 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/4] 2024-10-26 03:26:38,908 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7471308a3d00>)
[dl:0/4] 2024-10-26 03:26:38,908 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl:0/4] 2024-10-26 03:26:38,908 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
dl:979817:979817 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979817:979817 [0] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:979817:979817 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:979817:979817 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.20.5+cuda12.4
dl:979817:980016 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979817:980016 [0] NCCL INFO NET/IB : No device found.
dl:979817:980016 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979817:980016 [0] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:979817:980016 [0] NCCL INFO Using non-device net plugin version 0
dl:979817:980016 [0] NCCL INFO Using network Socket
dl:979817:980016 [0] NCCL INFO comm 0xa768d40 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 2000 commId 0x47133566ce6f830e - Init START
dl:979817:980016 [0] NCCL INFO NVLS multicast support is not available on dev 0
dl:979817:980016 [0] NCCL INFO comm 0xa768d40 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
dl:979817:980016 [0] NCCL INFO Channel 00/02 :    0   1   2   3
dl:979817:980016 [0] NCCL INFO Channel 01/02 :    0   1   2   3
dl:979817:980016 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
dl:979817:980016 [0] NCCL INFO P2P Chunksize set to 131072
dl:979817:980016 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
dl:979817:980016 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
dl:979817:980016 [0] NCCL INFO Connected all rings
dl:979817:980016 [0] NCCL INFO Connected all trees
dl:979817:980016 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:979817:980016 [0] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:979817:980016 [0] NCCL INFO comm 0xa768d40 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 2000 commId 0x47133566ce6f830e - Init COMPLETE
dl:979818:979818 [1] NCCL INFO cudaDriverVersion 12050
dl:979818:979818 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979818:979818 [1] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:979818:979818 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:979818:980019 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979818:980019 [1] NCCL INFO NET/IB : No device found.
dl:979818:980019 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979818:980019 [1] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:979818:980019 [1] NCCL INFO Using non-device net plugin version 0
dl:979818:980019 [1] NCCL INFO Using network Socket
dl:979818:980019 [1] NCCL INFO comm 0x5a602d70 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 21000 commId 0x47133566ce6f830e - Init START
dl:979818:980019 [1] NCCL INFO NVLS multicast support is not available on dev 1
dl:979818:980019 [1] NCCL INFO comm 0x5a602d70 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
dl:979818:980019 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
dl:979818:980019 [1] NCCL INFO P2P Chunksize set to 131072
dl:979818:980019 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
dl:979818:980019 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
dl:979818:980019 [1] NCCL INFO Connected all rings
dl:979818:980019 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
dl:979818:980019 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
dl:979818:980019 [1] NCCL INFO Connected all trees
dl:979818:980019 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:979818:980019 [1] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:979818:980019 [1] NCCL INFO comm 0x5a602d70 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 21000 commId 0x47133566ce6f830e - Init COMPLETE
[rank0]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
dl:979820:979820 [3] NCCL INFO cudaDriverVersion 12050
dl:979820:979820 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979820:979820 [3] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:979820:979820 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:979820:980017 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979820:980017 [3] NCCL INFO NET/IB : No device found.
dl:979820:980017 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979820:980017 [3] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:979820:980017 [3] NCCL INFO Using non-device net plugin version 0
dl:979820:980017 [3] NCCL INFO Using network Socket
dl:979820:980017 [3] NCCL INFO comm 0x5ae256f0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 4a000 commId 0x47133566ce6f830e - Init START
dl:979820:980017 [3] NCCL INFO NVLS multicast support is not available on dev 3
dl:979820:980017 [3] NCCL INFO comm 0x5ae256f0 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
dl:979820:980017 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
dl:979820:980017 [3] NCCL INFO P2P Chunksize set to 131072
dl:979820:980017 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM
dl:979820:980017 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/CUMEM
dl:979820:980017 [3] NCCL INFO Connected all rings
dl:979820:980017 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
dl:979820:980017 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
dl:979820:980017 [3] NCCL INFO Connected all trees
dl:979820:980017 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:979820:980017 [3] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:979820:980017 [3] NCCL INFO comm 0x5ae256f0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 4a000 commId 0x47133566ce6f830e - Init COMPLETE
[rank1]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
[rank3]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
dl:979819:979819 [2] NCCL INFO cudaDriverVersion 12050
dl:979819:979819 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979819:979819 [2] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:979819:979819 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:979819:980018 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979819:980018 [2] NCCL INFO NET/IB : No device found.
dl:979819:980018 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979819:980018 [2] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:979819:980018 [2] NCCL INFO Using non-device net plugin version 0
dl:979819:980018 [2] NCCL INFO Using network Socket
dl:979819:980018 [2] NCCL INFO comm 0x5af571c0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 49000 commId 0x47133566ce6f830e - Init START
dl:979819:980018 [2] NCCL INFO NVLS multicast support is not available on dev 2
dl:979819:980018 [2] NCCL INFO comm 0x5af571c0 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
dl:979819:980018 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
dl:979819:980018 [2] NCCL INFO P2P Chunksize set to 131072
dl:979819:980018 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
dl:979819:980018 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
dl:979819:980018 [2] NCCL INFO Connected all rings
dl:979819:980018 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
dl:979819:980018 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
dl:979819:980018 [2] NCCL INFO Connected all trees
dl:979819:980018 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:979819:980018 [2] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:979819:980018 [2] NCCL INFO comm 0x5af571c0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 49000 commId 0x47133566ce6f830e - Init COMPLETE
[rank2]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
[dl:0/4] 2024-10-26 03:26:40,360 (trainer:311) INFO: 1/70epoch started
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Process SpawnProcess-1:
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 614, in train_one_epoch
    retval = model(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 237, in forward
    encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 378, in encode
    feats, feats_lengths = self._extract_feats(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 445, in _extract_feats
    feats, feats_lengths = self.frontend(speech, speech_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/frontend/s3prl.py", line 99, in forward
    feats, feats_lens = self.upstream(input, input_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/nn/upstream.py", line 209, in forward
    hidden_states = self.upstream(wavs_list)["hidden_states"]
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/interfaces.py", line 103, in __call__
    result = super().__call__(wavs, *args, **kwargs) or {}
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/expert.py", line 83, in forward
    features, feat_padding_mask = self.model.extract_features(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 389, in extract_features
    x, layer_results = self.encoder(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 592, in forward
    x, layer_results = self.extract_features(x, padding_mask, streaming_mask, layer)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 626, in extract_features
    x, z, pos_bias = layer(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 725, in forward
    x, attn, pos_bias = self.self_attn(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/modules.py", line 556, in forward
    x, attn = F.multi_head_attention_forward(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py", line 5504, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 252.00 MiB. GPU 
dl:979817:980021 [0] NCCL INFO [Service thread] Connection closed by localRank 0
dl:979817:981444 [0] NCCL INFO comm 0xa768d40 rank 0 nranks 4 cudaDev 0 busId 2000 - Abort COMPLETE
W1026 03:27:34.863000 123852957493056 torch/multiprocessing/spawn.py:145] Terminating process 979818 via signal SIGTERM
W1026 03:27:34.864000 123852957493056 torch/multiprocessing/spawn.py:145] Terminating process 979819 via signal SIGTERM
W1026 03:27:34.864000 123852957493056 torch/multiprocessing/spawn.py:145] Terminating process 979820 via signal SIGTERM
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1273, in main
    while not ProcessContext(processes, error_files).join():
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 177, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with exit code 1
# Accounting: time=105 threads=1
# Ended (code 1) at Sat Oct 26 03:27:37 EDT 2024, elapsed time 105 seconds

2024-10-26T03:28:21 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 8 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-26T03:28:22 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-26T03:28:22 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-26T03:28:22 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-26T03:28:22 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log'
2024-10-26 03:28:22,972 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-26 03:28:22,988 (launch:237) INFO: single-node with 4gpu on distributed mode
2024-10-26 03:28:23,004 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log
2024-10-26T04:11:34 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-26T04:11:34 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-26T04:11:34 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-26T04:11:34 (asr.sh:1407:main) Generate 'exp/asr_train_asr_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-26T04:11:34 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_onlyctc_raw_en_bpe5000/train.log'
2024-10-26 04:11:36,224 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_onlyctc_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_onlyctc_raw_en_bpe5000 --config conf/train_asr_onlyctc.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-26 04:11:36,239 (launch:237) INFO: single-node with 4gpu on distributed mode
2024-10-26 04:11:36,257 (launch:348) INFO: log file: exp/asr_train_asr_onlyctc_raw_en_bpe5000/train.log
2024-10-26T15:08:42 (asr.sh:1813:main) Successfully finished. [elapsed=39428s]
2024-10-26T15:47:30 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 8 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-26T15:47:30 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-26T15:47:30 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-26T15:47:30 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-26T15:47:30 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log'
2024-10-26 15:47:30,661 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-26 15:47:30,675 (launch:237) INFO: single-node with 4gpu on distributed mode
2024-10-26 15:47:30,678 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log
2024-10-26T16:59:00 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 8 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-26T16:59:00 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-26T16:59:00 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-26T16:59:00 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-26T16:59:00 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log'
2024-10-26 16:59:00,503 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-26 16:59:00,519 (launch:237) INFO: single-node with 4gpu on distributed mode
2024-10-26 16:59:00,519 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log
2024-10-26T16:59:33 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 8 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-26T16:59:33 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-26T16:59:33 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-26T16:59:33 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-26T16:59:33 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log'
2024-10-26 16:59:33,280 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-26 16:59:33,293 (launch:237) INFO: single-node with 4gpu on distributed mode
2024-10-26 16:59:33,294 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log
2024-10-26T16:59:47 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 8 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-26T16:59:47 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-26T16:59:47 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-26T16:59:47 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-26T16:59:47 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log'
2024-10-26 16:59:47,846 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-26 16:59:47,861 (launch:237) INFO: single-node with 4gpu on distributed mode
2024-10-26 16:59:47,862 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log
<<<<<<< HEAD
2024-10-27T17:15:02 (asr.sh:1813:main) Successfully finished. [elapsed=87315s]
2024-10-27T17:29:48 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 8 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-27T17:29:48 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-27T17:29:48 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000
2024-10-27T17:29:48 (asr.sh:1508:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/run.sh'. You can resume the process from stage 12 using this script
2024-10-27T17:29:48 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/asr_inference.*.log'
run.pl: 8 / 8 failed, log is in exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/asr_inference.*.log
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.1.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.1 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:29:48 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.1.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.1 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:29:56,423 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:29:56,578 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:29:56,808 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:29:56,808 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:30:02,643 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:30:13,129 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
=======
2024-10-30T23:11:56 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_onlyctc_finetune.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-30T23:11:57 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-30T23:11:57 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-30T23:11:57 (asr.sh:1407:main) Generate 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-30T23:11:57 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log'
2024-10-30 23:11:59,536 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log' --log exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000 --config conf/train_asr_onlyctc_finetune.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-30 23:11:59,571 (launch:237) INFO: single-node with 2gpu on distributed mode
2024-10-30 23:11:59,572 (launch:348) INFO: log file: exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log
2024-10-31T20:45:13 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_onlyctc_finetune.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-31T20:45:13 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-31T20:45:13 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-31T20:45:13 (asr.sh:1407:main) Generate 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-31T20:45:13 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log'
2024-10-31 20:45:13,632 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log' --log exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000 --config conf/train_asr_onlyctc_finetune.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-31 20:45:13,645 (launch:237) INFO: single-node with 2gpu on distributed mode
2024-10-31 20:45:13,646 (launch:348) INFO: log file: exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log', '--gpu', '2', 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000', '--config', 'conf/train_asr_onlyctc_finetune.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '2', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
>>>>>>> cea3af74175b4f1b718b2c4afeb0a5f028e5698c
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
<<<<<<< HEAD
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 813, in inference
    speech2text = Speech2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 698, in from_pretrained
    return Speech2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 140, in __init__
    asr_model, asr_train_args = task.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
=======
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000 --config conf/train_asr_onlyctc_finetune.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed True 
# Started at Thu Oct 31 20:45:13 PDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000 --config conf/train_asr_onlyctc_finetune.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed True
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[lambda-Lambda-Vector:0/2] 2024-10-31 20:45:25,055 (asr:523) INFO: Vocabulary size: 5000
[lambda-Lambda-Vector:0/2] 2024-10-31 20:45:25,224 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
Process SpawnProcess-2:
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1466, in main_worker
    load_pretrained_model(
  File "/data/mohan/workdir/espnet/espnet2/torch_utils/load_pretrained_model.py", line 99, in load_pretrained_model
    src_state = torch.load(path, map_location=map_location)
>>>>>>> cea3af74175b4f1b718b2c4afeb0a5f028e5698c
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
<<<<<<< HEAD
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth'
# Accounting: time=26 threads=1
# Ended (code 1) at Sun Oct 27 17:30:14 EDT 2024, elapsed time 26 seconds
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.2.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.2 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:29:48 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.2.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.2 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:29:56,354 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:29:56,510 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:29:56,814 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:29:56,815 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:30:02,643 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:30:13,316 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 813, in inference
    speech2text = Speech2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 698, in from_pretrained
    return Speech2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 140, in __init__
    asr_model, asr_train_args = task.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth'
# Accounting: time=26 threads=1
# Ended (code 1) at Sun Oct 27 17:30:14 EDT 2024, elapsed time 26 seconds
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.3.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.3 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:29:48 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.3.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.3 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:29:56,232 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:29:56,398 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:29:56,813 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:29:56,864 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:30:02,643 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:30:13,168 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 813, in inference
    speech2text = Speech2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 698, in from_pretrained
    return Speech2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 140, in __init__
    asr_model, asr_train_args = task.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth'
# Accounting: time=26 threads=1
# Ended (code 1) at Sun Oct 27 17:30:14 EDT 2024, elapsed time 26 seconds
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.4.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.4 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:29:48 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.4.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.4 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:29:56,363 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:29:56,522 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:29:56,814 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:29:56,814 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:30:02,643 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:30:13,167 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 813, in inference
    speech2text = Speech2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 698, in from_pretrained
    return Speech2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 140, in __init__
    asr_model, asr_train_args = task.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth'
# Accounting: time=26 threads=1
# Ended (code 1) at Sun Oct 27 17:30:14 EDT 2024, elapsed time 26 seconds
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.5.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.5 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:29:48 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.5.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.5 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:29:56,820 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:29:56,974 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:29:57,044 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:29:57,044 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:30:02,643 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:30:13,027 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 813, in inference
    speech2text = Speech2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 698, in from_pretrained
    return Speech2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 140, in __init__
    asr_model, asr_train_args = task.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth'
# Accounting: time=26 threads=1
# Ended (code 1) at Sun Oct 27 17:30:14 EDT 2024, elapsed time 26 seconds
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.6.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.6 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:29:48 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.6.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.6 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:29:56,232 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:29:56,401 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:29:56,813 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:29:56,814 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:30:02,643 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:30:13,123 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 813, in inference
    speech2text = Speech2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 698, in from_pretrained
    return Speech2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 140, in __init__
    asr_model, asr_train_args = task.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth'
# Accounting: time=26 threads=1
# Ended (code 1) at Sun Oct 27 17:30:14 EDT 2024, elapsed time 26 seconds
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.7.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.7 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:29:48 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.7.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.7 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:29:56,611 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:29:56,767 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:29:56,835 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:29:56,836 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:30:02,643 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:30:13,029 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 813, in inference
    speech2text = Speech2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 698, in from_pretrained
    return Speech2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 140, in __init__
    asr_model, asr_train_args = task.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth'
# Accounting: time=26 threads=1
# Ended (code 1) at Sun Oct 27 17:30:14 EDT 2024, elapsed time 26 seconds
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.8.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.8 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:29:48 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.8.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.8 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:29:56,811 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:29:56,964 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:29:57,033 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:29:57,034 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:30:02,643 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:30:13,119 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 813, in inference
    speech2text = Speech2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 698, in from_pretrained
    return Speech2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 140, in __init__
    asr_model, asr_train_args = task.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth'
# Accounting: time=26 threads=1
# Ended (code 1) at Sun Oct 27 17:30:14 EDT 2024, elapsed time 26 seconds
2024-10-27T17:30:57 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 8 --inference_asr_model valid.cer_ctc.ave.pth --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-27T17:30:57 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-27T17:30:57 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000
2024-10-27T17:30:57 (asr.sh:1508:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/run.sh'. You can resume the process from stage 12 using this script
2024-10-27T17:30:57 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/asr_inference.*.log'
run.pl: 2 / 8 failed, log is in exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/asr_inference.*.log
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/keys.1.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.cer_ctc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/output.1 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:30:57 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/keys.1.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.cer_ctc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/output.1 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:31:06,011 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:31:06,167 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:31:06,237 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:31:06,238 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:31:07,210 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:31:16,230 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
2024-10-27 17:31:18,748 (asr_inference:372) INFO: BatchBeamSearch implementation is selected.
2024-10-27 17:31:18,748 (asr_inference:383) INFO: Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict()
)
2024-10-27 17:31:18,748 (asr_inference:384) INFO: Decoding device=cpu, dtype=float32
2024-10-27 17:31:18,750 (asr_inference:462) INFO: Text tokenizer: SentencepiecesTokenizer(model="data/en_token_list/bpe_unigram5000/bpe.model")
2024-10-27 17:31:18,752 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
2024-10-27 17:31:20,138 (asr_inference:509) INFO: speech length: 51000
2024-10-27 17:31:22,331 (beam_search:428) INFO: decoder input length: 39
2024-10-27 17:31:22,331 (beam_search:429) INFO: max output length: 39
2024-10-27 17:31:22,331 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:22,381 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:22,381 (beam_search:476) INFO:  -1.07 * 1.0 =  -1.07 for ctc
2024-10-27 17:31:22,381 (beam_search:479) INFO: total log probability: -1.07
2024-10-27 17:31:22,381 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:31:22,381 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:22,381 (beam_search:483) INFO: best hypo: IAMREALLYGOODHOWABOUTYOU

2024-10-27 17:31:22,389 (asr_inference:509) INFO: speech length: 131718
2024-10-27 17:31:27,516 (beam_search:428) INFO: decoder input length: 102
2024-10-27 17:31:27,517 (beam_search:429) INFO: max output length: 102
2024-10-27 17:31:27,517 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:27,595 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:27,596 (beam_search:476) INFO:  -2.51 * 1.0 =  -2.51 for ctc
2024-10-27 17:31:27,596 (beam_search:479) INFO: total log probability: -2.51
2024-10-27 17:31:27,596 (beam_search:480) INFO: normalized log probability: -0.42
2024-10-27 17:31:27,596 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:27,596 (beam_search:483) INFO: best hypo: BEENABOUTANDENERGY

2024-10-27 17:31:27,598 (asr_inference:509) INFO: speech length: 103798
2024-10-27 17:31:31,593 (beam_search:428) INFO: decoder input length: 80
2024-10-27 17:31:31,594 (beam_search:429) INFO: max output length: 80
2024-10-27 17:31:31,594 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:31,738 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:31,739 (beam_search:476) INFO:  -2.80 * 1.0 =  -2.80 for ctc
2024-10-27 17:31:31,739 (beam_search:479) INFO: total log probability: -2.80
2024-10-27 17:31:31,739 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:31:31,739 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:31,739 (beam_search:483) INFO: best hypo: ISADCELLBATTERYWIRESANDALIGHTBULB

2024-10-27 17:31:31,741 (asr_inference:509) INFO: speech length: 141660
2024-10-27 17:31:37,561 (beam_search:428) INFO: decoder input length: 110
2024-10-27 17:31:37,561 (beam_search:429) INFO: max output length: 110
2024-10-27 17:31:37,561 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:37,916 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:37,916 (beam_search:476) INFO:  -1.78 * 1.0 =  -1.78 for ctc
2024-10-27 17:31:37,916 (beam_search:479) INFO: total log probability: -1.78
2024-10-27 17:31:37,916 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:31:37,916 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:37,916 (beam_search:483) INFO: best hypo: SOYOUCANPOWERYOURORLIGHTBULBSOITWILLLIGHTUPORSHOWYOUTHATTHEENERGY

2024-10-27 17:31:37,919 (asr_inference:509) INFO: speech length: 136837
2024-10-27 17:31:42,855 (beam_search:428) INFO: decoder input length: 106
2024-10-27 17:31:42,855 (beam_search:429) INFO: max output length: 106
2024-10-27 17:31:42,855 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:43,142 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:43,142 (beam_search:476) INFO:  -1.43 * 1.0 =  -1.43 for ctc
2024-10-27 17:31:43,142 (beam_search:479) INFO: total log probability: -1.43
2024-10-27 17:31:43,142 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:31:43,142 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:43,142 (beam_search:483) INFO: best hypo: THEYHELPTHEENERGYGOTHROUGHTHEDSOTHEBULBCANACTUALLYLIGHTUP

2024-10-27 17:31:43,144 (asr_inference:509) INFO: speech length: 41378
2024-10-27 17:31:44,777 (beam_search:428) INFO: decoder input length: 31
2024-10-27 17:31:44,778 (beam_search:429) INFO: max output length: 31
2024-10-27 17:31:44,778 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:44,793 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:44,793 (beam_search:476) INFO:  -0.23 * 1.0 =  -0.23 for ctc
2024-10-27 17:31:44,794 (beam_search:479) INFO: total log probability: -0.23
2024-10-27 17:31:44,794 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:31:44,794 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:44,794 (beam_search:483) INFO: best hypo: ITHINK

2024-10-27 17:31:44,796 (asr_inference:509) INFO: speech length: 115167
2024-10-27 17:31:49,021 (beam_search:428) INFO: decoder input length: 89
2024-10-27 17:31:49,021 (beam_search:429) INFO: max output length: 89
2024-10-27 17:31:49,021 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:49,196 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:49,196 (beam_search:476) INFO:  -2.80 * 1.0 =  -2.80 for ctc
2024-10-27 17:31:49,196 (beam_search:479) INFO: total log probability: -2.80
2024-10-27 17:31:49,196 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:31:49,196 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:49,196 (beam_search:483) INFO: best hypo: APATHWAYFORTHEENERGYTOGOSOSOYOUCANTHE

2024-10-27 17:31:49,198 (asr_inference:509) INFO: speech length: 77898
2024-10-27 17:31:52,037 (beam_search:428) INFO: decoder input length: 60
2024-10-27 17:31:52,037 (beam_search:429) INFO: max output length: 60
2024-10-27 17:31:52,037 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:52,066 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:52,066 (beam_search:476) INFO:  -0.82 * 1.0 =  -0.82 for ctc
2024-10-27 17:31:52,066 (beam_search:479) INFO: total log probability: -0.82
2024-10-27 17:31:52,066 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:31:52,066 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:52,066 (beam_search:483) INFO: best hypo: ENERGYAND

2024-10-27 17:31:52,068 (asr_inference:509) INFO: speech length: 139616
2024-10-27 17:31:57,089 (beam_search:428) INFO: decoder input length: 108
2024-10-27 17:31:57,090 (beam_search:429) INFO: max output length: 108
2024-10-27 17:31:57,090 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:57,326 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:57,326 (beam_search:476) INFO:  -2.60 * 1.0 =  -2.60 for ctc
2024-10-27 17:31:57,326 (beam_search:479) INFO: total log probability: -2.60
2024-10-27 17:31:57,326 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:31:57,326 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:57,326 (beam_search:483) INFO: best hypo: LIGHTBULBSARESOYOUCANSEETHATTHEENERGYANDYOUCAN

2024-10-27 17:31:57,328 (asr_inference:509) INFO: speech length: 25707
2024-10-27 17:31:58,463 (beam_search:428) INFO: decoder input length: 19
2024-10-27 17:31:58,463 (beam_search:429) INFO: max output length: 19
2024-10-27 17:31:58,463 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:58,478 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:58,479 (beam_search:476) INFO:  -0.58 * 1.0 =  -0.58 for ctc
2024-10-27 17:31:58,479 (beam_search:479) INFO: total log probability: -0.58
2024-10-27 17:31:58,479 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:31:58,479 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:58,479 (beam_search:483) INFO: best hypo: YOUMOREABOUTWHAT

2024-10-27 17:31:58,481 (asr_inference:509) INFO: speech length: 32263
2024-10-27 17:31:59,809 (beam_search:428) INFO: decoder input length: 24
2024-10-27 17:31:59,809 (beam_search:429) INFO: max output length: 24
2024-10-27 17:31:59,809 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:59,833 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:59,833 (beam_search:476) INFO:  -0.94 * 1.0 =  -0.94 for ctc
2024-10-27 17:31:59,833 (beam_search:479) INFO: total log probability: -0.94
2024-10-27 17:31:59,833 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:31:59,833 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:59,833 (beam_search:483) INFO: best hypo: THEENERGYTOLIGHTUP

2024-10-27 17:31:59,835 (asr_inference:509) INFO: speech length: 30317
2024-10-27 17:32:01,123 (beam_search:428) INFO: decoder input length: 23
2024-10-27 17:32:01,123 (beam_search:429) INFO: max output length: 23
2024-10-27 17:32:01,123 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:01,142 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:01,142 (beam_search:476) INFO:  -0.44 * 1.0 =  -0.44 for ctc
2024-10-27 17:32:01,142 (beam_search:479) INFO: total log probability: -0.44
2024-10-27 17:32:01,142 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:32:01,142 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:01,142 (beam_search:483) INFO: best hypo: ISEEACIRCUIT

2024-10-27 17:32:01,144 (asr_inference:509) INFO: speech length: 245972
2024-10-27 17:32:11,417 (beam_search:428) INFO: decoder input length: 191
2024-10-27 17:32:11,417 (beam_search:429) INFO: max output length: 191
2024-10-27 17:32:11,417 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:12,199 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:12,199 (beam_search:476) INFO:  -5.38 * 1.0 =  -5.38 for ctc
2024-10-27 17:32:12,199 (beam_search:479) INFO: total log probability: -5.38
2024-10-27 17:32:12,199 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:32:12,199 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:12,199 (beam_search:483) INFO: best hypo: ISTHROUGHTHEPOSITIVEANDGOINGOUTTHESIDEINTOTHEBULBBUTISOUTTHEBULBTOTHEPOSITIVEITACIRCUIT

2024-10-27 17:32:12,202 (asr_inference:509) INFO: speech length: 244624
2024-10-27 17:32:22,481 (beam_search:428) INFO: decoder input length: 190
2024-10-27 17:32:22,481 (beam_search:429) INFO: max output length: 190
2024-10-27 17:32:22,481 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:23,570 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:23,570 (beam_search:476) INFO:  -6.65 * 1.0 =  -6.65 for ctc
2024-10-27 17:32:23,570 (beam_search:479) INFO: total log probability: -6.65
2024-10-27 17:32:23,570 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:32:23,570 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:23,570 (beam_search:483) INFO: best hypo: ITBYTHROUGHTHEWIRESGOINGTOTHETHEBULBANDTHEREISANOTHERONECONNECTEDTOTHEBOTTOMOFTHEBULBOUTTOTHEPOSITIVETHEBULBHAVEACIRCUITITUP

2024-10-27 17:32:23,573 (asr_inference:509) INFO: speech length: 321835
2024-10-27 17:32:37,175 (beam_search:428) INFO: decoder input length: 250
2024-10-27 17:32:37,175 (beam_search:429) INFO: max output length: 250
2024-10-27 17:32:37,175 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:37,510 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:37,510 (beam_search:476) INFO:  -1.58 * 1.0 =  -1.58 for ctc
2024-10-27 17:32:37,510 (beam_search:479) INFO: total log probability: -1.58
2024-10-27 17:32:37,510 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:32:37,510 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:37,510 (beam_search:483) INFO: best hypo: I'MTALKINGONMESOME

2024-10-27 17:32:37,513 (asr_inference:509) INFO: speech length: 50375
2024-10-27 17:32:39,364 (beam_search:428) INFO: decoder input length: 38
2024-10-27 17:32:39,364 (beam_search:429) INFO: max output length: 38
2024-10-27 17:32:39,364 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:39,392 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:39,392 (beam_search:476) INFO:  -1.00 * 1.0 =  -1.00 for ctc
2024-10-27 17:32:39,392 (beam_search:479) INFO: total log probability: -1.00
2024-10-27 17:32:39,392 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:32:39,392 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:39,392 (beam_search:483) INFO: best hypo: ITHATITLIKE

2024-10-27 17:32:39,394 (asr_inference:509) INFO: speech length: 61737
2024-10-27 17:32:41,671 (beam_search:428) INFO: decoder input length: 47
2024-10-27 17:32:41,671 (beam_search:429) INFO: max output length: 47
2024-10-27 17:32:41,671 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:41,688 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:41,688 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 17:32:41,688 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 17:32:41,688 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:32:41,688 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:41,688 (beam_search:483) INFO: best hypo: THE

2024-10-27 17:32:41,691 (asr_inference:509) INFO: speech length: 65092
2024-10-27 17:32:44,136 (beam_search:428) INFO: decoder input length: 50
2024-10-27 17:32:44,136 (beam_search:429) INFO: max output length: 50
2024-10-27 17:32:44,136 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:44,219 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:44,219 (beam_search:476) INFO:  -1.39 * 1.0 =  -1.39 for ctc
2024-10-27 17:32:44,219 (beam_search:479) INFO: total log probability: -1.39
2024-10-27 17:32:44,219 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:32:44,219 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:44,219 (beam_search:483) INFO: best hypo: ITHATIT'STHELIGHTBULBUP

2024-10-27 17:32:44,221 (asr_inference:509) INFO: speech length: 125325
2024-10-27 17:32:49,066 (beam_search:428) INFO: decoder input length: 97
2024-10-27 17:32:49,066 (beam_search:429) INFO: max output length: 97
2024-10-27 17:32:49,066 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:49,303 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:49,303 (beam_search:476) INFO:  -3.76 * 1.0 =  -3.76 for ctc
2024-10-27 17:32:49,303 (beam_search:479) INFO: total log probability: -3.76
2024-10-27 17:32:49,303 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:32:49,303 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:49,304 (beam_search:483) INFO: best hypo: BECONNECTEDTOMAKETHEENERGYTHROUGHTHEWIRESSOTHESOTHELIGHTCAN

2024-10-27 17:32:49,305 (asr_inference:509) INFO: speech length: 230094
2024-10-27 17:32:58,367 (beam_search:428) INFO: decoder input length: 179
2024-10-27 17:32:58,367 (beam_search:429) INFO: max output length: 179
2024-10-27 17:32:58,367 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:59,603 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:59,603 (beam_search:476) INFO:  -5.67 * 1.0 =  -5.67 for ctc
2024-10-27 17:32:59,603 (beam_search:479) INFO: total log probability: -5.67
2024-10-27 17:32:59,603 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:32:59,603 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:59,603 (beam_search:483) INFO: best hypo: THATITWILLNOTLIGHTUPBECAUSETHERENEEDSONETOBEATTHEOFTHEBULBFORTHEENERGYTOOUTIT'SATTHEBOTTOMTHENTHEREISONLYJUSTONEBULBTOITOR

2024-10-27 17:32:59,606 (asr_inference:509) INFO: speech length: 35675
2024-10-27 17:33:00,994 (beam_search:428) INFO: decoder input length: 27
2024-10-27 17:33:00,994 (beam_search:429) INFO: max output length: 27
2024-10-27 17:33:00,994 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:01,032 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:01,032 (beam_search:476) INFO:  -1.38 * 1.0 =  -1.38 for ctc
2024-10-27 17:33:01,032 (beam_search:479) INFO: total log probability: -1.38
2024-10-27 17:33:01,032 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:33:01,033 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:01,033 (beam_search:483) INFO: best hypo: INOTICEDTHATIT'SLIGHTINGUPOF

2024-10-27 17:33:01,035 (asr_inference:509) INFO: speech length: 124270
2024-10-27 17:33:05,522 (beam_search:428) INFO: decoder input length: 96
2024-10-27 17:33:05,522 (beam_search:429) INFO: max output length: 96
2024-10-27 17:33:05,522 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:05,751 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:05,751 (beam_search:476) INFO:  -2.05 * 1.0 =  -2.05 for ctc
2024-10-27 17:33:05,751 (beam_search:479) INFO: total log probability: -2.05
2024-10-27 17:33:05,751 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:33:05,751 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:05,751 (beam_search:483) INFO: best hypo: WE'VEBEENANDHOWTHEYANDTOMAKEHOWTOMAKEA

2024-10-27 17:33:05,753 (asr_inference:509) INFO: speech length: 174326
2024-10-27 17:33:12,459 (beam_search:428) INFO: decoder input length: 135
2024-10-27 17:33:12,459 (beam_search:429) INFO: max output length: 135
2024-10-27 17:33:12,460 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:12,905 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:12,905 (beam_search:476) INFO:  -3.92 * 1.0 =  -3.92 for ctc
2024-10-27 17:33:12,905 (beam_search:479) INFO: total log probability: -3.92
2024-10-27 17:33:12,905 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:33:12,905 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:12,906 (beam_search:483) INFO: best hypo: WIRESCONNECTINGTOTHEDGOINGTOTHEMOTORTHEMOTORRUNANDITISGOINGTOTHROUGHA

2024-10-27 17:33:12,908 (asr_inference:509) INFO: speech length: 182507
2024-10-27 17:33:19,808 (beam_search:428) INFO: decoder input length: 142
2024-10-27 17:33:19,808 (beam_search:429) INFO: max output length: 142
2024-10-27 17:33:19,808 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:20,318 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:20,318 (beam_search:476) INFO:  -3.20 * 1.0 =  -3.20 for ctc
2024-10-27 17:33:20,318 (beam_search:479) INFO: total log probability: -3.20
2024-10-27 17:33:20,318 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:33:20,318 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:20,318 (beam_search:483) INFO: best hypo: THEISANDITISIMPORTANTANDITISIMPORTANTBECAUSETHENYOUKNOWTHEENERGYISACTUALLYGOINGTHROUGHOR

2024-10-27 17:33:20,321 (asr_inference:509) INFO: speech length: 87561
2024-10-27 17:33:23,439 (beam_search:428) INFO: decoder input length: 67
2024-10-27 17:33:23,439 (beam_search:429) INFO: max output length: 67
2024-10-27 17:33:23,439 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:23,551 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:23,551 (beam_search:476) INFO:  -2.86 * 1.0 =  -2.86 for ctc
2024-10-27 17:33:23,551 (beam_search:479) INFO: total log probability: -2.86
2024-10-27 17:33:23,551 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:33:23,551 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:23,551 (beam_search:483) INFO: best hypo: THEISGOINGTHROUGHACIRCUITGOINGTOTHEIT

2024-10-27 17:33:23,553 (asr_inference:509) INFO: speech length: 248676
2024-10-27 17:33:34,047 (beam_search:428) INFO: decoder input length: 193
2024-10-27 17:33:34,047 (beam_search:429) INFO: max output length: 193
2024-10-27 17:33:34,047 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:34,687 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:34,688 (beam_search:476) INFO:  -4.84 * 1.0 =  -4.84 for ctc
2024-10-27 17:33:34,688 (beam_search:479) INFO: total log probability: -4.84
2024-10-27 17:33:34,688 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:33:34,688 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:34,688 (beam_search:483) INFO: best hypo: WHATISENERGYGOTHROUGHTHEWIRESGOINGTHROUGHACIRCUITGOINGTOTHEMOTORITTHEGOTO

2024-10-27 17:33:34,690 (asr_inference:509) INFO: speech length: 163727
2024-10-27 17:33:40,809 (beam_search:428) INFO: decoder input length: 127
2024-10-27 17:33:40,810 (beam_search:429) INFO: max output length: 127
2024-10-27 17:33:40,810 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:41,150 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:41,150 (beam_search:476) INFO:  -3.57 * 1.0 =  -3.57 for ctc
2024-10-27 17:33:41,151 (beam_search:479) INFO: total log probability: -3.57
2024-10-27 17:33:41,151 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:33:41,151 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:41,151 (beam_search:483) INFO: best hypo: THISISABOUTTHESOITYOUCANTURNITOFFANDONMOVINGTHE

2024-10-27 17:33:41,153 (asr_inference:509) INFO: speech length: 16962
2024-10-27 17:33:41,919 (beam_search:428) INFO: decoder input length: 12
2024-10-27 17:33:41,919 (beam_search:429) INFO: max output length: 12
2024-10-27 17:33:41,919 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:41,928 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:41,928 (beam_search:476) INFO:  -0.91 * 1.0 =  -0.91 for ctc
2024-10-27 17:33:41,928 (beam_search:479) INFO: total log probability: -0.91
2024-10-27 17:33:41,928 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:33:41,928 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:41,928 (beam_search:483) INFO: best hypo: THEIS

2024-10-27 17:33:41,931 (asr_inference:509) INFO: speech length: 333295
2024-10-27 17:33:57,134 (beam_search:428) INFO: decoder input length: 259
2024-10-27 17:33:57,134 (beam_search:429) INFO: max output length: 259
2024-10-27 17:33:57,134 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:59,292 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:59,292 (beam_search:476) INFO:  -7.45 * 1.0 =  -7.45 for ctc
2024-10-27 17:33:59,292 (beam_search:479) INFO: total log probability: -7.45
2024-10-27 17:33:59,292 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:33:59,292 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:59,292 (beam_search:483) INFO: best hypo: THEENERGYWASN'TTOTHEOTHERWIREITGOINGITNOTGOINGTOAITNOTMOVEORDOANYTHINGLIKETHATBUTNOWTHATTHEWIRETHATTHESWITCHHASBEENCONNECTEDITISGOINGAGAINBECAUSEITISACIRCUITNOW

2024-10-27 17:33:59,295 (asr_inference:509) INFO: speech length: 69826
2024-10-27 17:34:01,766 (beam_search:428) INFO: decoder input length: 54
2024-10-27 17:34:01,766 (beam_search:429) INFO: max output length: 54
2024-10-27 17:34:01,766 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:01,819 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:01,819 (beam_search:476) INFO:  -0.66 * 1.0 =  -0.66 for ctc
2024-10-27 17:34:01,819 (beam_search:479) INFO: total log probability: -0.66
2024-10-27 17:34:01,819 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:34:01,819 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:01,819 (beam_search:483) INFO: best hypo: CAUSEITISNOTACIRCUIT

2024-10-27 17:34:01,822 (asr_inference:509) INFO: speech length: 420308
2024-10-27 17:34:21,780 (beam_search:428) INFO: decoder input length: 327
2024-10-27 17:34:21,780 (beam_search:429) INFO: max output length: 327
2024-10-27 17:34:21,780 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:25,475 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:25,475 (beam_search:476) INFO: -11.51 * 1.0 = -11.51 for ctc
2024-10-27 17:34:25,475 (beam_search:479) INFO: total log probability: -11.51
2024-10-27 17:34:25,475 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:34:25,475 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:25,475 (beam_search:483) INFO: best hypo: ACIRCUITSOYOUHAVETOTHESWITCHBACKBECAUSETHEENERGYHASCONNECTEDTOTHATTHINGTHATTHEWIRECONNECTEDTOWITHTHEANDTHESWITCHHASTOCONNECTTOTHEOTHERORMETALSOTHEENERGYISCONNECTEDTOTHEMETALSOITWILLGOTHROUGHTOMAKEACIRCUITGOINGUPTHEWHATYOUOR

2024-10-27 17:34:25,478 (asr_inference:509) INFO: speech length: 233892
2024-10-27 17:34:35,068 (beam_search:428) INFO: decoder input length: 182
2024-10-27 17:34:35,068 (beam_search:429) INFO: max output length: 182
2024-10-27 17:34:35,068 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:35,756 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:35,756 (beam_search:476) INFO:  -4.91 * 1.0 =  -4.91 for ctc
2024-10-27 17:34:35,756 (beam_search:479) INFO: total log probability: -4.91
2024-10-27 17:34:35,756 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:34:35,756 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:35,756 (beam_search:483) INFO: best hypo: ITHATWHENTHETOTHATOTHERMETALCONNECTEDTOTHEWIREITBUTISNOTANDITISITNOTGO

2024-10-27 17:34:35,759 (asr_inference:509) INFO: speech length: 210514
2024-10-27 17:34:44,361 (beam_search:428) INFO: decoder input length: 163
2024-10-27 17:34:44,361 (beam_search:429) INFO: max output length: 163
2024-10-27 17:34:44,361 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:45,079 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:45,080 (beam_search:476) INFO:  -3.92 * 1.0 =  -3.92 for ctc
2024-10-27 17:34:45,080 (beam_search:479) INFO: total log probability: -3.92
2024-10-27 17:34:45,080 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:34:45,080 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:45,080 (beam_search:483) INFO: best hypo: WHENTHESWITCHISOPENITNOTITNOTTHEENERGYDOESNOTFLOWTHROUGHTOTHEMOTORITNOTCONNECTANDNOTMAKINGITGO

2024-10-27 17:34:45,082 (asr_inference:509) INFO: speech length: 268425
2024-10-27 17:34:56,176 (beam_search:428) INFO: decoder input length: 209
2024-10-27 17:34:56,176 (beam_search:429) INFO: max output length: 209
2024-10-27 17:34:56,176 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:57,714 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:57,714 (beam_search:476) INFO:  -9.49 * 1.0 =  -9.49 for ctc
2024-10-27 17:34:57,714 (beam_search:479) INFO: total log probability: -9.49
2024-10-27 17:34:57,714 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:34:57,714 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:57,714 (beam_search:483) INFO: best hypo: ABOUTIFYOUDON'TAYOUCANGOOUTINTHEANDTHENITWILLANDTHENITWILLGETENERGYFROMTHESUNIFITISCONNECTEDTOTHETHEENERGYWILLTHROUGHTHEWIRETOYOUTOIT

2024-10-27 17:34:57,717 (asr_inference:509) INFO: speech length: 238996
2024-10-27 17:35:07,391 (beam_search:428) INFO: decoder input length: 186
2024-10-27 17:35:07,391 (beam_search:429) INFO: max output length: 186
2024-10-27 17:35:07,391 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:08,493 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:08,493 (beam_search:476) INFO:  -5.87 * 1.0 =  -5.87 for ctc
2024-10-27 17:35:08,493 (beam_search:479) INFO: total log probability: -5.87
2024-10-27 17:35:08,493 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:35:08,493 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:08,493 (beam_search:483) INFO: best hypo: ISISTHESUNMAKINGTHESENERGYCONNECTTHROUGHCONNECTTHROUGHTOTHETOTHELARTOTHESOLARCELLTHEENERGYGOTHROUGHTHEWIRESTOTHEITGO

2024-10-27 17:35:08,496 (asr_inference:509) INFO: speech length: 64137
2024-10-27 17:35:10,813 (beam_search:428) INFO: decoder input length: 49
2024-10-27 17:35:10,813 (beam_search:429) INFO: max output length: 49
2024-10-27 17:35:10,813 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:10,900 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:10,900 (beam_search:476) INFO:  -1.06 * 1.0 =  -1.06 for ctc
2024-10-27 17:35:10,900 (beam_search:479) INFO: total log probability: -1.06
2024-10-27 17:35:10,900 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:35:10,900 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:10,900 (beam_search:483) INFO: best hypo: THESOLARCELLHASTOBECONNECTEDTOWIRESTHE

2024-10-27 17:35:10,902 (asr_inference:509) INFO: speech length: 130569
2024-10-27 17:35:15,656 (beam_search:428) INFO: decoder input length: 101
2024-10-27 17:35:15,656 (beam_search:429) INFO: max output length: 101
2024-10-27 17:35:15,656 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:15,988 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:15,988 (beam_search:476) INFO:  -2.38 * 1.0 =  -2.38 for ctc
2024-10-27 17:35:15,988 (beam_search:479) INFO: total log probability: -2.38
2024-10-27 17:35:15,988 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:35:15,988 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:15,988 (beam_search:483) INFO: best hypo: THESOLARCELLISWITHTHESUNSENERGYINGITGOTHROUGHTHEWIRESTOTHEMOTOR

2024-10-27 17:35:15,990 (asr_inference:509) INFO: speech length: 74839
2024-10-27 17:35:18,697 (beam_search:428) INFO: decoder input length: 57
2024-10-27 17:35:18,698 (beam_search:429) INFO: max output length: 57
2024-10-27 17:35:18,698 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:18,840 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:18,841 (beam_search:476) INFO:  -1.97 * 1.0 =  -1.97 for ctc
2024-10-27 17:35:18,841 (beam_search:479) INFO: total log probability: -1.97
2024-10-27 17:35:18,841 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:35:18,841 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:18,841 (beam_search:483) INFO: best hypo: THEIMPORTANTABOUTTHATISTHATTHEENERGYISGOINGTHROUGHTHEWIRESTOTHE

2024-10-27 17:35:18,843 (asr_inference:509) INFO: speech length: 74632
2024-10-27 17:35:21,507 (beam_search:428) INFO: decoder input length: 57
2024-10-27 17:35:21,507 (beam_search:429) INFO: max output length: 57
2024-10-27 17:35:21,507 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:21,619 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:21,619 (beam_search:476) INFO:  -2.42 * 1.0 =  -2.42 for ctc
2024-10-27 17:35:21,619 (beam_search:479) INFO: total log probability: -2.42
2024-10-27 17:35:21,619 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:35:21,619 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:21,619 (beam_search:483) INFO: best hypo: THESOLARCELLWOULDNOTENERGYFROMTHEANDITWOULD

2024-10-27 17:35:21,622 (asr_inference:509) INFO: speech length: 141077
2024-10-27 17:35:27,215 (beam_search:428) INFO: decoder input length: 109
2024-10-27 17:35:27,215 (beam_search:429) INFO: max output length: 109
2024-10-27 17:35:27,215 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:27,574 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:27,574 (beam_search:476) INFO:  -2.27 * 1.0 =  -2.27 for ctc
2024-10-27 17:35:27,574 (beam_search:479) INFO: total log probability: -2.27
2024-10-27 17:35:27,574 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:35:27,574 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:27,574 (beam_search:483) INFO: best hypo: IFTHESUNISOUTTHENTHEREISENERGYFLOWINGFROMTHETOTHESOLARCELLITLIGHTUP

2024-10-27 17:35:27,576 (asr_inference:509) INFO: speech length: 20742
2024-10-27 17:35:28,482 (beam_search:428) INFO: decoder input length: 15
2024-10-27 17:35:28,482 (beam_search:429) INFO: max output length: 15
2024-10-27 17:35:28,482 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:28,491 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:28,491 (beam_search:476) INFO:  -0.50 * 1.0 =  -0.50 for ctc
2024-10-27 17:35:28,491 (beam_search:479) INFO: total log probability: -0.50
2024-10-27 17:35:28,491 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:35:28,491 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:28,491 (beam_search:483) INFO: best hypo: ITIS

2024-10-27 17:35:28,493 (asr_inference:509) INFO: speech length: 191161
2024-10-27 17:35:35,746 (beam_search:428) INFO: decoder input length: 148
2024-10-27 17:35:35,746 (beam_search:429) INFO: max output length: 148
2024-10-27 17:35:35,746 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:36,177 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:36,177 (beam_search:476) INFO:  -5.22 * 1.0 =  -5.22 for ctc
2024-10-27 17:35:36,177 (beam_search:479) INFO: total log probability: -5.22
2024-10-27 17:35:36,177 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:35:36,177 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:36,177 (beam_search:483) INFO: best hypo: THATTHEENERGYGOTHROUGHTHEWIRESMAKINGITGOTOTHEMOTORITHAVEENERGYITGO

2024-10-27 17:35:36,179 (asr_inference:509) INFO: speech length: 40751
2024-10-27 17:35:37,758 (beam_search:428) INFO: decoder input length: 31
2024-10-27 17:35:37,759 (beam_search:429) INFO: max output length: 31
2024-10-27 17:35:37,759 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:37,792 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:37,793 (beam_search:476) INFO:  -2.19 * 1.0 =  -2.19 for ctc
2024-10-27 17:35:37,793 (beam_search:479) INFO: total log probability: -2.19
2024-10-27 17:35:37,793 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 17:35:37,793 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:37,793 (beam_search:483) INFO: best hypo: ITHATITISNOTGOING

2024-10-27 17:35:37,795 (asr_inference:509) INFO: speech length: 58003
2024-10-27 17:35:40,139 (beam_search:428) INFO: decoder input length: 44
2024-10-27 17:35:40,139 (beam_search:429) INFO: max output length: 44
2024-10-27 17:35:40,139 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:40,198 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:40,198 (beam_search:476) INFO:  -1.31 * 1.0 =  -1.31 for ctc
2024-10-27 17:35:40,198 (beam_search:479) INFO: total log probability: -1.31
2024-10-27 17:35:40,198 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:35:40,198 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:40,198 (beam_search:483) INFO: best hypo: WHATIS'TINTHEISENERGY

2024-10-27 17:35:40,201 (asr_inference:509) INFO: speech length: 258863
2024-10-27 17:35:50,812 (beam_search:428) INFO: decoder input length: 201
2024-10-27 17:35:50,812 (beam_search:429) INFO: max output length: 201
2024-10-27 17:35:50,812 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:51,694 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:51,694 (beam_search:476) INFO:  -5.94 * 1.0 =  -5.94 for ctc
2024-10-27 17:35:51,694 (beam_search:479) INFO: total log probability: -5.94
2024-10-27 17:35:51,694 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:35:51,694 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:51,694 (beam_search:483) INFO: best hypo: THATTHATTHEENERGYISGOINGTHROUGHTHEWIRESACIRCUITTHETHEENERGYTOTHEWIREITGOINGANDSYOUAWIRE

2024-10-27 17:35:51,697 (asr_inference:509) INFO: speech length: 29000
2024-10-27 17:35:53,012 (beam_search:428) INFO: decoder input length: 22
2024-10-27 17:35:53,012 (beam_search:429) INFO: max output length: 22
2024-10-27 17:35:53,012 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:53,027 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:53,027 (beam_search:476) INFO:  -0.61 * 1.0 =  -0.61 for ctc
2024-10-27 17:35:53,027 (beam_search:479) INFO: total log probability: -0.61
2024-10-27 17:35:53,027 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:35:53,027 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:53,027 (beam_search:483) INFO: best hypo: HOWAREYOU

2024-10-27 17:35:53,030 (asr_inference:509) INFO: speech length: 209556
2024-10-27 17:36:01,242 (beam_search:428) INFO: decoder input length: 163
2024-10-27 17:36:01,242 (beam_search:429) INFO: max output length: 163
2024-10-27 17:36:01,242 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:01,575 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:01,575 (beam_search:476) INFO:  -2.98 * 1.0 =  -2.98 for ctc
2024-10-27 17:36:01,575 (beam_search:479) INFO: total log probability: -2.98
2024-10-27 17:36:01,575 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:36:01,575 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:01,575 (beam_search:483) INFO: best hypo: BEENABOUTOTHERMETALTHATHAVETHATCANENERGYTOENERGY

2024-10-27 17:36:01,578 (asr_inference:509) INFO: speech length: 460409
2024-10-27 17:36:24,067 (beam_search:428) INFO: decoder input length: 359
2024-10-27 17:36:24,068 (beam_search:429) INFO: max output length: 359
2024-10-27 17:36:24,068 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:28,088 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:28,088 (beam_search:476) INFO:  -6.18 * 1.0 =  -6.18 for ctc
2024-10-27 17:36:28,088 (beam_search:479) INFO: total log probability: -6.18
2024-10-27 17:36:28,088 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:36:28,088 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:28,088 (beam_search:483) INFO: best hypo: THEDCELLBATTERYHASAWIRETHROUGHTHESIDEGOINGTOTHETHEISOPENBUTTHENAILISANDITSMETALSOTHEENERGYCANFLOWTHROUGHGOINGTOTHEOTHERGOINGTOTHEOTHERWIRETOTHEMOTORITGOANDTHENTHESIDEISJUSTTHEWIREGOINGTOTHEMOTOR

2024-10-27 17:36:28,091 (asr_inference:509) INFO: speech length: 44009
2024-10-27 17:36:29,708 (beam_search:428) INFO: decoder input length: 33
2024-10-27 17:36:29,708 (beam_search:429) INFO: max output length: 33
2024-10-27 17:36:29,708 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:29,721 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:29,721 (beam_search:476) INFO:  -1.59 * 1.0 =  -1.59 for ctc
2024-10-27 17:36:29,721 (beam_search:479) INFO: total log probability: -1.59
2024-10-27 17:36:29,721 (beam_search:480) INFO: normalized log probability: -0.53
2024-10-27 17:36:29,721 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:29,721 (beam_search:483) INFO: best hypo: OR

2024-10-27 17:36:29,724 (asr_inference:509) INFO: speech length: 241360
2024-10-27 17:36:39,273 (beam_search:428) INFO: decoder input length: 188
2024-10-27 17:36:39,273 (beam_search:429) INFO: max output length: 188
2024-10-27 17:36:39,273 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:40,375 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:40,375 (beam_search:476) INFO:  -5.38 * 1.0 =  -5.38 for ctc
2024-10-27 17:36:40,375 (beam_search:479) INFO: total log probability: -5.38
2024-10-27 17:36:40,375 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:36:40,375 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:40,376 (beam_search:483) INFO: best hypo: THATTHEPARTSOFTHECIRCUITARETHETHATISTHATISENERGYISFLOWINGBACKTHROUGHTHEDLIKEITISUMTHEENERGYMAKINGITTHROUGHTHEDCELL

2024-10-27 17:36:40,378 (asr_inference:509) INFO: speech length: 55213
2024-10-27 17:36:42,402 (beam_search:428) INFO: decoder input length: 42
2024-10-27 17:36:42,402 (beam_search:429) INFO: max output length: 42
2024-10-27 17:36:42,402 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:42,452 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:42,452 (beam_search:476) INFO:  -1.30 * 1.0 =  -1.30 for ctc
2024-10-27 17:36:42,452 (beam_search:479) INFO: total log probability: -1.30
2024-10-27 17:36:42,452 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:36:42,452 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:42,452 (beam_search:483) INFO: best hypo: THEANDTHEANDALLTHATSTUFF

2024-10-27 17:36:42,454 (asr_inference:509) INFO: speech length: 186686
2024-10-27 17:36:49,540 (beam_search:428) INFO: decoder input length: 145
2024-10-27 17:36:49,540 (beam_search:429) INFO: max output length: 145
2024-10-27 17:36:49,540 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:49,959 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:49,959 (beam_search:476) INFO:  -2.90 * 1.0 =  -2.90 for ctc
2024-10-27 17:36:49,959 (beam_search:479) INFO: total log probability: -2.90
2024-10-27 17:36:49,959 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:36:49,959 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:49,959 (beam_search:483) INFO: best hypo: ITHATTHEISANDTHATMETALTHEENERGYISFLOWINGTHROUGHANDGOINGTOTHE

2024-10-27 17:36:49,961 (asr_inference:509) INFO: speech length: 69977
2024-10-27 17:36:52,387 (beam_search:428) INFO: decoder input length: 54
2024-10-27 17:36:52,387 (beam_search:429) INFO: max output length: 54
2024-10-27 17:36:52,387 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:52,470 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:52,471 (beam_search:476) INFO:  -0.98 * 1.0 =  -0.98 for ctc
2024-10-27 17:36:52,471 (beam_search:479) INFO: total log probability: -0.98
2024-10-27 17:36:52,471 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:36:52,471 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:52,471 (beam_search:483) INFO: best hypo: THEENERGYISGOINGTHROUGHTHEBECAUSETHEIS

2024-10-27 17:36:52,473 (asr_inference:509) INFO: speech length: 188346
2024-10-27 17:36:59,763 (beam_search:428) INFO: decoder input length: 146
2024-10-27 17:36:59,763 (beam_search:429) INFO: max output length: 146
2024-10-27 17:36:59,763 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:00,299 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:00,300 (beam_search:476) INFO:  -3.71 * 1.0 =  -3.71 for ctc
2024-10-27 17:37:00,300 (beam_search:479) INFO: total log probability: -3.71
2024-10-27 17:37:00,300 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:37:00,300 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:00,300 (beam_search:483) INFO: best hypo: THATTHESTICKISNOTMETALANDITSOTHEENERGYCANTTHROUGHTHATSOTHEISNOTRUNNING

2024-10-27 17:37:00,303 (asr_inference:509) INFO: speech length: 141012
2024-10-27 17:37:05,353 (beam_search:428) INFO: decoder input length: 109
2024-10-27 17:37:05,354 (beam_search:429) INFO: max output length: 109
2024-10-27 17:37:05,354 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:05,535 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:05,536 (beam_search:476) INFO:  -2.50 * 1.0 =  -2.50 for ctc
2024-10-27 17:37:05,536 (beam_search:479) INFO: total log probability: -2.50
2024-10-27 17:37:05,536 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:37:05,536 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:05,536 (beam_search:483) INFO: best hypo: ISNOTBECAUSETHETHESTHEISNOT

2024-10-27 17:37:05,538 (asr_inference:509) INFO: speech length: 113339
2024-10-27 17:37:09,630 (beam_search:428) INFO: decoder input length: 88
2024-10-27 17:37:09,630 (beam_search:429) INFO: max output length: 88
2024-10-27 17:37:09,630 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:09,843 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:09,844 (beam_search:476) INFO:  -4.37 * 1.0 =  -4.37 for ctc
2024-10-27 17:37:09,844 (beam_search:479) INFO: total log probability: -4.37
2024-10-27 17:37:09,844 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:37:09,844 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:09,844 (beam_search:483) INFO: best hypo: ITHASTODOWITHBECAUSEITISNOTMETALNOTTHROUGHITWEJUST

2024-10-27 17:37:09,846 (asr_inference:509) INFO: speech length: 84810
2024-10-27 17:37:12,908 (beam_search:428) INFO: decoder input length: 65
2024-10-27 17:37:12,908 (beam_search:429) INFO: max output length: 65
2024-10-27 17:37:12,908 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:13,010 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:13,010 (beam_search:476) INFO:  -2.07 * 1.0 =  -2.07 for ctc
2024-10-27 17:37:13,010 (beam_search:479) INFO: total log probability: -2.07
2024-10-27 17:37:13,010 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:37:13,010 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:13,010 (beam_search:483) INFO: best hypo: BECAUSEIFITISMETALTHENITNEEDSTO

2024-10-27 17:37:13,012 (asr_inference:509) INFO: speech length: 67458
2024-10-27 17:37:15,392 (beam_search:428) INFO: decoder input length: 52
2024-10-27 17:37:15,392 (beam_search:429) INFO: max output length: 52
2024-10-27 17:37:15,392 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:15,457 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:15,458 (beam_search:476) INFO:  -1.39 * 1.0 =  -1.39 for ctc
2024-10-27 17:37:15,458 (beam_search:479) INFO: total log probability: -1.39
2024-10-27 17:37:15,458 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:37:15,458 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:15,458 (beam_search:483) INFO: best hypo: DIFFERENTBECAUSETHEYARETMADEOF

2024-10-27 17:37:15,460 (asr_inference:509) INFO: speech length: 76733
2024-10-27 17:37:18,336 (beam_search:428) INFO: decoder input length: 59
2024-10-27 17:37:18,336 (beam_search:429) INFO: max output length: 59
2024-10-27 17:37:18,336 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:18,483 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:18,484 (beam_search:476) INFO:  -1.99 * 1.0 =  -1.99 for ctc
2024-10-27 17:37:18,484 (beam_search:479) INFO: total log probability: -1.99
2024-10-27 17:37:18,484 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:37:18,484 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:18,484 (beam_search:483) INFO: best hypo: THEYAREBECAUSETHEY'RENOTMADEOUTOFMETALANDENERGYCANNOTTHROUGH

2024-10-27 17:37:18,486 (asr_inference:509) INFO: speech length: 72739
2024-10-27 17:37:21,079 (beam_search:428) INFO: decoder input length: 56
2024-10-27 17:37:21,079 (beam_search:429) INFO: max output length: 56
2024-10-27 17:37:21,079 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:21,154 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:21,154 (beam_search:476) INFO:  -2.48 * 1.0 =  -2.48 for ctc
2024-10-27 17:37:21,154 (beam_search:479) INFO: total log probability: -2.48
2024-10-27 17:37:21,154 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:37:21,154 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:21,155 (beam_search:483) INFO: best hypo: BECAUSETHEY'RENOTOUTOFMETAL

2024-10-27 17:37:21,157 (asr_inference:509) INFO: speech length: 99645
2024-10-27 17:37:24,785 (beam_search:428) INFO: decoder input length: 77
2024-10-27 17:37:24,785 (beam_search:429) INFO: max output length: 77
2024-10-27 17:37:24,785 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:24,973 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:24,973 (beam_search:476) INFO:  -2.17 * 1.0 =  -2.17 for ctc
2024-10-27 17:37:24,973 (beam_search:479) INFO: total log probability: -2.17
2024-10-27 17:37:24,973 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:37:24,973 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:24,973 (beam_search:483) INFO: best hypo: THEYAREBECAUSETHEY'REMADEOUTOFMETALANDENERGYCANMETAL

2024-10-27 17:37:24,976 (asr_inference:509) INFO: speech length: 30907
2024-10-27 17:37:26,279 (beam_search:428) INFO: decoder input length: 23
2024-10-27 17:37:26,279 (beam_search:429) INFO: max output length: 23
2024-10-27 17:37:26,279 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:26,295 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:26,295 (beam_search:476) INFO:  -0.87 * 1.0 =  -0.87 for ctc
2024-10-27 17:37:26,295 (beam_search:479) INFO: total log probability: -0.87
2024-10-27 17:37:26,295 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:37:26,296 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:26,296 (beam_search:483) INFO: best hypo: THEYAREALL

2024-10-27 17:37:26,298 (asr_inference:509) INFO: speech length: 124698
2024-10-27 17:37:30,850 (beam_search:428) INFO: decoder input length: 96
2024-10-27 17:37:30,850 (beam_search:429) INFO: max output length: 96
2024-10-27 17:37:30,850 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:30,986 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:30,986 (beam_search:476) INFO:  -0.49 * 1.0 =  -0.49 for ctc
2024-10-27 17:37:30,986 (beam_search:479) INFO: total log probability: -0.49
2024-10-27 17:37:30,986 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:37:30,986 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:30,986 (beam_search:483) INFO: best hypo: WEHAVEBEENLEARNINGABOUTSANDHOW

2024-10-27 17:37:30,989 (asr_inference:509) INFO: speech length: 185267
2024-10-27 17:37:37,780 (beam_search:428) INFO: decoder input length: 144
2024-10-27 17:37:37,780 (beam_search:429) INFO: max output length: 144
2024-10-27 17:37:37,780 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:38,089 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:38,089 (beam_search:476) INFO:  -3.06 * 1.0 =  -3.06 for ctc
2024-10-27 17:37:38,089 (beam_search:479) INFO: total log probability: -3.06
2024-10-27 17:37:38,089 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:37:38,089 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:38,089 (beam_search:483) INFO: best hypo: IAAANDISAONTHEANDTHENTHEIS

2024-10-27 17:37:38,093 (asr_inference:509) INFO: speech length: 149559
2024-10-27 17:37:43,702 (beam_search:428) INFO: decoder input length: 116
2024-10-27 17:37:43,702 (beam_search:429) INFO: max output length: 116
2024-10-27 17:37:43,702 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:43,999 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:43,999 (beam_search:476) INFO:  -3.43 * 1.0 =  -3.43 for ctc
2024-10-27 17:37:43,999 (beam_search:479) INFO: total log probability: -3.43
2024-10-27 17:37:43,999 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:37:43,999 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:43,999 (beam_search:483) INFO: best hypo: BECAUSEITISWHICHISTHEENERGYITSOKNOWITISENERGYNOTOTHER

2024-10-27 17:37:44,002 (asr_inference:509) INFO: speech length: 182284
2024-10-27 17:37:50,795 (beam_search:428) INFO: decoder input length: 141
2024-10-27 17:37:50,795 (beam_search:429) INFO: max output length: 141
2024-10-27 17:37:50,795 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:51,440 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:51,440 (beam_search:476) INFO:  -7.71 * 1.0 =  -7.71 for ctc
2024-10-27 17:37:51,440 (beam_search:479) INFO: total log probability: -7.71
2024-10-27 17:37:51,440 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 17:37:51,440 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:51,441 (beam_search:483) INFO: best hypo: BECAUSEWEDIDN'THAVEENERGYWE'THAVELIGHTANDLIKETHATANDWEWOULDNHAVETHEBECAUSETHEISMADEOFENERGY

2024-10-27 17:37:51,443 (asr_inference:509) INFO: speech length: 76976
2024-10-27 17:37:54,198 (beam_search:428) INFO: decoder input length: 59
2024-10-27 17:37:54,198 (beam_search:429) INFO: max output length: 59
2024-10-27 17:37:54,198 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:54,269 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:54,270 (beam_search:476) INFO:  -0.76 * 1.0 =  -0.76 for ctc
2024-10-27 17:37:54,270 (beam_search:479) INFO: total log probability: -0.76
2024-10-27 17:37:54,270 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:37:54,270 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:54,270 (beam_search:483) INFO: best hypo: THATITISBECAUSEOFTHEAND

2024-10-27 17:37:54,272 (asr_inference:509) INFO: speech length: 60480
2024-10-27 17:37:56,573 (beam_search:428) INFO: decoder input length: 46
2024-10-27 17:37:56,573 (beam_search:429) INFO: max output length: 46
2024-10-27 17:37:56,573 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:56,615 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:56,615 (beam_search:476) INFO:  -2.41 * 1.0 =  -2.41 for ctc
2024-10-27 17:37:56,615 (beam_search:479) INFO: total log probability: -2.41
2024-10-27 17:37:56,615 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 17:37:56,615 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:56,615 (beam_search:483) INFO: best hypo: ISTHETHINGMAKESIT

2024-10-27 17:37:56,618 (asr_inference:509) INFO: speech length: 36913
2024-10-27 17:37:58,153 (beam_search:428) INFO: decoder input length: 28
2024-10-27 17:37:58,153 (beam_search:429) INFO: max output length: 28
2024-10-27 17:37:58,153 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:58,176 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:58,176 (beam_search:476) INFO:  -0.72 * 1.0 =  -0.72 for ctc
2024-10-27 17:37:58,176 (beam_search:479) INFO: total log probability: -0.72
2024-10-27 17:37:58,176 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:37:58,176 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:58,176 (beam_search:483) INFO: best hypo: IT'TLIKE

2024-10-27 17:37:58,178 (asr_inference:509) INFO: speech length: 261916
2024-10-27 17:38:08,885 (beam_search:428) INFO: decoder input length: 204
2024-10-27 17:38:08,886 (beam_search:429) INFO: max output length: 204
2024-10-27 17:38:08,886 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:09,990 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:09,990 (beam_search:476) INFO:  -3.73 * 1.0 =  -3.73 for ctc
2024-10-27 17:38:09,990 (beam_search:479) INFO: total log probability: -3.73
2024-10-27 17:38:09,990 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:38:09,991 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:09,991 (beam_search:483) INFO: best hypo: ISEETHATTHEISANDTHEDENERGYFROMTHEISGOINGTOTHEGOINGTOTHEITLIGHTBUTITISINGTHEWAXISINGAWAYENERGY

2024-10-27 17:38:09,993 (asr_inference:509) INFO: speech length: 40464
2024-10-27 17:38:11,558 (beam_search:428) INFO: decoder input length: 31
2024-10-27 17:38:11,558 (beam_search:429) INFO: max output length: 31
2024-10-27 17:38:11,558 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:11,579 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:11,579 (beam_search:476) INFO:  -1.10 * 1.0 =  -1.10 for ctc
2024-10-27 17:38:11,579 (beam_search:479) INFO: total log probability: -1.10
2024-10-27 17:38:11,579 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:38:11,579 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:11,579 (beam_search:483) INFO: best hypo: ISANDAND

2024-10-27 17:38:11,581 (asr_inference:509) INFO: speech length: 168041
2024-10-27 17:38:17,850 (beam_search:428) INFO: decoder input length: 130
2024-10-27 17:38:17,850 (beam_search:429) INFO: max output length: 130
2024-10-27 17:38:17,850 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:18,142 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:18,142 (beam_search:476) INFO:  -1.68 * 1.0 =  -1.68 for ctc
2024-10-27 17:38:18,142 (beam_search:479) INFO: total log probability: -1.68
2024-10-27 17:38:18,142 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:38:18,142 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:18,142 (beam_search:483) INFO: best hypo: ENERGYISOUTTHEBECAUSEITISBEINGANDENERGYISTHE

2024-10-27 17:38:18,145 (asr_inference:509) INFO: speech length: 149225
2024-10-27 17:38:23,665 (beam_search:428) INFO: decoder input length: 116
2024-10-27 17:38:23,665 (beam_search:429) INFO: max output length: 116
2024-10-27 17:38:23,665 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:24,034 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:24,035 (beam_search:476) INFO:  -5.52 * 1.0 =  -5.52 for ctc
2024-10-27 17:38:24,035 (beam_search:479) INFO: total log probability: -5.52
2024-10-27 17:38:24,035 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 17:38:24,035 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:24,035 (beam_search:483) INFO: best hypo: USINGENERGYBECAUSESHE'SGOINGABUTSHE'SALSOENERGYBECAUSETHEHASIT

2024-10-27 17:38:24,037 (asr_inference:509) INFO: speech length: 79123
2024-10-27 17:38:26,965 (beam_search:428) INFO: decoder input length: 61
2024-10-27 17:38:26,965 (beam_search:429) INFO: max output length: 61
2024-10-27 17:38:26,965 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:27,037 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:27,037 (beam_search:476) INFO:  -1.22 * 1.0 =  -1.22 for ctc
2024-10-27 17:38:27,037 (beam_search:479) INFO: total log probability: -1.22
2024-10-27 17:38:27,037 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:38:27,037 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:27,037 (beam_search:483) INFO: best hypo: ISINGITENERGYANDTHEIS

2024-10-27 17:38:27,040 (asr_inference:509) INFO: speech length: 62715
2024-10-27 17:38:29,357 (beam_search:428) INFO: decoder input length: 48
2024-10-27 17:38:29,358 (beam_search:429) INFO: max output length: 48
2024-10-27 17:38:29,358 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:29,395 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:29,395 (beam_search:476) INFO:  -0.58 * 1.0 =  -0.58 for ctc
2024-10-27 17:38:29,395 (beam_search:479) INFO: total log probability: -0.58
2024-10-27 17:38:29,395 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:38:29,395 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:29,395 (beam_search:483) INFO: best hypo: CANBECAUSEALLTHAT

2024-10-27 17:38:29,397 (asr_inference:509) INFO: speech length: 80935
2024-10-27 17:38:32,362 (beam_search:428) INFO: decoder input length: 62
2024-10-27 17:38:32,363 (beam_search:429) INFO: max output length: 62
2024-10-27 17:38:32,363 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:32,432 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:32,432 (beam_search:476) INFO:  -1.83 * 1.0 =  -1.83 for ctc
2024-10-27 17:38:32,432 (beam_search:479) INFO: total log probability: -1.83
2024-10-27 17:38:32,432 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:38:32,432 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:32,432 (beam_search:483) INFO: best hypo: ANDENERGYWHICHISOFBUT

2024-10-27 17:38:32,435 (asr_inference:509) INFO: speech length: 278942
2024-10-27 17:38:44,071 (beam_search:428) INFO: decoder input length: 217
2024-10-27 17:38:44,072 (beam_search:429) INFO: max output length: 217
2024-10-27 17:38:44,072 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:44,721 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:44,721 (beam_search:476) INFO:  -5.15 * 1.0 =  -5.15 for ctc
2024-10-27 17:38:44,721 (beam_search:479) INFO: total log probability: -5.15
2024-10-27 17:38:44,721 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 17:38:44,721 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:44,722 (beam_search:483) INFO: best hypo: WHICHSOTHETHETHEISDENERGYTHEHEATSOTHEANDTHEMOREMUCH

2024-10-27 17:38:44,724 (asr_inference:509) INFO: speech length: 36911
2024-10-27 17:38:46,233 (beam_search:428) INFO: decoder input length: 28
2024-10-27 17:38:46,233 (beam_search:429) INFO: max output length: 28
2024-10-27 17:38:46,233 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:46,253 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:46,253 (beam_search:476) INFO:  -1.09 * 1.0 =  -1.09 for ctc
2024-10-27 17:38:46,253 (beam_search:479) INFO: total log probability: -1.09
2024-10-27 17:38:46,253 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:38:46,253 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:46,253 (beam_search:483) INFO: best hypo: WILLBURNTO

2024-10-27 17:38:46,256 (asr_inference:509) INFO: speech length: 19601
2024-10-27 17:38:47,165 (beam_search:428) INFO: decoder input length: 14
2024-10-27 17:38:47,166 (beam_search:429) INFO: max output length: 14
2024-10-27 17:38:47,166 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:47,174 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:47,175 (beam_search:476) INFO:  -1.28 * 1.0 =  -1.28 for ctc
2024-10-27 17:38:47,175 (beam_search:479) INFO: total log probability: -1.28
2024-10-27 17:38:47,175 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 17:38:47,175 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:47,175 (beam_search:483) INFO: best hypo: STHAT

2024-10-27 17:38:47,177 (asr_inference:509) INFO: speech length: 118320
2024-10-27 17:38:51,412 (beam_search:428) INFO: decoder input length: 91
2024-10-27 17:38:51,412 (beam_search:429) INFO: max output length: 91
2024-10-27 17:38:51,412 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:51,532 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:51,532 (beam_search:476) INFO:  -1.51 * 1.0 =  -1.51 for ctc
2024-10-27 17:38:51,532 (beam_search:479) INFO: total log probability: -1.51
2024-10-27 17:38:51,532 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:38:51,532 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:51,532 (beam_search:483) INFO: best hypo: ITHINKSISASOLAR

2024-10-27 17:38:51,535 (asr_inference:509) INFO: speech length: 140065
2024-10-27 17:38:56,705 (beam_search:428) INFO: decoder input length: 108
2024-10-27 17:38:56,706 (beam_search:429) INFO: max output length: 108
2024-10-27 17:38:56,706 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:57,108 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:57,108 (beam_search:476) INFO:  -5.38 * 1.0 =  -5.38 for ctc
2024-10-27 17:38:57,108 (beam_search:479) INFO: total log probability: -5.38
2024-10-27 17:38:57,108 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:38:57,108 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:57,108 (beam_search:483) INFO: best hypo: THETOTHELARTOSOLARENERGYANDITWILLGOTHROUGHTHATINGTOTHEWIRESGOINGTOTHEIT

2024-10-27 17:38:57,112 (asr_inference:509) INFO: speech length: 51200
2024-10-27 17:38:59,194 (beam_search:428) INFO: decoder input length: 39
2024-10-27 17:38:59,194 (beam_search:429) INFO: max output length: 39
2024-10-27 17:38:59,194 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:59,224 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:59,224 (beam_search:476) INFO:  -1.42 * 1.0 =  -1.42 for ctc
2024-10-27 17:38:59,224 (beam_search:479) INFO: total log probability: -1.42
2024-10-27 17:38:59,224 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 17:38:59,224 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:59,224 (beam_search:483) INFO: best hypo: OKAYWETHIS

2024-10-27 17:38:59,226 (asr_inference:509) INFO: speech length: 147498
2024-10-27 17:39:04,599 (beam_search:428) INFO: decoder input length: 114
2024-10-27 17:39:04,600 (beam_search:429) INFO: max output length: 114
2024-10-27 17:39:04,600 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:04,889 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:04,889 (beam_search:476) INFO:  -3.08 * 1.0 =  -3.08 for ctc
2024-10-27 17:39:04,889 (beam_search:479) INFO: total log probability: -3.08
2024-10-27 17:39:04,890 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:39:04,890 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:04,890 (beam_search:483) INFO: best hypo: ITISONLYAONETIMEANDCOALISJUSTANDITCANFORA

2024-10-27 17:39:04,893 (asr_inference:509) INFO: speech length: 22500
2024-10-27 17:39:05,912 (beam_search:428) INFO: decoder input length: 17
2024-10-27 17:39:05,912 (beam_search:429) INFO: max output length: 17
2024-10-27 17:39:05,912 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:05,927 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:05,927 (beam_search:476) INFO:  -0.83 * 1.0 =  -0.83 for ctc
2024-10-27 17:39:05,927 (beam_search:479) INFO: total log probability: -0.83
2024-10-27 17:39:05,927 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:39:05,927 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:05,927 (beam_search:483) INFO: best hypo: ITWILLTHEAIR

2024-10-27 17:39:05,929 (asr_inference:509) INFO: speech length: 18500
2024-10-27 17:39:06,829 (beam_search:428) INFO: decoder input length: 13
2024-10-27 17:39:06,829 (beam_search:429) INFO: max output length: 13
2024-10-27 17:39:06,829 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:06,844 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:06,844 (beam_search:476) INFO:  -0.01 * 1.0 =  -0.01 for ctc
2024-10-27 17:39:06,844 (beam_search:479) INFO: total log probability: -0.01
2024-10-27 17:39:06,844 (beam_search:480) INFO: normalized log probability: -0.00
2024-10-27 17:39:06,844 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:06,844 (beam_search:483) INFO: best hypo: IDON'TKNOW

2024-10-27 17:39:06,847 (asr_inference:509) INFO: speech length: 23918
2024-10-27 17:39:07,924 (beam_search:428) INFO: decoder input length: 18
2024-10-27 17:39:07,925 (beam_search:429) INFO: max output length: 18
2024-10-27 17:39:07,925 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:07,930 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:07,930 (beam_search:476) INFO:  -0.42 * 1.0 =  -0.42 for ctc
2024-10-27 17:39:07,930 (beam_search:479) INFO: total log probability: -0.42
2024-10-27 17:39:07,930 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:39:07,930 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:07,930 (beam_search:483) INFO: best hypo: 

2024-10-27 17:39:07,932 (asr_inference:509) INFO: speech length: 87500
2024-10-27 17:39:11,199 (beam_search:428) INFO: decoder input length: 67
2024-10-27 17:39:11,199 (beam_search:429) INFO: max output length: 67
2024-10-27 17:39:11,199 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:11,250 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:11,250 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 17:39:11,250 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 17:39:11,250 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:39:11,250 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:11,250 (beam_search:483) INFO: best hypo: WELLNOTINOT

2024-10-27 17:39:11,252 (asr_inference:509) INFO: speech length: 27984
2024-10-27 17:39:12,412 (beam_search:428) INFO: decoder input length: 21
2024-10-27 17:39:12,412 (beam_search:429) INFO: max output length: 21
2024-10-27 17:39:12,412 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:12,433 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:12,433 (beam_search:476) INFO:  -0.34 * 1.0 =  -0.34 for ctc
2024-10-27 17:39:12,433 (beam_search:479) INFO: total log probability: -0.34
2024-10-27 17:39:12,433 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:39:12,433 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:12,433 (beam_search:483) INFO: best hypo: COULDYOUSAYTHATAGAIN

2024-10-27 17:39:12,435 (asr_inference:509) INFO: speech length: 42914
2024-10-27 17:39:14,104 (beam_search:428) INFO: decoder input length: 33
2024-10-27 17:39:14,104 (beam_search:429) INFO: max output length: 33
2024-10-27 17:39:14,104 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:14,134 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:14,134 (beam_search:476) INFO:  -1.24 * 1.0 =  -1.24 for ctc
2024-10-27 17:39:14,134 (beam_search:479) INFO: total log probability: -1.24
2024-10-27 17:39:14,134 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:39:14,134 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:14,134 (beam_search:483) INFO: best hypo: THISISAPARALLELCIRCUIT

2024-10-27 17:39:14,136 (asr_inference:509) INFO: speech length: 270455
2024-10-27 17:39:25,315 (beam_search:428) INFO: decoder input length: 210
2024-10-27 17:39:25,315 (beam_search:429) INFO: max output length: 210
2024-10-27 17:39:25,315 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:26,313 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:26,313 (beam_search:476) INFO:  -5.37 * 1.0 =  -5.37 for ctc
2024-10-27 17:39:26,313 (beam_search:479) INFO: total log probability: -5.37
2024-10-27 17:39:26,313 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:39:26,313 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:26,313 (beam_search:483) INFO: best hypo: ISGOINGAROUNDANDITISAANDITISACIRCUITANDITISLIGHTINGUPTHEBULBSWITHONEDANDWEDIDTHISINAND

2024-10-27 17:39:26,316 (asr_inference:509) INFO: speech length: 73187
2024-10-27 17:39:28,965 (beam_search:428) INFO: decoder input length: 56
2024-10-27 17:39:28,965 (beam_search:429) INFO: max output length: 56
2024-10-27 17:39:28,965 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:28,989 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:28,989 (beam_search:476) INFO:  -0.29 * 1.0 =  -0.29 for ctc
2024-10-27 17:39:28,989 (beam_search:479) INFO: total log probability: -0.29
2024-10-27 17:39:28,989 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:39:28,989 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:28,989 (beam_search:483) INFO: best hypo: AND

2024-10-27 17:39:28,992 (asr_inference:509) INFO: speech length: 110606
2024-10-27 17:39:33,328 (beam_search:428) INFO: decoder input length: 85
2024-10-27 17:39:33,328 (beam_search:429) INFO: max output length: 85
2024-10-27 17:39:33,328 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:33,420 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:33,420 (beam_search:476) INFO:  -0.53 * 1.0 =  -0.53 for ctc
2024-10-27 17:39:33,420 (beam_search:479) INFO: total log probability: -0.53
2024-10-27 17:39:33,420 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:39:33,420 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:33,420 (beam_search:483) INFO: best hypo: ITHINKTHATBECAUSEITIS

2024-10-27 17:39:33,422 (asr_inference:509) INFO: speech length: 152653
2024-10-27 17:39:39,023 (beam_search:428) INFO: decoder input length: 118
2024-10-27 17:39:39,023 (beam_search:429) INFO: max output length: 118
2024-10-27 17:39:39,023 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:39,184 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:39,185 (beam_search:476) INFO:  -1.66 * 1.0 =  -1.66 for ctc
2024-10-27 17:39:39,185 (beam_search:479) INFO: total log probability: -1.66
2024-10-27 17:39:39,185 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:39:39,185 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:39,185 (beam_search:483) INFO: best hypo: ITHATTHATITISAACIRCUIT

2024-10-27 17:39:39,187 (asr_inference:509) INFO: speech length: 133741
2024-10-27 17:39:43,985 (beam_search:428) INFO: decoder input length: 103
2024-10-27 17:39:43,986 (beam_search:429) INFO: max output length: 103
2024-10-27 17:39:43,986 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:44,114 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:44,114 (beam_search:476) INFO:  -1.72 * 1.0 =  -1.72 for ctc
2024-10-27 17:39:44,114 (beam_search:479) INFO: total log probability: -1.72
2024-10-27 17:39:44,114 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:39:44,114 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:44,114 (beam_search:483) INFO: best hypo: THATISGOINGTHROUGHATHATIS

2024-10-27 17:39:44,116 (asr_inference:509) INFO: speech length: 88621
2024-10-27 17:39:47,283 (beam_search:428) INFO: decoder input length: 68
2024-10-27 17:39:47,283 (beam_search:429) INFO: max output length: 68
2024-10-27 17:39:47,283 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:47,383 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:47,383 (beam_search:476) INFO:  -3.44 * 1.0 =  -3.44 for ctc
2024-10-27 17:39:47,383 (beam_search:479) INFO: total log probability: -3.44
2024-10-27 17:39:47,383 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 17:39:47,383 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:47,383 (beam_search:483) INFO: best hypo: THEYAREWITHONLYANDTHATVERYTO

2024-10-27 17:39:47,385 (asr_inference:509) INFO: speech length: 181949
2024-10-27 17:39:54,296 (beam_search:428) INFO: decoder input length: 141
2024-10-27 17:39:54,296 (beam_search:429) INFO: max output length: 141
2024-10-27 17:39:54,296 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:54,425 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:54,425 (beam_search:476) INFO:  -4.30 * 1.0 =  -4.30 for ctc
2024-10-27 17:39:54,425 (beam_search:479) INFO: total log probability: -4.30
2024-10-27 17:39:54,425 (beam_search:480) INFO: normalized log probability: -0.61
2024-10-27 17:39:54,425 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:54,425 (beam_search:483) INFO: best hypo: THEANDSTUFFITA

2024-10-27 17:39:54,427 (asr_inference:509) INFO: speech length: 82654
2024-10-27 17:39:57,438 (beam_search:428) INFO: decoder input length: 64
2024-10-27 17:39:57,438 (beam_search:429) INFO: max output length: 64
2024-10-27 17:39:57,438 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:57,529 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:57,529 (beam_search:476) INFO:  -1.61 * 1.0 =  -1.61 for ctc
2024-10-27 17:39:57,530 (beam_search:479) INFO: total log probability: -1.61
2024-10-27 17:39:57,530 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:39:57,530 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:57,530 (beam_search:483) INFO: best hypo: ITHATTHEBULBSARELIGHTINGINA

2024-10-27 17:39:57,533 (asr_inference:509) INFO: speech length: 91132
2024-10-27 17:40:00,795 (beam_search:428) INFO: decoder input length: 70
2024-10-27 17:40:00,795 (beam_search:429) INFO: max output length: 70
2024-10-27 17:40:00,795 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:00,861 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:00,861 (beam_search:476) INFO:  -0.64 * 1.0 =  -0.64 for ctc
2024-10-27 17:40:00,862 (beam_search:479) INFO: total log probability: -0.64
2024-10-27 17:40:00,862 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:40:00,862 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:00,862 (beam_search:483) INFO: best hypo: THATSIDONOT

2024-10-27 17:40:00,864 (asr_inference:509) INFO: speech length: 29672
2024-10-27 17:40:02,056 (beam_search:428) INFO: decoder input length: 22
2024-10-27 17:40:02,056 (beam_search:429) INFO: max output length: 22
2024-10-27 17:40:02,056 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:02,074 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:02,074 (beam_search:476) INFO:  -1.13 * 1.0 =  -1.13 for ctc
2024-10-27 17:40:02,074 (beam_search:479) INFO: total log probability: -1.13
2024-10-27 17:40:02,074 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:40:02,074 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:02,074 (beam_search:483) INFO: best hypo: COULDYOUTHATAGAIN

2024-10-27 17:40:02,077 (asr_inference:509) INFO: speech length: 59977
2024-10-27 17:40:04,279 (beam_search:428) INFO: decoder input length: 46
2024-10-27 17:40:04,279 (beam_search:429) INFO: max output length: 46
2024-10-27 17:40:04,279 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:04,346 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:04,346 (beam_search:476) INFO:  -0.92 * 1.0 =  -0.92 for ctc
2024-10-27 17:40:04,346 (beam_search:479) INFO: total log probability: -0.92
2024-10-27 17:40:04,346 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:40:04,346 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:04,346 (beam_search:483) INFO: best hypo: THEYAREINACIRCUITANDTHEYARE

2024-10-27 17:40:04,348 (asr_inference:509) INFO: speech length: 22500
2024-10-27 17:40:05,398 (beam_search:428) INFO: decoder input length: 17
2024-10-27 17:40:05,398 (beam_search:429) INFO: max output length: 17
2024-10-27 17:40:05,398 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:05,416 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:05,416 (beam_search:476) INFO:  -0.04 * 1.0 =  -0.04 for ctc
2024-10-27 17:40:05,416 (beam_search:479) INFO: total log probability: -0.04
2024-10-27 17:40:05,416 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 17:40:05,416 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:05,416 (beam_search:483) INFO: best hypo: IDON'TKNOW

2024-10-27 17:40:05,418 (asr_inference:509) INFO: speech length: 78188
2024-10-27 17:40:08,191 (beam_search:428) INFO: decoder input length: 60
2024-10-27 17:40:08,191 (beam_search:429) INFO: max output length: 60
2024-10-27 17:40:08,191 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:08,305 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:08,305 (beam_search:476) INFO:  -1.12 * 1.0 =  -1.12 for ctc
2024-10-27 17:40:08,305 (beam_search:479) INFO: total log probability: -1.12
2024-10-27 17:40:08,306 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:40:08,306 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:08,306 (beam_search:483) INFO: best hypo: ITGOESBACKINTOTHEPOSITIVESIDEANDTHENEGATIVESIDE

2024-10-27 17:40:08,308 (asr_inference:509) INFO: speech length: 55918
2024-10-27 17:40:10,513 (beam_search:428) INFO: decoder input length: 43
2024-10-27 17:40:10,513 (beam_search:429) INFO: max output length: 43
2024-10-27 17:40:10,513 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:10,571 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:10,571 (beam_search:476) INFO:  -1.99 * 1.0 =  -1.99 for ctc
2024-10-27 17:40:10,571 (beam_search:479) INFO: total log probability: -1.99
2024-10-27 17:40:10,571 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:40:10,571 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:10,571 (beam_search:483) INFO: best hypo: THATENERGYISANDITISACIRCUIT

2024-10-27 17:40:10,574 (asr_inference:509) INFO: speech length: 85554
2024-10-27 17:40:13,720 (beam_search:428) INFO: decoder input length: 66
2024-10-27 17:40:13,720 (beam_search:429) INFO: max output length: 66
2024-10-27 17:40:13,720 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:13,827 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:13,827 (beam_search:476) INFO:  -1.19 * 1.0 =  -1.19 for ctc
2024-10-27 17:40:13,827 (beam_search:479) INFO: total log probability: -1.19
2024-10-27 17:40:13,827 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:40:13,827 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:13,828 (beam_search:483) INFO: best hypo: IDON'TKNOWTHEAREGOINGIN

2024-10-27 17:40:13,830 (asr_inference:509) INFO: speech length: 305190
2024-10-27 17:40:26,932 (beam_search:428) INFO: decoder input length: 237
2024-10-27 17:40:26,932 (beam_search:429) INFO: max output length: 237
2024-10-27 17:40:26,932 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:28,275 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:28,276 (beam_search:476) INFO:  -3.72 * 1.0 =  -3.72 for ctc
2024-10-27 17:40:28,276 (beam_search:479) INFO: total log probability: -3.72
2024-10-27 17:40:28,276 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:40:28,276 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:28,276 (beam_search:483) INFO: best hypo: ITHATWHENITISITISGOINGBUTWHENITIS'TCONNECTEDITISGOINGFORSOMEBUTIYOURSOITHINKI'MRIGHT

2024-10-27 17:40:28,278 (asr_inference:509) INFO: speech length: 49331
2024-10-27 17:40:30,112 (beam_search:428) INFO: decoder input length: 38
2024-10-27 17:40:30,112 (beam_search:429) INFO: max output length: 38
2024-10-27 17:40:30,112 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:30,138 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:30,138 (beam_search:476) INFO:  -0.03 * 1.0 =  -0.03 for ctc
2024-10-27 17:40:30,138 (beam_search:479) INFO: total log probability: -0.03
2024-10-27 17:40:30,138 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 17:40:30,138 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:30,138 (beam_search:483) INFO: best hypo: ITISTHE

2024-10-27 17:40:30,141 (asr_inference:509) INFO: speech length: 189576
2024-10-27 17:40:37,376 (beam_search:428) INFO: decoder input length: 147
2024-10-27 17:40:37,376 (beam_search:429) INFO: max output length: 147
2024-10-27 17:40:37,376 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:37,650 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:37,650 (beam_search:476) INFO:  -2.37 * 1.0 =  -2.37 for ctc
2024-10-27 17:40:37,650 (beam_search:479) INFO: total log probability: -2.37
2024-10-27 17:40:37,650 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:40:37,650 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:37,650 (beam_search:483) INFO: best hypo: IT'SBECAUSEITISANDANDLIKEYEAHYEAH

2024-10-27 17:40:37,653 (asr_inference:509) INFO: speech length: 36479
2024-10-27 17:40:39,031 (beam_search:428) INFO: decoder input length: 27
2024-10-27 17:40:39,032 (beam_search:429) INFO: max output length: 27
2024-10-27 17:40:39,032 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:39,046 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:39,046 (beam_search:476) INFO:  -1.43 * 1.0 =  -1.43 for ctc
2024-10-27 17:40:39,046 (beam_search:479) INFO: total log probability: -1.43
2024-10-27 17:40:39,046 (beam_search:480) INFO: normalized log probability: -0.36
2024-10-27 17:40:39,046 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:39,046 (beam_search:483) INFO: best hypo: DONT

2024-10-27 17:40:39,048 (asr_inference:509) INFO: speech length: 58548
2024-10-27 17:40:41,225 (beam_search:428) INFO: decoder input length: 45
2024-10-27 17:40:41,225 (beam_search:429) INFO: max output length: 45
2024-10-27 17:40:41,225 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:41,255 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:41,255 (beam_search:476) INFO:  -0.58 * 1.0 =  -0.58 for ctc
2024-10-27 17:40:41,255 (beam_search:479) INFO: total log probability: -0.58
2024-10-27 17:40:41,255 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:40:41,255 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:41,255 (beam_search:483) INFO: best hypo: BECAUSETHEYDO

2024-10-27 17:40:41,258 (asr_inference:509) INFO: speech length: 50179
2024-10-27 17:40:43,253 (beam_search:428) INFO: decoder input length: 38
2024-10-27 17:40:43,254 (beam_search:429) INFO: max output length: 38
2024-10-27 17:40:43,254 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:43,305 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:43,305 (beam_search:476) INFO:  -2.15 * 1.0 =  -2.15 for ctc
2024-10-27 17:40:43,305 (beam_search:479) INFO: total log probability: -2.15
2024-10-27 17:40:43,305 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:40:43,305 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:43,305 (beam_search:483) INFO: best hypo: ITHECIRCUITWOULDBEFORAPARALLEL

2024-10-27 17:40:43,308 (asr_inference:509) INFO: speech length: 327275
2024-10-27 17:40:57,567 (beam_search:428) INFO: decoder input length: 255
2024-10-27 17:40:57,567 (beam_search:429) INFO: max output length: 255
2024-10-27 17:40:57,567 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:58,558 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:58,558 (beam_search:476) INFO:  -5.51 * 1.0 =  -5.51 for ctc
2024-10-27 17:40:58,558 (beam_search:479) INFO: total log probability: -5.51
2024-10-27 17:40:58,558 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:40:58,558 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:58,558 (beam_search:483) INFO: best hypo: ITHATTHETHATTWOMAGNETSHAVEANDWHICHISANDANDTHATTHEYARETOTHEWHICHISOFAND

2024-10-27 17:40:58,562 (asr_inference:509) INFO: speech length: 255482
2024-10-27 17:41:09,212 (beam_search:428) INFO: decoder input length: 199
2024-10-27 17:41:09,212 (beam_search:429) INFO: max output length: 199
2024-10-27 17:41:09,212 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:09,700 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:09,700 (beam_search:476) INFO:  -4.76 * 1.0 =  -4.76 for ctc
2024-10-27 17:41:09,700 (beam_search:479) INFO: total log probability: -4.76
2024-10-27 17:41:09,700 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 17:41:09,700 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:09,700 (beam_search:483) INFO: best hypo: THEYSTICKTOASSTEELANDBUTNOTOTHERSTUFFSTICKTO

2024-10-27 17:41:09,703 (asr_inference:509) INFO: speech length: 206532
2024-10-27 17:41:18,362 (beam_search:428) INFO: decoder input length: 160
2024-10-27 17:41:18,362 (beam_search:429) INFO: max output length: 160
2024-10-27 17:41:18,362 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:18,684 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:18,684 (beam_search:476) INFO:  -3.16 * 1.0 =  -3.16 for ctc
2024-10-27 17:41:18,684 (beam_search:479) INFO: total log probability: -3.16
2024-10-27 17:41:18,684 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:41:18,684 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:18,684 (beam_search:483) INFO: best hypo: BECAUSETHEISANDTHEYLIKETOSTICKTOMETALAND

2024-10-27 17:41:18,687 (asr_inference:509) INFO: speech length: 42885
2024-10-27 17:41:20,376 (beam_search:428) INFO: decoder input length: 33
2024-10-27 17:41:20,376 (beam_search:429) INFO: max output length: 33
2024-10-27 17:41:20,376 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:20,417 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:20,418 (beam_search:476) INFO:  -2.17 * 1.0 =  -2.17 for ctc
2024-10-27 17:41:20,418 (beam_search:479) INFO: total log probability: -2.17
2024-10-27 17:41:20,418 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:41:20,418 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:20,418 (beam_search:483) INFO: best hypo: THENAILOUTOFSTEELANDIRON

2024-10-27 17:41:20,420 (asr_inference:509) INFO: speech length: 378809
2024-10-27 17:41:37,641 (beam_search:428) INFO: decoder input length: 295
2024-10-27 17:41:37,641 (beam_search:429) INFO: max output length: 295
2024-10-27 17:41:37,641 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:37,836 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:37,836 (beam_search:476) INFO:  -1.05 * 1.0 =  -1.05 for ctc
2024-10-27 17:41:37,836 (beam_search:479) INFO: total log probability: -1.05
2024-10-27 17:41:37,836 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:41:37,836 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:37,836 (beam_search:483) INFO: best hypo: PAPERANDPLASTIC

2024-10-27 17:41:37,838 (asr_inference:509) INFO: speech length: 212833
2024-10-27 17:41:46,246 (beam_search:428) INFO: decoder input length: 165
2024-10-27 17:41:46,246 (beam_search:429) INFO: max output length: 165
2024-10-27 17:41:46,246 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:46,744 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:46,744 (beam_search:476) INFO:  -4.69 * 1.0 =  -4.69 for ctc
2024-10-27 17:41:46,744 (beam_search:479) INFO: total log probability: -4.69
2024-10-27 17:41:46,744 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:41:46,744 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:46,744 (beam_search:483) INFO: best hypo: THENAILISOUTOFSTEELANDIRONITSTICKTOTHEMAGNETBUTTHEISNOT

2024-10-27 17:41:46,746 (asr_inference:509) INFO: speech length: 128130
2024-10-27 17:41:51,446 (beam_search:428) INFO: decoder input length: 99
2024-10-27 17:41:51,446 (beam_search:429) INFO: max output length: 99
2024-10-27 17:41:51,446 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:51,499 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:51,499 (beam_search:476) INFO:  -1.20 * 1.0 =  -1.20 for ctc
2024-10-27 17:41:51,499 (beam_search:479) INFO: total log probability: -1.20
2024-10-27 17:41:51,499 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 17:41:51,499 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:51,499 (beam_search:483) INFO: best hypo: ANDOTHER

2024-10-27 17:41:51,501 (asr_inference:509) INFO: speech length: 236101
2024-10-27 17:42:00,961 (beam_search:428) INFO: decoder input length: 183
2024-10-27 17:42:00,961 (beam_search:429) INFO: max output length: 183
2024-10-27 17:42:00,961 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:01,259 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:01,259 (beam_search:476) INFO:  -5.63 * 1.0 =  -5.63 for ctc
2024-10-27 17:42:01,259 (beam_search:479) INFO: total log probability: -5.63
2024-10-27 17:42:01,259 (beam_search:480) INFO: normalized log probability: -0.51
2024-10-27 17:42:01,259 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:01,259 (beam_search:483) INFO: best hypo: HAVEORINITOROTHERSONOTSTICK

2024-10-27 17:42:01,262 (asr_inference:509) INFO: speech length: 409252
2024-10-27 17:42:21,043 (beam_search:428) INFO: decoder input length: 319
2024-10-27 17:42:21,043 (beam_search:429) INFO: max output length: 319
2024-10-27 17:42:21,043 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:23,223 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:23,224 (beam_search:476) INFO: -11.33 * 1.0 = -11.33 for ctc
2024-10-27 17:42:23,224 (beam_search:479) INFO: total log probability: -11.33
2024-10-27 17:42:23,224 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 17:42:23,224 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:23,224 (beam_search:483) INFO: best hypo: N'TBECAUSEITDOESN'THAVEANYBECAUSEITDOESN'THAVEANYOTHERORANDITN'THAVEABUTTHENAILDOESHAVEIRONSTEELITHATA

2024-10-27 17:42:23,226 (asr_inference:509) INFO: speech length: 23864
2024-10-27 17:42:24,287 (beam_search:428) INFO: decoder input length: 18
2024-10-27 17:42:24,288 (beam_search:429) INFO: max output length: 18
2024-10-27 17:42:24,288 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:24,299 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:24,299 (beam_search:476) INFO:  -0.34 * 1.0 =  -0.34 for ctc
2024-10-27 17:42:24,299 (beam_search:479) INFO: total log probability: -0.34
2024-10-27 17:42:24,299 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:42:24,299 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:24,299 (beam_search:483) INFO: best hypo: THEYHAVE

2024-10-27 17:42:24,301 (asr_inference:509) INFO: speech length: 89521
2024-10-27 17:42:27,562 (beam_search:428) INFO: decoder input length: 69
2024-10-27 17:42:27,562 (beam_search:429) INFO: max output length: 69
2024-10-27 17:42:27,562 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:27,630 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:27,630 (beam_search:476) INFO:  -0.19 * 1.0 =  -0.19 for ctc
2024-10-27 17:42:27,630 (beam_search:479) INFO: total log probability: -0.19
2024-10-27 17:42:27,630 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 17:42:27,630 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:27,630 (beam_search:483) INFO: best hypo: IDON'TKNOW

2024-10-27 17:42:27,633 (asr_inference:509) INFO: speech length: 76698
2024-10-27 17:42:30,502 (beam_search:428) INFO: decoder input length: 59
2024-10-27 17:42:30,502 (beam_search:429) INFO: max output length: 59
2024-10-27 17:42:30,502 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:30,612 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:30,612 (beam_search:476) INFO:  -2.64 * 1.0 =  -2.64 for ctc
2024-10-27 17:42:30,612 (beam_search:479) INFO: total log probability: -2.64
2024-10-27 17:42:30,612 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:42:30,612 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:30,612 (beam_search:483) INFO: best hypo: I'MANDIDON'TREALLYHAVEFOR

2024-10-27 17:42:30,614 (asr_inference:509) INFO: speech length: 96391
2024-10-27 17:42:34,095 (beam_search:428) INFO: decoder input length: 74
2024-10-27 17:42:34,095 (beam_search:429) INFO: max output length: 74
2024-10-27 17:42:34,095 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:34,179 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:34,179 (beam_search:476) INFO:  -0.49 * 1.0 =  -0.49 for ctc
2024-10-27 17:42:34,179 (beam_search:479) INFO: total log probability: -0.49
2024-10-27 17:42:34,179 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:42:34,179 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:34,179 (beam_search:483) INFO: best hypo: I'MSORTOFREALLY

2024-10-27 17:42:34,181 (asr_inference:509) INFO: speech length: 80405
2024-10-27 17:42:37,258 (beam_search:428) INFO: decoder input length: 62
2024-10-27 17:42:37,258 (beam_search:429) INFO: max output length: 62
2024-10-27 17:42:37,258 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:37,368 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:37,369 (beam_search:476) INFO:  -0.80 * 1.0 =  -0.80 for ctc
2024-10-27 17:42:37,369 (beam_search:479) INFO: total log probability: -0.80
2024-10-27 17:42:37,369 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:42:37,369 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:37,369 (beam_search:483) INFO: best hypo: THEISNOTBECAUSEIT'SNOTOFTHE

2024-10-27 17:42:37,371 (asr_inference:509) INFO: speech length: 73535
2024-10-27 17:42:40,067 (beam_search:428) INFO: decoder input length: 56
2024-10-27 17:42:40,067 (beam_search:429) INFO: max output length: 56
2024-10-27 17:42:40,067 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:40,192 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:40,192 (beam_search:476) INFO:  -3.38 * 1.0 =  -3.38 for ctc
2024-10-27 17:42:40,192 (beam_search:479) INFO: total log probability: -3.38
2024-10-27 17:42:40,192 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:42:40,192 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:40,192 (beam_search:483) INFO: best hypo: ISMADEOUTOFANDSTEELBUTTHEOTHERONEISMADEOUTOF

2024-10-27 17:42:40,194 (asr_inference:509) INFO: speech length: 66629
2024-10-27 17:42:42,619 (beam_search:428) INFO: decoder input length: 51
2024-10-27 17:42:42,620 (beam_search:429) INFO: max output length: 51
2024-10-27 17:42:42,620 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:42,708 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:42,709 (beam_search:476) INFO:  -2.13 * 1.0 =  -2.13 for ctc
2024-10-27 17:42:42,709 (beam_search:479) INFO: total log probability: -2.13
2024-10-27 17:42:42,709 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:42:42,709 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:42,709 (beam_search:483) INFO: best hypo: WELLONEISANDONEISOUTOFSTEELANDTHE

2024-10-27 17:42:42,711 (asr_inference:509) INFO: speech length: 73237
2024-10-27 17:42:45,348 (beam_search:428) INFO: decoder input length: 56
2024-10-27 17:42:45,349 (beam_search:429) INFO: max output length: 56
2024-10-27 17:42:45,349 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:45,480 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:45,480 (beam_search:476) INFO:  -2.64 * 1.0 =  -2.64 for ctc
2024-10-27 17:42:45,480 (beam_search:479) INFO: total log probability: -2.64
2024-10-27 17:42:45,480 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:42:45,480 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:45,480 (beam_search:483) INFO: best hypo: IDON'TKNOWBUTIDON'TREALLYHAVEFORPLEASEUP

2024-10-27 17:42:45,482 (asr_inference:509) INFO: speech length: 78525
2024-10-27 17:42:48,362 (beam_search:428) INFO: decoder input length: 60
2024-10-27 17:42:48,362 (beam_search:429) INFO: max output length: 60
2024-10-27 17:42:48,362 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:48,411 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:48,411 (beam_search:476) INFO:  -0.75 * 1.0 =  -0.75 for ctc
2024-10-27 17:42:48,411 (beam_search:479) INFO: total log probability: -0.75
2024-10-27 17:42:48,412 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:42:48,412 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:48,412 (beam_search:483) INFO: best hypo: ANDANDOTHERAND

2024-10-27 17:42:48,415 (asr_inference:509) INFO: speech length: 188327
2024-10-27 17:42:55,872 (beam_search:428) INFO: decoder input length: 146
2024-10-27 17:42:55,872 (beam_search:429) INFO: max output length: 146
2024-10-27 17:42:55,872 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:56,195 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:56,195 (beam_search:476) INFO:  -2.93 * 1.0 =  -2.93 for ctc
2024-10-27 17:42:56,195 (beam_search:479) INFO: total log probability: -2.93
2024-10-27 17:42:56,195 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:42:56,195 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:56,195 (beam_search:483) INFO: best hypo: ITHINKTHETHATTHATHAVEHAVETHETHATITSTICKTOTHE

2024-10-27 17:42:56,197 (asr_inference:509) INFO: speech length: 105843
2024-10-27 17:42:59,899 (beam_search:428) INFO: decoder input length: 82
2024-10-27 17:42:59,899 (beam_search:429) INFO: max output length: 82
2024-10-27 17:42:59,899 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:00,096 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:00,097 (beam_search:476) INFO:  -1.14 * 1.0 =  -1.14 for ctc
2024-10-27 17:43:00,097 (beam_search:479) INFO: total log probability: -1.14
2024-10-27 17:43:00,097 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:43:00,097 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:00,097 (beam_search:483) INFO: best hypo: I'MI'MSOIDON'TREALLYKNOWIS

2024-10-27 17:43:00,099 (asr_inference:509) INFO: speech length: 18913
2024-10-27 17:43:01,044 (beam_search:428) INFO: decoder input length: 14
2024-10-27 17:43:01,044 (beam_search:429) INFO: max output length: 14
2024-10-27 17:43:01,044 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:01,053 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:01,054 (beam_search:476) INFO:  -0.84 * 1.0 =  -0.84 for ctc
2024-10-27 17:43:01,054 (beam_search:479) INFO: total log probability: -0.84
2024-10-27 17:43:01,054 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:43:01,054 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:01,054 (beam_search:483) INFO: best hypo: WEBEEN

2024-10-27 17:43:01,057 (asr_inference:509) INFO: speech length: 84384
2024-10-27 17:43:04,178 (beam_search:428) INFO: decoder input length: 65
2024-10-27 17:43:04,178 (beam_search:429) INFO: max output length: 65
2024-10-27 17:43:04,178 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:04,231 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:04,231 (beam_search:476) INFO:  -0.90 * 1.0 =  -0.90 for ctc
2024-10-27 17:43:04,231 (beam_search:479) INFO: total log probability: -0.90
2024-10-27 17:43:04,231 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:43:04,231 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:04,231 (beam_search:483) INFO: best hypo: THEYAREANDVERY

2024-10-27 17:43:04,233 (asr_inference:509) INFO: speech length: 63720
2024-10-27 17:43:06,530 (beam_search:428) INFO: decoder input length: 49
2024-10-27 17:43:06,530 (beam_search:429) INFO: max output length: 49
2024-10-27 17:43:06,530 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:06,557 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:06,557 (beam_search:476) INFO:  -0.29 * 1.0 =  -0.29 for ctc
2024-10-27 17:43:06,557 (beam_search:479) INFO: total log probability: -0.29
2024-10-27 17:43:06,557 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:43:06,557 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:06,557 (beam_search:483) INFO: best hypo: THEYWOULD

2024-10-27 17:43:06,560 (asr_inference:509) INFO: speech length: 192753
2024-10-27 17:43:14,532 (beam_search:428) INFO: decoder input length: 150
2024-10-27 17:43:14,532 (beam_search:429) INFO: max output length: 150
2024-10-27 17:43:14,532 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:15,010 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:15,010 (beam_search:476) INFO:  -3.59 * 1.0 =  -3.59 for ctc
2024-10-27 17:43:15,010 (beam_search:479) INFO: total log probability: -3.59
2024-10-27 17:43:15,010 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:43:15,010 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:15,010 (beam_search:483) INFO: best hypo: THEEACHOTHERBECAUSEYOUTHEANDORANDCANTGOTOGETHERIYOUWERETHANTHIS

2024-10-27 17:43:15,013 (asr_inference:509) INFO: speech length: 170536
2024-10-27 17:43:21,613 (beam_search:428) INFO: decoder input length: 132
2024-10-27 17:43:21,613 (beam_search:429) INFO: max output length: 132
2024-10-27 17:43:21,613 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:21,817 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:21,817 (beam_search:476) INFO:  -3.95 * 1.0 =  -3.95 for ctc
2024-10-27 17:43:21,817 (beam_search:479) INFO: total log probability: -3.95
2024-10-27 17:43:21,817 (beam_search:480) INFO: normalized log probability: -0.36
2024-10-27 17:43:21,817 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:21,817 (beam_search:483) INFO: best hypo: ANDNORTHANDANDTHEOFIAYOU

2024-10-27 17:43:21,819 (asr_inference:509) INFO: speech length: 69713
2024-10-27 17:43:24,376 (beam_search:428) INFO: decoder input length: 53
2024-10-27 17:43:24,376 (beam_search:429) INFO: max output length: 53
2024-10-27 17:43:24,376 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:24,420 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:24,421 (beam_search:476) INFO:  -1.33 * 1.0 =  -1.33 for ctc
2024-10-27 17:43:24,421 (beam_search:479) INFO: total log probability: -1.33
2024-10-27 17:43:24,421 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:43:24,421 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:24,421 (beam_search:483) INFO: best hypo: ITSANDAND

2024-10-27 17:43:24,424 (asr_inference:509) INFO: speech length: 121814
2024-10-27 17:43:28,825 (beam_search:428) INFO: decoder input length: 94
2024-10-27 17:43:28,825 (beam_search:429) INFO: max output length: 94
2024-10-27 17:43:28,825 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:28,966 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:28,966 (beam_search:476) INFO:  -3.59 * 1.0 =  -3.59 for ctc
2024-10-27 17:43:28,966 (beam_search:479) INFO: total log probability: -3.59
2024-10-27 17:43:28,966 (beam_search:480) INFO: normalized log probability: -0.36
2024-10-27 17:43:28,966 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:28,966 (beam_search:483) INFO: best hypo: SOMEARETHEOFIAYOUMUCH

2024-10-27 17:43:28,968 (asr_inference:509) INFO: speech length: 315781
2024-10-27 17:43:42,548 (beam_search:428) INFO: decoder input length: 246
2024-10-27 17:43:42,548 (beam_search:429) INFO: max output length: 246
2024-10-27 17:43:42,548 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:43,874 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:43,874 (beam_search:476) INFO:  -8.39 * 1.0 =  -8.39 for ctc
2024-10-27 17:43:43,874 (beam_search:479) INFO: total log probability: -8.39
2024-10-27 17:43:43,874 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:43:43,874 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:43,874 (beam_search:483) INFO: best hypo: THENORTHSIDEOFTHEANDTHESOUTHTHEOFTHEBECAUSEWEHAVEBECAUSETHENORTHTHENORTHANDTHEAREMAGNETICTOUSFROMTHESSOLAR

2024-10-27 17:43:43,878 (asr_inference:509) INFO: speech length: 75041
2024-10-27 17:43:46,637 (beam_search:428) INFO: decoder input length: 58
2024-10-27 17:43:46,637 (beam_search:429) INFO: max output length: 58
2024-10-27 17:43:46,638 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:46,674 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:46,674 (beam_search:476) INFO:  -1.31 * 1.0 =  -1.31 for ctc
2024-10-27 17:43:46,674 (beam_search:479) INFO: total log probability: -1.31
2024-10-27 17:43:46,674 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:43:46,674 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:46,674 (beam_search:483) INFO: best hypo: AWITHA

2024-10-27 17:43:46,676 (asr_inference:509) INFO: speech length: 166169
2024-10-27 17:43:53,160 (beam_search:428) INFO: decoder input length: 129
2024-10-27 17:43:53,160 (beam_search:429) INFO: max output length: 129
2024-10-27 17:43:53,160 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:53,331 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:53,331 (beam_search:476) INFO:  -2.55 * 1.0 =  -2.55 for ctc
2024-10-27 17:43:53,331 (beam_search:479) INFO: total log probability: -2.55
2024-10-27 17:43:53,331 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 17:43:53,331 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:53,331 (beam_search:483) INFO: best hypo: ITISAMAGNETICANDITTO

2024-10-27 17:43:53,334 (asr_inference:509) INFO: speech length: 35952
2024-10-27 17:43:54,761 (beam_search:428) INFO: decoder input length: 27
2024-10-27 17:43:54,761 (beam_search:429) INFO: max output length: 27
2024-10-27 17:43:54,761 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:54,780 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:54,780 (beam_search:476) INFO:  -0.38 * 1.0 =  -0.38 for ctc
2024-10-27 17:43:54,780 (beam_search:479) INFO: total log probability: -0.38
2024-10-27 17:43:54,781 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:43:54,781 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:54,781 (beam_search:483) INFO: best hypo: BECAUSEOFTHE

2024-10-27 17:43:54,782 (asr_inference:509) INFO: speech length: 59510
2024-10-27 17:43:56,975 (beam_search:428) INFO: decoder input length: 45
2024-10-27 17:43:56,975 (beam_search:429) INFO: max output length: 45
2024-10-27 17:43:56,975 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:57,016 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:57,016 (beam_search:476) INFO:  -0.52 * 1.0 =  -0.52 for ctc
2024-10-27 17:43:57,016 (beam_search:479) INFO: total log probability: -0.52
2024-10-27 17:43:57,016 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:43:57,016 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:57,016 (beam_search:483) INFO: best hypo: ITISGOINGTHROUGHTHE

2024-10-27 17:43:57,018 (asr_inference:509) INFO: speech length: 45095
2024-10-27 17:43:58,723 (beam_search:428) INFO: decoder input length: 34
2024-10-27 17:43:58,723 (beam_search:429) INFO: max output length: 34
2024-10-27 17:43:58,723 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:58,751 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:58,751 (beam_search:476) INFO:  -0.13 * 1.0 =  -0.13 for ctc
2024-10-27 17:43:58,751 (beam_search:479) INFO: total log probability: -0.13
2024-10-27 17:43:58,751 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 17:43:58,751 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:58,751 (beam_search:483) INFO: best hypo: BECAUSETHEREISLIKE

2024-10-27 17:43:58,753 (asr_inference:509) INFO: speech length: 57820
2024-10-27 17:44:00,802 (beam_search:428) INFO: decoder input length: 44
2024-10-27 17:44:00,802 (beam_search:429) INFO: max output length: 44
2024-10-27 17:44:00,802 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:00,835 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:00,836 (beam_search:476) INFO:  -0.29 * 1.0 =  -0.29 for ctc
2024-10-27 17:44:00,836 (beam_search:479) INFO: total log probability: -0.29
2024-10-27 17:44:00,836 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:44:00,836 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:00,836 (beam_search:483) INFO: best hypo: BECAUSETHEYHAVEAND

2024-10-27 17:44:00,838 (asr_inference:509) INFO: speech length: 179151
2024-10-27 17:44:07,809 (beam_search:428) INFO: decoder input length: 139
2024-10-27 17:44:07,809 (beam_search:429) INFO: max output length: 139
2024-10-27 17:44:07,809 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:08,146 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:08,146 (beam_search:476) INFO:  -2.26 * 1.0 =  -2.26 for ctc
2024-10-27 17:44:08,146 (beam_search:479) INFO: total log probability: -2.26
2024-10-27 17:44:08,146 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:44:08,146 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:08,146 (beam_search:483) INFO: best hypo: THEWOULDN'TBUTTHEWOULDBECAUSEIT'SFORMAGNETISMTO

2024-10-27 17:44:08,149 (asr_inference:509) INFO: speech length: 182457
2024-10-27 17:44:15,296 (beam_search:428) INFO: decoder input length: 142
2024-10-27 17:44:15,297 (beam_search:429) INFO: max output length: 142
2024-10-27 17:44:15,297 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:15,760 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:15,760 (beam_search:476) INFO:  -2.56 * 1.0 =  -2.56 for ctc
2024-10-27 17:44:15,760 (beam_search:479) INFO: total log probability: -2.56
2024-10-27 17:44:15,761 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:44:15,761 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:15,761 (beam_search:483) INFO: best hypo: ITWORKEDWELLTHEONEDIDN'TWORKANDTHEIONEYEAHTIMEANDID

2024-10-27 17:44:15,763 (asr_inference:509) INFO: speech length: 81499
2024-10-27 17:44:18,664 (beam_search:428) INFO: decoder input length: 63
2024-10-27 17:44:18,664 (beam_search:429) INFO: max output length: 63
2024-10-27 17:44:18,664 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:18,741 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:18,741 (beam_search:476) INFO:  -1.91 * 1.0 =  -1.91 for ctc
2024-10-27 17:44:18,741 (beam_search:479) INFO: total log probability: -1.91
2024-10-27 17:44:18,741 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:44:18,741 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:18,741 (beam_search:483) INFO: best hypo: ITSTOGOTHROUGHBECAUSETHE

2024-10-27 17:44:18,743 (asr_inference:509) INFO: speech length: 35953
2024-10-27 17:44:20,173 (beam_search:428) INFO: decoder input length: 27
2024-10-27 17:44:20,173 (beam_search:429) INFO: max output length: 27
2024-10-27 17:44:20,173 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:20,191 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:20,191 (beam_search:476) INFO:  -1.07 * 1.0 =  -1.07 for ctc
2024-10-27 17:44:20,191 (beam_search:479) INFO: total log probability: -1.07
2024-10-27 17:44:20,191 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:44:20,191 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:20,191 (beam_search:483) INFO: best hypo: ITISNOT

2024-10-27 17:44:20,194 (asr_inference:509) INFO: speech length: 224293
2024-10-27 17:44:29,316 (beam_search:428) INFO: decoder input length: 174
2024-10-27 17:44:29,316 (beam_search:429) INFO: max output length: 174
2024-10-27 17:44:29,316 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:29,791 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:29,791 (beam_search:476) INFO:  -6.75 * 1.0 =  -6.75 for ctc
2024-10-27 17:44:29,791 (beam_search:479) INFO: total log probability: -6.75
2024-10-27 17:44:29,791 (beam_search:480) INFO: normalized log probability: -0.40
2024-10-27 17:44:29,791 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:29,791 (beam_search:483) INFO: best hypo: THEISINGTHROUGHTHENAILWHENYOUTHESOMEINTHENAILTHESTICK

2024-10-27 17:44:29,793 (asr_inference:509) INFO: speech length: 41375
2024-10-27 17:44:31,345 (beam_search:428) INFO: decoder input length: 31
2024-10-27 17:44:31,345 (beam_search:429) INFO: max output length: 31
2024-10-27 17:44:31,345 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:31,369 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:31,369 (beam_search:476) INFO:  -1.35 * 1.0 =  -1.35 for ctc
2024-10-27 17:44:31,369 (beam_search:479) INFO: total log probability: -1.35
2024-10-27 17:44:31,369 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:44:31,369 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:31,369 (beam_search:483) INFO: best hypo: WETHATINSCIENCE

2024-10-27 17:44:31,371 (asr_inference:509) INFO: speech length: 44802
2024-10-27 17:44:33,069 (beam_search:428) INFO: decoder input length: 34
2024-10-27 17:44:33,069 (beam_search:429) INFO: max output length: 34
2024-10-27 17:44:33,069 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:33,086 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:33,087 (beam_search:476) INFO:  -0.56 * 1.0 =  -0.56 for ctc
2024-10-27 17:44:33,087 (beam_search:479) INFO: total log probability: -0.56
2024-10-27 17:44:33,087 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:44:33,087 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:33,087 (beam_search:483) INFO: best hypo: THEYARE

2024-10-27 17:44:33,089 (asr_inference:509) INFO: speech length: 54165
2024-10-27 17:44:35,173 (beam_search:428) INFO: decoder input length: 41
2024-10-27 17:44:35,173 (beam_search:429) INFO: max output length: 41
2024-10-27 17:44:35,173 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:35,189 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:35,189 (beam_search:476) INFO:  -0.31 * 1.0 =  -0.31 for ctc
2024-10-27 17:44:35,189 (beam_search:479) INFO: total log probability: -0.31
2024-10-27 17:44:35,189 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:44:35,189 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:35,189 (beam_search:483) INFO: best hypo: AND

2024-10-27 17:44:35,191 (asr_inference:509) INFO: speech length: 50938
2024-10-27 17:44:37,217 (beam_search:428) INFO: decoder input length: 39
2024-10-27 17:44:37,217 (beam_search:429) INFO: max output length: 39
2024-10-27 17:44:37,217 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:37,228 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:37,229 (beam_search:476) INFO:  -0.07 * 1.0 =  -0.07 for ctc
2024-10-27 17:44:37,229 (beam_search:479) INFO: total log probability: -0.07
2024-10-27 17:44:37,229 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:44:37,229 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:37,229 (beam_search:483) INFO: best hypo: 

2024-10-27 17:44:37,231 (asr_inference:509) INFO: speech length: 115447
2024-10-27 17:44:41,525 (beam_search:428) INFO: decoder input length: 89
2024-10-27 17:44:41,525 (beam_search:429) INFO: max output length: 89
2024-10-27 17:44:41,525 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:41,607 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:41,607 (beam_search:476) INFO:  -1.39 * 1.0 =  -1.39 for ctc
2024-10-27 17:44:41,607 (beam_search:479) INFO: total log probability: -1.39
2024-10-27 17:44:41,607 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:44:41,607 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:41,607 (beam_search:483) INFO: best hypo: ITWOULDNOTNOMORE

2024-10-27 17:44:41,609 (asr_inference:509) INFO: speech length: 36431
2024-10-27 17:44:42,991 (beam_search:428) INFO: decoder input length: 27
2024-10-27 17:44:42,991 (beam_search:429) INFO: max output length: 27
2024-10-27 17:44:42,991 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:43,005 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:43,005 (beam_search:476) INFO:  -0.44 * 1.0 =  -0.44 for ctc
2024-10-27 17:44:43,005 (beam_search:479) INFO: total log probability: -0.44
2024-10-27 17:44:43,005 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:44:43,005 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:43,005 (beam_search:483) INFO: best hypo: NOTMUCH

2024-10-27 17:44:43,007 (asr_inference:509) INFO: speech length: 20897
2024-10-27 17:44:43,998 (beam_search:428) INFO: decoder input length: 15
2024-10-27 17:44:43,998 (beam_search:429) INFO: max output length: 15
2024-10-27 17:44:43,998 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:44,015 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:44,015 (beam_search:476) INFO:  -0.02 * 1.0 =  -0.02 for ctc
2024-10-27 17:44:44,015 (beam_search:479) INFO: total log probability: -0.02
2024-10-27 17:44:44,015 (beam_search:480) INFO: normalized log probability: -0.00
2024-10-27 17:44:44,015 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:44,015 (beam_search:483) INFO: best hypo: IDON'TKNOW

2024-10-27 17:44:44,017 (asr_inference:509) INFO: speech length: 55027
2024-10-27 17:44:46,210 (beam_search:428) INFO: decoder input length: 42
2024-10-27 17:44:46,210 (beam_search:429) INFO: max output length: 42
2024-10-27 17:44:46,210 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:46,241 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:46,242 (beam_search:476) INFO:  -2.04 * 1.0 =  -2.04 for ctc
2024-10-27 17:44:46,242 (beam_search:479) INFO: total log probability: -2.04
2024-10-27 17:44:46,242 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 17:44:46,242 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:46,242 (beam_search:483) INFO: best hypo: THEOFISGOING

2024-10-27 17:44:46,244 (asr_inference:509) INFO: speech length: 123157
2024-10-27 17:44:50,728 (beam_search:428) INFO: decoder input length: 95
2024-10-27 17:44:50,728 (beam_search:429) INFO: max output length: 95
2024-10-27 17:44:50,728 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:50,857 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:50,857 (beam_search:476) INFO:  -2.11 * 1.0 =  -2.11 for ctc
2024-10-27 17:44:50,857 (beam_search:479) INFO: total log probability: -2.11
2024-10-27 17:44:50,857 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:44:50,857 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:50,857 (beam_search:483) INFO: best hypo: THEMAGNETTURNSOFFITSOFF

2024-10-27 17:44:50,859 (asr_inference:509) INFO: speech length: 44508
2024-10-27 17:44:52,666 (beam_search:428) INFO: decoder input length: 34
2024-10-27 17:44:52,667 (beam_search:429) INFO: max output length: 34
2024-10-27 17:44:52,667 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:52,705 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:52,705 (beam_search:476) INFO:  -0.39 * 1.0 =  -0.39 for ctc
2024-10-27 17:44:52,705 (beam_search:479) INFO: total log probability: -0.39
2024-10-27 17:44:52,705 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:44:52,705 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:52,705 (beam_search:483) INFO: best hypo: DON'TDON'T

2024-10-27 17:44:52,707 (asr_inference:509) INFO: speech length: 44498
2024-10-27 17:44:54,411 (beam_search:428) INFO: decoder input length: 34
2024-10-27 17:44:54,411 (beam_search:429) INFO: max output length: 34
2024-10-27 17:44:54,411 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:54,429 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:54,429 (beam_search:476) INFO:  -0.59 * 1.0 =  -0.59 for ctc
2024-10-27 17:44:54,429 (beam_search:479) INFO: total log probability: -0.59
2024-10-27 17:44:54,429 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:44:54,429 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:54,429 (beam_search:483) INFO: best hypo: ITIS

2024-10-27 17:44:54,432 (asr_inference:509) INFO: speech length: 194705
2024-10-27 17:45:02,030 (beam_search:428) INFO: decoder input length: 151
2024-10-27 17:45:02,030 (beam_search:429) INFO: max output length: 151
2024-10-27 17:45:02,030 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:02,395 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:02,395 (beam_search:476) INFO:  -3.11 * 1.0 =  -3.11 for ctc
2024-10-27 17:45:02,395 (beam_search:479) INFO: total log probability: -3.11
2024-10-27 17:45:02,395 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:45:02,395 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:02,395 (beam_search:483) INFO: best hypo: THEREAREWASHERSINTHEOTHERTHATARETOTHEMAGNETICOFTHETWO

2024-10-27 17:45:02,398 (asr_inference:509) INFO: speech length: 26869
2024-10-27 17:45:03,564 (beam_search:428) INFO: decoder input length: 20
2024-10-27 17:45:03,564 (beam_search:429) INFO: max output length: 20
2024-10-27 17:45:03,564 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:03,572 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:03,573 (beam_search:476) INFO:  -0.64 * 1.0 =  -0.64 for ctc
2024-10-27 17:45:03,573 (beam_search:479) INFO: total log probability: -0.64
2024-10-27 17:45:03,573 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:45:03,573 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:03,573 (beam_search:483) INFO: best hypo: I

2024-10-27 17:45:03,575 (asr_inference:509) INFO: speech length: 156229
2024-10-27 17:45:09,383 (beam_search:428) INFO: decoder input length: 121
2024-10-27 17:45:09,384 (beam_search:429) INFO: max output length: 121
2024-10-27 17:45:09,384 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:09,519 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:09,519 (beam_search:476) INFO:  -2.39 * 1.0 =  -2.39 for ctc
2024-10-27 17:45:09,519 (beam_search:479) INFO: total log probability: -2.39
2024-10-27 17:45:09,519 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 17:45:09,519 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:09,519 (beam_search:483) INFO: best hypo: ITHATTHEARELIKETHE

2024-10-27 17:45:09,522 (asr_inference:509) INFO: speech length: 47746
2024-10-27 17:45:11,353 (beam_search:428) INFO: decoder input length: 36
2024-10-27 17:45:11,353 (beam_search:429) INFO: max output length: 36
2024-10-27 17:45:11,353 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:11,367 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:11,367 (beam_search:476) INFO:  -0.68 * 1.0 =  -0.68 for ctc
2024-10-27 17:45:11,367 (beam_search:479) INFO: total log probability: -0.68
2024-10-27 17:45:11,367 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:45:11,367 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:11,367 (beam_search:483) INFO: best hypo: IS

2024-10-27 17:45:11,369 (asr_inference:509) INFO: speech length: 62159
2024-10-27 17:45:13,765 (beam_search:428) INFO: decoder input length: 48
2024-10-27 17:45:13,765 (beam_search:429) INFO: max output length: 48
2024-10-27 17:45:13,765 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:13,776 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:13,776 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 17:45:13,776 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 17:45:13,776 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 17:45:13,776 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:13,776 (beam_search:483) INFO: best hypo: 

2024-10-27 17:45:13,778 (asr_inference:509) INFO: speech length: 27865
2024-10-27 17:45:14,945 (beam_search:428) INFO: decoder input length: 21
2024-10-27 17:45:14,946 (beam_search:429) INFO: max output length: 21
2024-10-27 17:45:14,946 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:14,964 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:14,964 (beam_search:476) INFO:  -0.94 * 1.0 =  -0.94 for ctc
2024-10-27 17:45:14,964 (beam_search:479) INFO: total log probability: -0.94
2024-10-27 17:45:14,964 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:45:14,964 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:14,964 (beam_search:483) INFO: best hypo: WHATISALLABOUT

2024-10-27 17:45:14,966 (asr_inference:509) INFO: speech length: 72495
2024-10-27 17:45:17,663 (beam_search:428) INFO: decoder input length: 56
2024-10-27 17:45:17,664 (beam_search:429) INFO: max output length: 56
2024-10-27 17:45:17,664 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:17,732 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:17,733 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 17:45:17,733 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 17:45:17,733 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:45:17,733 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:17,733 (beam_search:483) INFO: best hypo: HOWCANIPUTTHEMTOGETHERPLEASE

2024-10-27 17:45:17,735 (asr_inference:509) INFO: speech length: 69064
2024-10-27 17:45:20,298 (beam_search:428) INFO: decoder input length: 53
2024-10-27 17:45:20,298 (beam_search:429) INFO: max output length: 53
2024-10-27 17:45:20,298 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:20,340 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:20,340 (beam_search:476) INFO:  -0.76 * 1.0 =  -0.76 for ctc
2024-10-27 17:45:20,340 (beam_search:479) INFO: total log probability: -0.76
2024-10-27 17:45:20,340 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:45:20,340 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:20,340 (beam_search:483) INFO: best hypo: THENSOMETHINGWILLI

2024-10-27 17:45:20,342 (asr_inference:509) INFO: speech length: 92547
2024-10-27 17:45:23,587 (beam_search:428) INFO: decoder input length: 71
2024-10-27 17:45:23,587 (beam_search:429) INFO: max output length: 71
2024-10-27 17:45:23,587 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:23,704 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:23,704 (beam_search:476) INFO:  -2.20 * 1.0 =  -2.20 for ctc
2024-10-27 17:45:23,704 (beam_search:479) INFO: total log probability: -2.20
2024-10-27 17:45:23,704 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:45:23,704 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:23,704 (beam_search:483) INFO: best hypo: IWHATYOU'STALKINGABOUTYOUOFME

2024-10-27 17:45:23,706 (asr_inference:509) INFO: speech length: 124649
2024-10-27 17:45:28,233 (beam_search:428) INFO: decoder input length: 96
2024-10-27 17:45:28,233 (beam_search:429) INFO: max output length: 96
2024-10-27 17:45:28,233 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:28,435 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:28,435 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 17:45:28,435 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 17:45:28,435 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:45:28,435 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:28,435 (beam_search:483) INFO: best hypo: IDIDN'TSAYTHATATALLIDIDSAYTHAT

2024-10-27 17:45:28,437 (asr_inference:509) INFO: speech length: 187820
2024-10-27 17:45:35,816 (beam_search:428) INFO: decoder input length: 146
2024-10-27 17:45:35,816 (beam_search:429) INFO: max output length: 146
2024-10-27 17:45:35,816 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:36,003 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:36,003 (beam_search:476) INFO:  -2.82 * 1.0 =  -2.82 for ctc
2024-10-27 17:45:36,003 (beam_search:479) INFO: total log probability: -2.82
2024-10-27 17:45:36,003 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 17:45:36,003 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:36,003 (beam_search:483) INFO: best hypo: SHOWITOTHETHATSIT

2024-10-27 17:45:36,006 (asr_inference:509) INFO: speech length: 144332
2024-10-27 17:45:41,639 (beam_search:428) INFO: decoder input length: 112
2024-10-27 17:45:41,639 (beam_search:429) INFO: max output length: 112
2024-10-27 17:45:41,639 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:41,923 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:41,924 (beam_search:476) INFO:  -1.87 * 1.0 =  -1.87 for ctc
2024-10-27 17:45:41,924 (beam_search:479) INFO: total log probability: -1.87
2024-10-27 17:45:41,924 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:45:41,924 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:41,924 (beam_search:483) INFO: best hypo: THAT'SWHYITWAS'TBECAUSEITTHEGETIT

2024-10-27 17:45:41,926 (asr_inference:509) INFO: speech length: 198047
2024-10-27 17:45:49,649 (beam_search:428) INFO: decoder input length: 154
2024-10-27 17:45:49,649 (beam_search:429) INFO: max output length: 154
2024-10-27 17:45:49,649 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:50,231 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:50,232 (beam_search:476) INFO:  -3.23 * 1.0 =  -3.23 for ctc
2024-10-27 17:45:50,232 (beam_search:479) INFO: total log probability: -3.23
2024-10-27 17:45:50,232 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:45:50,232 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:50,232 (beam_search:483) INFO: best hypo: SNOIJUSTDOTHISANDTHENYOUANDYOUMYHANDYOUDON'TTODOIT

2024-10-27 17:45:50,234 (asr_inference:509) INFO: speech length: 59461
2024-10-27 17:45:52,483 (beam_search:428) INFO: decoder input length: 45
2024-10-27 17:45:52,483 (beam_search:429) INFO: max output length: 45
2024-10-27 17:45:52,483 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:52,529 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:52,529 (beam_search:476) INFO:  -0.78 * 1.0 =  -0.78 for ctc
2024-10-27 17:45:52,529 (beam_search:479) INFO: total log probability: -0.78
2024-10-27 17:45:52,529 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:45:52,529 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:52,529 (beam_search:483) INFO: best hypo: IT'SNOTAMY

2024-10-27 17:45:52,532 (asr_inference:509) INFO: speech length: 151906
2024-10-27 17:45:57,954 (beam_search:428) INFO: decoder input length: 118
2024-10-27 17:45:57,955 (beam_search:429) INFO: max output length: 118
2024-10-27 17:45:57,955 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:58,144 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:58,145 (beam_search:476) INFO:  -2.86 * 1.0 =  -2.86 for ctc
2024-10-27 17:45:58,145 (beam_search:479) INFO: total log probability: -2.86
2024-10-27 17:45:58,145 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:45:58,145 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:58,145 (beam_search:483) INFO: best hypo: SORRYNOTHOWITGOESITYOU'RE

2024-10-27 17:45:58,147 (asr_inference:509) INFO: speech length: 216874
2024-10-27 17:46:07,129 (beam_search:428) INFO: decoder input length: 168
2024-10-27 17:46:07,129 (beam_search:429) INFO: max output length: 168
2024-10-27 17:46:07,129 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:07,331 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:07,331 (beam_search:476) INFO:  -1.41 * 1.0 =  -1.41 for ctc
2024-10-27 17:46:07,331 (beam_search:479) INFO: total log probability: -1.41
2024-10-27 17:46:07,332 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:46:07,332 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:07,332 (beam_search:483) INFO: best hypo: ITHASALIKEANDTOGETHER

2024-10-27 17:46:07,335 (asr_inference:509) INFO: speech length: 93108
2024-10-27 17:46:10,951 (beam_search:428) INFO: decoder input length: 72
2024-10-27 17:46:10,951 (beam_search:429) INFO: max output length: 72
2024-10-27 17:46:10,951 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:10,986 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:10,986 (beam_search:476) INFO:  -1.16 * 1.0 =  -1.16 for ctc
2024-10-27 17:46:10,986 (beam_search:479) INFO: total log probability: -1.16
2024-10-27 17:46:10,986 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 17:46:10,986 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:10,986 (beam_search:483) INFO: best hypo: WITHA

2024-10-27 17:46:10,989 (asr_inference:509) INFO: speech length: 157709
2024-10-27 17:46:16,695 (beam_search:428) INFO: decoder input length: 122
2024-10-27 17:46:16,695 (beam_search:429) INFO: max output length: 122
2024-10-27 17:46:16,695 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:16,886 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:16,887 (beam_search:476) INFO:  -1.84 * 1.0 =  -1.84 for ctc
2024-10-27 17:46:16,887 (beam_search:479) INFO: total log probability: -1.84
2024-10-27 17:46:16,887 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:46:16,887 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:16,887 (beam_search:483) INFO: best hypo: WASWITHAWASWITHSWITHA

2024-10-27 17:46:16,890 (asr_inference:509) INFO: speech length: 30944
2024-10-27 17:46:18,192 (beam_search:428) INFO: decoder input length: 23
2024-10-27 17:46:18,192 (beam_search:429) INFO: max output length: 23
2024-10-27 17:46:18,192 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:18,212 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:18,212 (beam_search:476) INFO:  -0.39 * 1.0 =  -0.39 for ctc
2024-10-27 17:46:18,212 (beam_search:479) INFO: total log probability: -0.39
2024-10-27 17:46:18,212 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:46:18,212 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:18,212 (beam_search:483) INFO: best hypo: ITGOESTOTHE

2024-10-27 17:46:18,215 (asr_inference:509) INFO: speech length: 66586
2024-10-27 17:46:20,642 (beam_search:428) INFO: decoder input length: 51
2024-10-27 17:46:20,642 (beam_search:429) INFO: max output length: 51
2024-10-27 17:46:20,642 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:20,706 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:20,707 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 17:46:20,707 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 17:46:20,707 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:46:20,707 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:20,707 (beam_search:483) INFO: best hypo: EVERYTHINGTHERE'SAINYOU

2024-10-27 17:46:20,709 (asr_inference:509) INFO: speech length: 130418
2024-10-27 17:46:25,698 (beam_search:428) INFO: decoder input length: 101
2024-10-27 17:46:25,698 (beam_search:429) INFO: max output length: 101
2024-10-27 17:46:25,698 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:25,910 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:25,910 (beam_search:476) INFO:  -1.78 * 1.0 =  -1.78 for ctc
2024-10-27 17:46:25,910 (beam_search:479) INFO: total log probability: -1.78
2024-10-27 17:46:25,910 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:46:25,910 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:25,910 (beam_search:483) INFO: best hypo: IMGOODANDIHAVEBEENABOUTANDHOWTHEY

2024-10-27 17:46:25,913 (asr_inference:509) INFO: speech length: 265623
2024-10-27 17:46:37,330 (beam_search:428) INFO: decoder input length: 207
2024-10-27 17:46:37,330 (beam_search:429) INFO: max output length: 207
2024-10-27 17:46:37,330 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:37,971 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:37,971 (beam_search:476) INFO:  -5.47 * 1.0 =  -5.47 for ctc
2024-10-27 17:46:37,971 (beam_search:479) INFO: total log probability: -5.47
2024-10-27 17:46:37,971 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 17:46:37,971 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:37,971 (beam_search:483) INFO: best hypo: THEYAREAISISTHETHEWIRESTHEENERGYANDALIGHTBULBTHEOFENERGY

2024-10-27 17:46:37,973 (asr_inference:509) INFO: speech length: 28270
2024-10-27 17:46:39,117 (beam_search:428) INFO: decoder input length: 21
2024-10-27 17:46:39,117 (beam_search:429) INFO: max output length: 21
2024-10-27 17:46:39,117 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:39,138 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:39,138 (beam_search:476) INFO:  -0.37 * 1.0 =  -0.37 for ctc
2024-10-27 17:46:39,138 (beam_search:479) INFO: total log probability: -0.37
2024-10-27 17:46:39,139 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:46:39,139 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:39,139 (beam_search:483) INFO: best hypo: I'MNOTSURE

2024-10-27 17:46:39,141 (asr_inference:509) INFO: speech length: 19387
2024-10-27 17:46:39,993 (beam_search:428) INFO: decoder input length: 14
2024-10-27 17:46:39,993 (beam_search:429) INFO: max output length: 14
2024-10-27 17:46:39,993 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:40,004 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:40,004 (beam_search:476) INFO:  -1.21 * 1.0 =  -1.21 for ctc
2024-10-27 17:46:40,004 (beam_search:479) INFO: total log probability: -1.21
2024-10-27 17:46:40,004 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:46:40,004 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:40,004 (beam_search:483) INFO: best hypo: ADBATTERY

2024-10-27 17:46:40,007 (asr_inference:509) INFO: speech length: 85956
2024-10-27 17:46:43,159 (beam_search:428) INFO: decoder input length: 66
2024-10-27 17:46:43,159 (beam_search:429) INFO: max output length: 66
2024-10-27 17:46:43,159 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:43,276 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:43,276 (beam_search:476) INFO:  -3.31 * 1.0 =  -3.31 for ctc
2024-10-27 17:46:43,276 (beam_search:479) INFO: total log probability: -3.31
2024-10-27 17:46:43,276 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 17:46:43,276 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:43,276 (beam_search:483) INFO: best hypo: WIRESANDTHEYTHEYENERGYFROMTHETOTHEBULB

2024-10-27 17:46:43,278 (asr_inference:509) INFO: speech length: 159209
2024-10-27 17:46:50,006 (beam_search:428) INFO: decoder input length: 123
2024-10-27 17:46:50,006 (beam_search:429) INFO: max output length: 123
2024-10-27 17:46:50,006 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:50,277 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:50,277 (beam_search:476) INFO:  -2.09 * 1.0 =  -2.09 for ctc
2024-10-27 17:46:50,277 (beam_search:479) INFO: total log probability: -2.09
2024-10-27 17:46:50,277 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:46:50,277 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:50,277 (beam_search:483) INFO: best hypo: THEYARETHESOURCETOTHEBATTERYTOLIGHTTHELIGHTBULB

2024-10-27 17:46:50,280 (asr_inference:509) INFO: speech length: 141831
2024-10-27 17:46:55,435 (beam_search:428) INFO: decoder input length: 110
2024-10-27 17:46:55,435 (beam_search:429) INFO: max output length: 110
2024-10-27 17:46:55,435 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:55,833 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:55,833 (beam_search:476) INFO:  -3.33 * 1.0 =  -3.33 for ctc
2024-10-27 17:46:55,833 (beam_search:479) INFO: total log probability: -3.33
2024-10-27 17:46:55,833 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:46:55,833 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:55,833 (beam_search:483) INFO: best hypo: THATTHEDCELLISASOURCEOFENERGYBECAUSEYOUCANTLIGHTALIGHTBULBWITHOUTAOFENERGY

2024-10-27 17:46:55,836 (asr_inference:509) INFO: speech length: 76508
2024-10-27 17:46:58,613 (beam_search:428) INFO: decoder input length: 59
2024-10-27 17:46:58,613 (beam_search:429) INFO: max output length: 59
2024-10-27 17:46:58,613 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:58,667 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:58,667 (beam_search:476) INFO:  -1.64 * 1.0 =  -1.64 for ctc
2024-10-27 17:46:58,668 (beam_search:479) INFO: total log probability: -1.64
2024-10-27 17:46:58,668 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:46:58,668 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:58,668 (beam_search:483) INFO: best hypo: ADENERGYINA

2024-10-27 17:46:58,670 (asr_inference:509) INFO: speech length: 84514
2024-10-27 17:47:01,756 (beam_search:428) INFO: decoder input length: 65
2024-10-27 17:47:01,756 (beam_search:429) INFO: max output length: 65
2024-10-27 17:47:01,756 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:01,829 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:01,829 (beam_search:476) INFO:  -4.00 * 1.0 =  -4.00 for ctc
2024-10-27 17:47:01,829 (beam_search:479) INFO: total log probability: -4.00
2024-10-27 17:47:01,829 (beam_search:480) INFO: normalized log probability: -0.50
2024-10-27 17:47:01,829 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:01,829 (beam_search:483) INFO: best hypo: THEYENERGYFROMTHETOTHE

2024-10-27 17:47:01,832 (asr_inference:509) INFO: speech length: 192302
2024-10-27 17:47:09,600 (beam_search:428) INFO: decoder input length: 149
2024-10-27 17:47:09,600 (beam_search:429) INFO: max output length: 149
2024-10-27 17:47:09,600 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:10,153 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:10,153 (beam_search:476) INFO:  -3.56 * 1.0 =  -3.56 for ctc
2024-10-27 17:47:10,153 (beam_search:479) INFO: total log probability: -3.56
2024-10-27 17:47:10,153 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:47:10,153 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:10,153 (beam_search:483) INFO: best hypo: ITTHATTHEWIRESAREAPATHWAYSOTHATTHEENERGYFROMTHEDCANMOVETOTHELIGHTBULB

2024-10-27 17:47:10,156 (asr_inference:509) INFO: speech length: 64758
2024-10-27 17:47:12,494 (beam_search:428) INFO: decoder input length: 50
2024-10-27 17:47:12,494 (beam_search:429) INFO: max output length: 50
2024-10-27 17:47:12,494 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:12,539 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:12,540 (beam_search:476) INFO:  -0.62 * 1.0 =  -0.62 for ctc
2024-10-27 17:47:12,540 (beam_search:479) INFO: total log probability: -0.62
2024-10-27 17:47:12,540 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:47:12,540 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:12,540 (beam_search:483) INFO: best hypo: WIRESELECTRICITYINACIRCUIT

2024-10-27 17:47:12,542 (asr_inference:509) INFO: speech length: 192292
2024-10-27 17:47:20,275 (beam_search:428) INFO: decoder input length: 149
2024-10-27 17:47:20,275 (beam_search:429) INFO: max output length: 149
2024-10-27 17:47:20,275 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:20,764 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:20,765 (beam_search:476) INFO:  -2.82 * 1.0 =  -2.82 for ctc
2024-10-27 17:47:20,765 (beam_search:479) INFO: total log probability: -2.82
2024-10-27 17:47:20,765 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:47:20,765 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:20,765 (beam_search:483) INFO: best hypo: MAKEMAKELIGHTBYUSINGATHESUPUNTILIT'SENOUGHTHATIT

2024-10-27 17:47:20,768 (asr_inference:509) INFO: speech length: 351257
2024-10-27 17:47:36,791 (beam_search:428) INFO: decoder input length: 273
2024-10-27 17:47:36,792 (beam_search:429) INFO: max output length: 273
2024-10-27 17:47:36,792 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:38,227 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:38,228 (beam_search:476) INFO:  -7.78 * 1.0 =  -7.78 for ctc
2024-10-27 17:47:38,228 (beam_search:479) INFO: total log probability: -7.78
2024-10-27 17:47:38,228 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:47:38,228 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:38,228 (beam_search:483) INFO: best hypo: THEYUSETHEDTOTOTOENERGYTHROUGHTHEANDTHENTHEENERGYTHEENERGYISGOINGTHROUGHITMAKESWHENIT'STHEISIT

2024-10-27 17:47:38,231 (asr_inference:509) INFO: speech length: 106944
2024-10-27 17:47:42,046 (beam_search:428) INFO: decoder input length: 83
2024-10-27 17:47:42,046 (beam_search:429) INFO: max output length: 83
2024-10-27 17:47:42,047 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:42,183 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:42,183 (beam_search:476) INFO:  -1.57 * 1.0 =  -1.57 for ctc
2024-10-27 17:47:42,183 (beam_search:479) INFO: total log probability: -1.57
2024-10-27 17:47:42,183 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:47:42,183 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:42,183 (beam_search:483) INFO: best hypo: ITTHATITSTHEENERGYANDSINTO

2024-10-27 17:47:42,185 (asr_inference:509) INFO: speech length: 259507
2024-10-27 17:47:52,769 (beam_search:428) INFO: decoder input length: 202
2024-10-27 17:47:52,769 (beam_search:429) INFO: max output length: 202
2024-10-27 17:47:52,769 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:53,290 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:53,290 (beam_search:476) INFO:  -2.17 * 1.0 =  -2.17 for ctc
2024-10-27 17:47:53,290 (beam_search:479) INFO: total log probability: -2.17
2024-10-27 17:47:53,291 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:47:53,291 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:53,291 (beam_search:483) INFO: best hypo: ITTHATTHEENERGYISTHROUGHANDTHATTHEENERGYISINTOENERGYENERGY

2024-10-27 17:47:53,294 (asr_inference:509) INFO: speech length: 186079
2024-10-27 17:48:00,692 (beam_search:428) INFO: decoder input length: 144
2024-10-27 17:48:00,692 (beam_search:429) INFO: max output length: 144
2024-10-27 17:48:00,692 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:01,418 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:01,418 (beam_search:476) INFO:  -1.96 * 1.0 =  -1.96 for ctc
2024-10-27 17:48:01,418 (beam_search:479) INFO: total log probability: -1.96
2024-10-27 17:48:01,418 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:48:01,418 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:01,418 (beam_search:483) INFO: best hypo: ISEETHATTHEISEETHATTHEENERGYISTHROUGHTHEWIRESTOTHELIGHTBULBANDTHATIT'SANDTHELIGHTBULBISLIGHTINGUP

2024-10-27 17:48:01,422 (asr_inference:509) INFO: speech length: 198459
2024-10-27 17:48:09,116 (beam_search:428) INFO: decoder input length: 154
2024-10-27 17:48:09,116 (beam_search:429) INFO: max output length: 154
2024-10-27 17:48:09,116 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:09,762 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:09,762 (beam_search:476) INFO:  -2.52 * 1.0 =  -2.52 for ctc
2024-10-27 17:48:09,762 (beam_search:479) INFO: total log probability: -2.52
2024-10-27 17:48:09,762 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:48:09,762 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:09,762 (beam_search:483) INFO: best hypo: THETHATTHEENERGYISISINISFROMTHEPOSITIVEANDTHETHETHATTHEENERGYISOUTOFISTHENEGATIVE

2024-10-27 17:48:09,765 (asr_inference:509) INFO: speech length: 149060
2024-10-27 17:48:15,345 (beam_search:428) INFO: decoder input length: 115
2024-10-27 17:48:15,345 (beam_search:429) INFO: max output length: 115
2024-10-27 17:48:15,345 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:15,590 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:15,590 (beam_search:476) INFO:  -0.81 * 1.0 =  -0.81 for ctc
2024-10-27 17:48:15,590 (beam_search:479) INFO: total log probability: -0.81
2024-10-27 17:48:15,590 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:48:15,590 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:15,590 (beam_search:483) INFO: best hypo: ELECTRICITYINFROMTHEPOSITIVEANDELECTRICITYOUTFROMTHENEGATIVE

2024-10-27 17:48:15,593 (asr_inference:509) INFO: speech length: 180060
2024-10-27 17:48:22,823 (beam_search:428) INFO: decoder input length: 140
2024-10-27 17:48:22,823 (beam_search:429) INFO: max output length: 140
2024-10-27 17:48:22,823 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:23,274 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:23,274 (beam_search:476) INFO:  -1.82 * 1.0 =  -1.82 for ctc
2024-10-27 17:48:23,274 (beam_search:479) INFO: total log probability: -1.82
2024-10-27 17:48:23,274 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:48:23,274 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:23,274 (beam_search:483) INFO: best hypo: THENEGATIVESIDEOFTHEOFTHETHEDCELLBATTERYISISWHERETHEENERGYISFLOWINGOUT

2024-10-27 17:48:23,277 (asr_inference:509) INFO: speech length: 51406
2024-10-27 17:48:25,124 (beam_search:428) INFO: decoder input length: 39
2024-10-27 17:48:25,124 (beam_search:429) INFO: max output length: 39
2024-10-27 17:48:25,124 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:25,159 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:25,160 (beam_search:476) INFO:  -0.94 * 1.0 =  -0.94 for ctc
2024-10-27 17:48:25,160 (beam_search:479) INFO: total log probability: -0.94
2024-10-27 17:48:25,160 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:48:25,160 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:25,160 (beam_search:483) INFO: best hypo: ELECTRICITYISGETTINGBYTHE

2024-10-27 17:48:25,162 (asr_inference:509) INFO: speech length: 91296
2024-10-27 17:48:28,467 (beam_search:428) INFO: decoder input length: 70
2024-10-27 17:48:28,467 (beam_search:429) INFO: max output length: 70
2024-10-27 17:48:28,467 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:28,555 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:28,555 (beam_search:476) INFO:  -1.15 * 1.0 =  -1.15 for ctc
2024-10-27 17:48:28,556 (beam_search:479) INFO: total log probability: -1.15
2024-10-27 17:48:28,556 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:48:28,556 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:28,556 (beam_search:483) INFO: best hypo: THEENERGYTHEPOSITIVESIDEOFTHE

2024-10-27 17:48:28,559 (asr_inference:509) INFO: speech length: 71341
2024-10-27 17:48:31,193 (beam_search:428) INFO: decoder input length: 55
2024-10-27 17:48:31,193 (beam_search:429) INFO: max output length: 55
2024-10-27 17:48:31,193 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:31,234 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:31,235 (beam_search:476) INFO:  -1.31 * 1.0 =  -1.31 for ctc
2024-10-27 17:48:31,235 (beam_search:479) INFO: total log probability: -1.31
2024-10-27 17:48:31,235 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:48:31,235 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:31,235 (beam_search:483) INFO: best hypo: THEBLUEELECTRICITYFLOWING

2024-10-27 17:48:31,237 (asr_inference:509) INFO: speech length: 225061
2024-10-27 17:48:40,364 (beam_search:428) INFO: decoder input length: 175
2024-10-27 17:48:40,364 (beam_search:429) INFO: max output length: 175
2024-10-27 17:48:40,364 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:40,904 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:40,904 (beam_search:476) INFO:  -2.10 * 1.0 =  -2.10 for ctc
2024-10-27 17:48:40,904 (beam_search:479) INFO: total log probability: -2.10
2024-10-27 17:48:40,904 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:48:40,904 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:40,905 (beam_search:483) INFO: best hypo: THEENERGYINTOTHEPOSITIVESIDEANDTHEENERGYOUTFROMTHEPOSITIVEFROMTHESIDE

2024-10-27 17:48:40,907 (asr_inference:509) INFO: speech length: 93843
2024-10-27 17:48:44,377 (beam_search:428) INFO: decoder input length: 72
2024-10-27 17:48:44,377 (beam_search:429) INFO: max output length: 72
2024-10-27 17:48:44,377 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:44,502 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:44,503 (beam_search:476) INFO:  -2.52 * 1.0 =  -2.52 for ctc
2024-10-27 17:48:44,503 (beam_search:479) INFO: total log probability: -2.52
2024-10-27 17:48:44,503 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:48:44,503 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:44,503 (beam_search:483) INFO: best hypo: THATTHELIGHTBULBWORKSINBOTHTHETHEARE

2024-10-27 17:48:44,505 (asr_inference:509) INFO: speech length: 391132
2024-10-27 17:49:02,650 (beam_search:428) INFO: decoder input length: 305
2024-10-27 17:49:02,650 (beam_search:429) INFO: max output length: 305
2024-10-27 17:49:02,650 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:03,096 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:03,097 (beam_search:476) INFO:  -2.06 * 1.0 =  -2.06 for ctc
2024-10-27 17:49:03,097 (beam_search:479) INFO: total log probability: -2.06
2024-10-27 17:49:03,097 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:49:03,097 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:03,097 (beam_search:483) INFO: best hypo: THEELECTRICITYISFLOWINGAWAYFROMTHESIDE

2024-10-27 17:49:03,099 (asr_inference:509) INFO: speech length: 143961
2024-10-27 17:49:08,373 (beam_search:428) INFO: decoder input length: 111
2024-10-27 17:49:08,373 (beam_search:429) INFO: max output length: 111
2024-10-27 17:49:08,373 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:08,684 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:08,685 (beam_search:476) INFO:  -3.98 * 1.0 =  -3.98 for ctc
2024-10-27 17:49:08,685 (beam_search:479) INFO: total log probability: -3.98
2024-10-27 17:49:08,685 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:49:08,685 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:08,685 (beam_search:483) INFO: best hypo: THATONEOFTHEISTHEBULBANDONEOFTHEWIRESISTOUCHINGTHEBULB

2024-10-27 17:49:08,687 (asr_inference:509) INFO: speech length: 168688
2024-10-27 17:49:15,070 (beam_search:428) INFO: decoder input length: 131
2024-10-27 17:49:15,070 (beam_search:429) INFO: max output length: 131
2024-10-27 17:49:15,070 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:15,452 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:15,452 (beam_search:476) INFO:  -4.07 * 1.0 =  -4.07 for ctc
2024-10-27 17:49:15,452 (beam_search:479) INFO: total log probability: -4.07
2024-10-27 17:49:15,452 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:49:15,452 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:15,452 (beam_search:483) INFO: best hypo: THATONEOFTHEWIRESISTOUCHINGTHEANDONEOFTHEWIRESISTHEBULB

2024-10-27 17:49:15,455 (asr_inference:509) INFO: speech length: 257346
2024-10-27 17:49:26,050 (beam_search:428) INFO: decoder input length: 200
2024-10-27 17:49:26,050 (beam_search:429) INFO: max output length: 200
2024-10-27 17:49:26,050 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:27,021 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:27,021 (beam_search:476) INFO:  -6.11 * 1.0 =  -6.11 for ctc
2024-10-27 17:49:27,021 (beam_search:479) INFO: total log probability: -6.11
2024-10-27 17:49:27,021 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:49:27,021 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:27,021 (beam_search:483) INFO: best hypo: THEWIRESHAVETOBETHENEGATIVEANDPOSITIVEOFTHETOELECTRICITYFLOWTHROUGHANDTHEOTHEROFTHEWIREHASTOTOTOUCHTHEANDTHE

2024-10-27 17:49:27,024 (asr_inference:509) INFO: speech length: 24433
2024-10-27 17:49:28,043 (beam_search:428) INFO: decoder input length: 18
2024-10-27 17:49:28,043 (beam_search:429) INFO: max output length: 18
2024-10-27 17:49:28,043 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:28,059 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:28,059 (beam_search:476) INFO:  -1.39 * 1.0 =  -1.39 for ctc
2024-10-27 17:49:28,059 (beam_search:479) INFO: total log probability: -1.39
2024-10-27 17:49:28,059 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:49:28,059 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:28,059 (beam_search:483) INFO: best hypo: ITWOULDNOTWORK

2024-10-27 17:49:28,061 (asr_inference:509) INFO: speech length: 107814
2024-10-27 17:49:31,919 (beam_search:428) INFO: decoder input length: 83
2024-10-27 17:49:31,919 (beam_search:429) INFO: max output length: 83
2024-10-27 17:49:31,919 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:32,081 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:32,081 (beam_search:476) INFO:  -3.42 * 1.0 =  -3.42 for ctc
2024-10-27 17:49:32,081 (beam_search:479) INFO: total log probability: -3.42
2024-10-27 17:49:32,081 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:49:32,081 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:32,081 (beam_search:483) INFO: best hypo: THATTHETWOWIRESHAVETOBETHEBULBANDTHE

2024-10-27 17:49:32,083 (asr_inference:509) INFO: speech length: 402462
2024-10-27 17:49:50,743 (beam_search:428) INFO: decoder input length: 313
2024-10-27 17:49:50,743 (beam_search:429) INFO: max output length: 313
2024-10-27 17:49:50,743 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:51,988 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:51,988 (beam_search:476) INFO:  -7.05 * 1.0 =  -7.05 for ctc
2024-10-27 17:49:51,990 (beam_search:479) INFO: total log probability: -7.05
2024-10-27 17:49:51,990 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 17:49:51,990 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:51,990 (beam_search:483) INFO: best hypo: THETHETHEARETOUCHINGTHEONTHEISISNOTBECAUSEBECAUSETHEYDON'TLETTHETHROUGH

2024-10-27 17:49:51,993 (asr_inference:509) INFO: speech length: 108925
2024-10-27 17:49:55,862 (beam_search:428) INFO: decoder input length: 84
2024-10-27 17:49:55,862 (beam_search:429) INFO: max output length: 84
2024-10-27 17:49:55,862 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:55,969 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:55,969 (beam_search:476) INFO:  -1.73 * 1.0 =  -1.73 for ctc
2024-10-27 17:49:55,969 (beam_search:479) INFO: total log probability: -1.73
2024-10-27 17:49:55,969 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:49:55,969 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:55,969 (beam_search:483) INFO: best hypo: WEHAVEBEENLEARNINGABOUTANDOTHER

2024-10-27 17:49:55,971 (asr_inference:509) INFO: speech length: 225573
2024-10-27 17:50:05,020 (beam_search:428) INFO: decoder input length: 175
2024-10-27 17:50:05,020 (beam_search:429) INFO: max output length: 175
2024-10-27 17:50:05,020 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:05,437 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:05,437 (beam_search:476) INFO:  -4.30 * 1.0 =  -4.30 for ctc
2024-10-27 17:50:05,437 (beam_search:479) INFO: total log probability: -4.30
2024-10-27 17:50:05,437 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 17:50:05,437 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:05,437 (beam_search:483) INFO: best hypo: THEISISINGTHEENERGYTOTHETHEWIRESANDTHENTHE

2024-10-27 17:50:05,440 (asr_inference:509) INFO: speech length: 155180
2024-10-27 17:50:11,161 (beam_search:428) INFO: decoder input length: 120
2024-10-27 17:50:11,161 (beam_search:429) INFO: max output length: 120
2024-10-27 17:50:11,161 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:11,462 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:11,462 (beam_search:476) INFO:  -1.62 * 1.0 =  -1.62 for ctc
2024-10-27 17:50:11,462 (beam_search:479) INFO: total log probability: -1.62
2024-10-27 17:50:11,462 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:50:11,462 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:11,462 (beam_search:483) INFO: best hypo: THEISANDITISIMPORTANTBECAUSEITTHATENERGYISTHROUGHTHEWIRES

2024-10-27 17:50:11,466 (asr_inference:509) INFO: speech length: 133339
2024-10-27 17:50:16,286 (beam_search:428) INFO: decoder input length: 103
2024-10-27 17:50:16,286 (beam_search:429) INFO: max output length: 103
2024-10-27 17:50:16,286 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:16,419 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:16,419 (beam_search:476) INFO:  -2.08 * 1.0 =  -2.08 for ctc
2024-10-27 17:50:16,419 (beam_search:479) INFO: total log probability: -2.08
2024-10-27 17:50:16,419 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:50:16,419 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:16,419 (beam_search:483) INFO: best hypo: THEITHATTHEENERGYISTO

2024-10-27 17:50:16,421 (asr_inference:509) INFO: speech length: 101956
2024-10-27 17:50:20,106 (beam_search:428) INFO: decoder input length: 79
2024-10-27 17:50:20,106 (beam_search:429) INFO: max output length: 79
2024-10-27 17:50:20,106 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:20,273 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:20,273 (beam_search:476) INFO:  -0.91 * 1.0 =  -0.91 for ctc
2024-10-27 17:50:20,273 (beam_search:479) INFO: total log probability: -0.91
2024-10-27 17:50:20,273 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:50:20,273 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:20,273 (beam_search:483) INFO: best hypo: ITCOULDBETHEWIRESOTHATTHEENERGYCANGOTHROUGH

2024-10-27 17:50:20,276 (asr_inference:509) INFO: speech length: 44118
2024-10-27 17:50:21,982 (beam_search:428) INFO: decoder input length: 33
2024-10-27 17:50:21,983 (beam_search:429) INFO: max output length: 33
2024-10-27 17:50:21,983 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:22,005 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:22,005 (beam_search:476) INFO:  -0.30 * 1.0 =  -0.30 for ctc
2024-10-27 17:50:22,005 (beam_search:479) INFO: total log probability: -0.30
2024-10-27 17:50:22,005 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:50:22,005 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:22,005 (beam_search:483) INFO: best hypo: THETHEENERGY

2024-10-27 17:50:22,007 (asr_inference:509) INFO: speech length: 41119
2024-10-27 17:50:23,549 (beam_search:428) INFO: decoder input length: 31
2024-10-27 17:50:23,549 (beam_search:429) INFO: max output length: 31
2024-10-27 17:50:23,549 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:23,569 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:23,570 (beam_search:476) INFO:  -1.31 * 1.0 =  -1.31 for ctc
2024-10-27 17:50:23,570 (beam_search:479) INFO: total log probability: -1.31
2024-10-27 17:50:23,570 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:50:23,570 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:23,570 (beam_search:483) INFO: best hypo: THISISA

2024-10-27 17:50:23,572 (asr_inference:509) INFO: speech length: 27884
2024-10-27 17:50:24,713 (beam_search:428) INFO: decoder input length: 21
2024-10-27 17:50:24,713 (beam_search:429) INFO: max output length: 21
2024-10-27 17:50:24,713 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:24,731 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:24,731 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 17:50:24,731 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 17:50:24,731 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:50:24,731 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:24,731 (beam_search:483) INFO: best hypo: THESWITCHISOFF

2024-10-27 17:50:24,734 (asr_inference:509) INFO: speech length: 57002
2024-10-27 17:50:26,979 (beam_search:428) INFO: decoder input length: 44
2024-10-27 17:50:26,979 (beam_search:429) INFO: max output length: 44
2024-10-27 17:50:26,979 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:27,019 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:27,019 (beam_search:476) INFO:  -0.99 * 1.0 =  -0.99 for ctc
2024-10-27 17:50:27,019 (beam_search:479) INFO: total log probability: -0.99
2024-10-27 17:50:27,019 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:50:27,019 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:27,019 (beam_search:483) INFO: best hypo: THEISTHEISMOVING

2024-10-27 17:50:27,021 (asr_inference:509) INFO: speech length: 116200
2024-10-27 17:50:31,332 (beam_search:428) INFO: decoder input length: 90
2024-10-27 17:50:31,332 (beam_search:429) INFO: max output length: 90
2024-10-27 17:50:31,332 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:31,468 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:31,468 (beam_search:476) INFO:  -1.61 * 1.0 =  -1.61 for ctc
2024-10-27 17:50:31,468 (beam_search:479) INFO: total log probability: -1.61
2024-10-27 17:50:31,468 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:50:31,468 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:31,469 (beam_search:483) INFO: best hypo: BECAUSETHESWITCHLETSTHEENERGYTHROUGH

2024-10-27 17:50:31,472 (asr_inference:509) INFO: speech length: 107371
2024-10-27 17:50:35,336 (beam_search:428) INFO: decoder input length: 83
2024-10-27 17:50:35,336 (beam_search:429) INFO: max output length: 83
2024-10-27 17:50:35,336 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:35,520 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:35,520 (beam_search:476) INFO:  -0.61 * 1.0 =  -0.61 for ctc
2024-10-27 17:50:35,520 (beam_search:479) INFO: total log probability: -0.61
2024-10-27 17:50:35,520 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:50:35,520 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:35,520 (beam_search:483) INFO: best hypo: WHENTHESWITCHISOPENTHETHEELECTRICITYISNOTFLOWINGTHROUGH

2024-10-27 17:50:35,522 (asr_inference:509) INFO: speech length: 116319
2024-10-27 17:50:39,761 (beam_search:428) INFO: decoder input length: 90
2024-10-27 17:50:39,761 (beam_search:429) INFO: max output length: 90
2024-10-27 17:50:39,761 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:39,871 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:39,872 (beam_search:476) INFO:  -0.41 * 1.0 =  -0.41 for ctc
2024-10-27 17:50:39,872 (beam_search:479) INFO: total log probability: -0.41
2024-10-27 17:50:39,872 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:50:39,872 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:39,872 (beam_search:483) INFO: best hypo: THETHESWITCHHASTOBE

2024-10-27 17:50:39,874 (asr_inference:509) INFO: speech length: 192340
2024-10-27 17:50:47,340 (beam_search:428) INFO: decoder input length: 149
2024-10-27 17:50:47,340 (beam_search:429) INFO: max output length: 149
2024-10-27 17:50:47,340 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:47,783 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:47,783 (beam_search:476) INFO:  -4.34 * 1.0 =  -4.34 for ctc
2024-10-27 17:50:47,783 (beam_search:479) INFO: total log probability: -4.34
2024-10-27 17:50:47,783 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:50:47,783 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:47,784 (beam_search:483) INFO: best hypo: ITHATEVERYTIMETHESWITCHISOPENTHETHEANDTHESWITCHISTHEMOTOR

2024-10-27 17:50:47,786 (asr_inference:509) INFO: speech length: 79280
2024-10-27 17:50:50,667 (beam_search:428) INFO: decoder input length: 61
2024-10-27 17:50:50,667 (beam_search:429) INFO: max output length: 61
2024-10-27 17:50:50,667 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:50,752 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:50,753 (beam_search:476) INFO:  -0.95 * 1.0 =  -0.95 for ctc
2024-10-27 17:50:50,753 (beam_search:479) INFO: total log probability: -0.95
2024-10-27 17:50:50,753 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:50:50,753 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:50,753 (beam_search:483) INFO: best hypo: THEYENERGYSOANDOTHERTHINGSCANRUN

2024-10-27 17:50:50,755 (asr_inference:509) INFO: speech length: 164385
2024-10-27 17:50:56,940 (beam_search:428) INFO: decoder input length: 127
2024-10-27 17:50:56,941 (beam_search:429) INFO: max output length: 127
2024-10-27 17:50:56,941 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:57,274 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:57,274 (beam_search:476) INFO:  -3.26 * 1.0 =  -3.26 for ctc
2024-10-27 17:50:57,274 (beam_search:479) INFO: total log probability: -3.26
2024-10-27 17:50:57,274 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:50:57,274 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:57,274 (beam_search:483) INFO: best hypo: THESOLARISINGFROMTHESUNANDTHATTOENERGYTOPOWERTHE

2024-10-27 17:50:57,276 (asr_inference:509) INFO: speech length: 146457
2024-10-27 17:51:02,712 (beam_search:428) INFO: decoder input length: 113
2024-10-27 17:51:02,712 (beam_search:429) INFO: max output length: 113
2024-10-27 17:51:02,712 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:02,990 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:02,990 (beam_search:476) INFO:  -3.88 * 1.0 =  -3.88 for ctc
2024-10-27 17:51:02,990 (beam_search:479) INFO: total log probability: -3.88
2024-10-27 17:51:02,990 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:51:02,991 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:02,991 (beam_search:483) INFO: best hypo: THEMOTORTHESOLARCELLHASTOTHEINFORTHEMOTORTO

2024-10-27 17:51:02,993 (asr_inference:509) INFO: speech length: 74245
2024-10-27 17:51:05,733 (beam_search:428) INFO: decoder input length: 57
2024-10-27 17:51:05,733 (beam_search:429) INFO: max output length: 57
2024-10-27 17:51:05,733 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:05,791 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:05,791 (beam_search:476) INFO:  -1.82 * 1.0 =  -1.82 for ctc
2024-10-27 17:51:05,791 (beam_search:479) INFO: total log probability: -1.82
2024-10-27 17:51:05,791 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:51:05,792 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:05,792 (beam_search:483) INFO: best hypo: THEISTHENTHESOLAR

2024-10-27 17:51:05,794 (asr_inference:509) INFO: speech length: 129851
2024-10-27 17:51:10,447 (beam_search:428) INFO: decoder input length: 100
2024-10-27 17:51:10,448 (beam_search:429) INFO: max output length: 100
2024-10-27 17:51:10,448 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:10,526 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:10,526 (beam_search:476) INFO:  -0.44 * 1.0 =  -0.44 for ctc
2024-10-27 17:51:10,526 (beam_search:479) INFO: total log probability: -0.44
2024-10-27 17:51:10,526 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:51:10,526 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:10,527 (beam_search:483) INFO: best hypo: ISIMPORTANTISTHAT

2024-10-27 17:51:10,529 (asr_inference:509) INFO: speech length: 18056
2024-10-27 17:51:11,350 (beam_search:428) INFO: decoder input length: 13
2024-10-27 17:51:11,351 (beam_search:429) INFO: max output length: 13
2024-10-27 17:51:11,351 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:11,363 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:11,363 (beam_search:476) INFO:  -1.20 * 1.0 =  -1.20 for ctc
2024-10-27 17:51:11,363 (beam_search:479) INFO: total log probability: -1.20
2024-10-27 17:51:11,363 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:51:11,363 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:11,363 (beam_search:483) INFO: best hypo: THEMOTOR'T

2024-10-27 17:51:11,365 (asr_inference:509) INFO: speech length: 163607
2024-10-27 17:51:17,538 (beam_search:428) INFO: decoder input length: 127
2024-10-27 17:51:17,538 (beam_search:429) INFO: max output length: 127
2024-10-27 17:51:17,538 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:17,825 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:17,825 (beam_search:476) INFO:  -0.46 * 1.0 =  -0.46 for ctc
2024-10-27 17:51:17,825 (beam_search:479) INFO: total log probability: -0.46
2024-10-27 17:51:17,825 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 17:51:17,825 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:17,825 (beam_search:483) INFO: best hypo: THEMOTORDOESN'TRUNWHENITISITIS

2024-10-27 17:51:17,827 (asr_inference:509) INFO: speech length: 116416
2024-10-27 17:51:22,147 (beam_search:428) INFO: decoder input length: 90
2024-10-27 17:51:22,147 (beam_search:429) INFO: max output length: 90
2024-10-27 17:51:22,147 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:22,471 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:22,472 (beam_search:476) INFO:  -1.81 * 1.0 =  -1.81 for ctc
2024-10-27 17:51:22,472 (beam_search:479) INFO: total log probability: -1.81
2024-10-27 17:51:22,472 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:51:22,472 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:22,472 (beam_search:483) INFO: best hypo: WHENTHEISOUTTHEMOTORBUTIFIT'SORTHERE'SNOTHEMOTORWON'T

2024-10-27 17:51:22,474 (asr_inference:509) INFO: speech length: 140310
2024-10-27 17:51:27,477 (beam_search:428) INFO: decoder input length: 109
2024-10-27 17:51:27,477 (beam_search:429) INFO: max output length: 109
2024-10-27 17:51:27,477 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:27,793 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:27,793 (beam_search:476) INFO:  -2.99 * 1.0 =  -2.99 for ctc
2024-10-27 17:51:27,793 (beam_search:479) INFO: total log probability: -2.99
2024-10-27 17:51:27,793 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:51:27,793 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:27,793 (beam_search:483) INFO: best hypo: WHENTHERE'SNOSUNTHEREISWHENTHEREISNOSUNTHEMOTORNOT

2024-10-27 17:51:27,795 (asr_inference:509) INFO: speech length: 64101
2024-10-27 17:51:30,284 (beam_search:428) INFO: decoder input length: 49
2024-10-27 17:51:30,284 (beam_search:429) INFO: max output length: 49
2024-10-27 17:51:30,284 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:30,361 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:30,361 (beam_search:476) INFO:  -1.02 * 1.0 =  -1.02 for ctc
2024-10-27 17:51:30,361 (beam_search:479) INFO: total log probability: -1.02
2024-10-27 17:51:30,361 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:51:30,361 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:30,362 (beam_search:483) INFO: best hypo: THESOLARCELLISNOTGETTINGENOUGHENERGY

2024-10-27 17:51:30,364 (asr_inference:509) INFO: speech length: 100460
2024-10-27 17:51:34,043 (beam_search:428) INFO: decoder input length: 77
2024-10-27 17:51:34,043 (beam_search:429) INFO: max output length: 77
2024-10-27 17:51:34,043 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:34,121 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:34,121 (beam_search:476) INFO:  -1.04 * 1.0 =  -1.04 for ctc
2024-10-27 17:51:34,121 (beam_search:479) INFO: total log probability: -1.04
2024-10-27 17:51:34,121 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:51:34,121 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:34,121 (beam_search:483) INFO: best hypo: THISISTHECIRCUITTHE

2024-10-27 17:51:34,124 (asr_inference:509) INFO: speech length: 54551
2024-10-27 17:51:36,098 (beam_search:428) INFO: decoder input length: 42
2024-10-27 17:51:36,098 (beam_search:429) INFO: max output length: 42
2024-10-27 17:51:36,098 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:36,163 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:36,163 (beam_search:476) INFO:  -1.65 * 1.0 =  -1.65 for ctc
2024-10-27 17:51:36,163 (beam_search:479) INFO: total log probability: -1.65
2024-10-27 17:51:36,163 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:51:36,163 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:36,163 (beam_search:483) INFO: best hypo: WE'VEBEENLEARNINGABOUTCIRCUITSANDOTHERTHINGS

2024-10-27 17:51:36,166 (asr_inference:509) INFO: speech length: 56710
2024-10-27 17:51:38,307 (beam_search:428) INFO: decoder input length: 43
2024-10-27 17:51:38,307 (beam_search:429) INFO: max output length: 43
2024-10-27 17:51:38,307 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:38,334 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:38,334 (beam_search:476) INFO:  -0.14 * 1.0 =  -0.14 for ctc
2024-10-27 17:51:38,334 (beam_search:479) INFO: total log probability: -0.14
2024-10-27 17:51:38,334 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 17:51:38,334 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:38,335 (beam_search:483) INFO: best hypo: ACIRCUITIS

2024-10-27 17:51:38,337 (asr_inference:509) INFO: speech length: 94510
2024-10-27 17:51:41,797 (beam_search:428) INFO: decoder input length: 73
2024-10-27 17:51:41,797 (beam_search:429) INFO: max output length: 73
2024-10-27 17:51:41,797 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:41,858 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:41,859 (beam_search:476) INFO:  -1.31 * 1.0 =  -1.31 for ctc
2024-10-27 17:51:41,859 (beam_search:479) INFO: total log probability: -1.31
2024-10-27 17:51:41,859 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:51:41,859 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:41,859 (beam_search:483) INFO: best hypo: ISANAILTHE

2024-10-27 17:51:41,861 (asr_inference:509) INFO: speech length: 57636
2024-10-27 17:51:44,008 (beam_search:428) INFO: decoder input length: 44
2024-10-27 17:51:44,008 (beam_search:429) INFO: max output length: 44
2024-10-27 17:51:44,008 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:44,043 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:44,043 (beam_search:476) INFO:  -1.92 * 1.0 =  -1.92 for ctc
2024-10-27 17:51:44,043 (beam_search:479) INFO: total log probability: -1.92
2024-10-27 17:51:44,043 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 17:51:44,043 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:44,043 (beam_search:483) INFO: best hypo: ALLMETALCONDUCTELECTRICITY

2024-10-27 17:51:44,045 (asr_inference:509) INFO: speech length: 129184
2024-10-27 17:51:48,695 (beam_search:428) INFO: decoder input length: 100
2024-10-27 17:51:48,695 (beam_search:429) INFO: max output length: 100
2024-10-27 17:51:48,695 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:48,821 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:48,821 (beam_search:476) INFO:  -0.75 * 1.0 =  -0.75 for ctc
2024-10-27 17:51:48,821 (beam_search:479) INFO: total log probability: -0.75
2024-10-27 17:51:48,821 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:51:48,821 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:48,821 (beam_search:483) INFO: best hypo: ALLOFTHEINTHEAREARE

2024-10-27 17:51:48,824 (asr_inference:509) INFO: speech length: 52227
2024-10-27 17:51:50,732 (beam_search:428) INFO: decoder input length: 40
2024-10-27 17:51:50,732 (beam_search:429) INFO: max output length: 40
2024-10-27 17:51:50,732 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:50,752 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:50,752 (beam_search:476) INFO:  -0.41 * 1.0 =  -0.41 for ctc
2024-10-27 17:51:50,752 (beam_search:479) INFO: total log probability: -0.41
2024-10-27 17:51:50,752 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:51:50,752 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:50,752 (beam_search:483) INFO: best hypo: ALLELECTRICITY

2024-10-27 17:51:50,754 (asr_inference:509) INFO: speech length: 32801
2024-10-27 17:51:52,037 (beam_search:428) INFO: decoder input length: 25
2024-10-27 17:51:52,037 (beam_search:429) INFO: max output length: 25
2024-10-27 17:51:52,037 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:52,048 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:52,048 (beam_search:476) INFO:  -0.50 * 1.0 =  -0.50 for ctc
2024-10-27 17:51:52,048 (beam_search:479) INFO: total log probability: -0.50
2024-10-27 17:51:52,048 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:51:52,048 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:52,048 (beam_search:483) INFO: best hypo: THE

2024-10-27 17:51:52,051 (asr_inference:509) INFO: speech length: 64380
2024-10-27 17:51:54,533 (beam_search:428) INFO: decoder input length: 49
2024-10-27 17:51:54,534 (beam_search:429) INFO: max output length: 49
2024-10-27 17:51:54,534 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:54,561 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:54,561 (beam_search:476) INFO:  -1.11 * 1.0 =  -1.11 for ctc
2024-10-27 17:51:54,561 (beam_search:479) INFO: total log probability: -1.11
2024-10-27 17:51:54,561 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 17:51:54,561 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:54,561 (beam_search:483) INFO: best hypo: IA

2024-10-27 17:51:54,564 (asr_inference:509) INFO: speech length: 131550
2024-10-27 17:51:59,507 (beam_search:428) INFO: decoder input length: 102
2024-10-27 17:51:59,508 (beam_search:429) INFO: max output length: 102
2024-10-27 17:51:59,508 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:59,571 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:59,572 (beam_search:476) INFO:  -1.26 * 1.0 =  -1.26 for ctc
2024-10-27 17:51:59,572 (beam_search:479) INFO: total log probability: -1.26
2024-10-27 17:51:59,572 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:51:59,572 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:59,572 (beam_search:483) INFO: best hypo: ALLALLELECTRICITY

2024-10-27 17:51:59,574 (asr_inference:509) INFO: speech length: 31749
2024-10-27 17:52:00,822 (beam_search:428) INFO: decoder input length: 24
2024-10-27 17:52:00,822 (beam_search:429) INFO: max output length: 24
2024-10-27 17:52:00,822 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:00,838 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:00,838 (beam_search:476) INFO:  -0.15 * 1.0 =  -0.15 for ctc
2024-10-27 17:52:00,838 (beam_search:479) INFO: total log probability: -0.15
2024-10-27 17:52:00,838 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 17:52:00,838 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:00,838 (beam_search:483) INFO: best hypo: ITISNOT

2024-10-27 17:52:00,840 (asr_inference:509) INFO: speech length: 151965
2024-10-27 17:52:06,383 (beam_search:428) INFO: decoder input length: 118
2024-10-27 17:52:06,383 (beam_search:429) INFO: max output length: 118
2024-10-27 17:52:06,383 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:06,493 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:06,493 (beam_search:476) INFO:  -2.00 * 1.0 =  -2.00 for ctc
2024-10-27 17:52:06,493 (beam_search:479) INFO: total log probability: -2.00
2024-10-27 17:52:06,493 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 17:52:06,494 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:06,494 (beam_search:483) INFO: best hypo: AWITHTHEWILLNOT

2024-10-27 17:52:06,496 (asr_inference:509) INFO: speech length: 258082
2024-10-27 17:52:17,623 (beam_search:428) INFO: decoder input length: 201
2024-10-27 17:52:17,623 (beam_search:429) INFO: max output length: 201
2024-10-27 17:52:17,623 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:18,312 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:18,312 (beam_search:476) INFO:  -2.33 * 1.0 =  -2.33 for ctc
2024-10-27 17:52:18,312 (beam_search:479) INFO: total log probability: -2.33
2024-10-27 17:52:18,313 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:52:18,313 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:18,313 (beam_search:483) INFO: best hypo: THETHATAREDON'TWORKWITHTHECIRCUITANDANDTHEAREANDTHEINSULATORSARENOT

2024-10-27 17:52:18,315 (asr_inference:509) INFO: speech length: 113471
2024-10-27 17:52:22,503 (beam_search:428) INFO: decoder input length: 88
2024-10-27 17:52:22,504 (beam_search:429) INFO: max output length: 88
2024-10-27 17:52:22,504 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:22,635 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:22,635 (beam_search:476) INFO:  -1.59 * 1.0 =  -1.59 for ctc
2024-10-27 17:52:22,635 (beam_search:479) INFO: total log probability: -1.59
2024-10-27 17:52:22,635 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:52:22,635 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:22,635 (beam_search:483) INFO: best hypo: ARENOTMADEOFTHECIRCUITWILLNOT

2024-10-27 17:52:22,637 (asr_inference:509) INFO: speech length: 33068
2024-10-27 17:52:24,015 (beam_search:428) INFO: decoder input length: 25
2024-10-27 17:52:24,015 (beam_search:429) INFO: max output length: 25
2024-10-27 17:52:24,015 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:24,035 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:24,035 (beam_search:476) INFO:  -0.29 * 1.0 =  -0.29 for ctc
2024-10-27 17:52:24,035 (beam_search:479) INFO: total log probability: -0.29
2024-10-27 17:52:24,036 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:52:24,036 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:24,036 (beam_search:483) INFO: best hypo: THEMOTORNOTRUN

2024-10-27 17:52:24,037 (asr_inference:509) INFO: speech length: 124422
2024-10-27 17:52:28,477 (beam_search:428) INFO: decoder input length: 96
2024-10-27 17:52:28,477 (beam_search:429) INFO: max output length: 96
2024-10-27 17:52:28,477 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:28,630 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:28,630 (beam_search:476) INFO:  -2.72 * 1.0 =  -2.72 for ctc
2024-10-27 17:52:28,630 (beam_search:479) INFO: total log probability: -2.72
2024-10-27 17:52:28,630 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:52:28,630 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:28,630 (beam_search:483) INFO: best hypo: WHENITOACIRCUITTHECIRCUITISNOT

2024-10-27 17:52:28,633 (asr_inference:509) INFO: speech length: 39443
2024-10-27 17:52:30,222 (beam_search:428) INFO: decoder input length: 30
2024-10-27 17:52:30,222 (beam_search:429) INFO: max output length: 30
2024-10-27 17:52:30,222 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:30,241 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:30,241 (beam_search:476) INFO:  -0.26 * 1.0 =  -0.26 for ctc
2024-10-27 17:52:30,241 (beam_search:479) INFO: total log probability: -0.26
2024-10-27 17:52:30,241 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:52:30,241 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:30,241 (beam_search:483) INFO: best hypo: THEISOPEN

2024-10-27 17:52:30,243 (asr_inference:509) INFO: speech length: 41953
2024-10-27 17:52:31,879 (beam_search:428) INFO: decoder input length: 32
2024-10-27 17:52:31,880 (beam_search:429) INFO: max output length: 32
2024-10-27 17:52:31,880 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:31,893 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:31,893 (beam_search:476) INFO:  -0.26 * 1.0 =  -0.26 for ctc
2024-10-27 17:52:31,893 (beam_search:479) INFO: total log probability: -0.26
2024-10-27 17:52:31,893 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:52:31,893 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:31,893 (beam_search:483) INFO: best hypo: ARE

2024-10-27 17:52:31,896 (asr_inference:509) INFO: speech length: 78258
2024-10-27 17:52:34,889 (beam_search:428) INFO: decoder input length: 60
2024-10-27 17:52:34,889 (beam_search:429) INFO: max output length: 60
2024-10-27 17:52:34,889 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:34,963 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:34,963 (beam_search:476) INFO:  -1.96 * 1.0 =  -1.96 for ctc
2024-10-27 17:52:34,963 (beam_search:479) INFO: total log probability: -1.96
2024-10-27 17:52:34,963 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:52:34,963 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:34,963 (beam_search:483) INFO: best hypo: WEHAVEBEENABOUTHOWENERGYIS

2024-10-27 17:52:34,965 (asr_inference:509) INFO: speech length: 118301
2024-10-27 17:52:39,254 (beam_search:428) INFO: decoder input length: 91
2024-10-27 17:52:39,254 (beam_search:429) INFO: max output length: 91
2024-10-27 17:52:39,254 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:39,371 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:39,371 (beam_search:476) INFO:  -1.19 * 1.0 =  -1.19 for ctc
2024-10-27 17:52:39,371 (beam_search:479) INFO: total log probability: -1.19
2024-10-27 17:52:39,371 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:52:39,371 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:39,371 (beam_search:483) INFO: best hypo: ITITSTHINGSLIKESAND

2024-10-27 17:52:39,373 (asr_inference:509) INFO: speech length: 59919
2024-10-27 17:52:41,527 (beam_search:428) INFO: decoder input length: 46
2024-10-27 17:52:41,527 (beam_search:429) INFO: max output length: 46
2024-10-27 17:52:41,527 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:41,568 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:41,569 (beam_search:476) INFO:  -0.88 * 1.0 =  -0.88 for ctc
2024-10-27 17:52:41,569 (beam_search:479) INFO: total log probability: -0.88
2024-10-27 17:52:41,569 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:52:41,569 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:41,569 (beam_search:483) INFO: best hypo: ISEETHEANDTHE

2024-10-27 17:52:41,572 (asr_inference:509) INFO: speech length: 47231
2024-10-27 17:52:43,389 (beam_search:428) INFO: decoder input length: 36
2024-10-27 17:52:43,389 (beam_search:429) INFO: max output length: 36
2024-10-27 17:52:43,389 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:43,403 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:43,403 (beam_search:476) INFO:  -0.07 * 1.0 =  -0.07 for ctc
2024-10-27 17:52:43,403 (beam_search:479) INFO: total log probability: -0.07
2024-10-27 17:52:43,403 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 17:52:43,403 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:43,403 (beam_search:483) INFO: best hypo: THE

2024-10-27 17:52:43,405 (asr_inference:509) INFO: speech length: 42919
2024-10-27 17:52:45,168 (beam_search:428) INFO: decoder input length: 33
2024-10-27 17:52:45,168 (beam_search:429) INFO: max output length: 33
2024-10-27 17:52:45,168 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:45,195 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:45,195 (beam_search:476) INFO:  -0.49 * 1.0 =  -0.49 for ctc
2024-10-27 17:52:45,195 (beam_search:479) INFO: total log probability: -0.49
2024-10-27 17:52:45,195 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:52:45,195 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:45,195 (beam_search:483) INFO: best hypo: ITTVERYHOT

2024-10-27 17:52:45,198 (asr_inference:509) INFO: speech length: 43172
2024-10-27 17:52:46,948 (beam_search:428) INFO: decoder input length: 33
2024-10-27 17:52:46,948 (beam_search:429) INFO: max output length: 33
2024-10-27 17:52:46,948 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:46,970 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:46,970 (beam_search:476) INFO:  -1.66 * 1.0 =  -1.66 for ctc
2024-10-27 17:52:46,970 (beam_search:479) INFO: total log probability: -1.66
2024-10-27 17:52:46,970 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 17:52:46,970 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:46,970 (beam_search:483) INFO: best hypo: THEYHADNO

2024-10-27 17:52:46,972 (asr_inference:509) INFO: speech length: 50952
2024-10-27 17:52:48,850 (beam_search:428) INFO: decoder input length: 39
2024-10-27 17:52:48,850 (beam_search:429) INFO: max output length: 39
2024-10-27 17:52:48,850 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:48,876 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:48,876 (beam_search:476) INFO:  -2.03 * 1.0 =  -2.03 for ctc
2024-10-27 17:52:48,876 (beam_search:479) INFO: total log probability: -2.03
2024-10-27 17:52:48,876 (beam_search:480) INFO: normalized log probability: -0.41
2024-10-27 17:52:48,876 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:48,876 (beam_search:483) INFO: best hypo: HAVEABOUTENERGY

2024-10-27 17:52:48,878 (asr_inference:509) INFO: speech length: 70638
2024-10-27 17:52:51,406 (beam_search:428) INFO: decoder input length: 54
2024-10-27 17:52:51,406 (beam_search:429) INFO: max output length: 54
2024-10-27 17:52:51,407 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:51,440 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:51,440 (beam_search:476) INFO:  -0.80 * 1.0 =  -0.80 for ctc
2024-10-27 17:52:51,440 (beam_search:479) INFO: total log probability: -0.80
2024-10-27 17:52:51,440 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:52:51,440 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:51,440 (beam_search:483) INFO: best hypo: FROMTOS

2024-10-27 17:52:51,442 (asr_inference:509) INFO: speech length: 83546
2024-10-27 17:52:54,492 (beam_search:428) INFO: decoder input length: 64
2024-10-27 17:52:54,492 (beam_search:429) INFO: max output length: 64
2024-10-27 17:52:54,492 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:54,532 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:54,532 (beam_search:476) INFO:  -1.14 * 1.0 =  -1.14 for ctc
2024-10-27 17:52:54,532 (beam_search:479) INFO: total log probability: -1.14
2024-10-27 17:52:54,532 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:52:54,532 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:54,532 (beam_search:483) INFO: best hypo: ITHEAND

2024-10-27 17:52:54,535 (asr_inference:509) INFO: speech length: 90250
2024-10-27 17:52:57,837 (beam_search:428) INFO: decoder input length: 70
2024-10-27 17:52:57,837 (beam_search:429) INFO: max output length: 70
2024-10-27 17:52:57,837 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:57,901 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:57,901 (beam_search:476) INFO:  -2.24 * 1.0 =  -2.24 for ctc
2024-10-27 17:52:57,901 (beam_search:479) INFO: total log probability: -2.24
2024-10-27 17:52:57,901 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 17:52:57,901 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:57,901 (beam_search:483) INFO: best hypo: THEANDSOTHEIS

2024-10-27 17:52:57,903 (asr_inference:509) INFO: speech length: 50954
2024-10-27 17:52:59,818 (beam_search:428) INFO: decoder input length: 39
2024-10-27 17:52:59,818 (beam_search:429) INFO: max output length: 39
2024-10-27 17:52:59,818 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:59,838 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:59,838 (beam_search:476) INFO:  -1.07 * 1.0 =  -1.07 for ctc
2024-10-27 17:52:59,838 (beam_search:479) INFO: total log probability: -1.07
2024-10-27 17:52:59,838 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 17:52:59,838 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:59,838 (beam_search:483) INFO: best hypo: THERENO

2024-10-27 17:52:59,841 (asr_inference:509) INFO: speech length: 436641
2024-10-27 17:53:20,967 (beam_search:428) INFO: decoder input length: 340
2024-10-27 17:53:20,967 (beam_search:429) INFO: max output length: 340
2024-10-27 17:53:20,967 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:21,608 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:21,608 (beam_search:476) INFO:  -2.32 * 1.0 =  -2.32 for ctc
2024-10-27 17:53:21,608 (beam_search:479) INFO: total log probability: -2.32
2024-10-27 17:53:21,608 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:53:21,608 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:21,608 (beam_search:483) INFO: best hypo: ISEEITANDANDITANDITMAKESTHE

2024-10-27 17:53:21,611 (asr_inference:509) INFO: speech length: 172658
2024-10-27 17:53:28,241 (beam_search:428) INFO: decoder input length: 134
2024-10-27 17:53:28,241 (beam_search:429) INFO: max output length: 134
2024-10-27 17:53:28,241 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:28,451 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:28,451 (beam_search:476) INFO:  -1.76 * 1.0 =  -1.76 for ctc
2024-10-27 17:53:28,451 (beam_search:479) INFO: total log probability: -1.76
2024-10-27 17:53:28,451 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:53:28,452 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:28,452 (beam_search:483) INFO: best hypo: THEISITMAKETHEANDTHEIS

2024-10-27 17:53:28,454 (asr_inference:509) INFO: speech length: 122940
2024-10-27 17:53:32,989 (beam_search:428) INFO: decoder input length: 95
2024-10-27 17:53:32,989 (beam_search:429) INFO: max output length: 95
2024-10-27 17:53:32,989 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:33,119 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:33,119 (beam_search:476) INFO:  -2.22 * 1.0 =  -2.22 for ctc
2024-10-27 17:53:33,120 (beam_search:479) INFO: total log probability: -2.22
2024-10-27 17:53:33,120 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:53:33,120 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:33,120 (beam_search:483) INFO: best hypo: THEISTHEHANDSONTHEIS

2024-10-27 17:53:33,122 (asr_inference:509) INFO: speech length: 128146
2024-10-27 17:53:37,887 (beam_search:428) INFO: decoder input length: 99
2024-10-27 17:53:37,887 (beam_search:429) INFO: max output length: 99
2024-10-27 17:53:37,887 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:38,070 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:38,071 (beam_search:476) INFO:  -3.94 * 1.0 =  -3.94 for ctc
2024-10-27 17:53:38,071 (beam_search:479) INFO: total log probability: -3.94
2024-10-27 17:53:38,071 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 17:53:38,071 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:38,071 (beam_search:483) INFO: best hypo: THEUSINGENERGYBECAUSESHESHENEEDFROMTHETO

2024-10-27 17:53:38,073 (asr_inference:509) INFO: speech length: 207211
2024-10-27 17:53:46,344 (beam_search:428) INFO: decoder input length: 161
2024-10-27 17:53:46,344 (beam_search:429) INFO: max output length: 161
2024-10-27 17:53:46,344 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:46,762 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:46,762 (beam_search:476) INFO:  -3.91 * 1.0 =  -3.91 for ctc
2024-10-27 17:53:46,762 (beam_search:479) INFO: total log probability: -3.91
2024-10-27 17:53:46,762 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:53:46,762 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:46,762 (beam_search:483) INFO: best hypo: THEOFENERGYISISSHEISUSINGFROMTHETOTOUPTHE

2024-10-27 17:53:46,765 (asr_inference:509) INFO: speech length: 89872
2024-10-27 17:53:50,115 (beam_search:428) INFO: decoder input length: 69
2024-10-27 17:53:50,116 (beam_search:429) INFO: max output length: 69
2024-10-27 17:53:50,116 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:50,150 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:50,151 (beam_search:476) INFO:  -1.64 * 1.0 =  -1.64 for ctc
2024-10-27 17:53:50,151 (beam_search:479) INFO: total log probability: -1.64
2024-10-27 17:53:50,151 (beam_search:480) INFO: normalized log probability: -0.41
2024-10-27 17:53:50,151 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:50,151 (beam_search:483) INFO: best hypo: ANDOF

2024-10-27 17:53:50,153 (asr_inference:509) INFO: speech length: 192199
2024-10-27 17:53:57,447 (beam_search:428) INFO: decoder input length: 149
2024-10-27 17:53:57,448 (beam_search:429) INFO: max output length: 149
2024-10-27 17:53:57,448 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:57,884 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:57,884 (beam_search:476) INFO:  -5.96 * 1.0 =  -5.96 for ctc
2024-10-27 17:53:57,884 (beam_search:479) INFO: total log probability: -5.96
2024-10-27 17:53:57,884 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 17:53:57,884 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:57,884 (beam_search:483) INFO: best hypo: WHENWEITWILLDIOXIDEISFORTHEBECAUSEITANDTHATTHEAIRWILLBE

2024-10-27 17:53:57,887 (asr_inference:509) INFO: speech length: 41301
2024-10-27 17:53:59,412 (beam_search:428) INFO: decoder input length: 31
2024-10-27 17:53:59,413 (beam_search:429) INFO: max output length: 31
2024-10-27 17:53:59,413 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:59,433 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:59,434 (beam_search:476) INFO:  -0.36 * 1.0 =  -0.36 for ctc
2024-10-27 17:53:59,434 (beam_search:479) INFO: total log probability: -0.36
2024-10-27 17:53:59,434 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:53:59,434 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:59,434 (beam_search:483) INFO: best hypo: ITGETSAND

2024-10-27 17:53:59,436 (asr_inference:509) INFO: speech length: 132445
2024-10-27 17:54:04,446 (beam_search:428) INFO: decoder input length: 102
2024-10-27 17:54:04,446 (beam_search:429) INFO: max output length: 102
2024-10-27 17:54:04,446 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:04,584 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:04,585 (beam_search:476) INFO:  -1.52 * 1.0 =  -1.52 for ctc
2024-10-27 17:54:04,585 (beam_search:479) INFO: total log probability: -1.52
2024-10-27 17:54:04,585 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:54:04,585 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:04,585 (beam_search:483) INFO: best hypo: THESOLARDOESNOTBUTTHE

2024-10-27 17:54:04,588 (asr_inference:509) INFO: speech length: 105874
2024-10-27 17:54:08,377 (beam_search:428) INFO: decoder input length: 82
2024-10-27 17:54:08,377 (beam_search:429) INFO: max output length: 82
2024-10-27 17:54:08,377 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:08,430 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:08,430 (beam_search:476) INFO:  -0.62 * 1.0 =  -0.62 for ctc
2024-10-27 17:54:08,430 (beam_search:479) INFO: total log probability: -0.62
2024-10-27 17:54:08,430 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:54:08,430 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:08,430 (beam_search:483) INFO: best hypo: THEFORIS

2024-10-27 17:54:08,433 (asr_inference:509) INFO: speech length: 90635
2024-10-27 17:54:11,628 (beam_search:428) INFO: decoder input length: 70
2024-10-27 17:54:11,628 (beam_search:429) INFO: max output length: 70
2024-10-27 17:54:11,628 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:11,675 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:11,676 (beam_search:476) INFO:  -0.55 * 1.0 =  -0.55 for ctc
2024-10-27 17:54:11,676 (beam_search:479) INFO: total log probability: -0.55
2024-10-27 17:54:11,676 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:54:11,676 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:11,676 (beam_search:483) INFO: best hypo: THEYCANBE

2024-10-27 17:54:11,678 (asr_inference:509) INFO: speech length: 114000
2024-10-27 17:54:16,042 (beam_search:428) INFO: decoder input length: 88
2024-10-27 17:54:16,042 (beam_search:429) INFO: max output length: 88
2024-10-27 17:54:16,042 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:16,112 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:16,112 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 17:54:16,112 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 17:54:16,112 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:54:16,112 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:16,112 (beam_search:483) INFO: best hypo: THEYTHEYWORKON

2024-10-27 17:54:16,114 (asr_inference:509) INFO: speech length: 68385
2024-10-27 17:54:18,623 (beam_search:428) INFO: decoder input length: 52
2024-10-27 17:54:18,624 (beam_search:429) INFO: max output length: 52
2024-10-27 17:54:18,624 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:18,663 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:18,663 (beam_search:476) INFO:  -0.78 * 1.0 =  -0.78 for ctc
2024-10-27 17:54:18,663 (beam_search:479) INFO: total log probability: -0.78
2024-10-27 17:54:18,663 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:54:18,663 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:18,663 (beam_search:483) INFO: best hypo: THEYBURNANDTHEY

2024-10-27 17:54:18,666 (asr_inference:509) INFO: speech length: 70006
2024-10-27 17:54:21,132 (beam_search:428) INFO: decoder input length: 54
2024-10-27 17:54:21,132 (beam_search:429) INFO: max output length: 54
2024-10-27 17:54:21,132 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:21,200 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:21,201 (beam_search:476) INFO:  -0.82 * 1.0 =  -0.82 for ctc
2024-10-27 17:54:21,201 (beam_search:479) INFO: total log probability: -0.82
2024-10-27 17:54:21,201 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:54:21,201 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:21,201 (beam_search:483) INFO: best hypo: ONEISAANDISASOLID

2024-10-27 17:54:21,203 (asr_inference:509) INFO: speech length: 124234
2024-10-27 17:54:25,880 (beam_search:428) INFO: decoder input length: 96
2024-10-27 17:54:25,881 (beam_search:429) INFO: max output length: 96
2024-10-27 17:54:25,881 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:26,091 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:26,092 (beam_search:476) INFO:  -3.00 * 1.0 =  -3.00 for ctc
2024-10-27 17:54:26,092 (beam_search:479) INFO: total log probability: -3.00
2024-10-27 17:54:26,092 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:54:26,092 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:26,092 (beam_search:483) INFO: best hypo: THESOLARN'TGIVEBUTTHEANDTHEGAS

2024-10-27 17:54:26,095 (asr_inference:509) INFO: speech length: 64641
2024-10-27 17:54:28,697 (beam_search:428) INFO: decoder input length: 50
2024-10-27 17:54:28,697 (beam_search:429) INFO: max output length: 50
2024-10-27 17:54:28,697 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:28,735 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:28,735 (beam_search:476) INFO:  -1.66 * 1.0 =  -1.66 for ctc
2024-10-27 17:54:28,735 (beam_search:479) INFO: total log probability: -1.66
2024-10-27 17:54:28,735 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 17:54:28,735 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:28,735 (beam_search:483) INFO: best hypo: THATISFORTHE

2024-10-27 17:54:28,737 (asr_inference:509) INFO: speech length: 301687
2024-10-27 17:54:41,271 (beam_search:428) INFO: decoder input length: 235
2024-10-27 17:54:41,271 (beam_search:429) INFO: max output length: 235
2024-10-27 17:54:41,271 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:41,938 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:41,938 (beam_search:476) INFO:  -1.60 * 1.0 =  -1.60 for ctc
2024-10-27 17:54:41,938 (beam_search:479) INFO: total log probability: -1.60
2024-10-27 17:54:41,938 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:54:41,938 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:41,938 (beam_search:483) INFO: best hypo: THECOALANDTHEGASOLINEISFORBUTTHESOLARANDTHEWILLNOT

2024-10-27 17:54:41,940 (asr_inference:509) INFO: speech length: 151088
2024-10-27 17:54:47,705 (beam_search:428) INFO: decoder input length: 117
2024-10-27 17:54:47,705 (beam_search:429) INFO: max output length: 117
2024-10-27 17:54:47,705 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:48,085 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:48,085 (beam_search:476) INFO:  -1.27 * 1.0 =  -1.27 for ctc
2024-10-27 17:54:48,085 (beam_search:479) INFO: total log probability: -1.27
2024-10-27 17:54:48,085 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:54:48,085 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:48,085 (beam_search:483) INFO: best hypo: ITWILLTHEAIRANDITANDITWILLRUNOUTIFYOUUSEITTOOMUCH

2024-10-27 17:54:48,088 (asr_inference:509) INFO: speech length: 64676
2024-10-27 17:54:50,546 (beam_search:428) INFO: decoder input length: 50
2024-10-27 17:54:50,546 (beam_search:429) INFO: max output length: 50
2024-10-27 17:54:50,546 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:50,571 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:50,572 (beam_search:476) INFO:  -0.91 * 1.0 =  -0.91 for ctc
2024-10-27 17:54:50,572 (beam_search:479) INFO: total log probability: -0.91
2024-10-27 17:54:50,572 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:54:50,572 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:50,572 (beam_search:483) INFO: best hypo: GASAND

2024-10-27 17:54:50,574 (asr_inference:509) INFO: speech length: 27165
2024-10-27 17:54:51,754 (beam_search:428) INFO: decoder input length: 20
2024-10-27 17:54:51,754 (beam_search:429) INFO: max output length: 20
2024-10-27 17:54:51,754 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:51,770 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:51,770 (beam_search:476) INFO:  -1.48 * 1.0 =  -1.48 for ctc
2024-10-27 17:54:51,770 (beam_search:479) INFO: total log probability: -1.48
2024-10-27 17:54:51,770 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 17:54:51,770 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:51,770 (beam_search:483) INFO: best hypo: WEHAVEBEEN

2024-10-27 17:54:51,772 (asr_inference:509) INFO: speech length: 50857
2024-10-27 17:54:53,661 (beam_search:428) INFO: decoder input length: 39
2024-10-27 17:54:53,661 (beam_search:429) INFO: max output length: 39
2024-10-27 17:54:53,661 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:53,692 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:53,692 (beam_search:476) INFO:  -1.61 * 1.0 =  -1.61 for ctc
2024-10-27 17:54:53,692 (beam_search:479) INFO: total log probability: -1.61
2024-10-27 17:54:53,692 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 17:54:53,692 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:53,692 (beam_search:483) INFO: best hypo: AISAPARALLEL

2024-10-27 17:54:53,695 (asr_inference:509) INFO: speech length: 195092
2024-10-27 17:55:01,326 (beam_search:428) INFO: decoder input length: 151
2024-10-27 17:55:01,326 (beam_search:429) INFO: max output length: 151
2024-10-27 17:55:01,326 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:01,697 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:01,697 (beam_search:476) INFO:  -3.78 * 1.0 =  -3.78 for ctc
2024-10-27 17:55:01,697 (beam_search:479) INFO: total log probability: -3.78
2024-10-27 17:55:01,697 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:55:01,697 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:01,697 (beam_search:483) INFO: best hypo: THEDISTHETHEWIRESARETHEENERGYANDTHEARETHEENERGY

2024-10-27 17:55:01,700 (asr_inference:509) INFO: speech length: 68981
2024-10-27 17:55:04,266 (beam_search:428) INFO: decoder input length: 53
2024-10-27 17:55:04,266 (beam_search:429) INFO: max output length: 53
2024-10-27 17:55:04,266 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:04,299 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:04,300 (beam_search:476) INFO:  -1.25 * 1.0 =  -1.25 for ctc
2024-10-27 17:55:04,300 (beam_search:479) INFO: total log probability: -1.25
2024-10-27 17:55:04,300 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:55:04,300 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:04,300 (beam_search:483) INFO: best hypo: ANDPARALLELCIRCUITS

2024-10-27 17:55:04,302 (asr_inference:509) INFO: speech length: 32288
2024-10-27 17:55:05,668 (beam_search:428) INFO: decoder input length: 24
2024-10-27 17:55:05,668 (beam_search:429) INFO: max output length: 24
2024-10-27 17:55:05,668 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:05,678 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:05,678 (beam_search:476) INFO:  -0.34 * 1.0 =  -0.34 for ctc
2024-10-27 17:55:05,678 (beam_search:479) INFO: total log probability: -0.34
2024-10-27 17:55:05,678 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:55:05,678 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:05,678 (beam_search:483) INFO: best hypo: IT

2024-10-27 17:55:05,681 (asr_inference:509) INFO: speech length: 140192
2024-10-27 17:55:10,776 (beam_search:428) INFO: decoder input length: 109
2024-10-27 17:55:10,776 (beam_search:429) INFO: max output length: 109
2024-10-27 17:55:10,776 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:10,963 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:10,963 (beam_search:476) INFO:  -0.50 * 1.0 =  -0.50 for ctc
2024-10-27 17:55:10,963 (beam_search:479) INFO: total log probability: -0.50
2024-10-27 17:55:10,963 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:55:10,963 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:10,963 (beam_search:483) INFO: best hypo: THETHELIGHTBULBINENERGYANDOUTENERGY

2024-10-27 17:55:10,965 (asr_inference:509) INFO: speech length: 20418
2024-10-27 17:55:11,948 (beam_search:428) INFO: decoder input length: 15
2024-10-27 17:55:11,949 (beam_search:429) INFO: max output length: 15
2024-10-27 17:55:11,949 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:11,959 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:11,959 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 17:55:11,959 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 17:55:11,959 (beam_search:480) INFO: normalized log probability: -0.38
2024-10-27 17:55:11,959 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:11,959 (beam_search:483) INFO: best hypo: ITMORE

2024-10-27 17:55:11,962 (asr_inference:509) INFO: speech length: 211825
2024-10-27 17:55:20,448 (beam_search:428) INFO: decoder input length: 164
2024-10-27 17:55:20,448 (beam_search:429) INFO: max output length: 164
2024-10-27 17:55:20,448 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:20,872 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:20,872 (beam_search:476) INFO:  -3.09 * 1.0 =  -3.09 for ctc
2024-10-27 17:55:20,872 (beam_search:479) INFO: total log probability: -3.09
2024-10-27 17:55:20,872 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:55:20,872 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:20,872 (beam_search:483) INFO: best hypo: SOTHECIRCUITITDOESN'TASASAPARALLELCIRCUIT

2024-10-27 17:55:20,874 (asr_inference:509) INFO: speech length: 167579
2024-10-27 17:55:27,192 (beam_search:428) INFO: decoder input length: 130
2024-10-27 17:55:27,193 (beam_search:429) INFO: max output length: 130
2024-10-27 17:55:27,193 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:27,326 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:27,326 (beam_search:476) INFO:  -1.75 * 1.0 =  -1.75 for ctc
2024-10-27 17:55:27,326 (beam_search:479) INFO: total log probability: -1.75
2024-10-27 17:55:27,326 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:55:27,326 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:27,326 (beam_search:483) INFO: best hypo: FROMONEENDANDANOTHER

2024-10-27 17:55:27,330 (asr_inference:509) INFO: speech length: 199851
2024-10-27 17:55:35,203 (beam_search:428) INFO: decoder input length: 155
2024-10-27 17:55:35,203 (beam_search:429) INFO: max output length: 155
2024-10-27 17:55:35,203 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:35,838 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:35,838 (beam_search:476) INFO:  -5.03 * 1.0 =  -5.03 for ctc
2024-10-27 17:55:35,838 (beam_search:479) INFO: total log probability: -5.03
2024-10-27 17:55:35,838 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:55:35,838 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:35,838 (beam_search:483) INFO: best hypo: IENERGYGOESINTOONELIGHTBULBCOMESOUTANDTHENLIGHTBULBANDTHENTOTHELIGHTBULBLIGHTANDBLAH

2024-10-27 17:55:35,841 (asr_inference:509) INFO: speech length: 21732
2024-10-27 17:55:36,844 (beam_search:428) INFO: decoder input length: 16
2024-10-27 17:55:36,844 (beam_search:429) INFO: max output length: 16
2024-10-27 17:55:36,844 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:36,852 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:36,853 (beam_search:476) INFO:  -0.19 * 1.0 =  -0.19 for ctc
2024-10-27 17:55:36,853 (beam_search:479) INFO: total log probability: -0.19
2024-10-27 17:55:36,853 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:55:36,853 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:36,853 (beam_search:483) INFO: best hypo: YOU

2024-10-27 17:55:36,856 (asr_inference:509) INFO: speech length: 34557
2024-10-27 17:55:38,326 (beam_search:428) INFO: decoder input length: 26
2024-10-27 17:55:38,326 (beam_search:429) INFO: max output length: 26
2024-10-27 17:55:38,326 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:38,340 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:38,340 (beam_search:476) INFO:  -0.73 * 1.0 =  -0.73 for ctc
2024-10-27 17:55:38,340 (beam_search:479) INFO: total log probability: -0.73
2024-10-27 17:55:38,340 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:55:38,340 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:38,340 (beam_search:483) INFO: best hypo: INEED

2024-10-27 17:55:38,343 (asr_inference:509) INFO: speech length: 251672
2024-10-27 17:55:48,621 (beam_search:428) INFO: decoder input length: 196
2024-10-27 17:55:48,621 (beam_search:429) INFO: max output length: 196
2024-10-27 17:55:48,621 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:49,266 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:49,266 (beam_search:476) INFO:  -3.96 * 1.0 =  -3.96 for ctc
2024-10-27 17:55:49,266 (beam_search:479) INFO: total log probability: -3.96
2024-10-27 17:55:49,266 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:55:49,266 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:49,266 (beam_search:483) INFO: best hypo: THETWODON'TMAKEJUSTMAKETHEMMAKEMAKETHEMNOTNOTNOTMOREPOWERFUL

2024-10-27 17:55:49,269 (asr_inference:509) INFO: speech length: 58147
2024-10-27 17:55:51,562 (beam_search:428) INFO: decoder input length: 44
2024-10-27 17:55:51,562 (beam_search:429) INFO: max output length: 44
2024-10-27 17:55:51,562 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:51,594 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:51,595 (beam_search:476) INFO:  -0.54 * 1.0 =  -0.54 for ctc
2024-10-27 17:55:51,595 (beam_search:479) INFO: total log probability: -0.54
2024-10-27 17:55:51,595 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:55:51,595 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:51,595 (beam_search:483) INFO: best hypo: THEYALLHAVETWO

2024-10-27 17:55:51,597 (asr_inference:509) INFO: speech length: 101022
2024-10-27 17:55:55,185 (beam_search:428) INFO: decoder input length: 78
2024-10-27 17:55:55,185 (beam_search:429) INFO: max output length: 78
2024-10-27 17:55:55,185 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:55,340 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:55,340 (beam_search:476) INFO:  -0.97 * 1.0 =  -0.97 for ctc
2024-10-27 17:55:55,340 (beam_search:479) INFO: total log probability: -0.97
2024-10-27 17:55:55,340 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:55:55,340 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:55,340 (beam_search:483) INFO: best hypo: THEENERGYHASTOGOTHROUGHMANYMOREWIRESTHEENERGYWILL

2024-10-27 17:55:55,342 (asr_inference:509) INFO: speech length: 102972
2024-10-27 17:55:59,020 (beam_search:428) INFO: decoder input length: 79
2024-10-27 17:55:59,020 (beam_search:429) INFO: max output length: 79
2024-10-27 17:55:59,020 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:59,134 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:59,135 (beam_search:476) INFO:  -1.02 * 1.0 =  -1.02 for ctc
2024-10-27 17:55:59,135 (beam_search:479) INFO: total log probability: -1.02
2024-10-27 17:55:59,135 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:55:59,135 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:59,135 (beam_search:483) INFO: best hypo: THEENERGYHASTOGOTHROUGHMOREWIRES

2024-10-27 17:55:59,137 (asr_inference:509) INFO: speech length: 38008
2024-10-27 17:56:00,674 (beam_search:428) INFO: decoder input length: 29
2024-10-27 17:56:00,674 (beam_search:429) INFO: max output length: 29
2024-10-27 17:56:00,674 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:00,689 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:00,690 (beam_search:476) INFO:  -0.80 * 1.0 =  -0.80 for ctc
2024-10-27 17:56:00,690 (beam_search:479) INFO: total log probability: -0.80
2024-10-27 17:56:00,690 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:56:00,690 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:00,690 (beam_search:483) INFO: best hypo: THEREIS

2024-10-27 17:56:00,692 (asr_inference:509) INFO: speech length: 18000
2024-10-27 17:56:01,636 (beam_search:428) INFO: decoder input length: 13
2024-10-27 17:56:01,636 (beam_search:429) INFO: max output length: 13
2024-10-27 17:56:01,636 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:01,649 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:01,649 (beam_search:476) INFO:  -0.23 * 1.0 =  -0.23 for ctc
2024-10-27 17:56:01,649 (beam_search:479) INFO: total log probability: -0.23
2024-10-27 17:56:01,649 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:56:01,649 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:01,649 (beam_search:483) INFO: best hypo: I'MGOOD

2024-10-27 17:56:01,652 (asr_inference:509) INFO: speech length: 78551
2024-10-27 17:56:04,626 (beam_search:428) INFO: decoder input length: 60
2024-10-27 17:56:04,626 (beam_search:429) INFO: max output length: 60
2024-10-27 17:56:04,626 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:04,681 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:04,681 (beam_search:476) INFO:  -2.18 * 1.0 =  -2.18 for ctc
2024-10-27 17:56:04,681 (beam_search:479) INFO: total log probability: -2.18
2024-10-27 17:56:04,681 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 17:56:04,681 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:04,681 (beam_search:483) INFO: best hypo: ILEARNINGABOUTHOWMAGNETS

2024-10-27 17:56:04,683 (asr_inference:509) INFO: speech length: 65619
2024-10-27 17:56:06,979 (beam_search:428) INFO: decoder input length: 50
2024-10-27 17:56:06,979 (beam_search:429) INFO: max output length: 50
2024-10-27 17:56:06,979 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:07,019 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:07,020 (beam_search:476) INFO:  -0.59 * 1.0 =  -0.59 for ctc
2024-10-27 17:56:07,020 (beam_search:479) INFO: total log probability: -0.59
2024-10-27 17:56:07,020 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:56:07,020 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:07,020 (beam_search:483) INFO: best hypo: THEARESTICKINGTOGETHER

2024-10-27 17:56:07,022 (asr_inference:509) INFO: speech length: 33540
2024-10-27 17:56:08,331 (beam_search:428) INFO: decoder input length: 25
2024-10-27 17:56:08,332 (beam_search:429) INFO: max output length: 25
2024-10-27 17:56:08,332 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:08,345 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:08,345 (beam_search:476) INFO:  -0.99 * 1.0 =  -0.99 for ctc
2024-10-27 17:56:08,345 (beam_search:479) INFO: total log probability: -0.99
2024-10-27 17:56:08,345 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:56:08,345 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:08,345 (beam_search:483) INFO: best hypo: THEYOTHER

2024-10-27 17:56:08,347 (asr_inference:509) INFO: speech length: 47107
2024-10-27 17:56:10,113 (beam_search:428) INFO: decoder input length: 36
2024-10-27 17:56:10,113 (beam_search:429) INFO: max output length: 36
2024-10-27 17:56:10,113 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:10,142 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:10,142 (beam_search:476) INFO:  -0.29 * 1.0 =  -0.29 for ctc
2024-10-27 17:56:10,142 (beam_search:479) INFO: total log probability: -0.29
2024-10-27 17:56:10,142 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:56:10,142 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:10,142 (beam_search:483) INFO: best hypo: THEAREEACHOTHER

2024-10-27 17:56:10,144 (asr_inference:509) INFO: speech length: 29195
2024-10-27 17:56:11,428 (beam_search:428) INFO: decoder input length: 22
2024-10-27 17:56:11,428 (beam_search:429) INFO: max output length: 22
2024-10-27 17:56:11,428 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:11,444 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:11,444 (beam_search:476) INFO:  -2.51 * 1.0 =  -2.51 for ctc
2024-10-27 17:56:11,444 (beam_search:479) INFO: total log probability: -2.51
2024-10-27 17:56:11,444 (beam_search:480) INFO: normalized log probability: -0.50
2024-10-27 17:56:11,444 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:11,444 (beam_search:483) INFO: best hypo: BECAUSEAAND

2024-10-27 17:56:11,446 (asr_inference:509) INFO: speech length: 165108
2024-10-27 17:56:17,664 (beam_search:428) INFO: decoder input length: 128
2024-10-27 17:56:17,664 (beam_search:429) INFO: max output length: 128
2024-10-27 17:56:17,664 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:17,793 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:17,793 (beam_search:476) INFO:  -2.24 * 1.0 =  -2.24 for ctc
2024-10-27 17:56:17,793 (beam_search:479) INFO: total log probability: -2.24
2024-10-27 17:56:17,793 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 17:56:17,793 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:17,793 (beam_search:483) INFO: best hypo: THEYEACHOTHERAREAND

2024-10-27 17:56:17,795 (asr_inference:509) INFO: speech length: 121692
2024-10-27 17:56:22,431 (beam_search:428) INFO: decoder input length: 94
2024-10-27 17:56:22,431 (beam_search:429) INFO: max output length: 94
2024-10-27 17:56:22,431 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:22,545 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:22,545 (beam_search:476) INFO:  -2.96 * 1.0 =  -2.96 for ctc
2024-10-27 17:56:22,545 (beam_search:479) INFO: total log probability: -2.96
2024-10-27 17:56:22,545 (beam_search:480) INFO: normalized log probability: -0.37
2024-10-27 17:56:22,545 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:22,545 (beam_search:483) INFO: best hypo: OFTHATTOEACHOTHERTHAT

2024-10-27 17:56:22,547 (asr_inference:509) INFO: speech length: 273849
2024-10-27 17:56:33,830 (beam_search:428) INFO: decoder input length: 213
2024-10-27 17:56:33,831 (beam_search:429) INFO: max output length: 213
2024-10-27 17:56:33,831 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:34,318 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:34,318 (beam_search:476) INFO:  -2.94 * 1.0 =  -2.94 for ctc
2024-10-27 17:56:34,318 (beam_search:479) INFO: total log probability: -2.94
2024-10-27 17:56:34,318 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:56:34,318 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:34,319 (beam_search:483) INFO: best hypo: AREMAGNETSANDTHEYLIKEANDONEOFTHEANDTHEARE

2024-10-27 17:56:34,322 (asr_inference:509) INFO: speech length: 37719
2024-10-27 17:56:35,770 (beam_search:428) INFO: decoder input length: 28
2024-10-27 17:56:35,770 (beam_search:429) INFO: max output length: 28
2024-10-27 17:56:35,770 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:35,780 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:35,780 (beam_search:476) INFO:  -1.06 * 1.0 =  -1.06 for ctc
2024-10-27 17:56:35,781 (beam_search:479) INFO: total log probability: -1.06
2024-10-27 17:56:35,781 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 17:56:35,781 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:35,781 (beam_search:483) INFO: best hypo: S

2024-10-27 17:56:35,782 (asr_inference:509) INFO: speech length: 75597
2024-10-27 17:56:38,643 (beam_search:428) INFO: decoder input length: 58
2024-10-27 17:56:38,643 (beam_search:429) INFO: max output length: 58
2024-10-27 17:56:38,643 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:38,703 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:38,704 (beam_search:476) INFO:  -3.19 * 1.0 =  -3.19 for ctc
2024-10-27 17:56:38,704 (beam_search:479) INFO: total log probability: -3.19
2024-10-27 17:56:38,704 (beam_search:480) INFO: normalized log probability: -0.40
2024-10-27 17:56:38,704 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:38,704 (beam_search:483) INFO: best hypo: THISISABOUTATHES

2024-10-27 17:56:38,706 (asr_inference:509) INFO: speech length: 176241
2024-10-27 17:56:45,359 (beam_search:428) INFO: decoder input length: 137
2024-10-27 17:56:45,360 (beam_search:429) INFO: max output length: 137
2024-10-27 17:56:45,360 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:45,427 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:45,427 (beam_search:476) INFO:  -1.88 * 1.0 =  -1.88 for ctc
2024-10-27 17:56:45,427 (beam_search:479) INFO: total log probability: -1.88
2024-10-27 17:56:45,428 (beam_search:480) INFO: normalized log probability: -0.47
2024-10-27 17:56:45,428 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:45,428 (beam_search:483) INFO: best hypo: BECAUSETHE

2024-10-27 17:56:45,430 (asr_inference:509) INFO: speech length: 54500
2024-10-27 17:56:47,432 (beam_search:428) INFO: decoder input length: 42
2024-10-27 17:56:47,433 (beam_search:429) INFO: max output length: 42
2024-10-27 17:56:47,433 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:47,476 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:47,476 (beam_search:476) INFO:  -1.42 * 1.0 =  -1.42 for ctc
2024-10-27 17:56:47,476 (beam_search:479) INFO: total log probability: -1.42
2024-10-27 17:56:47,476 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:56:47,476 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:47,477 (beam_search:483) INFO: best hypo: LEARNINGABOUTMAGNETSANDOTHERSTUFF

2024-10-27 17:56:47,478 (asr_inference:509) INFO: speech length: 100961
2024-10-27 17:56:51,152 (beam_search:428) INFO: decoder input length: 78
2024-10-27 17:56:51,152 (beam_search:429) INFO: max output length: 78
2024-10-27 17:56:51,152 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:51,243 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:51,243 (beam_search:476) INFO:  -0.85 * 1.0 =  -0.85 for ctc
2024-10-27 17:56:51,243 (beam_search:479) INFO: total log probability: -0.85
2024-10-27 17:56:51,243 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:56:51,243 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:51,243 (beam_search:483) INFO: best hypo: WEABOUTANDANDTHATSTUFF

2024-10-27 17:56:51,245 (asr_inference:509) INFO: speech length: 159110
2024-10-27 17:56:57,223 (beam_search:428) INFO: decoder input length: 123
2024-10-27 17:56:57,223 (beam_search:429) INFO: max output length: 123
2024-10-27 17:56:57,223 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:57,407 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:57,407 (beam_search:476) INFO:  -1.88 * 1.0 =  -1.88 for ctc
2024-10-27 17:56:57,407 (beam_search:479) INFO: total log probability: -1.88
2024-10-27 17:56:57,407 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:56:57,407 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:57,407 (beam_search:483) INFO: best hypo: THEGOINGTHROUGHTHETHESTICKTOTHE

2024-10-27 17:56:57,410 (asr_inference:509) INFO: speech length: 308233
2024-10-27 17:57:10,474 (beam_search:428) INFO: decoder input length: 240
2024-10-27 17:57:10,474 (beam_search:429) INFO: max output length: 240
2024-10-27 17:57:10,474 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:11,164 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:11,164 (beam_search:476) INFO:  -6.10 * 1.0 =  -6.10 for ctc
2024-10-27 17:57:11,164 (beam_search:479) INFO: total log probability: -6.10
2024-10-27 17:57:11,164 (beam_search:480) INFO: normalized log probability: -0.38
2024-10-27 17:57:11,164 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:11,164 (beam_search:483) INFO: best hypo: WHENTHEMAGNETHAVEASOIFTHEENOUGHTHECANGOTHROUGHIT

2024-10-27 17:57:11,167 (asr_inference:509) INFO: speech length: 175688
2024-10-27 17:57:18,218 (beam_search:428) INFO: decoder input length: 136
2024-10-27 17:57:18,218 (beam_search:429) INFO: max output length: 136
2024-10-27 17:57:18,218 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:18,465 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:18,465 (beam_search:476) INFO:  -3.87 * 1.0 =  -3.87 for ctc
2024-10-27 17:57:18,466 (beam_search:479) INFO: total log probability: -3.87
2024-10-27 17:57:18,466 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 17:57:18,466 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:18,466 (beam_search:483) INFO: best hypo: ISASOIFAORSTICKTOTHEMAGNET

2024-10-27 17:57:18,468 (asr_inference:509) INFO: speech length: 133225
2024-10-27 17:57:23,218 (beam_search:428) INFO: decoder input length: 103
2024-10-27 17:57:23,218 (beam_search:429) INFO: max output length: 103
2024-10-27 17:57:23,218 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:23,417 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:23,417 (beam_search:476) INFO:  -2.12 * 1.0 =  -2.12 for ctc
2024-10-27 17:57:23,417 (beam_search:479) INFO: total log probability: -2.12
2024-10-27 17:57:23,417 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:57:23,417 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:23,417 (beam_search:483) INFO: best hypo: THEFIRSTTHEPOWERCANGOTHROUGHBUTTHEIT

2024-10-27 17:57:23,420 (asr_inference:509) INFO: speech length: 198692
2024-10-27 17:57:31,440 (beam_search:428) INFO: decoder input length: 154
2024-10-27 17:57:31,441 (beam_search:429) INFO: max output length: 154
2024-10-27 17:57:31,441 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:31,859 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:31,859 (beam_search:476) INFO:  -2.25 * 1.0 =  -2.25 for ctc
2024-10-27 17:57:31,859 (beam_search:479) INFO: total log probability: -2.25
2024-10-27 17:57:31,859 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:57:31,859 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:31,859 (beam_search:483) INFO: best hypo: THEISCANGOTHROUGHTHETHINGTHETHECANONLYGOTHROUGHSOME

2024-10-27 17:57:31,862 (asr_inference:509) INFO: speech length: 33318
2024-10-27 17:57:33,294 (beam_search:428) INFO: decoder input length: 25
2024-10-27 17:57:33,294 (beam_search:429) INFO: max output length: 25
2024-10-27 17:57:33,294 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:33,309 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:33,309 (beam_search:476) INFO:  -0.17 * 1.0 =  -0.17 for ctc
2024-10-27 17:57:33,309 (beam_search:479) INFO: total log probability: -0.17
2024-10-27 17:57:33,309 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:57:33,309 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:33,309 (beam_search:483) INFO: best hypo: ITIT

2024-10-27 17:57:33,312 (asr_inference:509) INFO: speech length: 154746
2024-10-27 17:57:38,985 (beam_search:428) INFO: decoder input length: 120
2024-10-27 17:57:38,986 (beam_search:429) INFO: max output length: 120
2024-10-27 17:57:38,986 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:39,185 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:39,186 (beam_search:476) INFO:  -3.63 * 1.0 =  -3.63 for ctc
2024-10-27 17:57:39,186 (beam_search:479) INFO: total log probability: -3.63
2024-10-27 17:57:39,186 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 17:57:39,186 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:39,186 (beam_search:483) INFO: best hypo: THEITWILLTHROUGHITWON'TGO

2024-10-27 17:57:39,189 (asr_inference:509) INFO: speech length: 68264
2024-10-27 17:57:41,746 (beam_search:428) INFO: decoder input length: 52
2024-10-27 17:57:41,747 (beam_search:429) INFO: max output length: 52
2024-10-27 17:57:41,747 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:41,812 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:41,812 (beam_search:476) INFO:  -2.00 * 1.0 =  -2.00 for ctc
2024-10-27 17:57:41,812 (beam_search:479) INFO: total log probability: -2.00
2024-10-27 17:57:41,812 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:57:41,812 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:41,812 (beam_search:483) INFO: best hypo: THROUGHTHETHECANNOTGOTHROUGH

2024-10-27 17:57:41,814 (asr_inference:509) INFO: speech length: 100901
2024-10-27 17:57:45,488 (beam_search:428) INFO: decoder input length: 78
2024-10-27 17:57:45,488 (beam_search:429) INFO: max output length: 78
2024-10-27 17:57:45,488 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:45,615 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:45,615 (beam_search:476) INFO:  -2.54 * 1.0 =  -2.54 for ctc
2024-10-27 17:57:45,615 (beam_search:479) INFO: total log probability: -2.54
2024-10-27 17:57:45,615 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:57:45,615 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:45,615 (beam_search:483) INFO: best hypo: THEISNAILANDSTEELNAILSTICKTOTHE

2024-10-27 17:57:45,617 (asr_inference:509) INFO: speech length: 43212
2024-10-27 17:57:47,194 (beam_search:428) INFO: decoder input length: 33
2024-10-27 17:57:47,195 (beam_search:429) INFO: max output length: 33
2024-10-27 17:57:47,195 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:47,226 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:47,227 (beam_search:476) INFO:  -1.48 * 1.0 =  -1.48 for ctc
2024-10-27 17:57:47,227 (beam_search:479) INFO: total log probability: -1.48
2024-10-27 17:57:47,227 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:57:47,227 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:47,227 (beam_search:483) INFO: best hypo: WE'VELEARNINGABOUT

2024-10-27 17:57:47,229 (asr_inference:509) INFO: speech length: 42840
2024-10-27 17:57:48,884 (beam_search:428) INFO: decoder input length: 32
2024-10-27 17:57:48,884 (beam_search:429) INFO: max output length: 32
2024-10-27 17:57:48,884 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:48,896 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:48,896 (beam_search:476) INFO:  -0.52 * 1.0 =  -0.52 for ctc
2024-10-27 17:57:48,896 (beam_search:479) INFO: total log probability: -0.52
2024-10-27 17:57:48,896 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:57:48,896 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:48,897 (beam_search:483) INFO: best hypo: MAGNET

2024-10-27 17:57:48,899 (asr_inference:509) INFO: speech length: 89315
2024-10-27 17:57:52,209 (beam_search:428) INFO: decoder input length: 69
2024-10-27 17:57:52,209 (beam_search:429) INFO: max output length: 69
2024-10-27 17:57:52,209 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:52,266 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:52,266 (beam_search:476) INFO:  -1.10 * 1.0 =  -1.10 for ctc
2024-10-27 17:57:52,266 (beam_search:479) INFO: total log probability: -1.10
2024-10-27 17:57:52,266 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:57:52,266 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:52,266 (beam_search:483) INFO: best hypo: MORETOTHESIDE

2024-10-27 17:57:52,269 (asr_inference:509) INFO: speech length: 50929
2024-10-27 17:57:54,252 (beam_search:428) INFO: decoder input length: 39
2024-10-27 17:57:54,252 (beam_search:429) INFO: max output length: 39
2024-10-27 17:57:54,252 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:54,261 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:54,261 (beam_search:476) INFO:  -0.12 * 1.0 =  -0.12 for ctc
2024-10-27 17:57:54,261 (beam_search:479) INFO: total log probability: -0.12
2024-10-27 17:57:54,261 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:57:54,262 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:54,262 (beam_search:483) INFO: best hypo: 

2024-10-27 17:57:54,264 (asr_inference:509) INFO: speech length: 182917
2024-10-27 17:58:01,129 (beam_search:428) INFO: decoder input length: 142
2024-10-27 17:58:01,130 (beam_search:429) INFO: max output length: 142
2024-10-27 17:58:01,130 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:01,221 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:01,221 (beam_search:476) INFO:  -1.03 * 1.0 =  -1.03 for ctc
2024-10-27 17:58:01,221 (beam_search:479) INFO: total log probability: -1.03
2024-10-27 17:58:01,221 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:58:01,221 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:01,221 (beam_search:483) INFO: best hypo: SOYOUIN

2024-10-27 17:58:01,223 (asr_inference:509) INFO: speech length: 170450
2024-10-27 17:58:07,824 (beam_search:428) INFO: decoder input length: 132
2024-10-27 17:58:07,824 (beam_search:429) INFO: max output length: 132
2024-10-27 17:58:07,824 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:07,909 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:07,910 (beam_search:476) INFO:  -1.45 * 1.0 =  -1.45 for ctc
2024-10-27 17:58:07,910 (beam_search:479) INFO: total log probability: -1.45
2024-10-27 17:58:07,910 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 17:58:07,910 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:07,910 (beam_search:483) INFO: best hypo: MAGNETSAWASHERS

2024-10-27 17:58:07,912 (asr_inference:509) INFO: speech length: 136752
2024-10-27 17:58:12,744 (beam_search:428) INFO: decoder input length: 106
2024-10-27 17:58:12,744 (beam_search:429) INFO: max output length: 106
2024-10-27 17:58:12,744 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:12,897 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:12,897 (beam_search:476) INFO:  -1.37 * 1.0 =  -1.37 for ctc
2024-10-27 17:58:12,897 (beam_search:479) INFO: total log probability: -1.37
2024-10-27 17:58:12,897 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:58:12,897 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:12,897 (beam_search:483) INFO: best hypo: WEHAVEBEENMAKINGANDOUTHOWTHEY

2024-10-27 17:58:12,899 (asr_inference:509) INFO: speech length: 211031
2024-10-27 17:58:21,790 (beam_search:428) INFO: decoder input length: 164
2024-10-27 17:58:21,790 (beam_search:429) INFO: max output length: 164
2024-10-27 17:58:21,791 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:22,024 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:22,024 (beam_search:476) INFO:  -4.51 * 1.0 =  -4.51 for ctc
2024-10-27 17:58:22,024 (beam_search:479) INFO: total log probability: -4.51
2024-10-27 17:58:22,024 (beam_search:480) INFO: normalized log probability: -0.45
2024-10-27 17:58:22,024 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:22,024 (beam_search:483) INFO: best hypo: THEWORKSWHENITISISWITHA

2024-10-27 17:58:22,026 (asr_inference:509) INFO: speech length: 334901
2024-10-27 17:58:36,977 (beam_search:428) INFO: decoder input length: 261
2024-10-27 17:58:36,977 (beam_search:429) INFO: max output length: 261
2024-10-27 17:58:36,977 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:37,036 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:37,037 (beam_search:476) INFO:  -0.17 * 1.0 =  -0.17 for ctc
2024-10-27 17:58:37,037 (beam_search:479) INFO: total log probability: -0.17
2024-10-27 17:58:37,037 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:58:37,037 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:37,037 (beam_search:483) INFO: best hypo: 

2024-10-27 17:58:37,040 (asr_inference:509) INFO: speech length: 127907
2024-10-27 17:58:41,800 (beam_search:428) INFO: decoder input length: 99
2024-10-27 17:58:41,800 (beam_search:429) INFO: max output length: 99
2024-10-27 17:58:41,800 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:41,881 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:41,881 (beam_search:476) INFO:  -0.67 * 1.0 =  -0.67 for ctc
2024-10-27 17:58:41,881 (beam_search:479) INFO: total log probability: -0.67
2024-10-27 17:58:41,881 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:58:41,881 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:41,881 (beam_search:483) INFO: best hypo: TOMAKEITA

2024-10-27 17:58:41,884 (asr_inference:509) INFO: speech length: 115174
2024-10-27 17:58:46,156 (beam_search:428) INFO: decoder input length: 89
2024-10-27 17:58:46,157 (beam_search:429) INFO: max output length: 89
2024-10-27 17:58:46,157 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:46,201 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:46,201 (beam_search:476) INFO:  -1.37 * 1.0 =  -1.37 for ctc
2024-10-27 17:58:46,201 (beam_search:479) INFO: total log probability: -1.37
2024-10-27 17:58:46,201 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 17:58:46,201 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:46,201 (beam_search:483) INFO: best hypo: ELECTROMAGNETBY

2024-10-27 17:58:46,203 (asr_inference:509) INFO: speech length: 64246
2024-10-27 17:58:48,606 (beam_search:428) INFO: decoder input length: 49
2024-10-27 17:58:48,606 (beam_search:429) INFO: max output length: 49
2024-10-27 17:58:48,606 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:48,667 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:48,667 (beam_search:476) INFO:  -1.07 * 1.0 =  -1.07 for ctc
2024-10-27 17:58:48,667 (beam_search:479) INFO: total log probability: -1.07
2024-10-27 17:58:48,667 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:58:48,667 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:48,668 (beam_search:483) INFO: best hypo: YOUHAVETOWRAPITAROUNDA

2024-10-27 17:58:48,669 (asr_inference:509) INFO: speech length: 154117
2024-10-27 17:58:54,337 (beam_search:428) INFO: decoder input length: 119
2024-10-27 17:58:54,337 (beam_search:429) INFO: max output length: 119
2024-10-27 17:58:54,337 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:54,401 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:54,401 (beam_search:476) INFO:  -1.36 * 1.0 =  -1.36 for ctc
2024-10-27 17:58:54,401 (beam_search:479) INFO: total log probability: -1.36
2024-10-27 17:58:54,401 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 17:58:54,401 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:54,401 (beam_search:483) INFO: best hypo: MAGNETSON

2024-10-27 17:58:54,404 (asr_inference:509) INFO: speech length: 170472
2024-10-27 17:59:00,852 (beam_search:428) INFO: decoder input length: 132
2024-10-27 17:59:00,852 (beam_search:429) INFO: max output length: 132
2024-10-27 17:59:00,852 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:00,878 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:00,878 (beam_search:476) INFO:  -0.90 * 1.0 =  -0.90 for ctc
2024-10-27 17:59:00,878 (beam_search:479) INFO: total log probability: -0.90
2024-10-27 17:59:00,878 (beam_search:480) INFO: normalized log probability: -0.45
2024-10-27 17:59:00,878 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:00,878 (beam_search:483) INFO: best hypo: 

2024-10-27 17:59:00,881 (asr_inference:509) INFO: speech length: 21000
2024-10-27 17:59:01,817 (beam_search:428) INFO: decoder input length: 15
2024-10-27 17:59:01,817 (beam_search:429) INFO: max output length: 15
2024-10-27 17:59:01,817 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:01,826 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:01,827 (beam_search:476) INFO:  -0.30 * 1.0 =  -0.30 for ctc
2024-10-27 17:59:01,827 (beam_search:479) INFO: total log probability: -0.30
2024-10-27 17:59:01,827 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:59:01,827 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:01,827 (beam_search:483) INFO: best hypo: ONAND

2024-10-27 17:59:01,829 (asr_inference:509) INFO: speech length: 36743
2024-10-27 17:59:03,264 (beam_search:428) INFO: decoder input length: 28
2024-10-27 17:59:03,265 (beam_search:429) INFO: max output length: 28
2024-10-27 17:59:03,265 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:03,278 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:03,278 (beam_search:476) INFO:  -0.24 * 1.0 =  -0.24 for ctc
2024-10-27 17:59:03,278 (beam_search:479) INFO: total log probability: -0.24
2024-10-27 17:59:03,278 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:59:03,278 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:03,278 (beam_search:483) INFO: best hypo: ON

2024-10-27 17:59:03,280 (asr_inference:509) INFO: speech length: 22500
2024-10-27 17:59:04,291 (beam_search:428) INFO: decoder input length: 17
2024-10-27 17:59:04,291 (beam_search:429) INFO: max output length: 17
2024-10-27 17:59:04,291 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:04,308 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:04,308 (beam_search:476) INFO:  -0.29 * 1.0 =  -0.29 for ctc
2024-10-27 17:59:04,308 (beam_search:479) INFO: total log probability: -0.29
2024-10-27 17:59:04,308 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:59:04,308 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:04,308 (beam_search:483) INFO: best hypo: I'MGOOD

2024-10-27 17:59:04,311 (asr_inference:509) INFO: speech length: 138849
2024-10-27 17:59:09,360 (beam_search:428) INFO: decoder input length: 107
2024-10-27 17:59:09,361 (beam_search:429) INFO: max output length: 107
2024-10-27 17:59:09,361 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:09,562 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:09,562 (beam_search:476) INFO:  -3.08 * 1.0 =  -3.08 for ctc
2024-10-27 17:59:09,562 (beam_search:479) INFO: total log probability: -3.08
2024-10-27 17:59:09,562 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:59:09,562 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:09,562 (beam_search:483) INFO: best hypo: IVEBEENABOUTANDENERGYTHROUGHLIGHTBULBSAND

2024-10-27 17:59:09,564 (asr_inference:509) INFO: speech length: 95614
2024-10-27 17:59:13,100 (beam_search:428) INFO: decoder input length: 74
2024-10-27 17:59:13,100 (beam_search:429) INFO: max output length: 74
2024-10-27 17:59:13,100 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:13,230 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:13,231 (beam_search:476) INFO:  -2.68 * 1.0 =  -2.68 for ctc
2024-10-27 17:59:13,231 (beam_search:479) INFO: total log probability: -2.68
2024-10-27 17:59:13,231 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:59:13,231 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:13,231 (beam_search:483) INFO: best hypo: THEYINACIRCUITADALIGHTBULBAND

2024-10-27 17:59:13,233 (asr_inference:509) INFO: speech length: 256427
2024-10-27 17:59:23,685 (beam_search:428) INFO: decoder input length: 199
2024-10-27 17:59:23,685 (beam_search:429) INFO: max output length: 199
2024-10-27 17:59:23,685 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:24,912 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:24,913 (beam_search:476) INFO:  -5.63 * 1.0 =  -5.63 for ctc
2024-10-27 17:59:24,913 (beam_search:479) INFO: total log probability: -5.63
2024-10-27 17:59:24,913 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:59:24,913 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:24,913 (beam_search:483) INFO: best hypo: WELLTHEINACIRCUITPUTACIRCUITTOGETHERSOPARTATHERE'STHERE'SNOTHINGASACIRCUITACIRCUITISJUSTLIKEAPATHWAYFORTHEENERGYTO

2024-10-27 17:59:24,916 (asr_inference:509) INFO: speech length: 103826
2024-10-27 17:59:28,725 (beam_search:428) INFO: decoder input length: 80
2024-10-27 17:59:28,725 (beam_search:429) INFO: max output length: 80
2024-10-27 17:59:28,725 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:28,862 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:28,862 (beam_search:476) INFO:  -2.49 * 1.0 =  -2.49 for ctc
2024-10-27 17:59:28,862 (beam_search:479) INFO: total log probability: -2.49
2024-10-27 17:59:28,863 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:59:28,863 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:28,863 (beam_search:483) INFO: best hypo: HAVEDINSIDETHEMANDAREUSEDTOPOWER

2024-10-27 17:59:28,865 (asr_inference:509) INFO: speech length: 99526
2024-10-27 17:59:32,357 (beam_search:428) INFO: decoder input length: 77
2024-10-27 17:59:32,357 (beam_search:429) INFO: max output length: 77
2024-10-27 17:59:32,357 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:32,559 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:32,559 (beam_search:476) INFO:  -3.38 * 1.0 =  -3.38 for ctc
2024-10-27 17:59:32,559 (beam_search:479) INFO: total log probability: -3.38
2024-10-27 17:59:32,559 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:59:32,559 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:32,559 (beam_search:483) INFO: best hypo: IFTHED'TTHEBULBWOULDNOTLIGHTUPBECAUSETHEREISNOENERGY

2024-10-27 17:59:32,561 (asr_inference:509) INFO: speech length: 63626
2024-10-27 17:59:34,884 (beam_search:428) INFO: decoder input length: 49
2024-10-27 17:59:34,884 (beam_search:429) INFO: max output length: 49
2024-10-27 17:59:34,884 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:34,945 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:34,946 (beam_search:476) INFO:  -0.73 * 1.0 =  -0.73 for ctc
2024-10-27 17:59:34,946 (beam_search:479) INFO: total log probability: -0.73
2024-10-27 17:59:34,946 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:59:34,946 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:34,946 (beam_search:483) INFO: best hypo: THEDCELLISTHEOFENERGY

2024-10-27 17:59:34,948 (asr_inference:509) INFO: speech length: 61473
2024-10-27 17:59:37,227 (beam_search:428) INFO: decoder input length: 47
2024-10-27 17:59:37,227 (beam_search:429) INFO: max output length: 47
2024-10-27 17:59:37,227 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:37,286 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:37,286 (beam_search:476) INFO:  -0.90 * 1.0 =  -0.90 for ctc
2024-10-27 17:59:37,286 (beam_search:479) INFO: total log probability: -0.90
2024-10-27 17:59:37,286 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:59:37,286 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:37,286 (beam_search:483) INFO: best hypo: THEDCELLPOWERINTOTHECIRCUIT

2024-10-27 17:59:37,289 (asr_inference:509) INFO: speech length: 177107
2024-10-27 17:59:44,284 (beam_search:428) INFO: decoder input length: 137
2024-10-27 17:59:44,284 (beam_search:429) INFO: max output length: 137
2024-10-27 17:59:44,284 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:44,478 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:44,478 (beam_search:476) INFO:  -2.58 * 1.0 =  -2.58 for ctc
2024-10-27 17:59:44,478 (beam_search:479) INFO: total log probability: -2.58
2024-10-27 17:59:44,478 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:59:44,478 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:44,478 (beam_search:483) INFO: best hypo: THEENERGYTHETHETOITTHROUGHAND

2024-10-27 17:59:44,480 (asr_inference:509) INFO: speech length: 103025
2024-10-27 17:59:48,166 (beam_search:428) INFO: decoder input length: 79
2024-10-27 17:59:48,167 (beam_search:429) INFO: max output length: 79
2024-10-27 17:59:48,167 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:48,246 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:48,247 (beam_search:476) INFO:  -0.92 * 1.0 =  -0.92 for ctc
2024-10-27 17:59:48,247 (beam_search:479) INFO: total log probability: -0.92
2024-10-27 17:59:48,247 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:59:48,247 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:48,247 (beam_search:483) INFO: best hypo: WIRESTHEELECTRICITYORENERGY

2024-10-27 17:59:48,249 (asr_inference:509) INFO: speech length: 62399
2024-10-27 17:59:50,619 (beam_search:428) INFO: decoder input length: 48
2024-10-27 17:59:50,619 (beam_search:429) INFO: max output length: 48
2024-10-27 17:59:50,620 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:50,663 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:50,664 (beam_search:476) INFO:  -0.93 * 1.0 =  -0.93 for ctc
2024-10-27 17:59:50,664 (beam_search:479) INFO: total log probability: -0.93
2024-10-27 17:59:50,664 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:59:50,664 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:50,664 (beam_search:483) INFO: best hypo: WIRESCARRYTHEENERGYTO

2024-10-27 17:59:50,667 (asr_inference:509) INFO: speech length: 126533
2024-10-27 17:59:55,313 (beam_search:428) INFO: decoder input length: 98
2024-10-27 17:59:55,313 (beam_search:429) INFO: max output length: 98
2024-10-27 17:59:55,313 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:55,482 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:55,482 (beam_search:476) INFO:  -1.45 * 1.0 =  -1.45 for ctc
2024-10-27 17:59:55,482 (beam_search:479) INFO: total log probability: -1.45
2024-10-27 17:59:55,482 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:59:55,482 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:55,482 (beam_search:483) INFO: best hypo: THELIGHTBULBSARELIKEOFTHEENERGYANDLIGHT

2024-10-27 17:59:55,485 (asr_inference:509) INFO: speech length: 61139
2024-10-27 17:59:57,752 (beam_search:428) INFO: decoder input length: 47
2024-10-27 17:59:57,752 (beam_search:429) INFO: max output length: 47
2024-10-27 17:59:57,752 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:57,802 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:57,802 (beam_search:476) INFO:  -0.94 * 1.0 =  -0.94 for ctc
2024-10-27 17:59:57,802 (beam_search:479) INFO: total log probability: -0.94
2024-10-27 17:59:57,802 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:59:57,803 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:57,803 (beam_search:483) INFO: best hypo: BULBSTHEENERGYFROMTHE

2024-10-27 17:59:57,805 (asr_inference:509) INFO: speech length: 127773
2024-10-27 18:00:02,410 (beam_search:428) INFO: decoder input length: 99
2024-10-27 18:00:02,410 (beam_search:429) INFO: max output length: 99
2024-10-27 18:00:02,410 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:02,673 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:02,673 (beam_search:476) INFO:  -3.68 * 1.0 =  -3.68 for ctc
2024-10-27 18:00:02,673 (beam_search:479) INFO: total log probability: -3.68
2024-10-27 18:00:02,673 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:00:02,673 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:02,673 (beam_search:483) INFO: best hypo: THELIGHTBULBTHATENERGYANDMAKESTHEINTHEREGETSOHOTITLIGHT

2024-10-27 18:00:02,675 (asr_inference:509) INFO: speech length: 52627
2024-10-27 18:00:04,654 (beam_search:428) INFO: decoder input length: 40
2024-10-27 18:00:04,654 (beam_search:429) INFO: max output length: 40
2024-10-27 18:00:04,654 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:04,704 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:04,705 (beam_search:476) INFO:  -0.46 * 1.0 =  -0.46 for ctc
2024-10-27 18:00:04,705 (beam_search:479) INFO: total log probability: -0.46
2024-10-27 18:00:04,705 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:00:04,705 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:04,705 (beam_search:483) INFO: best hypo: THATENERGYISFLOWINGTHROUGHTHECIRCUIT

2024-10-27 18:00:04,707 (asr_inference:509) INFO: speech length: 320754
2024-10-27 18:00:18,900 (beam_search:428) INFO: decoder input length: 250
2024-10-27 18:00:18,900 (beam_search:429) INFO: max output length: 250
2024-10-27 18:00:18,900 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:20,745 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:20,745 (beam_search:476) INFO:  -6.18 * 1.0 =  -6.18 for ctc
2024-10-27 18:00:20,745 (beam_search:479) INFO: total log probability: -6.18
2024-10-27 18:00:20,745 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:00:20,745 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:20,745 (beam_search:483) INFO: best hypo: SEETHEENERGYFLOWINGTHROUGHTHEWIRESTHROUGHTHELIGHTBULBWELLSOTHEENERGYOUTOFTHEDCELLTHEWIRESITSTHEBULBGOESTHROUGHTHEANDOUTTHROUGHTHEBULBINTOTHEPOSITIVEOFTHE

2024-10-27 18:00:20,747 (asr_inference:509) INFO: speech length: 103576
2024-10-27 18:00:24,501 (beam_search:428) INFO: decoder input length: 80
2024-10-27 18:00:24,502 (beam_search:429) INFO: max output length: 80
2024-10-27 18:00:24,502 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:24,703 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:24,703 (beam_search:476) INFO:  -1.77 * 1.0 =  -1.77 for ctc
2024-10-27 18:00:24,703 (beam_search:479) INFO: total log probability: -1.77
2024-10-27 18:00:24,703 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:00:24,703 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:24,703 (beam_search:483) INFO: best hypo: SOTHEENERGY'SOUTTHENEGATIVESIDEANDINTHROUGHTHEPOSITIVESIDE

2024-10-27 18:00:24,706 (asr_inference:509) INFO: speech length: 111600
2024-10-27 18:00:28,726 (beam_search:428) INFO: decoder input length: 86
2024-10-27 18:00:28,726 (beam_search:429) INFO: max output length: 86
2024-10-27 18:00:28,726 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:28,907 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:28,907 (beam_search:476) INFO:  -2.73 * 1.0 =  -2.73 for ctc
2024-10-27 18:00:28,907 (beam_search:479) INFO: total log probability: -2.73
2024-10-27 18:00:28,907 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:00:28,907 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:28,907 (beam_search:483) INFO: best hypo: THEPOSITIVEISINGTHEENERGYTHATCOMESOUTOFTHESIDE

2024-10-27 18:00:28,910 (asr_inference:509) INFO: speech length: 118184
2024-10-27 18:00:33,233 (beam_search:428) INFO: decoder input length: 91
2024-10-27 18:00:33,234 (beam_search:429) INFO: max output length: 91
2024-10-27 18:00:33,234 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:33,426 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:33,427 (beam_search:476) INFO:  -2.46 * 1.0 =  -2.46 for ctc
2024-10-27 18:00:33,427 (beam_search:479) INFO: total log probability: -2.46
2024-10-27 18:00:33,427 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:00:33,427 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:33,427 (beam_search:483) INFO: best hypo: WELLELECTRICITYINTOTHEDCELLTHEPOSITIVEFROMTHELIGHTBULB

2024-10-27 18:00:33,430 (asr_inference:509) INFO: speech length: 48255
2024-10-27 18:00:35,322 (beam_search:428) INFO: decoder input length: 37
2024-10-27 18:00:35,322 (beam_search:429) INFO: max output length: 37
2024-10-27 18:00:35,322 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:35,346 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:35,346 (beam_search:476) INFO:  -0.87 * 1.0 =  -0.87 for ctc
2024-10-27 18:00:35,346 (beam_search:479) INFO: total log probability: -0.87
2024-10-27 18:00:35,346 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:00:35,346 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:35,346 (beam_search:483) INFO: best hypo: THEFLOWINGTHROUGH

2024-10-27 18:00:35,348 (asr_inference:509) INFO: speech length: 193218
2024-10-27 18:00:42,714 (beam_search:428) INFO: decoder input length: 150
2024-10-27 18:00:42,714 (beam_search:429) INFO: max output length: 150
2024-10-27 18:00:42,714 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:43,039 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:43,039 (beam_search:476) INFO:  -1.97 * 1.0 =  -1.97 for ctc
2024-10-27 18:00:43,041 (beam_search:479) INFO: total log probability: -1.97
2024-10-27 18:00:43,041 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:00:43,041 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:43,041 (beam_search:483) INFO: best hypo: WELLSOITSOTHENEGATIVETHEENERGYANDTHEPOSITIVETHE

2024-10-27 18:00:43,043 (asr_inference:509) INFO: speech length: 123033
2024-10-27 18:00:47,407 (beam_search:428) INFO: decoder input length: 95
2024-10-27 18:00:47,407 (beam_search:429) INFO: max output length: 95
2024-10-27 18:00:47,407 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:47,662 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:47,663 (beam_search:476) INFO:  -1.47 * 1.0 =  -1.47 for ctc
2024-10-27 18:00:47,663 (beam_search:479) INFO: total log probability: -1.47
2024-10-27 18:00:47,663 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:00:47,663 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:47,663 (beam_search:483) INFO: best hypo: ONTHEDCELLITHEFLOWINGOUTOFTHEANDINTHROUGHTHEPOSITIVE

2024-10-27 18:00:47,666 (asr_inference:509) INFO: speech length: 146728
2024-10-27 18:00:53,361 (beam_search:428) INFO: decoder input length: 114
2024-10-27 18:00:53,361 (beam_search:429) INFO: max output length: 114
2024-10-27 18:00:53,361 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:53,688 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:53,688 (beam_search:476) INFO:  -5.18 * 1.0 =  -5.18 for ctc
2024-10-27 18:00:53,688 (beam_search:479) INFO: total log probability: -5.18
2024-10-27 18:00:53,688 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:00:53,688 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:53,689 (beam_search:483) INFO: best hypo: WELLIFYOUITIITWILLTHROUGHTHEBULBANDTHEBULBWILLSTILLLIGHT

2024-10-27 18:00:53,692 (asr_inference:509) INFO: speech length: 124866
2024-10-27 18:00:58,186 (beam_search:428) INFO: decoder input length: 97
2024-10-27 18:00:58,186 (beam_search:429) INFO: max output length: 97
2024-10-27 18:00:58,186 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:58,413 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:58,413 (beam_search:476) INFO:  -2.23 * 1.0 =  -2.23 for ctc
2024-10-27 18:00:58,413 (beam_search:479) INFO: total log probability: -2.23
2024-10-27 18:00:58,413 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:00:58,413 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:58,414 (beam_search:483) INFO: best hypo: WELLWHENYOUTHEDCELLTHEENERGYITHINKSTILLTHEWAY

2024-10-27 18:00:58,417 (asr_inference:509) INFO: speech length: 260467
2024-10-27 18:01:09,029 (beam_search:428) INFO: decoder input length: 202
2024-10-27 18:01:09,029 (beam_search:429) INFO: max output length: 202
2024-10-27 18:01:09,029 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:10,293 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:10,293 (beam_search:476) INFO:  -6.07 * 1.0 =  -6.07 for ctc
2024-10-27 18:01:10,293 (beam_search:479) INFO: total log probability: -6.07
2024-10-27 18:01:10,294 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:01:10,294 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:10,294 (beam_search:483) INFO: best hypo: THEISTHATAWIRESTHEOFTHEDCELLANDTHENTHATWIRESTHEBULBOFTHEBULBANDTHENWITHTHEPOSITIVEANOTHERWIRESANDTHATSAMESTHE

2024-10-27 18:01:10,296 (asr_inference:509) INFO: speech length: 229315
2024-10-27 18:01:19,363 (beam_search:428) INFO: decoder input length: 178
2024-10-27 18:01:19,363 (beam_search:429) INFO: max output length: 178
2024-10-27 18:01:19,363 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:20,192 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:20,193 (beam_search:476) INFO:  -2.86 * 1.0 =  -2.86 for ctc
2024-10-27 18:01:20,193 (beam_search:479) INFO: total log probability: -2.86
2024-10-27 18:01:20,193 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:01:20,193 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:20,193 (beam_search:483) INFO: best hypo: WELLITLOOKSLIKETHATTHEENERGYISFLOWINGTHROUGHTHENEGATIVEOFTHEDTHROUGHTHEBULBANDBACKTHROUGHTHEDCELLPOSITIVE

2024-10-27 18:01:20,195 (asr_inference:509) INFO: speech length: 173196
2024-10-27 18:01:26,523 (beam_search:428) INFO: decoder input length: 134
2024-10-27 18:01:26,523 (beam_search:429) INFO: max output length: 134
2024-10-27 18:01:26,523 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:26,903 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:26,903 (beam_search:476) INFO:  -3.99 * 1.0 =  -3.99 for ctc
2024-10-27 18:01:26,903 (beam_search:479) INFO: total log probability: -3.99
2024-10-27 18:01:26,904 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:01:26,904 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:26,904 (beam_search:483) INFO: best hypo: THEHAVETOBECONNECTEDTOALLTHEPOINTSTOGETTHETOTHERIGHTWAY

2024-10-27 18:01:26,906 (asr_inference:509) INFO: speech length: 108678
2024-10-27 18:01:30,927 (beam_search:428) INFO: decoder input length: 84
2024-10-27 18:01:30,927 (beam_search:429) INFO: max output length: 84
2024-10-27 18:01:30,927 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:31,113 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:31,113 (beam_search:476) INFO:  -2.91 * 1.0 =  -2.91 for ctc
2024-10-27 18:01:31,113 (beam_search:479) INFO: total log probability: -2.91
2024-10-27 18:01:31,113 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:01:31,113 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:31,113 (beam_search:483) INFO: best hypo: THATONEOFTHEWOULDNOTGETEDANDTHEBULBWOULDTURN

2024-10-27 18:01:31,115 (asr_inference:509) INFO: speech length: 37501
2024-10-27 18:01:32,518 (beam_search:428) INFO: decoder input length: 28
2024-10-27 18:01:32,518 (beam_search:429) INFO: max output length: 28
2024-10-27 18:01:32,518 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:32,548 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:32,548 (beam_search:476) INFO:  -0.15 * 1.0 =  -0.15 for ctc
2024-10-27 18:01:32,549 (beam_search:479) INFO: total log probability: -0.15
2024-10-27 18:01:32,549 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:01:32,549 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:32,549 (beam_search:483) INFO: best hypo: THELIGHTBULBTURNSON

2024-10-27 18:01:32,551 (asr_inference:509) INFO: speech length: 114727
2024-10-27 18:01:36,774 (beam_search:428) INFO: decoder input length: 89
2024-10-27 18:01:36,775 (beam_search:429) INFO: max output length: 89
2024-10-27 18:01:36,775 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:36,961 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:36,961 (beam_search:476) INFO:  -2.03 * 1.0 =  -2.03 for ctc
2024-10-27 18:01:36,961 (beam_search:479) INFO: total log probability: -2.03
2024-10-27 18:01:36,961 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:01:36,961 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:36,961 (beam_search:483) INFO: best hypo: WE'VEBEENLEARNINGABOUTANDSOLARENERGYWITHWITHA

2024-10-27 18:01:36,964 (asr_inference:509) INFO: speech length: 35023
2024-10-27 18:01:38,356 (beam_search:428) INFO: decoder input length: 26
2024-10-27 18:01:38,356 (beam_search:429) INFO: max output length: 26
2024-10-27 18:01:38,356 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:38,377 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:38,377 (beam_search:476) INFO:  -1.73 * 1.0 =  -1.73 for ctc
2024-10-27 18:01:38,377 (beam_search:479) INFO: total log probability: -1.73
2024-10-27 18:01:38,377 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:01:38,377 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:38,377 (beam_search:483) INFO: best hypo: ISPOWERINGTHE

2024-10-27 18:01:38,379 (asr_inference:509) INFO: speech length: 149651
2024-10-27 18:01:43,894 (beam_search:428) INFO: decoder input length: 116
2024-10-27 18:01:43,894 (beam_search:429) INFO: max output length: 116
2024-10-27 18:01:43,894 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:44,168 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:44,169 (beam_search:476) INFO:  -2.56 * 1.0 =  -2.56 for ctc
2024-10-27 18:01:44,169 (beam_search:479) INFO: total log probability: -2.56
2024-10-27 18:01:44,169 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:01:44,169 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:44,169 (beam_search:483) INFO: best hypo: THEISANDTHEISSOTHATSOTHATITSHOWSTHATTHEIS

2024-10-27 18:01:44,171 (asr_inference:509) INFO: speech length: 30864
2024-10-27 18:01:45,414 (beam_search:428) INFO: decoder input length: 23
2024-10-27 18:01:45,414 (beam_search:429) INFO: max output length: 23
2024-10-27 18:01:45,414 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:45,427 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:45,427 (beam_search:476) INFO:  -0.82 * 1.0 =  -0.82 for ctc
2024-10-27 18:01:45,427 (beam_search:479) INFO: total log probability: -0.82
2024-10-27 18:01:45,427 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:01:45,427 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:45,427 (beam_search:483) INFO: best hypo: THEIS

2024-10-27 18:01:45,429 (asr_inference:509) INFO: speech length: 82171
2024-10-27 18:01:48,335 (beam_search:428) INFO: decoder input length: 63
2024-10-27 18:01:48,335 (beam_search:429) INFO: max output length: 63
2024-10-27 18:01:48,335 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:48,412 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:48,412 (beam_search:476) INFO:  -0.87 * 1.0 =  -0.87 for ctc
2024-10-27 18:01:48,412 (beam_search:479) INFO: total log probability: -0.87
2024-10-27 18:01:48,412 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:01:48,412 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:48,412 (beam_search:483) INFO: best hypo: THEISMOVINGENERGYISTHROUGHTHE

2024-10-27 18:01:48,414 (asr_inference:509) INFO: speech length: 125988
2024-10-27 18:01:52,922 (beam_search:428) INFO: decoder input length: 97
2024-10-27 18:01:52,922 (beam_search:429) INFO: max output length: 97
2024-10-27 18:01:52,922 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:53,135 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:53,135 (beam_search:476) INFO:  -2.67 * 1.0 =  -2.67 for ctc
2024-10-27 18:01:53,135 (beam_search:479) INFO: total log probability: -2.67
2024-10-27 18:01:53,135 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:01:53,135 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:53,135 (beam_search:483) INFO: best hypo: WELLTHEREWASTHEWIRESHAVETOBECONNECTEDTOTHERIGHT

2024-10-27 18:01:53,137 (asr_inference:509) INFO: speech length: 38931
2024-10-27 18:01:54,643 (beam_search:428) INFO: decoder input length: 29
2024-10-27 18:01:54,643 (beam_search:429) INFO: max output length: 29
2024-10-27 18:01:54,643 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:54,662 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:54,662 (beam_search:476) INFO:  -0.25 * 1.0 =  -0.25 for ctc
2024-10-27 18:01:54,663 (beam_search:479) INFO: total log probability: -0.25
2024-10-27 18:01:54,663 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:01:54,663 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:54,663 (beam_search:483) INFO: best hypo: ENERGYTOTHE

2024-10-27 18:01:54,665 (asr_inference:509) INFO: speech length: 94508
2024-10-27 18:01:58,287 (beam_search:428) INFO: decoder input length: 73
2024-10-27 18:01:58,287 (beam_search:429) INFO: max output length: 73
2024-10-27 18:01:58,287 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:58,424 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:58,424 (beam_search:476) INFO:  -0.84 * 1.0 =  -0.84 for ctc
2024-10-27 18:01:58,424 (beam_search:479) INFO: total log probability: -0.84
2024-10-27 18:01:58,424 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:01:58,424 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:58,425 (beam_search:483) INFO: best hypo: INTHECIRCUITTHEOPENBECAUSETHESWITCHISNOTDOWN

2024-10-27 18:01:58,427 (asr_inference:509) INFO: speech length: 64647
2024-10-27 18:02:00,813 (beam_search:428) INFO: decoder input length: 50
2024-10-27 18:02:00,813 (beam_search:429) INFO: max output length: 50
2024-10-27 18:02:00,813 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:00,869 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:00,869 (beam_search:476) INFO:  -2.10 * 1.0 =  -2.10 for ctc
2024-10-27 18:02:00,869 (beam_search:479) INFO: total log probability: -2.10
2024-10-27 18:02:00,869 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:02:00,869 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:00,869 (beam_search:483) INFO: best hypo: TOCAUSEALLTHEARETOUCHING

2024-10-27 18:02:00,871 (asr_inference:509) INFO: speech length: 85031
2024-10-27 18:02:03,877 (beam_search:428) INFO: decoder input length: 65
2024-10-27 18:02:03,877 (beam_search:429) INFO: max output length: 65
2024-10-27 18:02:03,877 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:03,982 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:03,982 (beam_search:476) INFO:  -2.43 * 1.0 =  -2.43 for ctc
2024-10-27 18:02:03,982 (beam_search:479) INFO: total log probability: -2.43
2024-10-27 18:02:03,982 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:02:03,982 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:03,982 (beam_search:483) INFO: best hypo: ISFLOWINGTHROUGHTHEALLTHEPOINTSARETOUCHING

2024-10-27 18:02:03,984 (asr_inference:509) INFO: speech length: 110922
2024-10-27 18:02:07,931 (beam_search:428) INFO: decoder input length: 86
2024-10-27 18:02:07,931 (beam_search:429) INFO: max output length: 86
2024-10-27 18:02:07,931 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:08,102 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:08,102 (beam_search:476) INFO:  -1.68 * 1.0 =  -1.68 for ctc
2024-10-27 18:02:08,102 (beam_search:479) INFO: total log probability: -1.68
2024-10-27 18:02:08,102 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:02:08,102 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:08,102 (beam_search:483) INFO: best hypo: STHEANDENERGYCAN'TBACKINTOTHEBATTERY

2024-10-27 18:02:08,105 (asr_inference:509) INFO: speech length: 34481
2024-10-27 18:02:09,471 (beam_search:428) INFO: decoder input length: 26
2024-10-27 18:02:09,471 (beam_search:429) INFO: max output length: 26
2024-10-27 18:02:09,471 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:09,495 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:09,495 (beam_search:476) INFO:  -0.14 * 1.0 =  -0.14 for ctc
2024-10-27 18:02:09,495 (beam_search:479) INFO: total log probability: -0.14
2024-10-27 18:02:09,495 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:02:09,495 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:09,496 (beam_search:483) INFO: best hypo: YOUHAVETOTHESWITCH

2024-10-27 18:02:09,498 (asr_inference:509) INFO: speech length: 152584
2024-10-27 18:02:15,013 (beam_search:428) INFO: decoder input length: 118
2024-10-27 18:02:15,013 (beam_search:429) INFO: max output length: 118
2024-10-27 18:02:15,013 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:15,370 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:15,370 (beam_search:476) INFO:  -3.03 * 1.0 =  -3.03 for ctc
2024-10-27 18:02:15,370 (beam_search:479) INFO: total log probability: -3.03
2024-10-27 18:02:15,370 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:02:15,370 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:15,370 (beam_search:483) INFO: best hypo: ITSANDTHEWHENTHESWITCHISTHEMOTORWILLRUNWHENTHEISOPENTHEMOTOR

2024-10-27 18:02:15,373 (asr_inference:509) INFO: speech length: 189858
2024-10-27 18:02:23,067 (beam_search:428) INFO: decoder input length: 147
2024-10-27 18:02:23,067 (beam_search:429) INFO: max output length: 147
2024-10-27 18:02:23,067 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:23,422 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:23,422 (beam_search:476) INFO:  -3.64 * 1.0 =  -3.64 for ctc
2024-10-27 18:02:23,422 (beam_search:479) INFO: total log probability: -3.64
2024-10-27 18:02:23,422 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:02:23,422 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:23,423 (beam_search:483) INFO: best hypo: SOLARTHESENERGYANDITINTOTHEELECTRICITYFORACIRCUIT

2024-10-27 18:02:23,426 (asr_inference:509) INFO: speech length: 90236
2024-10-27 18:02:26,698 (beam_search:428) INFO: decoder input length: 69
2024-10-27 18:02:26,698 (beam_search:429) INFO: max output length: 69
2024-10-27 18:02:26,698 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:26,845 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:26,845 (beam_search:476) INFO:  -3.93 * 1.0 =  -3.93 for ctc
2024-10-27 18:02:26,845 (beam_search:479) INFO: total log probability: -3.93
2024-10-27 18:02:26,845 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:02:26,845 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:26,845 (beam_search:483) INFO: best hypo: THESOLARCELLISTAKINGTHESENERGYANDTHEMOTORRUN

2024-10-27 18:02:26,848 (asr_inference:509) INFO: speech length: 44589
2024-10-27 18:02:28,548 (beam_search:428) INFO: decoder input length: 34
2024-10-27 18:02:28,548 (beam_search:429) INFO: max output length: 34
2024-10-27 18:02:28,548 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:28,589 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:28,589 (beam_search:476) INFO:  -1.08 * 1.0 =  -1.08 for ctc
2024-10-27 18:02:28,589 (beam_search:479) INFO: total log probability: -1.08
2024-10-27 18:02:28,589 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:02:28,589 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:28,589 (beam_search:483) INFO: best hypo: ENERGYHASTOFLOWTHROUGHTHECIRCUIT

2024-10-27 18:02:28,592 (asr_inference:509) INFO: speech length: 61115
2024-10-27 18:02:30,749 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:02:30,749 (beam_search:429) INFO: max output length: 47
2024-10-27 18:02:30,749 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:30,814 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:30,814 (beam_search:476) INFO:  -2.08 * 1.0 =  -2.08 for ctc
2024-10-27 18:02:30,814 (beam_search:479) INFO: total log probability: -2.08
2024-10-27 18:02:30,814 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:02:30,814 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:30,814 (beam_search:483) INFO: best hypo: THESUNSLIGHTISTHELARCELL

2024-10-27 18:02:30,817 (asr_inference:509) INFO: speech length: 266725
2024-10-27 18:02:41,988 (beam_search:428) INFO: decoder input length: 207
2024-10-27 18:02:41,988 (beam_search:429) INFO: max output length: 207
2024-10-27 18:02:41,988 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:43,319 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:43,319 (beam_search:476) INFO:  -4.00 * 1.0 =  -4.00 for ctc
2024-10-27 18:02:43,320 (beam_search:479) INFO: total log probability: -4.00
2024-10-27 18:02:43,320 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:02:43,320 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:43,320 (beam_search:483) INFO: best hypo: WELLIFTHEOUTEVERYTHINGWOULDBECAUSETHENTHEREWOULDBENOENERGYFORTHECIRCUITSOSOIFTHEIFITGOTTHEREWOULDBENOENERGYTOGOINTOTHATSOLARCELL

2024-10-27 18:02:43,323 (asr_inference:509) INFO: speech length: 68084
2024-10-27 18:02:45,714 (beam_search:428) INFO: decoder input length: 52
2024-10-27 18:02:45,714 (beam_search:429) INFO: max output length: 52
2024-10-27 18:02:45,714 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:45,786 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:45,786 (beam_search:476) INFO:  -1.49 * 1.0 =  -1.49 for ctc
2024-10-27 18:02:45,786 (beam_search:479) INFO: total log probability: -1.49
2024-10-27 18:02:45,787 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:02:45,787 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:45,787 (beam_search:483) INFO: best hypo: THEWHENYOUONTHEITMAKESTHE

2024-10-27 18:02:45,789 (asr_inference:509) INFO: speech length: 68879
2024-10-27 18:02:48,360 (beam_search:428) INFO: decoder input length: 53
2024-10-27 18:02:48,361 (beam_search:429) INFO: max output length: 53
2024-10-27 18:02:48,361 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:48,439 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:48,439 (beam_search:476) INFO:  -1.75 * 1.0 =  -1.75 for ctc
2024-10-27 18:02:48,440 (beam_search:479) INFO: total log probability: -1.75
2024-10-27 18:02:48,440 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:02:48,440 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:48,440 (beam_search:483) INFO: best hypo: CAUSETHEREISNOENERGYFORTORUN

2024-10-27 18:02:48,442 (asr_inference:509) INFO: speech length: 57536
2024-10-27 18:02:50,621 (beam_search:428) INFO: decoder input length: 44
2024-10-27 18:02:50,621 (beam_search:429) INFO: max output length: 44
2024-10-27 18:02:50,621 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:50,676 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:50,676 (beam_search:476) INFO:  -2.26 * 1.0 =  -2.26 for ctc
2024-10-27 18:02:50,676 (beam_search:479) INFO: total log probability: -2.26
2024-10-27 18:02:50,676 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:02:50,676 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:50,676 (beam_search:483) INFO: best hypo: THEMOTORSTARTSNOWTHEREISENERGY

2024-10-27 18:02:50,679 (asr_inference:509) INFO: speech length: 90795
2024-10-27 18:02:54,040 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:02:54,040 (beam_search:429) INFO: max output length: 70
2024-10-27 18:02:54,040 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:54,130 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:54,131 (beam_search:476) INFO:  -0.99 * 1.0 =  -0.99 for ctc
2024-10-27 18:02:54,131 (beam_search:479) INFO: total log probability: -0.99
2024-10-27 18:02:54,131 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:02:54,131 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:54,131 (beam_search:483) INFO: best hypo: THATTHATTHATMAKESTHEMOTORRUN

2024-10-27 18:02:54,134 (asr_inference:509) INFO: speech length: 105887
2024-10-27 18:02:57,858 (beam_search:428) INFO: decoder input length: 82
2024-10-27 18:02:57,858 (beam_search:429) INFO: max output length: 82
2024-10-27 18:02:57,858 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:58,030 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:58,030 (beam_search:476) INFO:  -1.69 * 1.0 =  -1.69 for ctc
2024-10-27 18:02:58,030 (beam_search:479) INFO: total log probability: -1.69
2024-10-27 18:02:58,030 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:02:58,030 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:58,030 (beam_search:483) INFO: best hypo: ISTHEENERGYFROMTHETOTHEANDBACKTOTHEBATTERY

2024-10-27 18:02:58,033 (asr_inference:509) INFO: speech length: 62771
2024-10-27 18:03:00,321 (beam_search:428) INFO: decoder input length: 48
2024-10-27 18:03:00,321 (beam_search:429) INFO: max output length: 48
2024-10-27 18:03:00,321 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:00,371 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:00,371 (beam_search:476) INFO:  -2.36 * 1.0 =  -2.36 for ctc
2024-10-27 18:03:00,371 (beam_search:479) INFO: total log probability: -2.36
2024-10-27 18:03:00,371 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 18:03:00,371 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:00,371 (beam_search:483) INFO: best hypo: WEVELEARNINGABOUTANDINSULATORS

2024-10-27 18:03:00,373 (asr_inference:509) INFO: speech length: 143070
2024-10-27 18:03:05,797 (beam_search:428) INFO: decoder input length: 111
2024-10-27 18:03:05,797 (beam_search:429) INFO: max output length: 111
2024-10-27 18:03:05,797 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:06,046 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:06,046 (beam_search:476) INFO:  -2.25 * 1.0 =  -2.25 for ctc
2024-10-27 18:03:06,046 (beam_search:479) INFO: total log probability: -2.25
2024-10-27 18:03:06,046 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:03:06,046 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:06,047 (beam_search:483) INFO: best hypo: THEOPENBUTTHERE'SAOFTHINGSONTHESIDE

2024-10-27 18:03:06,049 (asr_inference:509) INFO: speech length: 187658
2024-10-27 18:03:13,146 (beam_search:428) INFO: decoder input length: 146
2024-10-27 18:03:13,146 (beam_search:429) INFO: max output length: 146
2024-10-27 18:03:13,146 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:13,610 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:13,610 (beam_search:476) INFO:  -4.38 * 1.0 =  -4.38 for ctc
2024-10-27 18:03:13,610 (beam_search:479) INFO: total log probability: -4.38
2024-10-27 18:03:13,610 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:03:13,610 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:13,610 (beam_search:483) INFO: best hypo: WHENYOUUSEATHEWILLBECAUSEENERGYISFLOWINGIFYOUUSEANTHEMOTORWILLNOT

2024-10-27 18:03:13,613 (asr_inference:509) INFO: speech length: 98639
2024-10-27 18:03:17,259 (beam_search:428) INFO: decoder input length: 76
2024-10-27 18:03:17,259 (beam_search:429) INFO: max output length: 76
2024-10-27 18:03:17,259 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:17,358 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:17,358 (beam_search:476) INFO:  -0.45 * 1.0 =  -0.45 for ctc
2024-10-27 18:03:17,358 (beam_search:479) INFO: total log probability: -0.45
2024-10-27 18:03:17,358 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:03:17,358 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:17,358 (beam_search:483) INFO: best hypo: ALLTHEMAKETHETHEMOTORRUN

2024-10-27 18:03:17,360 (asr_inference:509) INFO: speech length: 61864
2024-10-27 18:03:19,653 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:03:19,654 (beam_search:429) INFO: max output length: 47
2024-10-27 18:03:19,654 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:19,706 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:19,706 (beam_search:476) INFO:  -1.26 * 1.0 =  -1.26 for ctc
2024-10-27 18:03:19,706 (beam_search:479) INFO: total log probability: -1.26
2024-10-27 18:03:19,706 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:03:19,706 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:19,706 (beam_search:483) INFO: best hypo: WELLIFYOUPUTANON

2024-10-27 18:03:19,709 (asr_inference:509) INFO: speech length: 219161
2024-10-27 18:03:28,315 (beam_search:428) INFO: decoder input length: 170
2024-10-27 18:03:28,315 (beam_search:429) INFO: max output length: 170
2024-10-27 18:03:28,315 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:28,741 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:28,741 (beam_search:476) INFO:  -2.63 * 1.0 =  -2.63 for ctc
2024-10-27 18:03:28,742 (beam_search:479) INFO: total log probability: -2.63
2024-10-27 18:03:28,742 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:03:28,742 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:28,742 (beam_search:483) INFO: best hypo: WELLYOUCOULDLIKEASTEELNAILORYOUCOULDUSELIKEAOROR

2024-10-27 18:03:28,744 (asr_inference:509) INFO: speech length: 223307
2024-10-27 18:03:37,800 (beam_search:428) INFO: decoder input length: 173
2024-10-27 18:03:37,800 (beam_search:429) INFO: max output length: 173
2024-10-27 18:03:37,800 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:38,486 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:38,486 (beam_search:476) INFO:  -4.63 * 1.0 =  -4.63 for ctc
2024-10-27 18:03:38,486 (beam_search:479) INFO: total log probability: -4.63
2024-10-27 18:03:38,486 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:03:38,486 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:38,486 (beam_search:483) INFO: best hypo: THEOFARELIKETHEYAPATHWAYTHROUGHTHESWITCHANDINSULATORSAREJUSTLIKEATHATELECTRICITYFROMFLOWINGTHROUGHTHE

2024-10-27 18:03:38,490 (asr_inference:509) INFO: speech length: 19064
2024-10-27 18:03:39,440 (beam_search:428) INFO: decoder input length: 14
2024-10-27 18:03:39,440 (beam_search:429) INFO: max output length: 14
2024-10-27 18:03:39,440 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:39,449 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:39,449 (beam_search:476) INFO:  -0.78 * 1.0 =  -0.78 for ctc
2024-10-27 18:03:39,449 (beam_search:479) INFO: total log probability: -0.78
2024-10-27 18:03:39,449 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:03:39,449 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:39,449 (beam_search:483) INFO: best hypo: TO

2024-10-27 18:03:39,451 (asr_inference:509) INFO: speech length: 95296
2024-10-27 18:03:42,961 (beam_search:428) INFO: decoder input length: 73
2024-10-27 18:03:42,961 (beam_search:429) INFO: max output length: 73
2024-10-27 18:03:42,961 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:43,064 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:43,064 (beam_search:476) INFO:  -2.88 * 1.0 =  -2.88 for ctc
2024-10-27 18:03:43,064 (beam_search:479) INFO: total log probability: -2.88
2024-10-27 18:03:43,064 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:03:43,064 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:43,065 (beam_search:483) INFO: best hypo: ITISBECAUSESTICKDOESN'T

2024-10-27 18:03:43,067 (asr_inference:509) INFO: speech length: 153000
2024-10-27 18:03:48,783 (beam_search:428) INFO: decoder input length: 119
2024-10-27 18:03:48,783 (beam_search:429) INFO: max output length: 119
2024-10-27 18:03:48,783 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:49,039 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:49,039 (beam_search:476) INFO:  -2.97 * 1.0 =  -2.97 for ctc
2024-10-27 18:03:49,039 (beam_search:479) INFO: total log probability: -2.97
2024-10-27 18:03:49,039 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:03:49,039 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:49,039 (beam_search:483) INFO: best hypo: ANDDON'TTHEELECTRICITYN'TTHROUGHAN

2024-10-27 18:03:49,043 (asr_inference:509) INFO: speech length: 138637
2024-10-27 18:03:54,141 (beam_search:428) INFO: decoder input length: 107
2024-10-27 18:03:54,141 (beam_search:429) INFO: max output length: 107
2024-10-27 18:03:54,141 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:54,361 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:54,362 (beam_search:476) INFO:  -2.01 * 1.0 =  -2.01 for ctc
2024-10-27 18:03:54,362 (beam_search:479) INFO: total log probability: -2.01
2024-10-27 18:03:54,362 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:03:54,362 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:54,362 (beam_search:483) INFO: best hypo: KINDOFLIKELIKEAINGNOELECTRICITYTOFLOWTHROUGH

2024-10-27 18:03:54,364 (asr_inference:509) INFO: speech length: 105305
2024-10-27 18:03:58,140 (beam_search:428) INFO: decoder input length: 81
2024-10-27 18:03:58,141 (beam_search:429) INFO: max output length: 81
2024-10-27 18:03:58,141 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:58,285 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:58,285 (beam_search:476) INFO:  -2.02 * 1.0 =  -2.02 for ctc
2024-10-27 18:03:58,285 (beam_search:479) INFO: total log probability: -2.02
2024-10-27 18:03:58,285 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:03:58,285 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:58,285 (beam_search:483) INFO: best hypo: THEAREALLTHEPARTSTHATDONOTCONDUCTELECTRICITY

2024-10-27 18:03:58,288 (asr_inference:509) INFO: speech length: 39904
2024-10-27 18:03:59,758 (beam_search:428) INFO: decoder input length: 30
2024-10-27 18:03:59,759 (beam_search:429) INFO: max output length: 30
2024-10-27 18:03:59,759 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:59,766 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:59,766 (beam_search:476) INFO:  -0.49 * 1.0 =  -0.49 for ctc
2024-10-27 18:03:59,766 (beam_search:479) INFO: total log probability: -0.49
2024-10-27 18:03:59,766 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:03:59,766 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:59,766 (beam_search:483) INFO: best hypo: 

2024-10-27 18:03:59,768 (asr_inference:509) INFO: speech length: 88494
2024-10-27 18:04:02,972 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:04:02,972 (beam_search:429) INFO: max output length: 68
2024-10-27 18:04:02,972 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:03,072 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:03,072 (beam_search:476) INFO:  -1.70 * 1.0 =  -1.70 for ctc
2024-10-27 18:04:03,072 (beam_search:479) INFO: total log probability: -1.70
2024-10-27 18:04:03,072 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:04:03,072 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:03,072 (beam_search:483) INFO: best hypo: AREALLTHEPARTSTHATELECTRICITYLIKEAPATHWAY

2024-10-27 18:04:03,075 (asr_inference:509) INFO: speech length: 25497
2024-10-27 18:04:04,168 (beam_search:428) INFO: decoder input length: 19
2024-10-27 18:04:04,168 (beam_search:429) INFO: max output length: 19
2024-10-27 18:04:04,168 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:04,182 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:04,182 (beam_search:476) INFO:  -1.32 * 1.0 =  -1.32 for ctc
2024-10-27 18:04:04,182 (beam_search:479) INFO: total log probability: -1.32
2024-10-27 18:04:04,182 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:04:04,182 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:04,182 (beam_search:483) INFO: best hypo: 'REALL

2024-10-27 18:04:04,184 (asr_inference:509) INFO: speech length: 202053
2024-10-27 18:04:12,421 (beam_search:428) INFO: decoder input length: 157
2024-10-27 18:04:12,421 (beam_search:429) INFO: max output length: 157
2024-10-27 18:04:12,421 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:12,828 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:12,828 (beam_search:476) INFO:  -2.71 * 1.0 =  -2.71 for ctc
2024-10-27 18:04:12,828 (beam_search:479) INFO: total log probability: -2.71
2024-10-27 18:04:12,828 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:04:12,828 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:12,828 (beam_search:483) INFO: best hypo: THESTEELNAILTHETHETHETHETHENAILANDTHETHEBRASS

2024-10-27 18:04:12,830 (asr_inference:509) INFO: speech length: 178063
2024-10-27 18:04:19,762 (beam_search:428) INFO: decoder input length: 138
2024-10-27 18:04:19,762 (beam_search:429) INFO: max output length: 138
2024-10-27 18:04:19,762 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:19,963 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:19,964 (beam_search:476) INFO:  -1.54 * 1.0 =  -1.54 for ctc
2024-10-27 18:04:19,964 (beam_search:479) INFO: total log probability: -1.54
2024-10-27 18:04:19,964 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:04:19,964 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:19,964 (beam_search:483) INFO: best hypo: WEHAVEBEENDOINGWITHLIKEANDLIKE

2024-10-27 18:04:19,966 (asr_inference:509) INFO: speech length: 116393
2024-10-27 18:04:24,262 (beam_search:428) INFO: decoder input length: 90
2024-10-27 18:04:24,262 (beam_search:429) INFO: max output length: 90
2024-10-27 18:04:24,262 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:24,463 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:24,463 (beam_search:476) INFO:  -1.71 * 1.0 =  -1.71 for ctc
2024-10-27 18:04:24,465 (beam_search:479) INFO: total log probability: -1.71
2024-10-27 18:04:24,465 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:04:24,465 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:24,465 (beam_search:483) INFO: best hypo: ENERGYISPOWERTHATWEUSEDTOPUTTHINGSINTOORLIGHTOR

2024-10-27 18:04:24,467 (asr_inference:509) INFO: speech length: 60089
2024-10-27 18:04:26,591 (beam_search:428) INFO: decoder input length: 46
2024-10-27 18:04:26,591 (beam_search:429) INFO: max output length: 46
2024-10-27 18:04:26,591 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:26,608 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:26,609 (beam_search:476) INFO:  -0.28 * 1.0 =  -0.28 for ctc
2024-10-27 18:04:26,609 (beam_search:479) INFO: total log probability: -0.28
2024-10-27 18:04:26,609 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:04:26,609 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:26,609 (beam_search:483) INFO: best hypo: A

2024-10-27 18:04:26,611 (asr_inference:509) INFO: speech length: 31686
2024-10-27 18:04:27,920 (beam_search:428) INFO: decoder input length: 24
2024-10-27 18:04:27,920 (beam_search:429) INFO: max output length: 24
2024-10-27 18:04:27,920 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:27,930 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:27,930 (beam_search:476) INFO:  -0.11 * 1.0 =  -0.11 for ctc
2024-10-27 18:04:27,930 (beam_search:479) INFO: total log probability: -0.11
2024-10-27 18:04:27,930 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 18:04:27,930 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:27,930 (beam_search:483) INFO: best hypo: AND

2024-10-27 18:04:27,932 (asr_inference:509) INFO: speech length: 85578
2024-10-27 18:04:31,061 (beam_search:428) INFO: decoder input length: 66
2024-10-27 18:04:31,061 (beam_search:429) INFO: max output length: 66
2024-10-27 18:04:31,061 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:31,111 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:31,111 (beam_search:476) INFO:  -0.83 * 1.0 =  -0.83 for ctc
2024-10-27 18:04:31,111 (beam_search:479) INFO: total log probability: -0.83
2024-10-27 18:04:31,111 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:04:31,111 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:31,111 (beam_search:483) INFO: best hypo: SANDYOUALSO

2024-10-27 18:04:31,113 (asr_inference:509) INFO: speech length: 139049
2024-10-27 18:04:36,342 (beam_search:428) INFO: decoder input length: 108
2024-10-27 18:04:36,342 (beam_search:429) INFO: max output length: 108
2024-10-27 18:04:36,342 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:36,597 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:36,597 (beam_search:476) INFO:  -2.84 * 1.0 =  -2.84 for ctc
2024-10-27 18:04:36,597 (beam_search:479) INFO: total log probability: -2.84
2024-10-27 18:04:36,597 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:04:36,597 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:36,597 (beam_search:483) INFO: best hypo: THEISFROMTHETHEANDANDIT'SALSOINGLIGHT

2024-10-27 18:04:36,600 (asr_inference:509) INFO: speech length: 73444
2024-10-27 18:04:39,178 (beam_search:428) INFO: decoder input length: 56
2024-10-27 18:04:39,178 (beam_search:429) INFO: max output length: 56
2024-10-27 18:04:39,178 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:39,220 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:39,220 (beam_search:476) INFO:  -0.85 * 1.0 =  -0.85 for ctc
2024-10-27 18:04:39,220 (beam_search:479) INFO: total log probability: -0.85
2024-10-27 18:04:39,220 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:04:39,220 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:39,220 (beam_search:483) INFO: best hypo: ITISINGAND

2024-10-27 18:04:39,222 (asr_inference:509) INFO: speech length: 168376
2024-10-27 18:04:45,771 (beam_search:428) INFO: decoder input length: 131
2024-10-27 18:04:45,771 (beam_search:429) INFO: max output length: 131
2024-10-27 18:04:45,771 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:45,942 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:45,942 (beam_search:476) INFO:  -1.18 * 1.0 =  -1.18 for ctc
2024-10-27 18:04:45,942 (beam_search:479) INFO: total log probability: -1.18
2024-10-27 18:04:45,942 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:04:45,943 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:45,943 (beam_search:483) INFO: best hypo: THEISINGTHEISISENERGY

2024-10-27 18:04:45,945 (asr_inference:509) INFO: speech length: 29209
2024-10-27 18:04:47,117 (beam_search:428) INFO: decoder input length: 22
2024-10-27 18:04:47,117 (beam_search:429) INFO: max output length: 22
2024-10-27 18:04:47,117 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:47,134 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:47,134 (beam_search:476) INFO:  -0.70 * 1.0 =  -0.70 for ctc
2024-10-27 18:04:47,134 (beam_search:479) INFO: total log probability: -0.70
2024-10-27 18:04:47,134 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:04:47,134 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:47,134 (beam_search:483) INFO: best hypo: SHEISMOVING

2024-10-27 18:04:47,137 (asr_inference:509) INFO: speech length: 148839
2024-10-27 18:04:52,541 (beam_search:428) INFO: decoder input length: 115
2024-10-27 18:04:52,541 (beam_search:429) INFO: max output length: 115
2024-10-27 18:04:52,541 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:52,789 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:52,790 (beam_search:476) INFO:  -2.12 * 1.0 =  -2.12 for ctc
2024-10-27 18:04:52,790 (beam_search:479) INFO: total log probability: -2.12
2024-10-27 18:04:52,790 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:04:52,790 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:52,790 (beam_search:483) INFO: best hypo: THEENERGYAORKINDOFTHATTHATSAYSENERGYIS

2024-10-27 18:04:52,792 (asr_inference:509) INFO: speech length: 119702
2024-10-27 18:04:57,099 (beam_search:428) INFO: decoder input length: 93
2024-10-27 18:04:57,099 (beam_search:429) INFO: max output length: 93
2024-10-27 18:04:57,099 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:57,309 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:57,309 (beam_search:476) INFO:  -0.72 * 1.0 =  -0.72 for ctc
2024-10-27 18:04:57,309 (beam_search:479) INFO: total log probability: -0.72
2024-10-27 18:04:57,309 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:04:57,309 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:57,310 (beam_search:483) INFO: best hypo: YESYOUCANBECAUSEENERGYISGOINGTHROUGHTHETOMAKEITA

2024-10-27 18:04:57,312 (asr_inference:509) INFO: speech length: 95822
2024-10-27 18:05:00,894 (beam_search:428) INFO: decoder input length: 74
2024-10-27 18:05:00,894 (beam_search:429) INFO: max output length: 74
2024-10-27 18:05:00,894 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:01,005 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:01,006 (beam_search:476) INFO:  -2.02 * 1.0 =  -2.02 for ctc
2024-10-27 18:05:01,006 (beam_search:479) INFO: total log probability: -2.02
2024-10-27 18:05:01,006 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:05:01,006 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:01,006 (beam_search:483) INFO: best hypo: WELLTHEPERSON'SSUPA

2024-10-27 18:05:01,008 (asr_inference:509) INFO: speech length: 123075
2024-10-27 18:05:05,532 (beam_search:428) INFO: decoder input length: 95
2024-10-27 18:05:05,533 (beam_search:429) INFO: max output length: 95
2024-10-27 18:05:05,533 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:05,566 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:05,567 (beam_search:476) INFO:  -1.04 * 1.0 =  -1.04 for ctc
2024-10-27 18:05:05,567 (beam_search:479) INFO: total log probability: -1.04
2024-10-27 18:05:05,567 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:05:05,567 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:05,567 (beam_search:483) INFO: best hypo: AND

2024-10-27 18:05:05,569 (asr_inference:509) INFO: speech length: 150558
2024-10-27 18:05:11,140 (beam_search:428) INFO: decoder input length: 117
2024-10-27 18:05:11,141 (beam_search:429) INFO: max output length: 117
2024-10-27 18:05:11,141 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:11,506 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:11,506 (beam_search:476) INFO:  -3.00 * 1.0 =  -3.00 for ctc
2024-10-27 18:05:11,506 (beam_search:479) INFO: total log probability: -3.00
2024-10-27 18:05:11,506 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:05:11,506 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:11,506 (beam_search:483) INFO: best hypo: ITBESOTHAT'SWHYWESHOULDUSELIKESOLARENERGYANDENERGYANDWATER

2024-10-27 18:05:11,508 (asr_inference:509) INFO: speech length: 57758
2024-10-27 18:05:13,634 (beam_search:428) INFO: decoder input length: 44
2024-10-27 18:05:13,634 (beam_search:429) INFO: max output length: 44
2024-10-27 18:05:13,634 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:13,667 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:13,667 (beam_search:476) INFO:  -0.32 * 1.0 =  -0.32 for ctc
2024-10-27 18:05:13,667 (beam_search:479) INFO: total log probability: -0.32
2024-10-27 18:05:13,667 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:05:13,667 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:13,667 (beam_search:483) INFO: best hypo: ITSINTOTHE

2024-10-27 18:05:13,669 (asr_inference:509) INFO: speech length: 51118
2024-10-27 18:05:15,613 (beam_search:428) INFO: decoder input length: 39
2024-10-27 18:05:15,613 (beam_search:429) INFO: max output length: 39
2024-10-27 18:05:15,613 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:15,639 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:15,639 (beam_search:476) INFO:  -0.30 * 1.0 =  -0.30 for ctc
2024-10-27 18:05:15,639 (beam_search:479) INFO: total log probability: -0.30
2024-10-27 18:05:15,639 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:05:15,639 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:15,639 (beam_search:483) INFO: best hypo: ITWILLBE

2024-10-27 18:05:15,642 (asr_inference:509) INFO: speech length: 58480
2024-10-27 18:05:17,937 (beam_search:428) INFO: decoder input length: 45
2024-10-27 18:05:17,937 (beam_search:429) INFO: max output length: 45
2024-10-27 18:05:17,937 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:17,970 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:17,970 (beam_search:476) INFO:  -0.62 * 1.0 =  -0.62 for ctc
2024-10-27 18:05:17,970 (beam_search:479) INFO: total log probability: -0.62
2024-10-27 18:05:17,970 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:05:17,970 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:17,970 (beam_search:483) INFO: best hypo: THEWILLNOTBE

2024-10-27 18:05:17,972 (asr_inference:509) INFO: speech length: 206390
2024-10-27 18:05:26,180 (beam_search:428) INFO: decoder input length: 160
2024-10-27 18:05:26,180 (beam_search:429) INFO: max output length: 160
2024-10-27 18:05:26,180 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:26,699 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:26,699 (beam_search:476) INFO:  -2.05 * 1.0 =  -2.05 for ctc
2024-10-27 18:05:26,699 (beam_search:479) INFO: total log probability: -2.05
2024-10-27 18:05:26,699 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:05:26,699 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:26,699 (beam_search:483) INFO: best hypo: THEISANDFOODWINDWATERANDSOLARENERGYDONOTLIKETHEYDON'T

2024-10-27 18:05:26,702 (asr_inference:509) INFO: speech length: 34177
2024-10-27 18:05:28,131 (beam_search:428) INFO: decoder input length: 26
2024-10-27 18:05:28,132 (beam_search:429) INFO: max output length: 26
2024-10-27 18:05:28,132 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:28,158 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:28,158 (beam_search:476) INFO:  -0.98 * 1.0 =  -0.98 for ctc
2024-10-27 18:05:28,158 (beam_search:479) INFO: total log probability: -0.98
2024-10-27 18:05:28,158 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:05:28,158 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:28,158 (beam_search:483) INFO: best hypo: I'MNOTSURE

2024-10-27 18:05:28,160 (asr_inference:509) INFO: speech length: 176780
2024-10-27 18:05:35,090 (beam_search:428) INFO: decoder input length: 137
2024-10-27 18:05:35,090 (beam_search:429) INFO: max output length: 137
2024-10-27 18:05:35,090 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:35,438 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:35,438 (beam_search:476) INFO:  -3.21 * 1.0 =  -3.21 for ctc
2024-10-27 18:05:35,438 (beam_search:479) INFO: total log probability: -3.21
2024-10-27 18:05:35,438 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:05:35,438 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:35,438 (beam_search:483) INFO: best hypo: THEY'BOTHANDTHEYCANBEANDANDCOALAREANDRUNOUT

2024-10-27 18:05:35,441 (asr_inference:509) INFO: speech length: 79361
2024-10-27 18:05:38,355 (beam_search:428) INFO: decoder input length: 61
2024-10-27 18:05:38,355 (beam_search:429) INFO: max output length: 61
2024-10-27 18:05:38,355 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:38,434 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:38,434 (beam_search:476) INFO:  -1.48 * 1.0 =  -1.48 for ctc
2024-10-27 18:05:38,434 (beam_search:479) INFO: total log probability: -1.48
2024-10-27 18:05:38,434 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:05:38,434 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:38,434 (beam_search:483) INFO: best hypo: THEY'REBOTHANDTHEYBOTHNEED

2024-10-27 18:05:38,436 (asr_inference:509) INFO: speech length: 110466
2024-10-27 18:05:42,334 (beam_search:428) INFO: decoder input length: 85
2024-10-27 18:05:42,334 (beam_search:429) INFO: max output length: 85
2024-10-27 18:05:42,334 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:42,457 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:42,458 (beam_search:476) INFO:  -1.17 * 1.0 =  -1.17 for ctc
2024-10-27 18:05:42,458 (beam_search:479) INFO: total log probability: -1.17
2024-10-27 18:05:42,458 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:05:42,458 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:42,458 (beam_search:483) INFO: best hypo: THEYWILLTHEAIRANDUPTHEAND

2024-10-27 18:05:42,460 (asr_inference:509) INFO: speech length: 89676
2024-10-27 18:05:45,676 (beam_search:428) INFO: decoder input length: 69
2024-10-27 18:05:45,676 (beam_search:429) INFO: max output length: 69
2024-10-27 18:05:45,676 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:45,775 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:45,775 (beam_search:476) INFO:  -1.61 * 1.0 =  -1.61 for ctc
2024-10-27 18:05:45,775 (beam_search:479) INFO: total log probability: -1.61
2024-10-27 18:05:45,775 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:05:45,775 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:45,775 (beam_search:483) INFO: best hypo: THEYTHESOLARPANELSDONOTTHE

2024-10-27 18:05:45,777 (asr_inference:509) INFO: speech length: 106976
2024-10-27 18:05:49,721 (beam_search:428) INFO: decoder input length: 83
2024-10-27 18:05:49,721 (beam_search:429) INFO: max output length: 83
2024-10-27 18:05:49,721 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:49,848 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:49,849 (beam_search:476) INFO:  -0.52 * 1.0 =  -0.52 for ctc
2024-10-27 18:05:49,849 (beam_search:479) INFO: total log probability: -0.52
2024-10-27 18:05:49,849 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:05:49,849 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:49,849 (beam_search:483) INFO: best hypo: WELLWE'VEBEENLEARNINGABOUTHOWABOUT

2024-10-27 18:05:49,851 (asr_inference:509) INFO: speech length: 91949
2024-10-27 18:05:53,481 (beam_search:428) INFO: decoder input length: 71
2024-10-27 18:05:53,481 (beam_search:429) INFO: max output length: 71
2024-10-27 18:05:53,481 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:53,561 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:53,561 (beam_search:476) INFO:  -1.66 * 1.0 =  -1.66 for ctc
2024-10-27 18:05:53,561 (beam_search:479) INFO: total log probability: -1.66
2024-10-27 18:05:53,561 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:05:53,561 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:53,561 (beam_search:483) INFO: best hypo: AISAFROMTHETHAT

2024-10-27 18:05:53,564 (asr_inference:509) INFO: speech length: 161538
2024-10-27 18:05:59,722 (beam_search:428) INFO: decoder input length: 125
2024-10-27 18:05:59,723 (beam_search:429) INFO: max output length: 125
2024-10-27 18:05:59,723 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:00,198 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:00,198 (beam_search:476) INFO:  -1.87 * 1.0 =  -1.87 for ctc
2024-10-27 18:06:00,198 (beam_search:479) INFO: total log probability: -1.87
2024-10-27 18:06:00,198 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:06:00,198 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:00,198 (beam_search:483) INFO: best hypo: WELLTHEENERGYFLOWSFROMTHEBATTERYTHROUGHTHEWIRESTHROUGHTHEANDBACKTHROUGHTHEWIRESTOTHEDCELL

2024-10-27 18:06:00,201 (asr_inference:509) INFO: speech length: 287210
2024-10-27 18:06:12,476 (beam_search:428) INFO: decoder input length: 223
2024-10-27 18:06:12,476 (beam_search:429) INFO: max output length: 223
2024-10-27 18:06:12,476 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:13,802 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:13,802 (beam_search:476) INFO:  -5.72 * 1.0 =  -5.72 for ctc
2024-10-27 18:06:13,802 (beam_search:479) INFO: total log probability: -5.72
2024-10-27 18:06:13,802 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:06:13,802 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:13,802 (beam_search:483) INFO: best hypo: WELLTHEAREFLOWINGOUTTHENEGATIVESIDEOFTHETHETERMINALOFTHETHROUGHTHEWIREINTOTHEBULBOUTOFTHEANDINTOTHETHEPOSITIVETERMINALOFTHEDCELL

2024-10-27 18:06:13,805 (asr_inference:509) INFO: speech length: 107239
2024-10-27 18:06:17,685 (beam_search:428) INFO: decoder input length: 83
2024-10-27 18:06:17,685 (beam_search:429) INFO: max output length: 83
2024-10-27 18:06:17,685 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:17,830 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:17,831 (beam_search:476) INFO:  -1.86 * 1.0 =  -1.86 for ctc
2024-10-27 18:06:17,831 (beam_search:479) INFO: total log probability: -1.86
2024-10-27 18:06:17,831 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:06:17,831 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:17,831 (beam_search:483) INFO: best hypo: WELLELECTRICITYOUTOFTHENEGATIVEANDINTOTHEPOSITIVE

2024-10-27 18:06:17,833 (asr_inference:509) INFO: speech length: 129875
2024-10-27 18:06:22,538 (beam_search:428) INFO: decoder input length: 100
2024-10-27 18:06:22,538 (beam_search:429) INFO: max output length: 100
2024-10-27 18:06:22,538 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:22,720 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:22,721 (beam_search:476) INFO:  -2.96 * 1.0 =  -2.96 for ctc
2024-10-27 18:06:22,721 (beam_search:479) INFO: total log probability: -2.96
2024-10-27 18:06:22,721 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:06:22,721 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:22,721 (beam_search:483) INFO: best hypo: WELLTHEELECTRICITYHASTOTHROUGHBOTHOFALLTHE

2024-10-27 18:06:22,723 (asr_inference:509) INFO: speech length: 174405
2024-10-27 18:06:29,609 (beam_search:428) INFO: decoder input length: 135
2024-10-27 18:06:29,610 (beam_search:429) INFO: max output length: 135
2024-10-27 18:06:29,610 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:30,084 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:30,084 (beam_search:476) INFO:  -3.09 * 1.0 =  -3.09 for ctc
2024-10-27 18:06:30,084 (beam_search:479) INFO: total log probability: -3.09
2024-10-27 18:06:30,084 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:06:30,084 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:30,084 (beam_search:483) INFO: best hypo: THEYALLMAKETHEYALLMAKETHECIRCUITACIRCUITBECAUSEWITHOUTINTHECIRCUITTHENIT'S

2024-10-27 18:06:30,087 (asr_inference:509) INFO: speech length: 121399
2024-10-27 18:06:34,520 (beam_search:428) INFO: decoder input length: 94
2024-10-27 18:06:34,520 (beam_search:429) INFO: max output length: 94
2024-10-27 18:06:34,520 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:34,809 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:34,809 (beam_search:476) INFO:  -2.05 * 1.0 =  -2.05 for ctc
2024-10-27 18:06:34,809 (beam_search:479) INFO: total log probability: -2.05
2024-10-27 18:06:34,809 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:06:34,809 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:34,809 (beam_search:483) INFO: best hypo: THISISAPATHWAYTHROUGHTHELIGHTBULBFROMTHEBATTERYTHROUGHTHELIGHTBULBANDACIRCUIT

2024-10-27 18:06:34,811 (asr_inference:509) INFO: speech length: 129231
2024-10-27 18:06:39,508 (beam_search:428) INFO: decoder input length: 100
2024-10-27 18:06:39,508 (beam_search:429) INFO: max output length: 100
2024-10-27 18:06:39,508 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:39,695 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:39,695 (beam_search:476) INFO:  -2.76 * 1.0 =  -2.76 for ctc
2024-10-27 18:06:39,695 (beam_search:479) INFO: total log probability: -2.76
2024-10-27 18:06:39,695 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:06:39,695 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:39,695 (beam_search:483) INFO: best hypo: ELECTRICITYTHROUGHACIRCUITTHROUGHBOTHANDBACKTHROUGHTHE

2024-10-27 18:06:39,697 (asr_inference:509) INFO: speech length: 186586
2024-10-27 18:06:47,323 (beam_search:428) INFO: decoder input length: 145
2024-10-27 18:06:47,323 (beam_search:429) INFO: max output length: 145
2024-10-27 18:06:47,324 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:47,884 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:47,884 (beam_search:476) INFO:  -3.27 * 1.0 =  -3.27 for ctc
2024-10-27 18:06:47,884 (beam_search:479) INFO: total log probability: -3.27
2024-10-27 18:06:47,884 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:06:47,884 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:47,884 (beam_search:483) INFO: best hypo: WELLTHEELECTRICITYFLOWINGOUTOFTHESOLARPANELORTHEDTHROUGHTHELIGHTBULBSANDBACKTHROUGHTHEPOWER

2024-10-27 18:06:47,887 (asr_inference:509) INFO: speech length: 349040
2024-10-27 18:07:03,659 (beam_search:428) INFO: decoder input length: 272
2024-10-27 18:07:03,659 (beam_search:429) INFO: max output length: 272
2024-10-27 18:07:03,659 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:05,718 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:05,718 (beam_search:476) INFO:  -5.61 * 1.0 =  -5.61 for ctc
2024-10-27 18:07:05,719 (beam_search:479) INFO: total log probability: -5.61
2024-10-27 18:07:05,719 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:07:05,719 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:05,719 (beam_search:483) INFO: best hypo: IT'SFLOWINGOUTOFTHETHEDCELLANDTHENTHROUGHTHEBULBTHENOUTOFTHEOUTOFTHEANDTHENTHROUGHTHEBULBOFTHEOTHERBULBOUTOFTHEANDBACKTOTHEPOSITIVETHEDCELL

2024-10-27 18:07:05,721 (asr_inference:509) INFO: speech length: 61954
2024-10-27 18:07:08,071 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:07:08,071 (beam_search:429) INFO: max output length: 47
2024-10-27 18:07:08,071 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:08,131 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:08,131 (beam_search:476) INFO:  -1.37 * 1.0 =  -1.37 for ctc
2024-10-27 18:07:08,131 (beam_search:479) INFO: total log probability: -1.37
2024-10-27 18:07:08,131 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:07:08,131 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:08,131 (beam_search:483) INFO: best hypo: THOSEARETHEFLOWINGTHROUGHTHECIRCUIT

2024-10-27 18:07:08,134 (asr_inference:509) INFO: speech length: 76751
2024-10-27 18:07:11,072 (beam_search:428) INFO: decoder input length: 59
2024-10-27 18:07:11,072 (beam_search:429) INFO: max output length: 59
2024-10-27 18:07:11,072 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:11,179 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:11,179 (beam_search:476) INFO:  -2.06 * 1.0 =  -2.06 for ctc
2024-10-27 18:07:11,179 (beam_search:479) INFO: total log probability: -2.06
2024-10-27 18:07:11,179 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:07:11,179 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:11,179 (beam_search:483) INFO: best hypo: WELLTHETHEBLUEDOTSARETHEELECTRICITYFLOWINGTHROUGHTHE

2024-10-27 18:07:11,181 (asr_inference:509) INFO: speech length: 162611
2024-10-27 18:07:17,542 (beam_search:428) INFO: decoder input length: 126
2024-10-27 18:07:17,542 (beam_search:429) INFO: max output length: 126
2024-10-27 18:07:17,542 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:17,905 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:17,906 (beam_search:476) INFO:  -2.09 * 1.0 =  -2.09 for ctc
2024-10-27 18:07:17,906 (beam_search:479) INFO: total log probability: -2.09
2024-10-27 18:07:17,906 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:07:17,906 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:17,906 (beam_search:483) INFO: best hypo: OUTOFTHEANDTHENTHROUGHBOTHOFTHEBULBTHROUGHTHEANDGOESBACKTO

2024-10-27 18:07:17,909 (asr_inference:509) INFO: speech length: 288833
2024-10-27 18:07:30,075 (beam_search:428) INFO: decoder input length: 225
2024-10-27 18:07:30,076 (beam_search:429) INFO: max output length: 225
2024-10-27 18:07:30,076 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:31,685 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:31,686 (beam_search:476) INFO:  -4.72 * 1.0 =  -4.72 for ctc
2024-10-27 18:07:31,686 (beam_search:479) INFO: total log probability: -4.72
2024-10-27 18:07:31,686 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:07:31,686 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:31,686 (beam_search:483) INFO: best hypo: WELLTHETHEELECTRICITYISOUTOFTHETHROUGHTHEWIREINTOTHEBULBOUTOFTHEBULBINTOTHROUGHTHEWIREINTOTHEOTHERBULBOUTTHEOTHERBULBANDBACKINTOTHEPOSITIVETHEDCELL

2024-10-27 18:07:31,688 (asr_inference:509) INFO: speech length: 108519
2024-10-27 18:07:35,573 (beam_search:428) INFO: decoder input length: 84
2024-10-27 18:07:35,573 (beam_search:429) INFO: max output length: 84
2024-10-27 18:07:35,573 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:35,839 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:35,839 (beam_search:476) INFO:  -2.46 * 1.0 =  -2.46 for ctc
2024-10-27 18:07:35,839 (beam_search:479) INFO: total log probability: -2.46
2024-10-27 18:07:35,839 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:07:35,839 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:35,839 (beam_search:483) INFO: best hypo: THELIGHTWILLGETBECAUSETHEREISNOTTHERE'SNOTENOUGHENERGYTOPOWERALLOFTHEM

2024-10-27 18:07:35,841 (asr_inference:509) INFO: speech length: 95855
2024-10-27 18:07:39,406 (beam_search:428) INFO: decoder input length: 74
2024-10-27 18:07:39,406 (beam_search:429) INFO: max output length: 74
2024-10-27 18:07:39,406 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:39,578 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:39,579 (beam_search:476) INFO:  -3.61 * 1.0 =  -3.61 for ctc
2024-10-27 18:07:39,579 (beam_search:479) INFO: total log probability: -3.61
2024-10-27 18:07:39,579 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:07:39,579 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:39,579 (beam_search:483) INFO: best hypo: THELIGHTWILLGETBRIGHTERTHENTHERE'SMOREPOWERTOTHEENERGY

2024-10-27 18:07:39,581 (asr_inference:509) INFO: speech length: 212706
2024-10-27 18:07:47,852 (beam_search:428) INFO: decoder input length: 165
2024-10-27 18:07:47,852 (beam_search:429) INFO: max output length: 165
2024-10-27 18:07:47,852 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:48,303 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:48,303 (beam_search:476) INFO:  -1.51 * 1.0 =  -1.51 for ctc
2024-10-27 18:07:48,303 (beam_search:479) INFO: total log probability: -1.51
2024-10-27 18:07:48,303 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:07:48,303 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:48,303 (beam_search:483) INFO: best hypo: WHENYOUMORETHENTHELIGHTSTOOUTMOREBECAUSETHEYGETMOREENERGY

2024-10-27 18:07:48,306 (asr_inference:509) INFO: speech length: 85004
2024-10-27 18:07:51,398 (beam_search:428) INFO: decoder input length: 65
2024-10-27 18:07:51,398 (beam_search:429) INFO: max output length: 65
2024-10-27 18:07:51,398 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:51,466 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:51,466 (beam_search:476) INFO:  -1.45 * 1.0 =  -1.45 for ctc
2024-10-27 18:07:51,466 (beam_search:479) INFO: total log probability: -1.45
2024-10-27 18:07:51,466 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:07:51,466 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:51,467 (beam_search:483) INFO: best hypo: BECAUSETHEPOSITIVESARETOGETHER

2024-10-27 18:07:51,469 (asr_inference:509) INFO: speech length: 272545
2024-10-27 18:08:03,118 (beam_search:428) INFO: decoder input length: 212
2024-10-27 18:08:03,118 (beam_search:429) INFO: max output length: 212
2024-10-27 18:08:03,118 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:04,239 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:04,239 (beam_search:476) INFO:  -5.04 * 1.0 =  -5.04 for ctc
2024-10-27 18:08:04,239 (beam_search:479) INFO: total log probability: -5.04
2024-10-27 18:08:04,239 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:08:04,239 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:04,240 (beam_search:483) INFO: best hypo: WHENYOUTHEDITGOESTOPOSITIVETONEGATIVEANDDHAVETOBETHATWAYJUSTLIKEMAGNETSHAVETOBEJUSTLIKETHEHAVETOTHE

2024-10-27 18:08:04,243 (asr_inference:509) INFO: speech length: 93028
2024-10-27 18:08:07,667 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:08:07,667 (beam_search:429) INFO: max output length: 72
2024-10-27 18:08:07,667 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:07,737 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:07,737 (beam_search:476) INFO:  -1.29 * 1.0 =  -1.29 for ctc
2024-10-27 18:08:07,737 (beam_search:479) INFO: total log probability: -1.29
2024-10-27 18:08:07,737 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:08:07,737 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:07,737 (beam_search:483) INFO: best hypo: THEDCELLTHETHE

2024-10-27 18:08:07,740 (asr_inference:509) INFO: speech length: 131630
2024-10-27 18:08:12,736 (beam_search:428) INFO: decoder input length: 102
2024-10-27 18:08:12,736 (beam_search:429) INFO: max output length: 102
2024-10-27 18:08:12,736 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:13,015 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:13,015 (beam_search:476) INFO:  -2.32 * 1.0 =  -2.32 for ctc
2024-10-27 18:08:13,015 (beam_search:479) INFO: total log probability: -2.32
2024-10-27 18:08:13,015 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:08:13,015 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:13,015 (beam_search:483) INFO: best hypo: THEDTOMAKEITSOTHATTHEREISPOSITIVETONEGATIVEORNEGATIVETO

2024-10-27 18:08:13,017 (asr_inference:509) INFO: speech length: 68952
2024-10-27 18:08:15,580 (beam_search:428) INFO: decoder input length: 53
2024-10-27 18:08:15,580 (beam_search:429) INFO: max output length: 53
2024-10-27 18:08:15,580 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:15,644 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:15,644 (beam_search:476) INFO:  -2.65 * 1.0 =  -2.65 for ctc
2024-10-27 18:08:15,644 (beam_search:479) INFO: total log probability: -2.65
2024-10-27 18:08:15,644 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:08:15,644 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:15,644 (beam_search:483) INFO: best hypo: I'VEBEENLEARNINGABOUTAND

2024-10-27 18:08:15,646 (asr_inference:509) INFO: speech length: 146772
2024-10-27 18:08:21,067 (beam_search:428) INFO: decoder input length: 114
2024-10-27 18:08:21,067 (beam_search:429) INFO: max output length: 114
2024-10-27 18:08:21,067 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:21,380 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:21,380 (beam_search:476) INFO:  -3.25 * 1.0 =  -3.25 for ctc
2024-10-27 18:08:21,380 (beam_search:479) INFO: total log probability: -3.25
2024-10-27 18:08:21,380 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:08:21,380 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:21,380 (beam_search:483) INFO: best hypo: THEENERGYOUTTWOINTHENEGATIVEANDTHENFLOWSINTWOINTOTHEPOSITIVE

2024-10-27 18:08:21,382 (asr_inference:509) INFO: speech length: 46049
2024-10-27 18:08:23,210 (beam_search:428) INFO: decoder input length: 35
2024-10-27 18:08:23,210 (beam_search:429) INFO: max output length: 35
2024-10-27 18:08:23,210 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:23,245 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:23,245 (beam_search:476) INFO:  -2.05 * 1.0 =  -2.05 for ctc
2024-10-27 18:08:23,245 (beam_search:479) INFO: total log probability: -2.05
2024-10-27 18:08:23,245 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:08:23,245 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:23,245 (beam_search:483) INFO: best hypo: HASITSOWNPATHWAY

2024-10-27 18:08:23,248 (asr_inference:509) INFO: speech length: 174974
2024-10-27 18:08:30,015 (beam_search:428) INFO: decoder input length: 136
2024-10-27 18:08:30,015 (beam_search:429) INFO: max output length: 136
2024-10-27 18:08:30,015 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:30,595 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:30,595 (beam_search:476) INFO:  -6.02 * 1.0 =  -6.02 for ctc
2024-10-27 18:08:30,595 (beam_search:479) INFO: total log probability: -6.02
2024-10-27 18:08:30,596 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:08:30,596 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:30,596 (beam_search:483) INFO: best hypo: INACIRCUITSOONEOFWIRESTOOFTHELIGHTANDTHEOTHEROFWIRESGOESTOTHEOTHERLIGHTBULB

2024-10-27 18:08:30,599 (asr_inference:509) INFO: speech length: 52505
2024-10-27 18:08:32,597 (beam_search:428) INFO: decoder input length: 40
2024-10-27 18:08:32,597 (beam_search:429) INFO: max output length: 40
2024-10-27 18:08:32,597 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:32,640 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:32,640 (beam_search:476) INFO:  -2.23 * 1.0 =  -2.23 for ctc
2024-10-27 18:08:32,640 (beam_search:479) INFO: total log probability: -2.23
2024-10-27 18:08:32,640 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:08:32,640 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:32,640 (beam_search:483) INFO: best hypo: THEREISONEDANDTWO

2024-10-27 18:08:32,642 (asr_inference:509) INFO: speech length: 125311
2024-10-27 18:08:37,283 (beam_search:428) INFO: decoder input length: 97
2024-10-27 18:08:37,283 (beam_search:429) INFO: max output length: 97
2024-10-27 18:08:37,283 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:37,555 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:37,555 (beam_search:476) INFO:  -5.48 * 1.0 =  -5.48 for ctc
2024-10-27 18:08:37,557 (beam_search:479) INFO: total log probability: -5.48
2024-10-27 18:08:37,557 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:08:37,557 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:37,557 (beam_search:483) INFO: best hypo: THISISNOTAPARALLELCIRCUITIT'SACIRCUITBECAUSEEACHISCONNECTEDINTHE

2024-10-27 18:08:37,559 (asr_inference:509) INFO: speech length: 31217
2024-10-27 18:08:38,839 (beam_search:428) INFO: decoder input length: 23
2024-10-27 18:08:38,839 (beam_search:429) INFO: max output length: 23
2024-10-27 18:08:38,839 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:38,850 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:38,850 (beam_search:476) INFO:  -0.08 * 1.0 =  -0.08 for ctc
2024-10-27 18:08:38,850 (beam_search:479) INFO: total log probability: -0.08
2024-10-27 18:08:38,850 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:08:38,850 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:38,850 (beam_search:483) INFO: best hypo: AND

2024-10-27 18:08:38,853 (asr_inference:509) INFO: speech length: 259134
2024-10-27 18:08:49,460 (beam_search:428) INFO: decoder input length: 201
2024-10-27 18:08:49,461 (beam_search:429) INFO: max output length: 201
2024-10-27 18:08:49,461 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:49,977 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:49,977 (beam_search:476) INFO:  -5.94 * 1.0 =  -5.94 for ctc
2024-10-27 18:08:49,977 (beam_search:479) INFO: total log probability: -5.94
2024-10-27 18:08:49,977 (beam_search:480) INFO: normalized log probability: -0.37
2024-10-27 18:08:49,977 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:49,977 (beam_search:483) INFO: best hypo: BOTHTHETHEHAVEMOREENERGYFORTHEBULBSTHEBECAUSEITTHROUGHOF

2024-10-27 18:08:49,980 (asr_inference:509) INFO: speech length: 237295
2024-10-27 18:09:00,079 (beam_search:428) INFO: decoder input length: 184
2024-10-27 18:09:00,080 (beam_search:429) INFO: max output length: 184
2024-10-27 18:09:00,080 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:00,964 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:00,964 (beam_search:476) INFO:  -7.44 * 1.0 =  -7.44 for ctc
2024-10-27 18:09:00,964 (beam_search:479) INFO: total log probability: -7.44
2024-10-27 18:09:00,964 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:09:00,964 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:00,965 (beam_search:483) INFO: best hypo: BECAUSETHERETWOININACIRCUITTHERE'SINACIRCUITANDTHEREISSTILLONEINAEXCEPTTHERE'SLESS

2024-10-27 18:09:00,967 (asr_inference:509) INFO: speech length: 247212
2024-10-27 18:09:10,938 (beam_search:428) INFO: decoder input length: 192
2024-10-27 18:09:10,939 (beam_search:429) INFO: max output length: 192
2024-10-27 18:09:10,939 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:11,672 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:11,672 (beam_search:476) INFO:  -6.35 * 1.0 =  -6.35 for ctc
2024-10-27 18:09:11,672 (beam_search:479) INFO: total log probability: -6.35
2024-10-27 18:09:11,672 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:09:11,672 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:11,672 (beam_search:483) INFO: best hypo: THEHASLESSOFENERGYBECAUSEBECAUSELIGHTARECONNECTEDINONETHECIRCUITHASMOREENERGYBECAUSETHEYCONNECTEDTWO

2024-10-27 18:09:11,675 (asr_inference:509) INFO: speech length: 35889
2024-10-27 18:09:13,029 (beam_search:428) INFO: decoder input length: 27
2024-10-27 18:09:13,029 (beam_search:429) INFO: max output length: 27
2024-10-27 18:09:13,029 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:13,048 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:13,048 (beam_search:476) INFO:  -0.80 * 1.0 =  -0.80 for ctc
2024-10-27 18:09:13,048 (beam_search:479) INFO: total log probability: -0.80
2024-10-27 18:09:13,048 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:09:13,048 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:13,048 (beam_search:483) INFO: best hypo: THEWILLGET

2024-10-27 18:09:13,050 (asr_inference:509) INFO: speech length: 83867
2024-10-27 18:09:16,077 (beam_search:428) INFO: decoder input length: 65
2024-10-27 18:09:16,077 (beam_search:429) INFO: max output length: 65
2024-10-27 18:09:16,077 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:16,213 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:16,213 (beam_search:476) INFO:  -2.54 * 1.0 =  -2.54 for ctc
2024-10-27 18:09:16,213 (beam_search:479) INFO: total log probability: -2.54
2024-10-27 18:09:16,213 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:09:16,213 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:16,213 (beam_search:483) INFO: best hypo: ITWOULDMAKETHEBRIGHTERBECAUSETHENTHEREWOULDMOREENERGYTO

2024-10-27 18:09:16,215 (asr_inference:509) INFO: speech length: 160221
2024-10-27 18:09:22,286 (beam_search:428) INFO: decoder input length: 124
2024-10-27 18:09:22,286 (beam_search:429) INFO: max output length: 124
2024-10-27 18:09:22,286 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:22,651 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:22,651 (beam_search:476) INFO:  -3.18 * 1.0 =  -3.18 for ctc
2024-10-27 18:09:22,651 (beam_search:479) INFO: total log probability: -3.18
2024-10-27 18:09:22,651 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:09:22,651 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:22,652 (beam_search:483) INFO: best hypo: THEENERGYOUTOFTHETERMINALOFTHEBATTERYANDINTOTHEPOSITIVETERMINALOFTHE

2024-10-27 18:09:22,654 (asr_inference:509) INFO: speech length: 101694
2024-10-27 18:09:26,747 (beam_search:428) INFO: decoder input length: 78
2024-10-27 18:09:26,747 (beam_search:429) INFO: max output length: 78
2024-10-27 18:09:26,747 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:26,914 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:26,915 (beam_search:476) INFO:  -2.84 * 1.0 =  -2.84 for ctc
2024-10-27 18:09:26,915 (beam_search:479) INFO: total log probability: -2.84
2024-10-27 18:09:26,915 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:09:26,915 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:26,915 (beam_search:483) INFO: best hypo: THEISFLOWINGOUTOFTHENEGATIVEANDINTOTHEPOSITIVETERMINAL

2024-10-27 18:09:26,918 (asr_inference:509) INFO: speech length: 340531
2024-10-27 18:09:42,023 (beam_search:428) INFO: decoder input length: 265
2024-10-27 18:09:42,023 (beam_search:429) INFO: max output length: 265
2024-10-27 18:09:42,023 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:44,203 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:44,203 (beam_search:476) INFO:  -7.98 * 1.0 =  -7.98 for ctc
2024-10-27 18:09:44,203 (beam_search:479) INFO: total log probability: -7.98
2024-10-27 18:09:44,203 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:09:44,203 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:44,204 (beam_search:483) INFO: best hypo: THEENERGYISFLOWINGOUTOFTHENEGATIVETHROUGHTHEFIRSTLIGHTBULBANDTHENINTOTHESECONDLIGHTBULBBACKTHROUGHFROMTHETOTHEOTHERBULBANDBACKINTOTHEPOSITIVETHEIT'SONESOIT'SA

2024-10-27 18:09:44,206 (asr_inference:509) INFO: speech length: 75422
2024-10-27 18:09:46,963 (beam_search:428) INFO: decoder input length: 58
2024-10-27 18:09:46,963 (beam_search:429) INFO: max output length: 58
2024-10-27 18:09:46,963 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:47,028 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:47,028 (beam_search:476) INFO:  -2.25 * 1.0 =  -2.25 for ctc
2024-10-27 18:09:47,028 (beam_search:479) INFO: total log probability: -2.25
2024-10-27 18:09:47,028 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:09:47,028 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:47,029 (beam_search:483) INFO: best hypo: EACHBULBHASITSPATHWAY

2024-10-27 18:09:47,031 (asr_inference:509) INFO: speech length: 175885
2024-10-27 18:09:53,669 (beam_search:428) INFO: decoder input length: 136
2024-10-27 18:09:53,670 (beam_search:429) INFO: max output length: 136
2024-10-27 18:09:53,670 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:54,153 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:54,153 (beam_search:476) INFO:  -6.06 * 1.0 =  -6.06 for ctc
2024-10-27 18:09:54,153 (beam_search:479) INFO: total log probability: -6.06
2024-10-27 18:09:54,153 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:09:54,153 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:54,154 (beam_search:483) INFO: best hypo: SOTHEPATHWAYSBOTHCOMEOUTOFTHEBATTERYTHROUGHINSEPARATETOTHELIGHTANDBACKINTOTHE

2024-10-27 18:09:54,156 (asr_inference:509) INFO: speech length: 61916
2024-10-27 18:09:56,448 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:09:56,448 (beam_search:429) INFO: max output length: 47
2024-10-27 18:09:56,448 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:56,483 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:56,483 (beam_search:476) INFO:  -0.59 * 1.0 =  -0.59 for ctc
2024-10-27 18:09:56,484 (beam_search:479) INFO: total log probability: -0.59
2024-10-27 18:09:56,484 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:09:56,484 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:56,484 (beam_search:483) INFO: best hypo: ITONLYONEBULB

2024-10-27 18:09:56,486 (asr_inference:509) INFO: speech length: 35737
2024-10-27 18:09:57,872 (beam_search:428) INFO: decoder input length: 27
2024-10-27 18:09:57,872 (beam_search:429) INFO: max output length: 27
2024-10-27 18:09:57,872 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:57,894 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:57,894 (beam_search:476) INFO:  -1.68 * 1.0 =  -1.68 for ctc
2024-10-27 18:09:57,894 (beam_search:479) INFO: total log probability: -1.68
2024-10-27 18:09:57,894 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:09:57,894 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:57,894 (beam_search:483) INFO: best hypo: THERE'STWO

2024-10-27 18:09:57,896 (asr_inference:509) INFO: speech length: 57702
2024-10-27 18:10:00,007 (beam_search:428) INFO: decoder input length: 44
2024-10-27 18:10:00,007 (beam_search:429) INFO: max output length: 44
2024-10-27 18:10:00,007 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:00,059 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:00,059 (beam_search:476) INFO:  -2.92 * 1.0 =  -2.92 for ctc
2024-10-27 18:10:00,059 (beam_search:479) INFO: total log probability: -2.92
2024-10-27 18:10:00,059 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 18:10:00,059 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:00,059 (beam_search:483) INFO: best hypo: THEAREBECAUSETHERE'SMORE

2024-10-27 18:10:00,062 (asr_inference:509) INFO: speech length: 34937
2024-10-27 18:10:01,427 (beam_search:428) INFO: decoder input length: 26
2024-10-27 18:10:01,427 (beam_search:429) INFO: max output length: 26
2024-10-27 18:10:01,427 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:01,453 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:01,453 (beam_search:476) INFO:  -1.87 * 1.0 =  -1.87 for ctc
2024-10-27 18:10:01,453 (beam_search:479) INFO: total log probability: -1.87
2024-10-27 18:10:01,453 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:10:01,453 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:01,454 (beam_search:483) INFO: best hypo: CAUSETHEREISLESSENERGY

2024-10-27 18:10:01,456 (asr_inference:509) INFO: speech length: 25500
2024-10-27 18:10:02,552 (beam_search:428) INFO: decoder input length: 19
2024-10-27 18:10:02,552 (beam_search:429) INFO: max output length: 19
2024-10-27 18:10:02,552 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:02,564 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:02,564 (beam_search:476) INFO:  -0.99 * 1.0 =  -0.99 for ctc
2024-10-27 18:10:02,564 (beam_search:479) INFO: total log probability: -0.99
2024-10-27 18:10:02,564 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:10:02,564 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:02,565 (beam_search:483) INFO: best hypo: ARETWO

2024-10-27 18:10:02,566 (asr_inference:509) INFO: speech length: 203151
2024-10-27 18:10:10,733 (beam_search:428) INFO: decoder input length: 158
2024-10-27 18:10:10,733 (beam_search:429) INFO: max output length: 158
2024-10-27 18:10:10,733 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:11,350 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:11,350 (beam_search:476) INFO:  -4.43 * 1.0 =  -4.43 for ctc
2024-10-27 18:10:11,350 (beam_search:479) INFO: total log probability: -4.43
2024-10-27 18:10:11,350 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:10:11,350 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:11,350 (beam_search:483) INFO: best hypo: OFLIGHTSAREDTOBECONNECTEDINPARALLELSOTHATTHEYDON'TALLTHEBULBSWHENBULBOUT

2024-10-27 18:10:11,353 (asr_inference:509) INFO: speech length: 177026
2024-10-27 18:10:18,087 (beam_search:428) INFO: decoder input length: 137
2024-10-27 18:10:18,087 (beam_search:429) INFO: max output length: 137
2024-10-27 18:10:18,087 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:18,555 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:18,555 (beam_search:476) INFO:  -2.44 * 1.0 =  -2.44 for ctc
2024-10-27 18:10:18,555 (beam_search:479) INFO: total log probability: -2.44
2024-10-27 18:10:18,555 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:10:18,555 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:18,555 (beam_search:483) INFO: best hypo: AREINTHEOFLIGHTSANDWHENAOUTTHENALLTHELIGHTSGOOUTONTHE

2024-10-27 18:10:18,558 (asr_inference:509) INFO: speech length: 88332
2024-10-27 18:10:21,731 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:10:21,731 (beam_search:429) INFO: max output length: 68
2024-10-27 18:10:21,731 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:21,857 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:21,857 (beam_search:476) INFO:  -1.94 * 1.0 =  -1.94 for ctc
2024-10-27 18:10:21,857 (beam_search:479) INFO: total log probability: -1.94
2024-10-27 18:10:21,857 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:10:21,857 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:21,857 (beam_search:483) INFO: best hypo: IT'SACIRCUITANDANDTHESWITCHISOPEN

2024-10-27 18:10:21,860 (asr_inference:509) INFO: speech length: 44472
2024-10-27 18:10:23,587 (beam_search:428) INFO: decoder input length: 34
2024-10-27 18:10:23,587 (beam_search:429) INFO: max output length: 34
2024-10-27 18:10:23,587 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:23,611 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:23,612 (beam_search:476) INFO:  -0.71 * 1.0 =  -0.71 for ctc
2024-10-27 18:10:23,612 (beam_search:479) INFO: total log probability: -0.71
2024-10-27 18:10:23,612 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:10:23,612 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:23,612 (beam_search:483) INFO: best hypo: NOENERGYIS

2024-10-27 18:10:23,614 (asr_inference:509) INFO: speech length: 37500
2024-10-27 18:10:25,079 (beam_search:428) INFO: decoder input length: 28
2024-10-27 18:10:25,079 (beam_search:429) INFO: max output length: 28
2024-10-27 18:10:25,079 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:25,114 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:25,114 (beam_search:476) INFO:  -0.75 * 1.0 =  -0.75 for ctc
2024-10-27 18:10:25,114 (beam_search:479) INFO: total log probability: -0.75
2024-10-27 18:10:25,114 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:10:25,114 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:25,114 (beam_search:483) INFO: best hypo: THELIGHTTURNSONBECAUSETHE

2024-10-27 18:10:25,116 (asr_inference:509) INFO: speech length: 61288
2024-10-27 18:10:27,402 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:10:27,403 (beam_search:429) INFO: max output length: 47
2024-10-27 18:10:27,403 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:27,497 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:27,498 (beam_search:476) INFO:  -3.29 * 1.0 =  -3.29 for ctc
2024-10-27 18:10:27,498 (beam_search:479) INFO: total log probability: -3.29
2024-10-27 18:10:27,498 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:10:27,498 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:27,498 (beam_search:483) INFO: best hypo: THENALLTHEBULBSGOOUTIFTHEY'REINA

2024-10-27 18:10:27,500 (asr_inference:509) INFO: speech length: 221910
2024-10-27 18:10:36,280 (beam_search:428) INFO: decoder input length: 172
2024-10-27 18:10:36,280 (beam_search:429) INFO: max output length: 172
2024-10-27 18:10:36,280 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:37,294 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:37,294 (beam_search:476) INFO:  -5.93 * 1.0 =  -5.93 for ctc
2024-10-27 18:10:37,294 (beam_search:479) INFO: total log probability: -5.93
2024-10-27 18:10:37,294 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:10:37,294 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:37,294 (beam_search:483) INFO: best hypo: OFLIGHTSANDARECONNECTEDINONEPATHWAYSOIFTHEFILAMENTINABULBOUTIT'SLIKEANOPENSWITCHSONOENERGYCANFLOWTHROUGHTHECIRCUIT

2024-10-27 18:10:37,297 (asr_inference:509) INFO: speech length: 38320
2024-10-27 18:10:38,887 (beam_search:428) INFO: decoder input length: 29
2024-10-27 18:10:38,887 (beam_search:429) INFO: max output length: 29
2024-10-27 18:10:38,887 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:38,904 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:38,904 (beam_search:476) INFO:  -0.10 * 1.0 =  -0.10 for ctc
2024-10-27 18:10:38,904 (beam_search:479) INFO: total log probability: -0.10
2024-10-27 18:10:38,904 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:10:38,904 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:38,904 (beam_search:483) INFO: best hypo: THEOTHER

2024-10-27 18:10:38,906 (asr_inference:509) INFO: speech length: 127552
2024-10-27 18:10:43,563 (beam_search:428) INFO: decoder input length: 99
2024-10-27 18:10:43,563 (beam_search:429) INFO: max output length: 99
2024-10-27 18:10:43,563 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:43,884 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:43,885 (beam_search:476) INFO:  -3.14 * 1.0 =  -3.14 for ctc
2024-10-27 18:10:43,885 (beam_search:479) INFO: total log probability: -3.14
2024-10-27 18:10:43,885 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:10:43,885 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:43,885 (beam_search:483) INFO: best hypo: THEOTHERGOESOUTTOOBECAUSEONEOFISUPSOIT'SLIKEANOPENSWITCH

2024-10-27 18:10:43,887 (asr_inference:509) INFO: speech length: 268870
2024-10-27 18:10:55,312 (beam_search:428) INFO: decoder input length: 209
2024-10-27 18:10:55,312 (beam_search:429) INFO: max output length: 209
2024-10-27 18:10:55,312 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:55,818 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:55,818 (beam_search:476) INFO:  -6.47 * 1.0 =  -6.47 for ctc
2024-10-27 18:10:55,818 (beam_search:479) INFO: total log probability: -6.47
2024-10-27 18:10:55,818 (beam_search:480) INFO: normalized log probability: -0.43
2024-10-27 18:10:55,818 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:55,818 (beam_search:483) INFO: best hypo: THEENERGYISFLOWINGONEBULBISSOTHEOFAREUYOU

2024-10-27 18:10:55,821 (asr_inference:509) INFO: speech length: 77127
2024-10-27 18:10:58,632 (beam_search:428) INFO: decoder input length: 59
2024-10-27 18:10:58,632 (beam_search:429) INFO: max output length: 59
2024-10-27 18:10:58,632 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:58,703 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:58,704 (beam_search:476) INFO:  -0.34 * 1.0 =  -0.34 for ctc
2024-10-27 18:10:58,704 (beam_search:479) INFO: total log probability: -0.34
2024-10-27 18:10:58,704 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 18:10:58,704 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:58,704 (beam_search:483) INFO: best hypo: ALLTHEARECONNECTEDINONEPATHWAY

2024-10-27 18:10:58,706 (asr_inference:509) INFO: speech length: 131177
2024-10-27 18:11:03,376 (beam_search:428) INFO: decoder input length: 101
2024-10-27 18:11:03,377 (beam_search:429) INFO: max output length: 101
2024-10-27 18:11:03,377 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:03,672 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:03,672 (beam_search:476) INFO:  -2.36 * 1.0 =  -2.36 for ctc
2024-10-27 18:11:03,672 (beam_search:479) INFO: total log probability: -2.36
2024-10-27 18:11:03,672 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:11:03,672 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:03,672 (beam_search:483) INFO: best hypo: ASERIESCIRCUITBELIKETHISBECAUSEIT'SONEPATHWAYTHATSTHROUGHALLTHE

2024-10-27 18:11:03,675 (asr_inference:509) INFO: speech length: 185587
2024-10-27 18:11:10,770 (beam_search:428) INFO: decoder input length: 144
2024-10-27 18:11:10,770 (beam_search:429) INFO: max output length: 144
2024-10-27 18:11:10,771 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:11,129 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:11,129 (beam_search:476) INFO:  -5.55 * 1.0 =  -5.55 for ctc
2024-10-27 18:11:11,129 (beam_search:479) INFO: total log probability: -5.55
2024-10-27 18:11:11,129 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:11:11,129 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:11,129 (beam_search:483) INFO: best hypo: WELLWHENTHEFILAMENTITITLIKESTHESWITCHLIKEALIKEOPEN

2024-10-27 18:11:11,132 (asr_inference:509) INFO: speech length: 80818
2024-10-27 18:11:14,211 (beam_search:428) INFO: decoder input length: 62
2024-10-27 18:11:14,211 (beam_search:429) INFO: max output length: 62
2024-10-27 18:11:14,211 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:14,300 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:14,300 (beam_search:476) INFO:  -1.84 * 1.0 =  -1.84 for ctc
2024-10-27 18:11:14,301 (beam_search:479) INFO: total log probability: -1.84
2024-10-27 18:11:14,301 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:11:14,301 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:14,301 (beam_search:483) INFO: best hypo: ONEISOUTSOTHEOTHERBULBIS

2024-10-27 18:11:14,303 (asr_inference:509) INFO: speech length: 207435
2024-10-27 18:11:22,433 (beam_search:428) INFO: decoder input length: 161
2024-10-27 18:11:22,433 (beam_search:429) INFO: max output length: 161
2024-10-27 18:11:22,433 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:22,967 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:22,967 (beam_search:476) INFO:  -4.09 * 1.0 =  -4.09 for ctc
2024-10-27 18:11:22,967 (beam_search:479) INFO: total log probability: -4.09
2024-10-27 18:11:22,967 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:11:22,967 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:22,967 (beam_search:483) INFO: best hypo: THISISACIRCUITWITHENERGYFLOWINGTHEFILAMENTOUTSOBECAUSETHEOUTTHENTHEOTHERTURNS

2024-10-27 18:11:22,970 (asr_inference:509) INFO: speech length: 63831
2024-10-27 18:11:25,361 (beam_search:428) INFO: decoder input length: 49
2024-10-27 18:11:25,361 (beam_search:429) INFO: max output length: 49
2024-10-27 18:11:25,361 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:25,407 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:25,408 (beam_search:476) INFO:  -0.65 * 1.0 =  -0.65 for ctc
2024-10-27 18:11:25,408 (beam_search:479) INFO: total log probability: -0.65
2024-10-27 18:11:25,408 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:11:25,408 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:25,408 (beam_search:483) INFO: best hypo: AANDSTHEPATHWAY

2024-10-27 18:11:25,411 (asr_inference:509) INFO: speech length: 88405
2024-10-27 18:11:28,665 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:11:28,666 (beam_search:429) INFO: max output length: 68
2024-10-27 18:11:28,666 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:28,755 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:28,756 (beam_search:476) INFO:  -0.98 * 1.0 =  -0.98 for ctc
2024-10-27 18:11:28,756 (beam_search:479) INFO: total log probability: -0.98
2024-10-27 18:11:28,756 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:11:28,756 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:28,756 (beam_search:483) INFO: best hypo: WELLWHENABULBGOESOUTTHEELECTRICITY

2024-10-27 18:11:28,758 (asr_inference:509) INFO: speech length: 86083
2024-10-27 18:11:31,771 (beam_search:428) INFO: decoder input length: 66
2024-10-27 18:11:31,771 (beam_search:429) INFO: max output length: 66
2024-10-27 18:11:31,771 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:31,858 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:31,858 (beam_search:476) INFO:  -0.73 * 1.0 =  -0.73 for ctc
2024-10-27 18:11:31,858 (beam_search:479) INFO: total log probability: -0.73
2024-10-27 18:11:31,858 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:11:31,858 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:31,858 (beam_search:483) INFO: best hypo: THEISNOTCONNECTEDFORENERGYTO

2024-10-27 18:11:31,861 (asr_inference:509) INFO: speech length: 155511
2024-10-27 18:11:37,913 (beam_search:428) INFO: decoder input length: 120
2024-10-27 18:11:37,913 (beam_search:429) INFO: max output length: 120
2024-10-27 18:11:37,913 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:38,220 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:38,220 (beam_search:476) INFO:  -6.07 * 1.0 =  -6.07 for ctc
2024-10-27 18:11:38,220 (beam_search:479) INFO: total log probability: -6.07
2024-10-27 18:11:38,220 (beam_search:480) INFO: normalized log probability: -0.36
2024-10-27 18:11:38,221 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:38,221 (beam_search:483) INFO: best hypo: IT'SSOONEOUTTHENTHEOTHERONEKEEPSBECAUSETHERETWO

2024-10-27 18:11:38,223 (asr_inference:509) INFO: speech length: 85372
2024-10-27 18:11:41,253 (beam_search:428) INFO: decoder input length: 66
2024-10-27 18:11:41,253 (beam_search:429) INFO: max output length: 66
2024-10-27 18:11:41,253 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:41,404 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:41,404 (beam_search:476) INFO:  -1.37 * 1.0 =  -1.37 for ctc
2024-10-27 18:11:41,404 (beam_search:479) INFO: total log probability: -1.37
2024-10-27 18:11:41,404 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:11:41,404 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:41,405 (beam_search:483) INFO: best hypo: IT'STOTHEONETHAT'SOUTSOYOUCANIT

2024-10-27 18:11:41,407 (asr_inference:509) INFO: speech length: 120463
2024-10-27 18:11:45,802 (beam_search:428) INFO: decoder input length: 93
2024-10-27 18:11:45,802 (beam_search:429) INFO: max output length: 93
2024-10-27 18:11:45,802 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:46,053 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:46,053 (beam_search:476) INFO:  -2.47 * 1.0 =  -2.47 for ctc
2024-10-27 18:11:46,053 (beam_search:479) INFO: total log probability: -2.47
2024-10-27 18:11:46,053 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:11:46,053 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:46,053 (beam_search:483) INFO: best hypo: THEOTHERBULBGETSBECAUSETHEENERGYDOESN'THAVETOANDITON

2024-10-27 18:11:46,055 (asr_inference:509) INFO: speech length: 270583
2024-10-27 18:11:57,160 (beam_search:428) INFO: decoder input length: 210
2024-10-27 18:11:57,160 (beam_search:429) INFO: max output length: 210
2024-10-27 18:11:57,160 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:58,235 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:58,235 (beam_search:476) INFO:  -2.58 * 1.0 =  -2.58 for ctc
2024-10-27 18:11:58,235 (beam_search:479) INFO: total log probability: -2.58
2024-10-27 18:11:58,235 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:11:58,235 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:58,235 (beam_search:483) INFO: best hypo: IT'SAIT'SACIRCUITIT'SNOTINSOWHENONEOUTTHEBECAUSEONEPATHWAYANDTHEOFTHEARESTILL

2024-10-27 18:11:58,238 (asr_inference:509) INFO: speech length: 98516
2024-10-27 18:12:01,923 (beam_search:428) INFO: decoder input length: 76
2024-10-27 18:12:01,923 (beam_search:429) INFO: max output length: 76
2024-10-27 18:12:01,923 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:02,086 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:02,087 (beam_search:476) INFO:  -1.72 * 1.0 =  -1.72 for ctc
2024-10-27 18:12:02,087 (beam_search:479) INFO: total log probability: -1.72
2024-10-27 18:12:02,087 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:12:02,087 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:02,087 (beam_search:483) INFO: best hypo: THEENERGYFLOWINGBECAUSETHEBULBISWHICHISLIKEANOPEN

2024-10-27 18:12:02,090 (asr_inference:509) INFO: speech length: 44376
2024-10-27 18:12:03,739 (beam_search:428) INFO: decoder input length: 34
2024-10-27 18:12:03,739 (beam_search:429) INFO: max output length: 34
2024-10-27 18:12:03,739 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:03,775 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:03,776 (beam_search:476) INFO:  -2.02 * 1.0 =  -2.02 for ctc
2024-10-27 18:12:03,776 (beam_search:479) INFO: total log probability: -2.02
2024-10-27 18:12:03,776 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:12:03,776 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:03,776 (beam_search:483) INFO: best hypo: THEYAREALLCONNECTEDINPATHWAYS

2024-10-27 18:12:03,779 (asr_inference:509) INFO: speech length: 135270
2024-10-27 18:12:08,850 (beam_search:428) INFO: decoder input length: 105
2024-10-27 18:12:08,851 (beam_search:429) INFO: max output length: 105
2024-10-27 18:12:08,851 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:09,093 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:09,093 (beam_search:476) INFO:  -4.31 * 1.0 =  -4.31 for ctc
2024-10-27 18:12:09,093 (beam_search:479) INFO: total log probability: -4.31
2024-10-27 18:12:09,093 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 18:12:09,093 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:09,093 (beam_search:483) INFO: best hypo: SOSOTHEOTHERBULBGOESOFFIFIT'SIN

2024-10-27 18:12:09,096 (asr_inference:509) INFO: speech length: 96611
2024-10-27 18:12:12,464 (beam_search:428) INFO: decoder input length: 74
2024-10-27 18:12:12,464 (beam_search:429) INFO: max output length: 74
2024-10-27 18:12:12,464 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:12,653 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:12,653 (beam_search:476) INFO:  -3.43 * 1.0 =  -3.43 for ctc
2024-10-27 18:12:12,653 (beam_search:479) INFO: total log probability: -3.43
2024-10-27 18:12:12,653 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:12:12,653 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:12,653 (beam_search:483) INFO: best hypo: THISISASERIESCIRCUITANDONEBULBISOUTSOTHEOTHERONEGOES

2024-10-27 18:12:12,655 (asr_inference:509) INFO: speech length: 100986
2024-10-27 18:12:16,302 (beam_search:428) INFO: decoder input length: 78
2024-10-27 18:12:16,302 (beam_search:429) INFO: max output length: 78
2024-10-27 18:12:16,302 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:16,467 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:16,467 (beam_search:476) INFO:  -1.40 * 1.0 =  -1.40 for ctc
2024-10-27 18:12:16,467 (beam_search:479) INFO: total log probability: -1.40
2024-10-27 18:12:16,467 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:12:16,467 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:16,467 (beam_search:483) INFO: best hypo: BECAUSETHEFILAMENTBURNSOUTANDLIKESTHELIKEANOPEN

2024-10-27 18:12:16,470 (asr_inference:509) INFO: speech length: 45777
2024-10-27 18:12:18,166 (beam_search:428) INFO: decoder input length: 35
2024-10-27 18:12:18,166 (beam_search:429) INFO: max output length: 35
2024-10-27 18:12:18,166 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:18,206 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:18,206 (beam_search:476) INFO:  -1.05 * 1.0 =  -1.05 for ctc
2024-10-27 18:12:18,206 (beam_search:479) INFO: total log probability: -1.05
2024-10-27 18:12:18,206 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:12:18,206 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:18,206 (beam_search:483) INFO: best hypo: WE'VEBEENLEARNINGABOUTMAGNETS

2024-10-27 18:12:18,208 (asr_inference:509) INFO: speech length: 94678
2024-10-27 18:12:21,684 (beam_search:428) INFO: decoder input length: 73
2024-10-27 18:12:21,685 (beam_search:429) INFO: max output length: 73
2024-10-27 18:12:21,685 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:21,803 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:21,803 (beam_search:476) INFO:  -3.35 * 1.0 =  -3.35 for ctc
2024-10-27 18:12:21,803 (beam_search:479) INFO: total log probability: -3.35
2024-10-27 18:12:21,803 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 18:12:21,803 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:21,803 (beam_search:483) INFO: best hypo: WEHOWSTICKTOONLYTHINGSTHATAREOR

2024-10-27 18:12:21,806 (asr_inference:509) INFO: speech length: 172329
2024-10-27 18:12:28,257 (beam_search:428) INFO: decoder input length: 134
2024-10-27 18:12:28,257 (beam_search:429) INFO: max output length: 134
2024-10-27 18:12:28,257 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:28,570 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:28,571 (beam_search:476) INFO:  -2.47 * 1.0 =  -2.47 for ctc
2024-10-27 18:12:28,571 (beam_search:479) INFO: total log probability: -2.47
2024-10-27 18:12:28,571 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:12:28,571 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:28,571 (beam_search:483) INFO: best hypo: WELLAREMADEOUTOFANDTHEYSTICKTOOTHERMAGNETSOR

2024-10-27 18:12:28,574 (asr_inference:509) INFO: speech length: 63948
2024-10-27 18:12:30,956 (beam_search:428) INFO: decoder input length: 49
2024-10-27 18:12:30,956 (beam_search:429) INFO: max output length: 49
2024-10-27 18:12:30,956 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:31,012 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:31,012 (beam_search:476) INFO:  -1.19 * 1.0 =  -1.19 for ctc
2024-10-27 18:12:31,012 (beam_search:479) INFO: total log probability: -1.19
2024-10-27 18:12:31,012 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:12:31,012 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:31,012 (beam_search:483) INFO: best hypo: ITISMADEOFORSTEEL

2024-10-27 18:12:31,015 (asr_inference:509) INFO: speech length: 111132
2024-10-27 18:12:35,062 (beam_search:428) INFO: decoder input length: 86
2024-10-27 18:12:35,063 (beam_search:429) INFO: max output length: 86
2024-10-27 18:12:35,063 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:35,169 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:35,170 (beam_search:476) INFO:  -5.07 * 1.0 =  -5.07 for ctc
2024-10-27 18:12:35,170 (beam_search:479) INFO: total log probability: -5.07
2024-10-27 18:12:35,170 (beam_search:480) INFO: normalized log probability: -0.56
2024-10-27 18:12:35,170 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:35,170 (beam_search:483) INFO: best hypo: THEISTHENAILBECAUSEITSTEEL

2024-10-27 18:12:35,173 (asr_inference:509) INFO: speech length: 115404
2024-10-27 18:12:39,444 (beam_search:428) INFO: decoder input length: 89
2024-10-27 18:12:39,444 (beam_search:429) INFO: max output length: 89
2024-10-27 18:12:39,444 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:39,590 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:39,591 (beam_search:476) INFO:  -2.14 * 1.0 =  -2.14 for ctc
2024-10-27 18:12:39,591 (beam_search:479) INFO: total log probability: -2.14
2024-10-27 18:12:39,591 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:12:39,591 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:39,591 (beam_search:483) INFO: best hypo: BECAUSEITISORANDONLYTHOSESTICKTO

2024-10-27 18:12:39,593 (asr_inference:509) INFO: speech length: 101887
2024-10-27 18:12:43,302 (beam_search:428) INFO: decoder input length: 79
2024-10-27 18:12:43,302 (beam_search:429) INFO: max output length: 79
2024-10-27 18:12:43,303 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:43,376 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:43,376 (beam_search:476) INFO:  -2.73 * 1.0 =  -2.73 for ctc
2024-10-27 18:12:43,376 (beam_search:479) INFO: total log probability: -2.73
2024-10-27 18:12:43,377 (beam_search:480) INFO: normalized log probability: -0.39
2024-10-27 18:12:43,377 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:43,377 (beam_search:483) INFO: best hypo: SWHICHTOTHEMAGNET

2024-10-27 18:12:43,379 (asr_inference:509) INFO: speech length: 352199
2024-10-27 18:12:58,913 (beam_search:428) INFO: decoder input length: 274
2024-10-27 18:12:58,913 (beam_search:429) INFO: max output length: 274
2024-10-27 18:12:58,913 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:59,103 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:59,103 (beam_search:476) INFO:  -2.58 * 1.0 =  -2.58 for ctc
2024-10-27 18:12:59,104 (beam_search:479) INFO: total log probability: -2.58
2024-10-27 18:12:59,104 (beam_search:480) INFO: normalized log probability: -0.52
2024-10-27 18:12:59,104 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:59,104 (beam_search:483) INFO: best hypo: WOODANDEVEN

2024-10-27 18:12:59,107 (asr_inference:509) INFO: speech length: 296875
2024-10-27 18:13:12,533 (beam_search:428) INFO: decoder input length: 231
2024-10-27 18:13:12,533 (beam_search:429) INFO: max output length: 231
2024-10-27 18:13:12,533 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:13,396 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:13,396 (beam_search:476) INFO:  -5.40 * 1.0 =  -5.40 for ctc
2024-10-27 18:13:13,396 (beam_search:479) INFO: total log probability: -5.40
2024-10-27 18:13:13,396 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:13:13,396 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:13,396 (beam_search:483) INFO: best hypo: ONLYSTICKTOTHATHAVESINLIKEWOODHASOFTHATWOODNOSOITWON'TSTICK

2024-10-27 18:13:13,399 (asr_inference:509) INFO: speech length: 153627
2024-10-27 18:13:19,617 (beam_search:428) INFO: decoder input length: 119
2024-10-27 18:13:19,617 (beam_search:429) INFO: max output length: 119
2024-10-27 18:13:19,617 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:19,914 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:19,915 (beam_search:476) INFO:  -6.94 * 1.0 =  -6.94 for ctc
2024-10-27 18:13:19,915 (beam_search:479) INFO: total log probability: -6.94
2024-10-27 18:13:19,915 (beam_search:480) INFO: normalized log probability: -0.43
2024-10-27 18:13:19,915 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:19,915 (beam_search:483) INFO: best hypo: ISMADEOFTHENAILISSTEELWHICHTOANDTHEISNOTSTICK

2024-10-27 18:13:19,917 (asr_inference:509) INFO: speech length: 149264
2024-10-27 18:13:25,889 (beam_search:428) INFO: decoder input length: 116
2024-10-27 18:13:25,890 (beam_search:429) INFO: max output length: 116
2024-10-27 18:13:25,890 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:26,132 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:26,132 (beam_search:476) INFO:  -4.24 * 1.0 =  -4.24 for ctc
2024-10-27 18:13:26,132 (beam_search:479) INFO: total log probability: -4.24
2024-10-27 18:13:26,132 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 18:13:26,132 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:26,132 (beam_search:483) INFO: best hypo: ONLYSTICKTOTOWITHORANYTHINGTHATHASINIT

2024-10-27 18:13:26,134 (asr_inference:509) INFO: speech length: 149403
2024-10-27 18:13:32,026 (beam_search:428) INFO: decoder input length: 116
2024-10-27 18:13:32,026 (beam_search:429) INFO: max output length: 116
2024-10-27 18:13:32,026 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:32,363 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:32,363 (beam_search:476) INFO:  -5.71 * 1.0 =  -5.71 for ctc
2024-10-27 18:13:32,363 (beam_search:479) INFO: total log probability: -5.71
2024-10-27 18:13:32,364 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 18:13:32,364 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:32,364 (beam_search:483) INFO: best hypo: SOTHENOINITANDIT'SNOTIRONORSTEELSONOTSTICK

2024-10-27 18:13:32,366 (asr_inference:509) INFO: speech length: 167678
2024-10-27 18:13:38,475 (beam_search:428) INFO: decoder input length: 130
2024-10-27 18:13:38,475 (beam_search:429) INFO: max output length: 130
2024-10-27 18:13:38,475 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:38,816 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:38,816 (beam_search:476) INFO:  -5.58 * 1.0 =  -5.58 for ctc
2024-10-27 18:13:38,816 (beam_search:479) INFO: total log probability: -5.58
2024-10-27 18:13:38,816 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:13:38,816 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:38,816 (beam_search:483) INFO: best hypo: STICKTOTHEYMADEOFANDISMAGNETICSTEELHASSOFINIT

2024-10-27 18:13:38,820 (asr_inference:509) INFO: speech length: 250474
2024-10-27 18:13:49,099 (beam_search:428) INFO: decoder input length: 195
2024-10-27 18:13:49,099 (beam_search:429) INFO: max output length: 195
2024-10-27 18:13:49,099 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:49,966 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:49,966 (beam_search:476) INFO:  -8.07 * 1.0 =  -8.07 for ctc
2024-10-27 18:13:49,966 (beam_search:479) INFO: total log probability: -8.07
2024-10-27 18:13:49,966 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 18:13:49,966 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:49,966 (beam_search:483) INFO: best hypo: ONEISONEISSTICKTOBECAUSETHEYHAVESOFIRONINTHEMNOTSTICKTOTHEBECAUSEITHASNOOFINIT

2024-10-27 18:13:49,968 (asr_inference:509) INFO: speech length: 143063
2024-10-27 18:13:55,381 (beam_search:428) INFO: decoder input length: 111
2024-10-27 18:13:55,381 (beam_search:429) INFO: max output length: 111
2024-10-27 18:13:55,381 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:55,518 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:55,518 (beam_search:476) INFO:  -2.38 * 1.0 =  -2.38 for ctc
2024-10-27 18:13:55,518 (beam_search:479) INFO: total log probability: -2.38
2024-10-27 18:13:55,518 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 18:13:55,518 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:55,518 (beam_search:483) INFO: best hypo: STICKTONOTSTICKTOMAGNETS

2024-10-27 18:13:55,520 (asr_inference:509) INFO: speech length: 358940
2024-10-27 18:14:11,904 (beam_search:428) INFO: decoder input length: 279
2024-10-27 18:14:11,904 (beam_search:429) INFO: max output length: 279
2024-10-27 18:14:11,904 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:13,022 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:13,022 (beam_search:476) INFO:  -8.48 * 1.0 =  -8.48 for ctc
2024-10-27 18:14:13,022 (beam_search:479) INFO: total log probability: -8.48
2024-10-27 18:14:13,023 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:14:13,023 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:13,023 (beam_search:483) INFO: best hypo: MAKESMAGNETSTICKTOINITISLIKETHEITTHETHEMAGNETTOLIKEITBECAUSETHEMAGNETREALLYLIKES

2024-10-27 18:14:13,025 (asr_inference:509) INFO: speech length: 233302
2024-10-27 18:14:22,371 (beam_search:428) INFO: decoder input length: 181
2024-10-27 18:14:22,371 (beam_search:429) INFO: max output length: 181
2024-10-27 18:14:22,371 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:22,869 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:22,870 (beam_search:476) INFO:  -3.00 * 1.0 =  -3.00 for ctc
2024-10-27 18:14:22,870 (beam_search:479) INFO: total log probability: -3.00
2024-10-27 18:14:22,870 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:14:22,870 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:22,870 (beam_search:483) INFO: best hypo: STICKTOANDBECAUSETHEYBECAUSESTICKTOSTEELISWITHANDHASNOIRON

2024-10-27 18:14:22,872 (asr_inference:509) INFO: speech length: 226359
2024-10-27 18:14:31,870 (beam_search:428) INFO: decoder input length: 176
2024-10-27 18:14:31,870 (beam_search:429) INFO: max output length: 176
2024-10-27 18:14:31,870 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:32,622 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:32,622 (beam_search:476) INFO:  -2.94 * 1.0 =  -2.94 for ctc
2024-10-27 18:14:32,623 (beam_search:479) INFO: total log probability: -2.94
2024-10-27 18:14:32,623 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:14:32,623 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:32,623 (beam_search:483) INFO: best hypo: SOTHEONLYTHATDIDNOTSTICKWERETHETHEBRASSRINGANDTHEBECAUSETHEYHAVENOSOFIRONINTHEM

2024-10-27 18:14:32,625 (asr_inference:509) INFO: speech length: 92955
2024-10-27 18:14:35,894 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:14:35,894 (beam_search:429) INFO: max output length: 72
2024-10-27 18:14:35,894 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:36,071 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:36,071 (beam_search:476) INFO:  -3.63 * 1.0 =  -3.63 for ctc
2024-10-27 18:14:36,071 (beam_search:479) INFO: total log probability: -3.63
2024-10-27 18:14:36,071 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:14:36,071 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:36,071 (beam_search:483) INFO: best hypo: ITWILLDOBECAUSEITITSITHASNOINITBUTTHEBLACK

2024-10-27 18:14:36,073 (asr_inference:509) INFO: speech length: 88906
2024-10-27 18:14:39,272 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:14:39,272 (beam_search:429) INFO: max output length: 68
2024-10-27 18:14:39,272 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:39,324 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:39,324 (beam_search:476) INFO:  -2.00 * 1.0 =  -2.00 for ctc
2024-10-27 18:14:39,324 (beam_search:479) INFO: total log probability: -2.00
2024-10-27 18:14:39,324 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 18:14:39,324 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:39,324 (beam_search:483) INFO: best hypo: STICKBECAUSEITIS

2024-10-27 18:14:39,326 (asr_inference:509) INFO: speech length: 57233
2024-10-27 18:14:41,417 (beam_search:428) INFO: decoder input length: 44
2024-10-27 18:14:41,417 (beam_search:429) INFO: max output length: 44
2024-10-27 18:14:41,418 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:41,457 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:41,457 (beam_search:476) INFO:  -2.02 * 1.0 =  -2.02 for ctc
2024-10-27 18:14:41,457 (beam_search:479) INFO: total log probability: -2.02
2024-10-27 18:14:41,457 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:14:41,457 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:41,457 (beam_search:483) INFO: best hypo: ITHAVEIRONORAND

2024-10-27 18:14:41,460 (asr_inference:509) INFO: speech length: 52851
2024-10-27 18:14:43,386 (beam_search:428) INFO: decoder input length: 40
2024-10-27 18:14:43,386 (beam_search:429) INFO: max output length: 40
2024-10-27 18:14:43,386 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:43,452 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:43,452 (beam_search:476) INFO:  -1.60 * 1.0 =  -1.60 for ctc
2024-10-27 18:14:43,452 (beam_search:479) INFO: total log probability: -1.60
2024-10-27 18:14:43,452 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:14:43,452 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:43,452 (beam_search:483) INFO: best hypo: ITWILLBECAUSEIT'SMADEOUTOFSTEEL

2024-10-27 18:14:43,454 (asr_inference:509) INFO: speech length: 90554
2024-10-27 18:14:46,896 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:14:46,896 (beam_search:429) INFO: max output length: 70
2024-10-27 18:14:46,896 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:47,061 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:47,062 (beam_search:476) INFO:  -3.84 * 1.0 =  -3.84 for ctc
2024-10-27 18:14:47,062 (beam_search:479) INFO: total log probability: -3.84
2024-10-27 18:14:47,062 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:14:47,062 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:47,062 (beam_search:483) INFO: best hypo: ITBECAUSEITIT'SMADEOUTOFSTEELWHICHIRONINIT

2024-10-27 18:14:47,064 (asr_inference:509) INFO: speech length: 87082
2024-10-27 18:14:50,417 (beam_search:428) INFO: decoder input length: 67
2024-10-27 18:14:50,417 (beam_search:429) INFO: max output length: 67
2024-10-27 18:14:50,417 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:50,510 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:50,510 (beam_search:476) INFO:  -4.33 * 1.0 =  -4.33 for ctc
2024-10-27 18:14:50,510 (beam_search:479) INFO: total log probability: -4.33
2024-10-27 18:14:50,510 (beam_search:480) INFO: normalized log probability: -0.43
2024-10-27 18:14:50,510 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:50,510 (beam_search:483) INFO: best hypo: ITSBECAUSEITINITANDSTICKS

2024-10-27 18:14:50,513 (asr_inference:509) INFO: speech length: 165348
2024-10-27 18:14:57,074 (beam_search:428) INFO: decoder input length: 128
2024-10-27 18:14:57,074 (beam_search:429) INFO: max output length: 128
2024-10-27 18:14:57,074 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:57,676 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:57,677 (beam_search:476) INFO:  -4.73 * 1.0 =  -4.73 for ctc
2024-10-27 18:14:57,677 (beam_search:479) INFO: total log probability: -4.73
2024-10-27 18:14:57,677 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:14:57,677 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:57,677 (beam_search:483) INFO: best hypo: ITHASTOHAVEIRONINITSOTHATSOIFTHERE'NOIRONINITIT'THAVEBUTITHASIRONSOIT

2024-10-27 18:14:57,679 (asr_inference:509) INFO: speech length: 325445
2024-10-27 18:15:12,149 (beam_search:428) INFO: decoder input length: 253
2024-10-27 18:15:12,149 (beam_search:429) INFO: max output length: 253
2024-10-27 18:15:12,149 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:13,337 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:13,337 (beam_search:476) INFO:  -9.33 * 1.0 =  -9.33 for ctc
2024-10-27 18:15:13,337 (beam_search:479) INFO: total log probability: -9.33
2024-10-27 18:15:13,337 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 18:15:13,337 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:13,337 (beam_search:483) INFO: best hypo: WETALKEDABOUTHOWSTICKTOANDBECAUSETHEYHAVEINITMADEOUTOFSOSOSTICKTOMAGNETBECAUSEONLYATTRACTIRONOR

2024-10-27 18:15:13,340 (asr_inference:509) INFO: speech length: 94213
2024-10-27 18:15:16,701 (beam_search:428) INFO: decoder input length: 73
2024-10-27 18:15:16,701 (beam_search:429) INFO: max output length: 73
2024-10-27 18:15:16,701 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:16,786 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:16,786 (beam_search:476) INFO:  -2.01 * 1.0 =  -2.01 for ctc
2024-10-27 18:15:16,786 (beam_search:479) INFO: total log probability: -2.01
2024-10-27 18:15:16,786 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:15:16,786 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:16,786 (beam_search:483) INFO: best hypo: WEBEENLEARNINGABOUTWHATMAGNETS

2024-10-27 18:15:16,788 (asr_inference:509) INFO: speech length: 61827
2024-10-27 18:15:19,052 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:15:19,052 (beam_search:429) INFO: max output length: 47
2024-10-27 18:15:19,052 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:19,082 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:19,082 (beam_search:476) INFO:  -1.32 * 1.0 =  -1.32 for ctc
2024-10-27 18:15:19,082 (beam_search:479) INFO: total log probability: -1.32
2024-10-27 18:15:19,082 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:15:19,082 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:19,082 (beam_search:483) INFO: best hypo: THEANDARE

2024-10-27 18:15:19,085 (asr_inference:509) INFO: speech length: 130461
2024-10-27 18:15:24,012 (beam_search:428) INFO: decoder input length: 101
2024-10-27 18:15:24,013 (beam_search:429) INFO: max output length: 101
2024-10-27 18:15:24,013 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:24,209 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:24,209 (beam_search:476) INFO:  -2.98 * 1.0 =  -2.98 for ctc
2024-10-27 18:15:24,209 (beam_search:479) INFO: total log probability: -2.98
2024-10-27 18:15:24,209 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:15:24,209 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:24,209 (beam_search:483) INFO: best hypo: THEYWILLEACHOTHERBECAUSEITWILLBETORT

2024-10-27 18:15:24,211 (asr_inference:509) INFO: speech length: 80516
2024-10-27 18:15:27,213 (beam_search:428) INFO: decoder input length: 62
2024-10-27 18:15:27,213 (beam_search:429) INFO: max output length: 62
2024-10-27 18:15:27,213 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:27,303 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:27,304 (beam_search:476) INFO:  -2.15 * 1.0 =  -2.15 for ctc
2024-10-27 18:15:27,304 (beam_search:479) INFO: total log probability: -2.15
2024-10-27 18:15:27,304 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:15:27,304 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:27,304 (beam_search:483) INFO: best hypo: THEAREEACHOTHERBECAUSETHEARETHE

2024-10-27 18:15:27,306 (asr_inference:509) INFO: speech length: 171966
2024-10-27 18:15:33,575 (beam_search:428) INFO: decoder input length: 133
2024-10-27 18:15:33,575 (beam_search:429) INFO: max output length: 133
2024-10-27 18:15:33,575 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:33,886 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:33,886 (beam_search:476) INFO:  -4.93 * 1.0 =  -4.93 for ctc
2024-10-27 18:15:33,886 (beam_search:479) INFO: total log probability: -4.93
2024-10-27 18:15:33,886 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 18:15:33,886 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:33,886 (beam_search:483) INFO: best hypo: THEARETOOTHERTHEYATTRACTBUTTHEARETOEACHOTHERTHEY

2024-10-27 18:15:33,889 (asr_inference:509) INFO: speech length: 196248
2024-10-27 18:15:41,527 (beam_search:428) INFO: decoder input length: 152
2024-10-27 18:15:41,527 (beam_search:429) INFO: max output length: 152
2024-10-27 18:15:41,527 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:41,824 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:41,824 (beam_search:476) INFO:  -3.71 * 1.0 =  -3.71 for ctc
2024-10-27 18:15:41,824 (beam_search:479) INFO: total log probability: -3.71
2024-10-27 18:15:41,824 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:15:41,824 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:41,824 (beam_search:483) INFO: best hypo: THEANDARETHEANDANDANDONTHEMAGNETSARE

2024-10-27 18:15:41,827 (asr_inference:509) INFO: speech length: 117300
2024-10-27 18:15:46,066 (beam_search:428) INFO: decoder input length: 91
2024-10-27 18:15:46,066 (beam_search:429) INFO: max output length: 91
2024-10-27 18:15:46,066 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:46,223 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:46,223 (beam_search:476) INFO:  -2.64 * 1.0 =  -2.64 for ctc
2024-10-27 18:15:46,223 (beam_search:479) INFO: total log probability: -2.64
2024-10-27 18:15:46,223 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:15:46,223 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:46,223 (beam_search:483) INFO: best hypo: ARETHETHATARETOEACHOTHERARETHE

2024-10-27 18:15:46,225 (asr_inference:509) INFO: speech length: 200496
2024-10-27 18:15:53,966 (beam_search:428) INFO: decoder input length: 156
2024-10-27 18:15:53,966 (beam_search:429) INFO: max output length: 156
2024-10-27 18:15:53,966 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:54,328 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:54,328 (beam_search:476) INFO:  -2.36 * 1.0 =  -2.36 for ctc
2024-10-27 18:15:54,328 (beam_search:479) INFO: total log probability: -2.36
2024-10-27 18:15:54,328 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:15:54,328 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:54,329 (beam_search:483) INFO: best hypo: WELLTHEYHAVEANDANDTHOSEAREJUSTTHEYEACHOTHERTHEY

2024-10-27 18:15:54,331 (asr_inference:509) INFO: speech length: 161972
2024-10-27 18:16:00,388 (beam_search:428) INFO: decoder input length: 126
2024-10-27 18:16:00,388 (beam_search:429) INFO: max output length: 126
2024-10-27 18:16:00,388 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:00,711 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:00,711 (beam_search:476) INFO:  -4.98 * 1.0 =  -4.98 for ctc
2024-10-27 18:16:00,711 (beam_search:479) INFO: total log probability: -4.98
2024-10-27 18:16:00,711 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:16:00,711 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:00,711 (beam_search:483) INFO: best hypo: AREBECAUSETHEY'RETHEYDON'TTOTHEOTHERTHEY'RE

2024-10-27 18:16:00,713 (asr_inference:509) INFO: speech length: 84844
2024-10-27 18:16:03,743 (beam_search:428) INFO: decoder input length: 65
2024-10-27 18:16:03,743 (beam_search:429) INFO: max output length: 65
2024-10-27 18:16:03,743 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:03,820 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:03,820 (beam_search:476) INFO:  -1.24 * 1.0 =  -1.24 for ctc
2024-10-27 18:16:03,820 (beam_search:479) INFO: total log probability: -1.24
2024-10-27 18:16:03,820 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:16:03,820 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:03,820 (beam_search:483) INFO: best hypo: IT'SAONTHEEARTH

2024-10-27 18:16:03,822 (asr_inference:509) INFO: speech length: 81385
2024-10-27 18:16:06,929 (beam_search:428) INFO: decoder input length: 63
2024-10-27 18:16:06,930 (beam_search:429) INFO: max output length: 63
2024-10-27 18:16:06,930 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:07,014 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:07,014 (beam_search:476) INFO:  -2.07 * 1.0 =  -2.07 for ctc
2024-10-27 18:16:07,014 (beam_search:479) INFO: total log probability: -2.07
2024-10-27 18:16:07,014 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:16:07,014 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:07,014 (beam_search:483) INFO: best hypo: USINGATOWHATTHEYREGOING

2024-10-27 18:16:07,016 (asr_inference:509) INFO: speech length: 114164
2024-10-27 18:16:11,450 (beam_search:428) INFO: decoder input length: 88
2024-10-27 18:16:11,450 (beam_search:429) INFO: max output length: 88
2024-10-27 18:16:11,450 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:11,597 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:11,598 (beam_search:476) INFO:  -2.09 * 1.0 =  -2.09 for ctc
2024-10-27 18:16:11,598 (beam_search:479) INFO: total log probability: -2.09
2024-10-27 18:16:11,598 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:16:11,598 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:11,598 (beam_search:483) INFO: best hypo: ITTHEFLOWOFTHETHATISINSIDETHE

2024-10-27 18:16:11,600 (asr_inference:509) INFO: speech length: 92264
2024-10-27 18:16:14,876 (beam_search:428) INFO: decoder input length: 71
2024-10-27 18:16:14,876 (beam_search:429) INFO: max output length: 71
2024-10-27 18:16:14,876 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:14,968 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:14,968 (beam_search:476) INFO:  -2.23 * 1.0 =  -2.23 for ctc
2024-10-27 18:16:14,968 (beam_search:479) INFO: total log probability: -2.23
2024-10-27 18:16:14,968 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:16:14,968 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:14,968 (beam_search:483) INFO: best hypo: WEHAVEBEENLEARNINGABOUTAOF

2024-10-27 18:16:14,970 (asr_inference:509) INFO: speech length: 72190
2024-10-27 18:16:17,581 (beam_search:428) INFO: decoder input length: 55
2024-10-27 18:16:17,581 (beam_search:429) INFO: max output length: 55
2024-10-27 18:16:17,581 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:17,615 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:17,615 (beam_search:476) INFO:  -0.82 * 1.0 =  -0.82 for ctc
2024-10-27 18:16:17,615 (beam_search:479) INFO: total log probability: -0.82
2024-10-27 18:16:17,615 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:16:17,615 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:17,615 (beam_search:483) INFO: best hypo: THEYHAVEAND

2024-10-27 18:16:17,617 (asr_inference:509) INFO: speech length: 262109
2024-10-27 18:16:28,577 (beam_search:428) INFO: decoder input length: 204
2024-10-27 18:16:28,577 (beam_search:429) INFO: max output length: 204
2024-10-27 18:16:28,577 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:29,261 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:29,261 (beam_search:476) INFO:  -7.56 * 1.0 =  -7.56 for ctc
2024-10-27 18:16:29,261 (beam_search:479) INFO: total log probability: -7.56
2024-10-27 18:16:29,261 (beam_search:480) INFO: normalized log probability: -0.38
2024-10-27 18:16:29,261 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:29,261 (beam_search:483) INFO: best hypo: MAGNETSHAVEAANDAPOLEWHENOTHERTHEYATTRACTBUTWHENTWOOFTHESAMEOTHERTHEY

2024-10-27 18:16:29,264 (asr_inference:509) INFO: speech length: 93670
2024-10-27 18:16:32,668 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:16:32,668 (beam_search:429) INFO: max output length: 72
2024-10-27 18:16:32,668 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:32,719 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:32,720 (beam_search:476) INFO:  -0.86 * 1.0 =  -0.86 for ctc
2024-10-27 18:16:32,720 (beam_search:479) INFO: total log probability: -0.86
2024-10-27 18:16:32,720 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:16:32,720 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:32,720 (beam_search:483) INFO: best hypo: THEISTHE

2024-10-27 18:16:32,722 (asr_inference:509) INFO: speech length: 126746
2024-10-27 18:16:37,740 (beam_search:428) INFO: decoder input length: 98
2024-10-27 18:16:37,740 (beam_search:429) INFO: max output length: 98
2024-10-27 18:16:37,740 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:37,837 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:37,837 (beam_search:476) INFO:  -3.99 * 1.0 =  -3.99 for ctc
2024-10-27 18:16:37,837 (beam_search:479) INFO: total log probability: -3.99
2024-10-27 18:16:37,837 (beam_search:480) INFO: normalized log probability: -0.57
2024-10-27 18:16:37,837 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:37,837 (beam_search:483) INFO: best hypo: ISTOBOTHTOGETHERATTRACT

2024-10-27 18:16:37,840 (asr_inference:509) INFO: speech length: 173626
2024-10-27 18:16:44,261 (beam_search:428) INFO: decoder input length: 135
2024-10-27 18:16:44,261 (beam_search:429) INFO: max output length: 135
2024-10-27 18:16:44,261 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:44,474 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:44,475 (beam_search:476) INFO:  -3.65 * 1.0 =  -3.65 for ctc
2024-10-27 18:16:44,475 (beam_search:479) INFO: total log probability: -3.65
2024-10-27 18:16:44,475 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 18:16:44,475 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:44,475 (beam_search:483) INFO: best hypo: THEOFTHEISLIKETHATMAGNETSARETHROUGH

2024-10-27 18:16:44,477 (asr_inference:509) INFO: speech length: 273295
2024-10-27 18:16:56,296 (beam_search:428) INFO: decoder input length: 213
2024-10-27 18:16:56,297 (beam_search:429) INFO: max output length: 213
2024-10-27 18:16:56,297 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:56,891 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:56,891 (beam_search:476) INFO:  -5.94 * 1.0 =  -5.94 for ctc
2024-10-27 18:16:56,891 (beam_search:479) INFO: total log probability: -5.94
2024-10-27 18:16:56,891 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:16:56,891 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:56,891 (beam_search:483) INFO: best hypo: WELLITCANGOTHROUGHLIKEASASASTHEMAGNETORTHENOTTOO

2024-10-27 18:16:56,894 (asr_inference:509) INFO: speech length: 249027
2024-10-27 18:17:07,251 (beam_search:428) INFO: decoder input length: 194
2024-10-27 18:17:07,251 (beam_search:429) INFO: max output length: 194
2024-10-27 18:17:07,251 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:07,888 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:07,888 (beam_search:476) INFO: -10.72 * 1.0 = -10.72 for ctc
2024-10-27 18:17:07,888 (beam_search:479) INFO: total log probability: -10.72
2024-10-27 18:17:07,888 (beam_search:480) INFO: normalized log probability: -0.56
2024-10-27 18:17:07,888 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:07,889 (beam_search:483) INFO: best hypo: ITSIFTHENOTABLETOSOMETHINGELSETHATSBUTITSWILLBETO

2024-10-27 18:17:07,891 (asr_inference:509) INFO: speech length: 44753
2024-10-27 18:17:09,555 (beam_search:428) INFO: decoder input length: 34
2024-10-27 18:17:09,556 (beam_search:429) INFO: max output length: 34
2024-10-27 18:17:09,556 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:09,586 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:09,586 (beam_search:476) INFO:  -0.31 * 1.0 =  -0.31 for ctc
2024-10-27 18:17:09,587 (beam_search:479) INFO: total log probability: -0.31
2024-10-27 18:17:09,587 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 18:17:09,587 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:09,587 (beam_search:483) INFO: best hypo: BECAUSEIT'STOO

2024-10-27 18:17:09,590 (asr_inference:509) INFO: speech length: 220941
2024-10-27 18:17:18,664 (beam_search:428) INFO: decoder input length: 172
2024-10-27 18:17:18,664 (beam_search:429) INFO: max output length: 172
2024-10-27 18:17:18,664 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:19,216 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:19,216 (beam_search:476) INFO:  -5.72 * 1.0 =  -5.72 for ctc
2024-10-27 18:17:19,216 (beam_search:479) INFO: total log probability: -5.72
2024-10-27 18:17:19,216 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:17:19,216 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:19,216 (beam_search:483) INFO: best hypo: AISANAILWHICHISTHESTEELNAILSOBECAUSEITSITCANPICKUPA

2024-10-27 18:17:19,218 (asr_inference:509) INFO: speech length: 102298
2024-10-27 18:17:22,813 (beam_search:428) INFO: decoder input length: 79
2024-10-27 18:17:22,814 (beam_search:429) INFO: max output length: 79
2024-10-27 18:17:22,814 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:22,867 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:22,867 (beam_search:476) INFO:  -2.10 * 1.0 =  -2.10 for ctc
2024-10-27 18:17:22,867 (beam_search:479) INFO: total log probability: -2.10
2024-10-27 18:17:22,867 (beam_search:480) INFO: normalized log probability: -0.42
2024-10-27 18:17:22,867 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:22,867 (beam_search:483) INFO: best hypo: THETHROUGHNAIL

2024-10-27 18:17:22,870 (asr_inference:509) INFO: speech length: 122610
2024-10-27 18:17:27,419 (beam_search:428) INFO: decoder input length: 95
2024-10-27 18:17:27,419 (beam_search:429) INFO: max output length: 95
2024-10-27 18:17:27,419 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:27,522 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:27,522 (beam_search:476) INFO:  -3.33 * 1.0 =  -3.33 for ctc
2024-10-27 18:17:27,522 (beam_search:479) INFO: total log probability: -3.33
2024-10-27 18:17:27,522 (beam_search:480) INFO: normalized log probability: -0.42
2024-10-27 18:17:27,522 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:27,522 (beam_search:483) INFO: best hypo: ISTHETHROUGHTHENAILWHICH

2024-10-27 18:17:27,525 (asr_inference:509) INFO: speech length: 171267
2024-10-27 18:17:34,510 (beam_search:428) INFO: decoder input length: 133
2024-10-27 18:17:34,510 (beam_search:429) INFO: max output length: 133
2024-10-27 18:17:34,510 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:34,785 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:34,785 (beam_search:476) INFO:  -3.65 * 1.0 =  -3.65 for ctc
2024-10-27 18:17:34,785 (beam_search:479) INFO: total log probability: -3.65
2024-10-27 18:17:34,785 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 18:17:34,786 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:34,786 (beam_search:483) INFO: best hypo: MAGNETISMAKINGTHETEMPORARYMAGNETBYINGTHETHROUGH

2024-10-27 18:17:34,788 (asr_inference:509) INFO: speech length: 196199
2024-10-27 18:17:42,103 (beam_search:428) INFO: decoder input length: 152
2024-10-27 18:17:42,103 (beam_search:429) INFO: max output length: 152
2024-10-27 18:17:42,103 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:42,518 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:42,518 (beam_search:476) INFO:  -3.95 * 1.0 =  -3.95 for ctc
2024-10-27 18:17:42,518 (beam_search:479) INFO: total log probability: -3.95
2024-10-27 18:17:42,518 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:17:42,518 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:42,518 (beam_search:483) INFO: best hypo: THETOBEAMAGNETWHICHISWHICHITATEMPORARYMAGNETWHICHUPLIKE

2024-10-27 18:17:42,520 (asr_inference:509) INFO: speech length: 54425
2024-10-27 18:17:44,555 (beam_search:428) INFO: decoder input length: 42
2024-10-27 18:17:44,555 (beam_search:429) INFO: max output length: 42
2024-10-27 18:17:44,555 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:44,609 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:44,609 (beam_search:476) INFO:  -0.67 * 1.0 =  -0.67 for ctc
2024-10-27 18:17:44,609 (beam_search:479) INFO: total log probability: -0.67
2024-10-27 18:17:44,609 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:17:44,609 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:44,609 (beam_search:483) INFO: best hypo: ITHASTOBETOUCHINGTHENAIL

2024-10-27 18:17:44,612 (asr_inference:509) INFO: speech length: 92724
2024-10-27 18:17:47,925 (beam_search:428) INFO: decoder input length: 71
2024-10-27 18:17:47,925 (beam_search:429) INFO: max output length: 71
2024-10-27 18:17:47,925 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:47,993 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:47,993 (beam_search:476) INFO:  -2.80 * 1.0 =  -2.80 for ctc
2024-10-27 18:17:47,993 (beam_search:479) INFO: total log probability: -2.80
2024-10-27 18:17:47,993 (beam_search:480) INFO: normalized log probability: -0.40
2024-10-27 18:17:47,993 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:47,993 (beam_search:483) INFO: best hypo: ISTOUCHINGNAILTHENTHE

2024-10-27 18:17:47,995 (asr_inference:509) INFO: speech length: 115464
2024-10-27 18:17:52,328 (beam_search:428) INFO: decoder input length: 89
2024-10-27 18:17:52,328 (beam_search:429) INFO: max output length: 89
2024-10-27 18:17:52,328 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:52,399 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:52,399 (beam_search:476) INFO:  -1.68 * 1.0 =  -1.68 for ctc
2024-10-27 18:17:52,399 (beam_search:479) INFO: total log probability: -1.68
2024-10-27 18:17:52,399 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:17:52,399 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:52,399 (beam_search:483) INFO: best hypo: THEISTOUP

2024-10-27 18:17:52,402 (asr_inference:509) INFO: speech length: 163148
2024-10-27 18:17:59,035 (beam_search:428) INFO: decoder input length: 126
2024-10-27 18:17:59,035 (beam_search:429) INFO: max output length: 126
2024-10-27 18:17:59,035 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:59,445 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:59,445 (beam_search:476) INFO:  -9.06 * 1.0 =  -9.06 for ctc
2024-10-27 18:17:59,445 (beam_search:479) INFO: total log probability: -9.06
2024-10-27 18:17:59,445 (beam_search:480) INFO: normalized log probability: -0.45
2024-10-27 18:17:59,445 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:59,445 (beam_search:483) INFO: best hypo: WELLTHEISTHEMAGNETTHETEMPORARYITWONTWORKBECAUSETHERENOMAGNETICINTHETEMPORARY

2024-10-27 18:17:59,449 (asr_inference:509) INFO: speech length: 72508
2024-10-27 18:18:02,325 (beam_search:428) INFO: decoder input length: 56
2024-10-27 18:18:02,325 (beam_search:429) INFO: max output length: 56
2024-10-27 18:18:02,325 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:02,361 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:02,361 (beam_search:476) INFO:  -1.40 * 1.0 =  -1.40 for ctc
2024-10-27 18:18:02,361 (beam_search:479) INFO: total log probability: -1.40
2024-10-27 18:18:02,361 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:18:02,361 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:02,361 (beam_search:483) INFO: best hypo: THEISEVERYTHING

2024-10-27 18:18:02,363 (asr_inference:509) INFO: speech length: 145077
2024-10-27 18:18:08,015 (beam_search:428) INFO: decoder input length: 112
2024-10-27 18:18:08,015 (beam_search:429) INFO: max output length: 112
2024-10-27 18:18:08,015 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:08,172 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:08,172 (beam_search:476) INFO:  -4.19 * 1.0 =  -4.19 for ctc
2024-10-27 18:18:08,172 (beam_search:479) INFO: total log probability: -4.19
2024-10-27 18:18:08,172 (beam_search:480) INFO: normalized log probability: -0.42
2024-10-27 18:18:08,172 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:08,172 (beam_search:483) INFO: best hypo: WELLBYTHEMAGNETITEVERYTHINGTHATMAGNET

2024-10-27 18:18:08,175 (asr_inference:509) INFO: speech length: 96491
2024-10-27 18:18:11,597 (beam_search:428) INFO: decoder input length: 74
2024-10-27 18:18:11,597 (beam_search:429) INFO: max output length: 74
2024-10-27 18:18:11,597 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:11,723 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:11,723 (beam_search:476) INFO:  -3.62 * 1.0 =  -3.62 for ctc
2024-10-27 18:18:11,723 (beam_search:479) INFO: total log probability: -3.62
2024-10-27 18:18:11,723 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 18:18:11,723 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:11,723 (beam_search:483) INFO: best hypo: BECAUSEISNOMAGNETICFORCEITWILLNOTWORK

2024-10-27 18:18:11,725 (asr_inference:509) INFO: speech length: 18548
2024-10-27 18:18:12,582 (beam_search:428) INFO: decoder input length: 13
2024-10-27 18:18:12,582 (beam_search:429) INFO: max output length: 13
2024-10-27 18:18:12,582 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:12,593 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:12,593 (beam_search:476) INFO:  -0.39 * 1.0 =  -0.39 for ctc
2024-10-27 18:18:12,593 (beam_search:479) INFO: total log probability: -0.39
2024-10-27 18:18:12,593 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:18:12,593 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:12,593 (beam_search:483) INFO: best hypo: IT'S

2024-10-27 18:18:12,595 (asr_inference:509) INFO: speech length: 68536
2024-10-27 18:18:15,161 (beam_search:428) INFO: decoder input length: 53
2024-10-27 18:18:15,161 (beam_search:429) INFO: max output length: 53
2024-10-27 18:18:15,161 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:15,172 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:15,172 (beam_search:476) INFO:  -0.37 * 1.0 =  -0.37 for ctc
2024-10-27 18:18:15,172 (beam_search:479) INFO: total log probability: -0.37
2024-10-27 18:18:15,172 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:18:15,172 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:15,172 (beam_search:483) INFO: best hypo: 

2024-10-27 18:18:15,174 (asr_inference:509) INFO: speech length: 88904
2024-10-27 18:18:18,480 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:18:18,480 (beam_search:429) INFO: max output length: 68
2024-10-27 18:18:18,480 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:18,575 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:18,575 (beam_search:476) INFO:  -2.47 * 1.0 =  -2.47 for ctc
2024-10-27 18:18:18,575 (beam_search:479) INFO: total log probability: -2.47
2024-10-27 18:18:18,575 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:18:18,575 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:18,575 (beam_search:483) INFO: best hypo: THEISTHEMAGNETTONAILTOTHE

2024-10-27 18:18:18,577 (asr_inference:509) INFO: speech length: 116162
2024-10-27 18:18:22,921 (beam_search:428) INFO: decoder input length: 90
2024-10-27 18:18:22,921 (beam_search:429) INFO: max output length: 90
2024-10-27 18:18:22,921 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:23,037 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:23,037 (beam_search:476) INFO:  -2.00 * 1.0 =  -2.00 for ctc
2024-10-27 18:18:23,037 (beam_search:479) INFO: total log probability: -2.00
2024-10-27 18:18:23,037 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:18:23,037 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:23,037 (beam_search:483) INFO: best hypo: WEVEBEENABOUTTHEOFWITH

2024-10-27 18:18:23,040 (asr_inference:509) INFO: speech length: 81147
2024-10-27 18:18:26,087 (beam_search:428) INFO: decoder input length: 62
2024-10-27 18:18:26,087 (beam_search:429) INFO: max output length: 62
2024-10-27 18:18:26,087 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:26,155 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:26,156 (beam_search:476) INFO:  -1.59 * 1.0 =  -1.59 for ctc
2024-10-27 18:18:26,156 (beam_search:479) INFO: total log probability: -1.59
2024-10-27 18:18:26,156 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:18:26,156 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:26,156 (beam_search:483) INFO: best hypo: THEMAGNETISTHETHROUGHTHE

2024-10-27 18:18:26,158 (asr_inference:509) INFO: speech length: 85452
2024-10-27 18:18:29,218 (beam_search:428) INFO: decoder input length: 66
2024-10-27 18:18:29,218 (beam_search:429) INFO: max output length: 66
2024-10-27 18:18:29,218 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:29,302 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:29,302 (beam_search:476) INFO:  -2.26 * 1.0 =  -2.26 for ctc
2024-10-27 18:18:29,302 (beam_search:479) INFO: total log probability: -2.26
2024-10-27 18:18:29,302 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:18:29,302 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:29,302 (beam_search:483) INFO: best hypo: ISNOTENOUGHBECAUSEITISTOO

2024-10-27 18:18:29,304 (asr_inference:509) INFO: speech length: 124706
2024-10-27 18:18:33,908 (beam_search:428) INFO: decoder input length: 96
2024-10-27 18:18:33,908 (beam_search:429) INFO: max output length: 96
2024-10-27 18:18:33,908 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:34,106 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:34,106 (beam_search:476) INFO:  -2.85 * 1.0 =  -2.85 for ctc
2024-10-27 18:18:34,106 (beam_search:479) INFO: total log probability: -2.85
2024-10-27 18:18:34,106 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:18:34,106 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:34,106 (beam_search:483) INFO: best hypo: SOAREBEINGINTOAWHICHISBREAKINGTHEOFWITH

2024-10-27 18:18:34,108 (asr_inference:509) INFO: speech length: 164185
2024-10-27 18:18:40,572 (beam_search:428) INFO: decoder input length: 127
2024-10-27 18:18:40,573 (beam_search:429) INFO: max output length: 127
2024-10-27 18:18:40,573 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:40,907 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:40,907 (beam_search:476) INFO:  -6.21 * 1.0 =  -6.21 for ctc
2024-10-27 18:18:40,907 (beam_search:479) INFO: total log probability: -6.21
2024-10-27 18:18:40,907 (beam_search:480) INFO: normalized log probability: -0.37
2024-10-27 18:18:40,907 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:40,907 (beam_search:483) INFO: best hypo: WHENTHEREISTOOONTHEOTHERSIDEOFTHETHEISTOTOTHE

2024-10-27 18:18:40,909 (asr_inference:509) INFO: speech length: 289709
2024-10-27 18:18:53,563 (beam_search:428) INFO: decoder input length: 225
2024-10-27 18:18:53,564 (beam_search:429) INFO: max output length: 225
2024-10-27 18:18:53,564 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:54,335 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:54,336 (beam_search:476) INFO:  -7.14 * 1.0 =  -7.14 for ctc
2024-10-27 18:18:54,336 (beam_search:479) INFO: total log probability: -7.14
2024-10-27 18:18:54,336 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 18:18:54,336 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:54,336 (beam_search:483) INFO: best hypo: YOUDROPANDTHENISDOWNTHEWHICHISMAKINGBECAUSETHEISTHANTHEITTHEEACHOTHER

2024-10-27 18:18:54,338 (asr_inference:509) INFO: speech length: 163300
2024-10-27 18:19:01,061 (beam_search:428) INFO: decoder input length: 127
2024-10-27 18:19:01,062 (beam_search:429) INFO: max output length: 127
2024-10-27 18:19:01,062 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:01,270 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:01,270 (beam_search:476) INFO:  -3.93 * 1.0 =  -3.93 for ctc
2024-10-27 18:19:01,270 (beam_search:479) INFO: total log probability: -3.93
2024-10-27 18:19:01,270 (beam_search:480) INFO: normalized log probability: -0.36
2024-10-27 18:19:01,270 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:01,270 (beam_search:483) INFO: best hypo: YOUSEPARATETHEIFTHERETHENTHETWORK

2024-10-27 18:19:01,272 (asr_inference:509) INFO: speech length: 227702
2024-10-27 18:19:10,521 (beam_search:428) INFO: decoder input length: 177
2024-10-27 18:19:10,521 (beam_search:429) INFO: max output length: 177
2024-10-27 18:19:10,521 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:10,719 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:10,719 (beam_search:476) INFO:  -3.10 * 1.0 =  -3.10 for ctc
2024-10-27 18:19:10,719 (beam_search:479) INFO: total log probability: -3.10
2024-10-27 18:19:10,719 (beam_search:480) INFO: normalized log probability: -0.39
2024-10-27 18:19:10,719 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:10,719 (beam_search:483) INFO: best hypo: AREANDWASHERSYOUTHEAWAY

2024-10-27 18:19:10,723 (asr_inference:509) INFO: speech length: 130565
2024-10-27 18:19:15,863 (beam_search:428) INFO: decoder input length: 101
2024-10-27 18:19:15,864 (beam_search:429) INFO: max output length: 101
2024-10-27 18:19:15,864 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:16,058 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:16,058 (beam_search:476) INFO:  -3.60 * 1.0 =  -3.60 for ctc
2024-10-27 18:19:16,058 (beam_search:479) INFO: total log probability: -3.60
2024-10-27 18:19:16,058 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:19:16,058 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:16,058 (beam_search:483) INFO: best hypo: WHENYOUADDITWITHTHEMAGNETICITBECAUSEITS

2024-10-27 18:19:16,060 (asr_inference:509) INFO: speech length: 277108
2024-10-27 18:19:27,985 (beam_search:428) INFO: decoder input length: 215
2024-10-27 18:19:27,985 (beam_search:429) INFO: max output length: 215
2024-10-27 18:19:27,985 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:28,223 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:28,223 (beam_search:476) INFO:  -2.82 * 1.0 =  -2.82 for ctc
2024-10-27 18:19:28,223 (beam_search:479) INFO: total log probability: -2.82
2024-10-27 18:19:28,223 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:19:28,223 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:28,223 (beam_search:483) INFO: best hypo: THEINISTHEISTHE

2024-10-27 18:19:28,226 (asr_inference:509) INFO: speech length: 87375
2024-10-27 18:19:31,378 (beam_search:428) INFO: decoder input length: 67
2024-10-27 18:19:31,378 (beam_search:429) INFO: max output length: 67
2024-10-27 18:19:31,378 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:31,441 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:31,441 (beam_search:476) INFO:  -2.31 * 1.0 =  -2.31 for ctc
2024-10-27 18:19:31,441 (beam_search:479) INFO: total log probability: -2.31
2024-10-27 18:19:31,441 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 18:19:31,441 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:31,441 (beam_search:483) INFO: best hypo: YOUADDTHENSTHE

2024-10-27 18:19:31,443 (asr_inference:509) INFO: speech length: 157855
2024-10-27 18:19:37,321 (beam_search:428) INFO: decoder input length: 122
2024-10-27 18:19:37,321 (beam_search:429) INFO: max output length: 122
2024-10-27 18:19:37,321 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:37,439 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:37,440 (beam_search:476) INFO:  -2.48 * 1.0 =  -2.48 for ctc
2024-10-27 18:19:37,440 (beam_search:479) INFO: total log probability: -2.48
2024-10-27 18:19:37,440 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:19:37,440 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:37,440 (beam_search:483) INFO: best hypo: THETHEBUTTHETHE

2024-10-27 18:19:37,442 (asr_inference:509) INFO: speech length: 35754
2024-10-27 18:19:38,857 (beam_search:428) INFO: decoder input length: 27
2024-10-27 18:19:38,857 (beam_search:429) INFO: max output length: 27
2024-10-27 18:19:38,857 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:38,871 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:38,872 (beam_search:476) INFO:  -0.69 * 1.0 =  -0.69 for ctc
2024-10-27 18:19:38,872 (beam_search:479) INFO: total log probability: -0.69
2024-10-27 18:19:38,872 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:19:38,872 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:38,872 (beam_search:483) INFO: best hypo: ITTHE

2024-10-27 18:19:38,873 (asr_inference:509) INFO: speech length: 145557
2024-10-27 18:19:44,422 (beam_search:428) INFO: decoder input length: 113
2024-10-27 18:19:44,422 (beam_search:429) INFO: max output length: 113
2024-10-27 18:19:44,422 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:44,717 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:44,717 (beam_search:476) INFO:  -5.49 * 1.0 =  -5.49 for ctc
2024-10-27 18:19:44,717 (beam_search:479) INFO: total log probability: -5.49
2024-10-27 18:19:44,717 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 18:19:44,717 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:44,717 (beam_search:483) INFO: best hypo: THEFORCEMAGNETISMISBUTASYOUPUTINMOREOFITSTHEBECAUSE

2024-10-27 18:19:44,721 (asr_inference:509) INFO: speech length: 59204
2024-10-27 18:19:46,861 (beam_search:428) INFO: decoder input length: 45
2024-10-27 18:19:46,861 (beam_search:429) INFO: max output length: 45
2024-10-27 18:19:46,861 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:46,903 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:46,903 (beam_search:476) INFO:  -1.24 * 1.0 =  -1.24 for ctc
2024-10-27 18:19:46,903 (beam_search:479) INFO: total log probability: -1.24
2024-10-27 18:19:46,903 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:19:46,903 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:46,903 (beam_search:483) INFO: best hypo: WELLTOTHEYOUADD

2024-10-27 18:19:46,906 (asr_inference:509) INFO: speech length: 91289
2024-10-27 18:19:50,226 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:19:50,226 (beam_search:429) INFO: max output length: 70
2024-10-27 18:19:50,226 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:50,293 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:50,294 (beam_search:476) INFO:  -2.81 * 1.0 =  -2.81 for ctc
2024-10-27 18:19:50,294 (beam_search:479) INFO: total log probability: -2.81
2024-10-27 18:19:50,294 (beam_search:480) INFO: normalized log probability: -0.40
2024-10-27 18:19:50,294 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:50,294 (beam_search:483) INFO: best hypo: ITTAKETOTHEOF

2024-10-27 18:19:50,296 (asr_inference:509) INFO: speech length: 60102
2024-10-27 18:19:52,610 (beam_search:428) INFO: decoder input length: 46
2024-10-27 18:19:52,611 (beam_search:429) INFO: max output length: 46
2024-10-27 18:19:52,611 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:52,658 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:52,659 (beam_search:476) INFO:  -1.93 * 1.0 =  -1.93 for ctc
2024-10-27 18:19:52,659 (beam_search:479) INFO: total log probability: -1.93
2024-10-27 18:19:52,659 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:19:52,659 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:52,659 (beam_search:483) INFO: best hypo: WE'VEHOWTOMAKE

2024-10-27 18:19:52,661 (asr_inference:509) INFO: speech length: 226549
2024-10-27 18:20:01,870 (beam_search:428) INFO: decoder input length: 176
2024-10-27 18:20:01,871 (beam_search:429) INFO: max output length: 176
2024-10-27 18:20:01,871 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:02,446 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:02,446 (beam_search:476) INFO:  -6.87 * 1.0 =  -6.87 for ctc
2024-10-27 18:20:02,446 (beam_search:479) INFO: total log probability: -6.87
2024-10-27 18:20:02,446 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 18:20:02,446 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:02,446 (beam_search:483) INFO: best hypo: ITSAISASTEELNAILRIVETKINDOFLIKEATHATHASWIREDAROUNDIT

2024-10-27 18:20:02,448 (asr_inference:509) INFO: speech length: 62868
2024-10-27 18:20:04,779 (beam_search:428) INFO: decoder input length: 48
2024-10-27 18:20:04,780 (beam_search:429) INFO: max output length: 48
2024-10-27 18:20:04,780 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:04,828 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:04,829 (beam_search:476) INFO:  -1.68 * 1.0 =  -1.68 for ctc
2024-10-27 18:20:04,829 (beam_search:479) INFO: total log probability: -1.68
2024-10-27 18:20:04,829 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:20:04,829 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:04,829 (beam_search:483) INFO: best hypo: YOUTHEWITHWIREITTHE

2024-10-27 18:20:04,831 (asr_inference:509) INFO: speech length: 189690
2024-10-27 18:20:11,978 (beam_search:428) INFO: decoder input length: 147
2024-10-27 18:20:11,978 (beam_search:429) INFO: max output length: 147
2024-10-27 18:20:11,978 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:12,372 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:12,372 (beam_search:476) INFO:  -2.37 * 1.0 =  -2.37 for ctc
2024-10-27 18:20:12,372 (beam_search:479) INFO: total log probability: -2.37
2024-10-27 18:20:12,373 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:20:12,373 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:12,373 (beam_search:483) INFO: best hypo: THEENERGYGOESTHROUGHTHOSESANDTHATSMAGNETICENERGYWHICHCANPICKUP

2024-10-27 18:20:12,375 (asr_inference:509) INFO: speech length: 260743
2024-10-27 18:20:23,634 (beam_search:428) INFO: decoder input length: 203
2024-10-27 18:20:23,634 (beam_search:429) INFO: max output length: 203
2024-10-27 18:20:23,634 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:24,323 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:24,323 (beam_search:476) INFO:  -4.08 * 1.0 =  -4.08 for ctc
2024-10-27 18:20:24,323 (beam_search:479) INFO: total log probability: -4.08
2024-10-27 18:20:24,323 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:20:24,323 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:24,323 (beam_search:483) INFO: best hypo: THEENERGYTHROUGHTHOSESGOESREALLYFASTSOTHATCREATESAOFENERGYWHICHSTHE

2024-10-27 18:20:24,326 (asr_inference:509) INFO: speech length: 168778
2024-10-27 18:20:30,459 (beam_search:428) INFO: decoder input length: 131
2024-10-27 18:20:30,459 (beam_search:429) INFO: max output length: 131
2024-10-27 18:20:30,459 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:30,844 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:30,845 (beam_search:476) INFO:  -3.16 * 1.0 =  -3.16 for ctc
2024-10-27 18:20:30,845 (beam_search:479) INFO: total log probability: -3.16
2024-10-27 18:20:30,845 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:20:30,845 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:30,845 (beam_search:483) INFO: best hypo: SOIT'SACIRCUITANDIT'STHEISWITHAOFWIRE

2024-10-27 18:20:30,848 (asr_inference:509) INFO: speech length: 164962
2024-10-27 18:20:37,715 (beam_search:428) INFO: decoder input length: 128
2024-10-27 18:20:37,715 (beam_search:429) INFO: max output length: 128
2024-10-27 18:20:37,715 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:38,225 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:38,225 (beam_search:476) INFO:  -6.56 * 1.0 =  -6.56 for ctc
2024-10-27 18:20:38,225 (beam_search:479) INFO: total log probability: -6.56
2024-10-27 18:20:38,225 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:20:38,225 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:38,225 (beam_search:483) INFO: best hypo: WELLRIGHTNOWTHEREISNOENERGYSOITSJUSTAYOUCANTANYTHINGUPITSNOTAMAGNET

2024-10-27 18:20:38,227 (asr_inference:509) INFO: speech length: 32432
2024-10-27 18:20:39,539 (beam_search:428) INFO: decoder input length: 24
2024-10-27 18:20:39,540 (beam_search:429) INFO: max output length: 24
2024-10-27 18:20:39,540 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:39,553 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:39,553 (beam_search:476) INFO:  -0.09 * 1.0 =  -0.09 for ctc
2024-10-27 18:20:39,553 (beam_search:479) INFO: total log probability: -0.09
2024-10-27 18:20:39,553 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:20:39,553 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:39,554 (beam_search:483) INFO: best hypo: ITTHE

2024-10-27 18:20:39,556 (asr_inference:509) INFO: speech length: 262376
2024-10-27 18:20:50,423 (beam_search:428) INFO: decoder input length: 204
2024-10-27 18:20:50,423 (beam_search:429) INFO: max output length: 204
2024-10-27 18:20:50,423 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:51,179 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:51,179 (beam_search:476) INFO:  -3.51 * 1.0 =  -3.51 for ctc
2024-10-27 18:20:51,179 (beam_search:479) INFO: total log probability: -3.51
2024-10-27 18:20:51,179 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:20:51,179 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:51,179 (beam_search:483) INFO: best hypo: THEENERGYISFLOWINGFROMTHENEGATIVEGOINGTHROUGHTHESWITCHTHROUGHTHEANDTHATENERGYISINGREALLYWHICHS

2024-10-27 18:20:51,181 (asr_inference:509) INFO: speech length: 72659
2024-10-27 18:20:53,917 (beam_search:428) INFO: decoder input length: 56
2024-10-27 18:20:53,918 (beam_search:429) INFO: max output length: 56
2024-10-27 18:20:53,918 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:53,954 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:53,954 (beam_search:476) INFO:  -1.24 * 1.0 =  -1.24 for ctc
2024-10-27 18:20:53,954 (beam_search:479) INFO: total log probability: -1.24
2024-10-27 18:20:53,954 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:20:53,954 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:53,954 (beam_search:483) INFO: best hypo: AREINTOTHE

2024-10-27 18:20:53,956 (asr_inference:509) INFO: speech length: 313793
2024-10-27 18:21:07,602 (beam_search:428) INFO: decoder input length: 244
2024-10-27 18:21:07,602 (beam_search:429) INFO: max output length: 244
2024-10-27 18:21:07,602 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:08,712 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:08,713 (beam_search:476) INFO:  -5.95 * 1.0 =  -5.95 for ctc
2024-10-27 18:21:08,713 (beam_search:479) INFO: total log probability: -5.95
2024-10-27 18:21:08,713 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:21:08,713 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:08,713 (beam_search:483) INFO: best hypo: THEISIMPORTANTBECAUSETHEWIREMAKESTHATENERGYINTOTHEISALSOIMPORTANTBECAUSEITNEEDSTOBESOTHESCANDOTHEIR

2024-10-27 18:21:08,716 (asr_inference:509) INFO: speech length: 242478
2024-10-27 18:21:18,619 (beam_search:428) INFO: decoder input length: 188
2024-10-27 18:21:18,619 (beam_search:429) INFO: max output length: 188
2024-10-27 18:21:18,619 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:19,151 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:19,151 (beam_search:476) INFO:  -3.29 * 1.0 =  -3.29 for ctc
2024-10-27 18:21:19,151 (beam_search:479) INFO: total log probability: -3.29
2024-10-27 18:21:19,151 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:21:19,151 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:19,151 (beam_search:483) INFO: best hypo: WELLTHERETOBEABECAUSEOTHERTHEBATTERYALLITSENERGYAOFENERGY

2024-10-27 18:21:19,154 (asr_inference:509) INFO: speech length: 232712
2024-10-27 18:21:28,622 (beam_search:428) INFO: decoder input length: 181
2024-10-27 18:21:28,622 (beam_search:429) INFO: max output length: 181
2024-10-27 18:21:28,622 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:29,410 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:29,410 (beam_search:476) INFO:  -6.67 * 1.0 =  -6.67 for ctc
2024-10-27 18:21:29,410 (beam_search:479) INFO: total log probability: -6.67
2024-10-27 18:21:29,410 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:21:29,410 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:29,410 (beam_search:483) INFO: best hypo: YOUTHISTOGETHERWITHACIRCUITTHEENERGYFROMTHEBATTERYTHROUGHTHEANDTHROUGHALLTHOSESBACKTOTHETERMINALWHICHISA

2024-10-27 18:21:29,412 (asr_inference:509) INFO: speech length: 114872
2024-10-27 18:21:33,580 (beam_search:428) INFO: decoder input length: 89
2024-10-27 18:21:33,581 (beam_search:429) INFO: max output length: 89
2024-10-27 18:21:33,581 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:33,725 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:33,725 (beam_search:476) INFO:  -1.20 * 1.0 =  -1.20 for ctc
2024-10-27 18:21:33,725 (beam_search:479) INFO: total log probability: -1.20
2024-10-27 18:21:33,725 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:21:33,725 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:33,725 (beam_search:483) INFO: best hypo: WHICHTHEBATTERYFROMBECAUSEUSEAOFENERGY

2024-10-27 18:21:33,728 (asr_inference:509) INFO: speech length: 152543
2024-10-27 18:21:39,168 (beam_search:428) INFO: decoder input length: 118
2024-10-27 18:21:39,168 (beam_search:429) INFO: max output length: 118
2024-10-27 18:21:39,168 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:39,495 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:39,496 (beam_search:476) INFO:  -2.17 * 1.0 =  -2.17 for ctc
2024-10-27 18:21:39,496 (beam_search:479) INFO: total log probability: -2.17
2024-10-27 18:21:39,496 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:21:39,496 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:39,496 (beam_search:483) INFO: best hypo: IJUSTTOLDYOUBUTTHETHESWITCHMAKESTHATTHEENERGYANDTHEISNOT

2024-10-27 18:21:39,498 (asr_inference:509) INFO: speech length: 263818
2024-10-27 18:21:50,507 (beam_search:428) INFO: decoder input length: 205
2024-10-27 18:21:50,507 (beam_search:429) INFO: max output length: 205
2024-10-27 18:21:50,507 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:51,326 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:51,326 (beam_search:476) INFO:  -7.01 * 1.0 =  -7.01 for ctc
2024-10-27 18:21:51,326 (beam_search:479) INFO: total log probability: -7.01
2024-10-27 18:21:51,326 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:21:51,326 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:51,326 (beam_search:483) INFO: best hypo: WHENYOUTHESWITCHITTHEBUTWHENYOU'NOTITYOUTOOFFTHECIRCUITBYTHESOTHET

2024-10-27 18:21:51,329 (asr_inference:509) INFO: speech length: 248778
2024-10-27 18:22:01,376 (beam_search:428) INFO: decoder input length: 193
2024-10-27 18:22:01,376 (beam_search:429) INFO: max output length: 193
2024-10-27 18:22:01,376 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:02,027 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:02,028 (beam_search:476) INFO:  -3.39 * 1.0 =  -3.39 for ctc
2024-10-27 18:22:02,028 (beam_search:479) INFO: total log probability: -3.39
2024-10-27 18:22:02,028 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:22:02,028 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:02,028 (beam_search:483) INFO: best hypo: THESWITCHSTHEBATTERYFROMANDITSTHEOFENERGYINGTHEANDITBACKON

2024-10-27 18:22:02,030 (asr_inference:509) INFO: speech length: 340017
2024-10-27 18:22:17,122 (beam_search:428) INFO: decoder input length: 265
2024-10-27 18:22:17,122 (beam_search:429) INFO: max output length: 265
2024-10-27 18:22:17,122 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:18,522 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:18,522 (beam_search:476) INFO:  -5.07 * 1.0 =  -5.07 for ctc
2024-10-27 18:22:18,522 (beam_search:479) INFO: total log probability: -5.07
2024-10-27 18:22:18,522 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:22:18,522 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:18,522 (beam_search:483) INFO: best hypo: WELLTHELIKEWHENTHESWITCHISTHESWITCHISTHEMAGNETICSANDPICKSUPWASHERSWHENYOUOPENTHESWITCHTHEMAGNETICANDTHEWASHERS

2024-10-27 18:22:18,525 (asr_inference:509) INFO: speech length: 144046
2024-10-27 18:22:23,788 (beam_search:428) INFO: decoder input length: 112
2024-10-27 18:22:23,788 (beam_search:429) INFO: max output length: 112
2024-10-27 18:22:23,788 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:23,971 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:23,971 (beam_search:476) INFO:  -2.20 * 1.0 =  -2.20 for ctc
2024-10-27 18:22:23,971 (beam_search:479) INFO: total log probability: -2.20
2024-10-27 18:22:23,971 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:22:23,971 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:23,971 (beam_search:483) INFO: best hypo: ELECTRICITYISFLOWINGTHROUGHTHATWHICHSAMAGNETIC

2024-10-27 18:22:23,973 (asr_inference:509) INFO: speech length: 254760
2024-10-27 18:22:34,621 (beam_search:428) INFO: decoder input length: 198
2024-10-27 18:22:34,621 (beam_search:429) INFO: max output length: 198
2024-10-27 18:22:34,621 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:35,320 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:35,320 (beam_search:476) INFO:  -3.69 * 1.0 =  -3.69 for ctc
2024-10-27 18:22:35,320 (beam_search:479) INFO: total log probability: -3.69
2024-10-27 18:22:35,320 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:22:35,320 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:35,320 (beam_search:483) INFO: best hypo: THEISATHATSTHEBATTERYFROMANDITMAKESTHEMAGNETINGUPSTUFFOROROFF

2024-10-27 18:22:35,323 (asr_inference:509) INFO: speech length: 62960
2024-10-27 18:22:37,716 (beam_search:428) INFO: decoder input length: 48
2024-10-27 18:22:37,716 (beam_search:429) INFO: max output length: 48
2024-10-27 18:22:37,716 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:37,753 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:37,753 (beam_search:476) INFO:  -1.54 * 1.0 =  -1.54 for ctc
2024-10-27 18:22:37,753 (beam_search:479) INFO: total log probability: -1.54
2024-10-27 18:22:37,753 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:22:37,753 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:37,753 (beam_search:483) INFO: best hypo: AREDON'T

2024-10-27 18:22:37,756 (asr_inference:509) INFO: speech length: 34969
2024-10-27 18:22:39,120 (beam_search:428) INFO: decoder input length: 26
2024-10-27 18:22:39,121 (beam_search:429) INFO: max output length: 26
2024-10-27 18:22:39,121 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:39,134 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:39,134 (beam_search:476) INFO:  -0.13 * 1.0 =  -0.13 for ctc
2024-10-27 18:22:39,135 (beam_search:479) INFO: total log probability: -0.13
2024-10-27 18:22:39,135 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:22:39,135 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:39,135 (beam_search:483) INFO: best hypo: ITTHE

2024-10-27 18:22:39,137 (asr_inference:509) INFO: speech length: 66247
2024-10-27 18:22:41,522 (beam_search:428) INFO: decoder input length: 51
2024-10-27 18:22:41,522 (beam_search:429) INFO: max output length: 51
2024-10-27 18:22:41,522 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:41,577 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:41,578 (beam_search:476) INFO:  -1.49 * 1.0 =  -1.49 for ctc
2024-10-27 18:22:41,578 (beam_search:479) INFO: total log probability: -1.49
2024-10-27 18:22:41,578 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:22:41,578 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:41,578 (beam_search:483) INFO: best hypo: ISFLOWINGTHROUGHTHETOIT

2024-10-27 18:22:41,580 (asr_inference:509) INFO: speech length: 228574
2024-10-27 18:22:50,971 (beam_search:428) INFO: decoder input length: 178
2024-10-27 18:22:50,971 (beam_search:429) INFO: max output length: 178
2024-10-27 18:22:50,971 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:51,479 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:51,479 (beam_search:476) INFO:  -3.27 * 1.0 =  -3.27 for ctc
2024-10-27 18:22:51,479 (beam_search:479) INFO: total log probability: -3.27
2024-10-27 18:22:51,479 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:22:51,479 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:51,479 (beam_search:483) INFO: best hypo: THEISNOTFLOWINGTHEREISNOMAGNETICBECAUSETHEREISNOINTHETHE

2024-10-27 18:22:51,482 (asr_inference:509) INFO: speech length: 108409
2024-10-27 18:22:55,474 (beam_search:428) INFO: decoder input length: 84
2024-10-27 18:22:55,474 (beam_search:429) INFO: max output length: 84
2024-10-27 18:22:55,474 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:55,619 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:55,620 (beam_search:476) INFO:  -1.44 * 1.0 =  -1.44 for ctc
2024-10-27 18:22:55,620 (beam_search:479) INFO: total log probability: -1.44
2024-10-27 18:22:55,620 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:22:55,620 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:55,620 (beam_search:483) INFO: best hypo: THEENERGYHASTOBEFLOWINGTHROUGHTHATTOTHE

2024-10-27 18:22:55,622 (asr_inference:509) INFO: speech length: 91719
2024-10-27 18:22:58,796 (beam_search:428) INFO: decoder input length: 71
2024-10-27 18:22:58,796 (beam_search:429) INFO: max output length: 71
2024-10-27 18:22:58,797 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:58,921 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:58,922 (beam_search:476) INFO:  -2.92 * 1.0 =  -2.92 for ctc
2024-10-27 18:22:58,922 (beam_search:479) INFO: total log probability: -2.92
2024-10-27 18:22:58,922 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:22:58,922 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:58,922 (beam_search:483) INFO: best hypo: TOBEFROMORIRONBECAUSETHOSEARETHEONLY

2024-10-27 18:22:58,924 (asr_inference:509) INFO: speech length: 88669
2024-10-27 18:23:02,050 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:23:02,050 (beam_search:429) INFO: max output length: 68
2024-10-27 18:23:02,050 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:02,152 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:02,152 (beam_search:476) INFO:  -3.09 * 1.0 =  -3.09 for ctc
2024-10-27 18:23:02,152 (beam_search:479) INFO: total log probability: -3.09
2024-10-27 18:23:02,152 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:23:02,152 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:02,152 (beam_search:483) INFO: best hypo: HAVETOBEDAROUNDORSTEELTOTHE

2024-10-27 18:23:02,154 (asr_inference:509) INFO: speech length: 120150
2024-10-27 18:23:06,570 (beam_search:428) INFO: decoder input length: 93
2024-10-27 18:23:06,571 (beam_search:429) INFO: max output length: 93
2024-10-27 18:23:06,571 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:06,716 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:06,716 (beam_search:476) INFO:  -0.58 * 1.0 =  -0.58 for ctc
2024-10-27 18:23:06,716 (beam_search:479) INFO: total log probability: -0.58
2024-10-27 18:23:06,716 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:23:06,716 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:06,716 (beam_search:483) INFO: best hypo: WE'VEBEENLEARNINGABOUTCIRCUITSANDENERGY

2024-10-27 18:23:06,719 (asr_inference:509) INFO: speech length: 94770
2024-10-27 18:23:10,166 (beam_search:428) INFO: decoder input length: 73
2024-10-27 18:23:10,166 (beam_search:429) INFO: max output length: 73
2024-10-27 18:23:10,166 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:10,317 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:10,317 (beam_search:476) INFO:  -0.87 * 1.0 =  -0.87 for ctc
2024-10-27 18:23:10,317 (beam_search:479) INFO: total log probability: -0.87
2024-10-27 18:23:10,317 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:23:10,317 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:10,317 (beam_search:483) INFO: best hypo: UMTHEREISALIGHTBULBAWIREANDADCELL

2024-10-27 18:23:10,320 (asr_inference:509) INFO: speech length: 61867
2024-10-27 18:23:12,569 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:23:12,569 (beam_search:429) INFO: max output length: 47
2024-10-27 18:23:12,569 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:12,600 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:12,600 (beam_search:476) INFO:  -0.84 * 1.0 =  -0.84 for ctc
2024-10-27 18:23:12,600 (beam_search:479) INFO: total log probability: -0.84
2024-10-27 18:23:12,601 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:23:12,601 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:12,601 (beam_search:483) INFO: best hypo: THESEAREWIRES

2024-10-27 18:23:12,603 (asr_inference:509) INFO: speech length: 106514
2024-10-27 18:23:16,380 (beam_search:428) INFO: decoder input length: 82
2024-10-27 18:23:16,380 (beam_search:429) INFO: max output length: 82
2024-10-27 18:23:16,380 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:16,570 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:16,570 (beam_search:476) INFO:  -3.44 * 1.0 =  -3.44 for ctc
2024-10-27 18:23:16,570 (beam_search:479) INFO: total log probability: -3.44
2024-10-27 18:23:16,570 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:23:16,570 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:16,571 (beam_search:483) INFO: best hypo: THEYTHEYHAVEDENERGYINTHEMANDTHEYTHELIGHTBULBLIGHT

2024-10-27 18:23:16,573 (asr_inference:509) INFO: speech length: 47775
2024-10-27 18:23:18,318 (beam_search:428) INFO: decoder input length: 36
2024-10-27 18:23:18,319 (beam_search:429) INFO: max output length: 36
2024-10-27 18:23:18,319 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:18,353 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:18,354 (beam_search:476) INFO:  -0.84 * 1.0 =  -0.84 for ctc
2024-10-27 18:23:18,354 (beam_search:479) INFO: total log probability: -0.84
2024-10-27 18:23:18,354 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:23:18,354 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:18,354 (beam_search:483) INFO: best hypo: THEYHAVEDENERGYIN

2024-10-27 18:23:18,356 (asr_inference:509) INFO: speech length: 34562
2024-10-27 18:23:19,754 (beam_search:428) INFO: decoder input length: 26
2024-10-27 18:23:19,754 (beam_search:429) INFO: max output length: 26
2024-10-27 18:23:19,754 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:19,773 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:19,774 (beam_search:476) INFO:  -0.10 * 1.0 =  -0.10 for ctc
2024-10-27 18:23:19,774 (beam_search:479) INFO: total log probability: -0.10
2024-10-27 18:23:19,774 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:23:19,774 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:19,774 (beam_search:483) INFO: best hypo: ITISA

2024-10-27 18:23:19,776 (asr_inference:509) INFO: speech length: 59190
2024-10-27 18:23:21,991 (beam_search:428) INFO: decoder input length: 45
2024-10-27 18:23:21,991 (beam_search:429) INFO: max output length: 45
2024-10-27 18:23:21,991 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:22,039 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:22,039 (beam_search:476) INFO:  -0.58 * 1.0 =  -0.58 for ctc
2024-10-27 18:23:22,039 (beam_search:479) INFO: total log probability: -0.58
2024-10-27 18:23:22,039 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:23:22,039 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:22,039 (beam_search:483) INFO: best hypo: ITENERGYTOLIGHTTHEBULB

2024-10-27 18:23:22,041 (asr_inference:509) INFO: speech length: 75741
2024-10-27 18:23:24,763 (beam_search:428) INFO: decoder input length: 58
2024-10-27 18:23:24,764 (beam_search:429) INFO: max output length: 58
2024-10-27 18:23:24,764 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:24,845 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:24,845 (beam_search:476) INFO:  -1.31 * 1.0 =  -1.31 for ctc
2024-10-27 18:23:24,845 (beam_search:479) INFO: total log probability: -1.31
2024-10-27 18:23:24,845 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:23:24,845 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:24,845 (beam_search:483) INFO: best hypo: ITHINKTHATTHEBULBWOULDNOTLIGHT

2024-10-27 18:23:24,848 (asr_inference:509) INFO: speech length: 69735
2024-10-27 18:23:27,352 (beam_search:428) INFO: decoder input length: 53
2024-10-27 18:23:27,352 (beam_search:429) INFO: max output length: 53
2024-10-27 18:23:27,352 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:27,402 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:27,402 (beam_search:476) INFO:  -0.44 * 1.0 =  -0.44 for ctc
2024-10-27 18:23:27,402 (beam_search:479) INFO: total log probability: -0.44
2024-10-27 18:23:27,402 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:23:27,402 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:27,402 (beam_search:483) INFO: best hypo: THELIGHTBULBTOTHE

2024-10-27 18:23:27,404 (asr_inference:509) INFO: speech length: 72767
2024-10-27 18:23:30,051 (beam_search:428) INFO: decoder input length: 56
2024-10-27 18:23:30,052 (beam_search:429) INFO: max output length: 56
2024-10-27 18:23:30,052 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:30,137 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:30,137 (beam_search:476) INFO:  -3.17 * 1.0 =  -3.17 for ctc
2024-10-27 18:23:30,137 (beam_search:479) INFO: total log probability: -3.17
2024-10-27 18:23:30,137 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:23:30,137 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:30,137 (beam_search:483) INFO: best hypo: THEWIRESAREAFROMTHETOTHEBULB

2024-10-27 18:23:30,140 (asr_inference:509) INFO: speech length: 66523
2024-10-27 18:23:32,667 (beam_search:428) INFO: decoder input length: 51
2024-10-27 18:23:32,667 (beam_search:429) INFO: max output length: 51
2024-10-27 18:23:32,667 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:32,706 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:32,706 (beam_search:476) INFO:  -1.04 * 1.0 =  -1.04 for ctc
2024-10-27 18:23:32,706 (beam_search:479) INFO: total log probability: -1.04
2024-10-27 18:23:32,706 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:23:32,706 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:32,706 (beam_search:483) INFO: best hypo: ITHATELECTRICITYIN

2024-10-27 18:23:32,709 (asr_inference:509) INFO: speech length: 57493
2024-10-27 18:23:34,865 (beam_search:428) INFO: decoder input length: 44
2024-10-27 18:23:34,865 (beam_search:429) INFO: max output length: 44
2024-10-27 18:23:34,865 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:34,887 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:34,887 (beam_search:476) INFO:  -0.94 * 1.0 =  -0.94 for ctc
2024-10-27 18:23:34,887 (beam_search:479) INFO: total log probability: -0.94
2024-10-27 18:23:34,887 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:23:34,887 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:34,887 (beam_search:483) INFO: best hypo: INA

2024-10-27 18:23:34,889 (asr_inference:509) INFO: speech length: 91190
2024-10-27 18:23:38,223 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:23:38,223 (beam_search:429) INFO: max output length: 70
2024-10-27 18:23:38,223 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:38,289 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:38,289 (beam_search:476) INFO:  -0.66 * 1.0 =  -0.66 for ctc
2024-10-27 18:23:38,289 (beam_search:479) INFO: total log probability: -0.66
2024-10-27 18:23:38,289 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:23:38,289 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:38,289 (beam_search:483) INFO: best hypo: THEYANDTHEYUSEELECTRICITY

2024-10-27 18:23:38,292 (asr_inference:509) INFO: speech length: 107729
2024-10-27 18:23:42,206 (beam_search:428) INFO: decoder input length: 83
2024-10-27 18:23:42,206 (beam_search:429) INFO: max output length: 83
2024-10-27 18:23:42,206 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:42,303 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:42,303 (beam_search:476) INFO:  -1.16 * 1.0 =  -1.16 for ctc
2024-10-27 18:23:42,303 (beam_search:479) INFO: total log probability: -1.16
2024-10-27 18:23:42,303 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:23:42,304 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:42,304 (beam_search:483) INFO: best hypo: SITSELECTRICITYANDLIGHT

2024-10-27 18:23:42,307 (asr_inference:509) INFO: speech length: 126897
2024-10-27 18:23:47,022 (beam_search:428) INFO: decoder input length: 98
2024-10-27 18:23:47,022 (beam_search:429) INFO: max output length: 98
2024-10-27 18:23:47,022 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:47,229 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:47,229 (beam_search:476) INFO:  -1.54 * 1.0 =  -1.54 for ctc
2024-10-27 18:23:47,229 (beam_search:479) INFO: total log probability: -1.54
2024-10-27 18:23:47,229 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:23:47,230 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:47,230 (beam_search:483) INFO: best hypo: THEANDITINTOTHELIGHTBULBTOMAKEITLIGHT

2024-10-27 18:23:47,232 (asr_inference:509) INFO: speech length: 261840
2024-10-27 18:23:57,914 (beam_search:428) INFO: decoder input length: 204
2024-10-27 18:23:57,914 (beam_search:429) INFO: max output length: 204
2024-10-27 18:23:57,914 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:59,000 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:59,000 (beam_search:476) INFO:  -3.76 * 1.0 =  -3.76 for ctc
2024-10-27 18:23:59,000 (beam_search:479) INFO: total log probability: -3.76
2024-10-27 18:23:59,000 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:23:59,000 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:59,000 (beam_search:483) INFO: best hypo: THEELECTRICITYISGOINGTHROUGHTHEWIRESTOTHELIGHTBULBANDBACKDOWNTOTHETOTHEANDITKEEPSGOINGITMAKESTHELIGHTBULBLIGHT

2024-10-27 18:23:59,002 (asr_inference:509) INFO: speech length: 249700
2024-10-27 18:24:09,064 (beam_search:428) INFO: decoder input length: 194
2024-10-27 18:24:09,064 (beam_search:429) INFO: max output length: 194
2024-10-27 18:24:09,064 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:10,268 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:10,268 (beam_search:476) INFO:  -4.15 * 1.0 =  -4.15 for ctc
2024-10-27 18:24:10,268 (beam_search:479) INFO: total log probability: -4.15
2024-10-27 18:24:10,268 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:24:10,268 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:10,268 (beam_search:483) INFO: best hypo: ONEISTHEPOSITIVESIDEANDONESIDEISTHENEGATIVESIDEANDTHEPOSITIVEHASTHEITANDITGOESINTOTHEELECTRICITYGOESINTOTHEPOSITIVESIDEANDOUTTHENEGATIVESIDE

2024-10-27 18:24:10,271 (asr_inference:509) INFO: speech length: 202458
2024-10-27 18:24:18,494 (beam_search:428) INFO: decoder input length: 157
2024-10-27 18:24:18,494 (beam_search:429) INFO: max output length: 157
2024-10-27 18:24:18,494 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:19,219 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:19,220 (beam_search:476) INFO:  -2.97 * 1.0 =  -2.97 for ctc
2024-10-27 18:24:19,220 (beam_search:479) INFO: total log probability: -2.97
2024-10-27 18:24:19,220 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:24:19,220 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:19,220 (beam_search:483) INFO: best hypo: ITGOESITGOESINTOTHEPOSITIVESIDEANDOUTTHROUGHTHENEGATIVESIDEANDTHENGOESAROUNDANDTHENBACKINTHROUGHTHEPOSITIVESIDE

2024-10-27 18:24:19,223 (asr_inference:509) INFO: speech length: 40975
2024-10-27 18:24:20,923 (beam_search:428) INFO: decoder input length: 31
2024-10-27 18:24:20,923 (beam_search:429) INFO: max output length: 31
2024-10-27 18:24:20,923 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:20,945 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:20,945 (beam_search:476) INFO:  -0.62 * 1.0 =  -0.62 for ctc
2024-10-27 18:24:20,945 (beam_search:479) INFO: total log probability: -0.62
2024-10-27 18:24:20,945 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:24:20,945 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:20,945 (beam_search:483) INFO: best hypo: SIDETHESIDE

2024-10-27 18:24:20,948 (asr_inference:509) INFO: speech length: 65727
2024-10-27 18:24:23,432 (beam_search:428) INFO: decoder input length: 50
2024-10-27 18:24:23,432 (beam_search:429) INFO: max output length: 50
2024-10-27 18:24:23,432 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:23,518 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:23,518 (beam_search:476) INFO:  -1.07 * 1.0 =  -1.07 for ctc
2024-10-27 18:24:23,518 (beam_search:479) INFO: total log probability: -1.07
2024-10-27 18:24:23,518 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:24:23,518 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:23,518 (beam_search:483) INFO: best hypo: ISTHEPOSITIVESIDEANDITHASAONIT

2024-10-27 18:24:23,521 (asr_inference:509) INFO: speech length: 131623
2024-10-27 18:24:28,371 (beam_search:428) INFO: decoder input length: 102
2024-10-27 18:24:28,371 (beam_search:429) INFO: max output length: 102
2024-10-27 18:24:28,371 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:28,564 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:28,564 (beam_search:476) INFO:  -2.02 * 1.0 =  -2.02 for ctc
2024-10-27 18:24:28,564 (beam_search:479) INFO: total log probability: -2.02
2024-10-27 18:24:28,564 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:24:28,565 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:28,565 (beam_search:483) INFO: best hypo: THEELECTRICITYTOWARDSTHEANDGOESINTOTHEDCELL

2024-10-27 18:24:28,567 (asr_inference:509) INFO: speech length: 39215
2024-10-27 18:24:30,058 (beam_search:428) INFO: decoder input length: 30
2024-10-27 18:24:30,058 (beam_search:429) INFO: max output length: 30
2024-10-27 18:24:30,058 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:30,070 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:30,070 (beam_search:476) INFO:  -1.10 * 1.0 =  -1.10 for ctc
2024-10-27 18:24:30,070 (beam_search:479) INFO: total log probability: -1.10
2024-10-27 18:24:30,070 (beam_search:480) INFO: normalized log probability: -0.37
2024-10-27 18:24:30,070 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:30,070 (beam_search:483) INFO: best hypo: ELECTRICITY

2024-10-27 18:24:30,072 (asr_inference:509) INFO: speech length: 60942
2024-10-27 18:24:32,378 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:24:32,378 (beam_search:429) INFO: max output length: 47
2024-10-27 18:24:32,378 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:32,430 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:32,430 (beam_search:476) INFO:  -1.31 * 1.0 =  -1.31 for ctc
2024-10-27 18:24:32,430 (beam_search:479) INFO: total log probability: -1.31
2024-10-27 18:24:32,430 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:24:32,430 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:32,430 (beam_search:483) INFO: best hypo: THEELECTRICITYISTHEPOSITIVESIDE

2024-10-27 18:24:32,433 (asr_inference:509) INFO: speech length: 207738
2024-10-27 18:24:41,008 (beam_search:428) INFO: decoder input length: 161
2024-10-27 18:24:41,008 (beam_search:429) INFO: max output length: 161
2024-10-27 18:24:41,008 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:41,589 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:41,589 (beam_search:476) INFO:  -3.02 * 1.0 =  -3.02 for ctc
2024-10-27 18:24:41,589 (beam_search:479) INFO: total log probability: -3.02
2024-10-27 18:24:41,590 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:24:41,590 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:41,590 (beam_search:483) INFO: best hypo: ITHATITAROUNDANDTHEELECTRICITYISSTILLFLOWINGINTOTHEPOSITIVESIDEANDOUTTHENEGATIVESIDE

2024-10-27 18:24:41,593 (asr_inference:509) INFO: speech length: 105899
2024-10-27 18:24:45,385 (beam_search:428) INFO: decoder input length: 82
2024-10-27 18:24:45,385 (beam_search:429) INFO: max output length: 82
2024-10-27 18:24:45,385 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:45,465 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:45,465 (beam_search:476) INFO:  -1.19 * 1.0 =  -1.19 for ctc
2024-10-27 18:24:45,465 (beam_search:479) INFO: total log probability: -1.19
2024-10-27 18:24:45,465 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:24:45,465 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:45,465 (beam_search:483) INFO: best hypo: THETHEGOESTHEOTHER

2024-10-27 18:24:45,468 (asr_inference:509) INFO: speech length: 100672
2024-10-27 18:24:49,104 (beam_search:428) INFO: decoder input length: 78
2024-10-27 18:24:49,105 (beam_search:429) INFO: max output length: 78
2024-10-27 18:24:49,105 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:49,265 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:49,265 (beam_search:476) INFO:  -1.68 * 1.0 =  -1.68 for ctc
2024-10-27 18:24:49,265 (beam_search:479) INFO: total log probability: -1.68
2024-10-27 18:24:49,265 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:24:49,265 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:49,265 (beam_search:483) INFO: best hypo: THEOFTHEFLOWTURNSJUSTASTHEDCELL

2024-10-27 18:24:49,267 (asr_inference:509) INFO: speech length: 114634
2024-10-27 18:24:53,386 (beam_search:428) INFO: decoder input length: 89
2024-10-27 18:24:53,386 (beam_search:429) INFO: max output length: 89
2024-10-27 18:24:53,386 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:53,537 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:53,537 (beam_search:476) INFO:  -2.65 * 1.0 =  -2.65 for ctc
2024-10-27 18:24:53,537 (beam_search:479) INFO: total log probability: -2.65
2024-10-27 18:24:53,537 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:24:53,537 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:53,537 (beam_search:483) INFO: best hypo: ISFLOWINGFROMTHENEGATIVESIDEOFTHED

2024-10-27 18:24:53,539 (asr_inference:509) INFO: speech length: 254768
2024-10-27 18:25:03,863 (beam_search:428) INFO: decoder input length: 198
2024-10-27 18:25:03,864 (beam_search:429) INFO: max output length: 198
2024-10-27 18:25:03,864 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:04,798 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:04,799 (beam_search:476) INFO:  -7.63 * 1.0 =  -7.63 for ctc
2024-10-27 18:25:04,799 (beam_search:479) INFO: total log probability: -7.63
2024-10-27 18:25:04,799 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:25:04,799 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:04,799 (beam_search:483) INFO: best hypo: THEPOSITIVESIDETHETHATSTHEPOSITIVEISALSOTHEBULBANDTHEWIRETHATISTOUCHINGTHENEGATIVESIDEOFTHEDCELLTHE

2024-10-27 18:25:04,801 (asr_inference:509) INFO: speech length: 328155
2024-10-27 18:25:19,024 (beam_search:428) INFO: decoder input length: 255
2024-10-27 18:25:19,025 (beam_search:429) INFO: max output length: 255
2024-10-27 18:25:19,025 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:19,746 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:19,747 (beam_search:476) INFO:  -6.87 * 1.0 =  -6.87 for ctc
2024-10-27 18:25:19,747 (beam_search:479) INFO: total log probability: -6.87
2024-10-27 18:25:19,747 (beam_search:480) INFO: normalized log probability: -0.38
2024-10-27 18:25:19,747 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:19,747 (beam_search:483) INFO: best hypo: THETHETHATTHEPOSITIVEISTHEANDTHETHAT'STHESIDEISTHE

2024-10-27 18:25:19,749 (asr_inference:509) INFO: speech length: 328886
2024-10-27 18:25:34,208 (beam_search:428) INFO: decoder input length: 256
2024-10-27 18:25:34,208 (beam_search:429) INFO: max output length: 256
2024-10-27 18:25:34,208 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:35,365 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:35,365 (beam_search:476) INFO:  -7.72 * 1.0 =  -7.72 for ctc
2024-10-27 18:25:35,365 (beam_search:479) INFO: total log probability: -7.72
2024-10-27 18:25:35,365 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:25:35,365 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:35,365 (beam_search:483) INFO: best hypo: ITHASTOBEACOMPLETEONEHASTOTOUCHTHESIDEANDONEHASTHEPOSITIVEANDONETOTOUCHTHEANDTHEOTHERTHE

2024-10-27 18:25:35,368 (asr_inference:509) INFO: speech length: 53467
2024-10-27 18:25:37,340 (beam_search:428) INFO: decoder input length: 41
2024-10-27 18:25:37,340 (beam_search:429) INFO: max output length: 41
2024-10-27 18:25:37,340 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:37,368 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:37,369 (beam_search:476) INFO:  -1.35 * 1.0 =  -1.35 for ctc
2024-10-27 18:25:37,369 (beam_search:479) INFO: total log probability: -1.35
2024-10-27 18:25:37,369 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:25:37,369 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:37,369 (beam_search:483) INFO: best hypo: THEELECTRICITYWILL

2024-10-27 18:25:37,371 (asr_inference:509) INFO: speech length: 379041
2024-10-27 18:25:54,297 (beam_search:428) INFO: decoder input length: 295
2024-10-27 18:25:54,297 (beam_search:429) INFO: max output length: 295
2024-10-27 18:25:54,297 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:56,488 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:56,488 (beam_search:476) INFO:  -8.78 * 1.0 =  -8.78 for ctc
2024-10-27 18:25:56,488 (beam_search:479) INFO: total log probability: -8.78
2024-10-27 18:25:56,489 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:25:56,489 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:56,489 (beam_search:483) INFO: best hypo: ITHATIFONEONEWIREISONTHEBULBONTHEBASETHENTHELIGHTBULBLIGHTSBUTIFBOTHIFBOTHOFTHEWIRESAREONTHEBULBTHENTHELIGHTBULBWON'TLIGHT

2024-10-27 18:25:56,491 (asr_inference:509) INFO: speech length: 84462
2024-10-27 18:25:59,600 (beam_search:428) INFO: decoder input length: 65
2024-10-27 18:25:59,601 (beam_search:429) INFO: max output length: 65
2024-10-27 18:25:59,601 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:59,757 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:59,757 (beam_search:476) INFO:  -1.98 * 1.0 =  -1.98 for ctc
2024-10-27 18:25:59,757 (beam_search:479) INFO: total log probability: -1.98
2024-10-27 18:25:59,757 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:25:59,757 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:59,757 (beam_search:483) INFO: best hypo: WE'VEBEENDOINGWE'VEBEENLEARNINGABOUTCIRCUITSANDENERGY

2024-10-27 18:25:59,760 (asr_inference:509) INFO: speech length: 79016
2024-10-27 18:26:02,616 (beam_search:428) INFO: decoder input length: 61
2024-10-27 18:26:02,616 (beam_search:429) INFO: max output length: 61
2024-10-27 18:26:02,616 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:02,673 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:02,673 (beam_search:476) INFO:  -1.68 * 1.0 =  -1.68 for ctc
2024-10-27 18:26:02,673 (beam_search:479) INFO: total log probability: -1.68
2024-10-27 18:26:02,673 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:26:02,673 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:02,673 (beam_search:483) INFO: best hypo: THEISINGTHEMOTOR

2024-10-27 18:26:02,675 (asr_inference:509) INFO: speech length: 104096
2024-10-27 18:26:06,361 (beam_search:428) INFO: decoder input length: 80
2024-10-27 18:26:06,361 (beam_search:429) INFO: max output length: 80
2024-10-27 18:26:06,361 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:06,605 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:06,606 (beam_search:476) INFO:  -2.22 * 1.0 =  -2.22 for ctc
2024-10-27 18:26:06,606 (beam_search:479) INFO: total log probability: -2.22
2024-10-27 18:26:06,606 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:26:06,606 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:06,606 (beam_search:483) INFO: best hypo: ITHATTHEISMOVINGANDIT'SIMPORTANTBECAUSETHENIKNOWTHATTHEMOTORIS

2024-10-27 18:26:06,608 (asr_inference:509) INFO: speech length: 73283
2024-10-27 18:26:09,321 (beam_search:428) INFO: decoder input length: 56
2024-10-27 18:26:09,321 (beam_search:429) INFO: max output length: 56
2024-10-27 18:26:09,321 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:09,364 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:09,365 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 18:26:09,365 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 18:26:09,365 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:26:09,365 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:09,365 (beam_search:483) INFO: best hypo: ITMEANSTHATTHE

2024-10-27 18:26:09,367 (asr_inference:509) INFO: speech length: 133702
2024-10-27 18:26:14,251 (beam_search:428) INFO: decoder input length: 103
2024-10-27 18:26:14,251 (beam_search:429) INFO: max output length: 103
2024-10-27 18:26:14,251 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:14,419 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:14,419 (beam_search:476) INFO:  -0.29 * 1.0 =  -0.29 for ctc
2024-10-27 18:26:14,419 (beam_search:479) INFO: total log probability: -0.29
2024-10-27 18:26:14,419 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:26:14,419 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:14,420 (beam_search:483) INFO: best hypo: THETHEDCELLBATTERYANDTHEWIRES

2024-10-27 18:26:14,423 (asr_inference:509) INFO: speech length: 99681
2024-10-27 18:26:18,222 (beam_search:428) INFO: decoder input length: 77
2024-10-27 18:26:18,222 (beam_search:429) INFO: max output length: 77
2024-10-27 18:26:18,222 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:18,331 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:18,331 (beam_search:476) INFO:  -1.02 * 1.0 =  -1.02 for ctc
2024-10-27 18:26:18,331 (beam_search:479) INFO: total log probability: -1.02
2024-10-27 18:26:18,331 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:26:18,331 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:18,331 (beam_search:483) INFO: best hypo: THATMEANSTHATTHESENERGYANDIT

2024-10-27 18:26:18,333 (asr_inference:509) INFO: speech length: 90106
2024-10-27 18:26:21,586 (beam_search:428) INFO: decoder input length: 69
2024-10-27 18:26:21,586 (beam_search:429) INFO: max output length: 69
2024-10-27 18:26:21,587 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:21,640 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:21,641 (beam_search:476) INFO:  -1.28 * 1.0 =  -1.28 for ctc
2024-10-27 18:26:21,641 (beam_search:479) INFO: total log probability: -1.28
2024-10-27 18:26:21,641 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:26:21,641 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:21,641 (beam_search:483) INFO: best hypo: THEENERGYANDELECTRICITY

2024-10-27 18:26:21,643 (asr_inference:509) INFO: speech length: 209471
2024-10-27 18:26:29,857 (beam_search:428) INFO: decoder input length: 163
2024-10-27 18:26:29,857 (beam_search:429) INFO: max output length: 163
2024-10-27 18:26:29,857 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:30,265 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:30,265 (beam_search:476) INFO:  -4.67 * 1.0 =  -4.67 for ctc
2024-10-27 18:26:30,265 (beam_search:479) INFO: total log probability: -4.67
2024-10-27 18:26:30,265 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 18:26:30,265 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:30,265 (beam_search:483) INFO: best hypo: THEMOTORHAVEABATTERYANDITHAVEABATTERYANDTWOWIRES

2024-10-27 18:26:30,267 (asr_inference:509) INFO: speech length: 217832
2024-10-27 18:26:38,672 (beam_search:428) INFO: decoder input length: 169
2024-10-27 18:26:38,672 (beam_search:429) INFO: max output length: 169
2024-10-27 18:26:38,672 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:39,055 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:39,055 (beam_search:476) INFO:  -1.48 * 1.0 =  -1.48 for ctc
2024-10-27 18:26:39,055 (beam_search:479) INFO: total log probability: -1.48
2024-10-27 18:26:39,055 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:26:39,055 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:39,055 (beam_search:483) INFO: best hypo: ITHINKTHATTHEDITHINKTHATTHEENERGYISIS

2024-10-27 18:26:39,058 (asr_inference:509) INFO: speech length: 277823
2024-10-27 18:26:50,780 (beam_search:428) INFO: decoder input length: 216
2024-10-27 18:26:50,780 (beam_search:429) INFO: max output length: 216
2024-10-27 18:26:50,780 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:51,351 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:51,351 (beam_search:476) INFO:  -1.27 * 1.0 =  -1.27 for ctc
2024-10-27 18:26:51,351 (beam_search:479) INFO: total log probability: -1.27
2024-10-27 18:26:51,351 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:26:51,351 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:51,351 (beam_search:483) INFO: best hypo: THETHEBATTERYENERGYTOTHEMOTORANDTHENANDTHENENERGYGOESBACK

2024-10-27 18:26:51,354 (asr_inference:509) INFO: speech length: 342604
2024-10-27 18:27:06,322 (beam_search:428) INFO: decoder input length: 267
2024-10-27 18:27:06,322 (beam_search:429) INFO: max output length: 267
2024-10-27 18:27:06,322 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:07,987 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:07,987 (beam_search:476) INFO:  -3.86 * 1.0 =  -3.86 for ctc
2024-10-27 18:27:07,987 (beam_search:479) INFO: total log probability: -3.86
2024-10-27 18:27:07,987 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:27:07,987 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:07,987 (beam_search:483) INFO: best hypo: THEREISTHETHEMOTORANDTHEBATTERYCONNECTEDTOASWITCHANDWHENTHESWITCHISUPTHEMOTORDOESNOTRUNBUTIFTHEISDOWNTHENTHEMOTORDOES

2024-10-27 18:27:07,990 (asr_inference:509) INFO: speech length: 118830
2024-10-27 18:27:12,378 (beam_search:428) INFO: decoder input length: 92
2024-10-27 18:27:12,379 (beam_search:429) INFO: max output length: 92
2024-10-27 18:27:12,379 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:12,484 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:12,484 (beam_search:476) INFO:  -1.26 * 1.0 =  -1.26 for ctc
2024-10-27 18:27:12,484 (beam_search:479) INFO: total log probability: -1.26
2024-10-27 18:27:12,484 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:27:12,484 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:12,484 (beam_search:483) INFO: best hypo: THEISBECAUSETHEISDOWN

2024-10-27 18:27:12,486 (asr_inference:509) INFO: speech length: 49001
2024-10-27 18:27:14,320 (beam_search:428) INFO: decoder input length: 37
2024-10-27 18:27:14,320 (beam_search:429) INFO: max output length: 37
2024-10-27 18:27:14,320 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:14,355 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:14,355 (beam_search:476) INFO:  -0.01 * 1.0 =  -0.01 for ctc
2024-10-27 18:27:14,355 (beam_search:479) INFO: total log probability: -0.01
2024-10-27 18:27:14,355 (beam_search:480) INFO: normalized log probability: -0.00
2024-10-27 18:27:14,355 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:14,355 (beam_search:483) INFO: best hypo: IDON'TKNOW

2024-10-27 18:27:14,358 (asr_inference:509) INFO: speech length: 98813
2024-10-27 18:27:18,063 (beam_search:428) INFO: decoder input length: 76
2024-10-27 18:27:18,064 (beam_search:429) INFO: max output length: 76
2024-10-27 18:27:18,064 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:18,134 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:18,135 (beam_search:476) INFO:  -0.32 * 1.0 =  -0.32 for ctc
2024-10-27 18:27:18,135 (beam_search:479) INFO: total log probability: -0.32
2024-10-27 18:27:18,135 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:27:18,135 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:18,135 (beam_search:483) INFO: best hypo: INTHISISUPAND

2024-10-27 18:27:18,138 (asr_inference:509) INFO: speech length: 125750
2024-10-27 18:27:22,829 (beam_search:428) INFO: decoder input length: 97
2024-10-27 18:27:22,829 (beam_search:429) INFO: max output length: 97
2024-10-27 18:27:22,829 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:22,996 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:22,996 (beam_search:476) INFO:  -1.95 * 1.0 =  -1.95 for ctc
2024-10-27 18:27:22,996 (beam_search:479) INFO: total log probability: -1.95
2024-10-27 18:27:22,996 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:27:22,996 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:22,996 (beam_search:483) INFO: best hypo: THETHESWITCHGOESDOWNANDTHEMOTORSTARTS

2024-10-27 18:27:22,998 (asr_inference:509) INFO: speech length: 222181
2024-10-27 18:27:31,777 (beam_search:428) INFO: decoder input length: 173
2024-10-27 18:27:31,777 (beam_search:429) INFO: max output length: 173
2024-10-27 18:27:31,777 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:32,251 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:32,251 (beam_search:476) INFO:  -3.19 * 1.0 =  -3.19 for ctc
2024-10-27 18:27:32,251 (beam_search:479) INFO: total log probability: -3.19
2024-10-27 18:27:32,251 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:27:32,251 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:32,251 (beam_search:483) INFO: best hypo: THECIRCUITINTHISISISOPENBECAUSETHENOTISNOTNOTISCONNECTED

2024-10-27 18:27:32,253 (asr_inference:509) INFO: speech length: 151551
2024-10-27 18:27:37,882 (beam_search:428) INFO: decoder input length: 117
2024-10-27 18:27:37,882 (beam_search:429) INFO: max output length: 117
2024-10-27 18:27:37,882 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:38,079 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:38,079 (beam_search:476) INFO:  -1.22 * 1.0 =  -1.22 for ctc
2024-10-27 18:27:38,079 (beam_search:479) INFO: total log probability: -1.22
2024-10-27 18:27:38,079 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:27:38,079 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:38,079 (beam_search:483) INFO: best hypo: ITMEANSTHATTHETHATTHEENERGYISNOT

2024-10-27 18:27:38,082 (asr_inference:509) INFO: speech length: 94951
2024-10-27 18:27:41,707 (beam_search:428) INFO: decoder input length: 73
2024-10-27 18:27:41,707 (beam_search:429) INFO: max output length: 73
2024-10-27 18:27:41,707 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:41,813 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:41,813 (beam_search:476) INFO:  -1.65 * 1.0 =  -1.65 for ctc
2024-10-27 18:27:41,813 (beam_search:479) INFO: total log probability: -1.65
2024-10-27 18:27:41,813 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:27:41,813 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:41,813 (beam_search:483) INFO: best hypo: ITHATWHENTHESWITCHISTHERUNNING

2024-10-27 18:27:41,815 (asr_inference:509) INFO: speech length: 129590
2024-10-27 18:27:46,659 (beam_search:428) INFO: decoder input length: 100
2024-10-27 18:27:46,659 (beam_search:429) INFO: max output length: 100
2024-10-27 18:27:46,659 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:46,864 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:46,865 (beam_search:476) INFO:  -3.21 * 1.0 =  -3.21 for ctc
2024-10-27 18:27:46,865 (beam_search:479) INFO: total log probability: -3.21
2024-10-27 18:27:46,865 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:27:46,865 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:46,865 (beam_search:483) INFO: best hypo: ITHINKTHEISBECAUSEITISACIRCUITANDENERGY

2024-10-27 18:27:46,868 (asr_inference:509) INFO: speech length: 80370
2024-10-27 18:27:49,720 (beam_search:428) INFO: decoder input length: 62
2024-10-27 18:27:49,720 (beam_search:429) INFO: max output length: 62
2024-10-27 18:27:49,720 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:49,788 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:49,788 (beam_search:476) INFO:  -1.26 * 1.0 =  -1.26 for ctc
2024-10-27 18:27:49,788 (beam_search:479) INFO: total log probability: -1.26
2024-10-27 18:27:49,788 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:27:49,788 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:49,788 (beam_search:483) INFO: best hypo: THEWHENTHESWITCHISOPEN

2024-10-27 18:27:49,791 (asr_inference:509) INFO: speech length: 168075
2024-10-27 18:27:56,457 (beam_search:428) INFO: decoder input length: 130
2024-10-27 18:27:56,457 (beam_search:429) INFO: max output length: 130
2024-10-27 18:27:56,457 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:56,839 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:56,839 (beam_search:476) INFO:  -1.68 * 1.0 =  -1.68 for ctc
2024-10-27 18:27:56,839 (beam_search:479) INFO: total log probability: -1.68
2024-10-27 18:27:56,839 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:27:56,839 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:56,839 (beam_search:483) INFO: best hypo: ITHINKTHATTHETHESWITCHHASTOBESOTHATTHETHETHEMOTORCANRUN

2024-10-27 18:27:56,842 (asr_inference:509) INFO: speech length: 66759
2024-10-27 18:27:59,293 (beam_search:428) INFO: decoder input length: 51
2024-10-27 18:27:59,293 (beam_search:429) INFO: max output length: 51
2024-10-27 18:27:59,293 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:59,372 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:59,373 (beam_search:476) INFO:  -1.69 * 1.0 =  -1.69 for ctc
2024-10-27 18:27:59,373 (beam_search:479) INFO: total log probability: -1.69
2024-10-27 18:27:59,373 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:27:59,373 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:59,373 (beam_search:483) INFO: best hypo: ITHATTHEMOTORRUNNINGWHENTHESWITCHIS

2024-10-27 18:27:59,375 (asr_inference:509) INFO: speech length: 38192
2024-10-27 18:28:00,910 (beam_search:428) INFO: decoder input length: 29
2024-10-27 18:28:00,910 (beam_search:429) INFO: max output length: 29
2024-10-27 18:28:00,910 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:00,931 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:00,931 (beam_search:476) INFO:  -0.57 * 1.0 =  -0.57 for ctc
2024-10-27 18:28:00,931 (beam_search:479) INFO: total log probability: -0.57
2024-10-27 18:28:00,931 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:28:00,931 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:00,931 (beam_search:483) INFO: best hypo: THESWITCHIS

2024-10-27 18:28:00,934 (asr_inference:509) INFO: speech length: 88558
2024-10-27 18:28:04,030 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:28:04,030 (beam_search:429) INFO: max output length: 68
2024-10-27 18:28:04,030 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:04,084 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:04,084 (beam_search:476) INFO:  -1.04 * 1.0 =  -1.04 for ctc
2024-10-27 18:28:04,084 (beam_search:479) INFO: total log probability: -1.04
2024-10-27 18:28:04,084 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:28:04,085 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:04,085 (beam_search:483) INFO: best hypo: CIRCUITHASTOBE

2024-10-27 18:28:04,087 (asr_inference:509) INFO: speech length: 149223
2024-10-27 18:28:09,676 (beam_search:428) INFO: decoder input length: 116
2024-10-27 18:28:09,676 (beam_search:429) INFO: max output length: 116
2024-10-27 18:28:09,676 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:10,006 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:10,007 (beam_search:476) INFO:  -2.69 * 1.0 =  -2.69 for ctc
2024-10-27 18:28:10,007 (beam_search:479) INFO: total log probability: -2.69
2024-10-27 18:28:10,007 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:28:10,007 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:10,007 (beam_search:483) INFO: best hypo: WHENTHESWITCHISOPENTHETHEELECTRICITYNOTFLOWSOTHEMOTORDOESNOTRUN

2024-10-27 18:28:10,009 (asr_inference:509) INFO: speech length: 221924
2024-10-27 18:28:19,072 (beam_search:428) INFO: decoder input length: 172
2024-10-27 18:28:19,072 (beam_search:429) INFO: max output length: 172
2024-10-27 18:28:19,072 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:19,680 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:19,680 (beam_search:476) INFO:  -3.43 * 1.0 =  -3.43 for ctc
2024-10-27 18:28:19,680 (beam_search:479) INFO: total log probability: -3.43
2024-10-27 18:28:19,680 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:28:19,680 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:19,680 (beam_search:483) INFO: best hypo: THEYGETTHESANDTHENTHEYTHEYTHEYISTHEENERGYANDTHENITSITTOTHE

2024-10-27 18:28:19,683 (asr_inference:509) INFO: speech length: 196482
2024-10-27 18:28:27,434 (beam_search:428) INFO: decoder input length: 153
2024-10-27 18:28:27,434 (beam_search:429) INFO: max output length: 153
2024-10-27 18:28:27,434 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:28,005 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:28,005 (beam_search:476) INFO:  -1.72 * 1.0 =  -1.72 for ctc
2024-10-27 18:28:28,006 (beam_search:479) INFO: total log probability: -1.72
2024-10-27 18:28:28,006 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:28:28,006 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:28,006 (beam_search:483) INFO: best hypo: THESOLARPANELLIGHTSOANDTHELIGHTISTHEENERGYSOTHENITITTOTHEANDTHEMOTOR

2024-10-27 18:28:28,008 (asr_inference:509) INFO: speech length: 228533
2024-10-27 18:28:37,288 (beam_search:428) INFO: decoder input length: 178
2024-10-27 18:28:37,288 (beam_search:429) INFO: max output length: 178
2024-10-27 18:28:37,288 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:37,990 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:37,990 (beam_search:476) INFO:  -1.14 * 1.0 =  -1.14 for ctc
2024-10-27 18:28:37,990 (beam_search:479) INFO: total log probability: -1.14
2024-10-27 18:28:37,990 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:28:37,990 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:37,990 (beam_search:483) INFO: best hypo: THESOLARCELLHASTOBECONNECTEDTOTHEANDTHESOLARCELLHASTOBEHASTOHAVELIGHT

2024-10-27 18:28:37,992 (asr_inference:509) INFO: speech length: 255089
2024-10-27 18:28:48,953 (beam_search:428) INFO: decoder input length: 198
2024-10-27 18:28:48,953 (beam_search:429) INFO: max output length: 198
2024-10-27 18:28:48,953 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:49,786 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:49,786 (beam_search:476) INFO:  -3.07 * 1.0 =  -3.07 for ctc
2024-10-27 18:28:49,786 (beam_search:479) INFO: total log probability: -3.07
2024-10-27 18:28:49,786 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:28:49,786 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:49,786 (beam_search:483) INFO: best hypo: THESOLARCELLISTHESUNSENERGYANDTHENITGETSTOANDITINTOTHEMOTORSOTHEMOTOR

2024-10-27 18:28:49,789 (asr_inference:509) INFO: speech length: 93964
2024-10-27 18:28:53,123 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:28:53,123 (beam_search:429) INFO: max output length: 72
2024-10-27 18:28:53,123 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:53,226 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:53,226 (beam_search:476) INFO:  -0.33 * 1.0 =  -0.33 for ctc
2024-10-27 18:28:53,226 (beam_search:479) INFO: total log probability: -0.33
2024-10-27 18:28:53,226 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:28:53,226 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:53,226 (beam_search:483) INFO: best hypo: ITHINKTHATTHEIDON'T

2024-10-27 18:28:53,230 (asr_inference:509) INFO: speech length: 188916
2024-10-27 18:29:00,326 (beam_search:428) INFO: decoder input length: 147
2024-10-27 18:29:00,327 (beam_search:429) INFO: max output length: 147
2024-10-27 18:29:00,327 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:00,991 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:00,992 (beam_search:476) INFO:  -2.38 * 1.0 =  -2.38 for ctc
2024-10-27 18:29:00,992 (beam_search:479) INFO: total log probability: -2.38
2024-10-27 18:29:00,992 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:29:00,992 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:00,992 (beam_search:483) INFO: best hypo: ITHINKTHATTHESOLARCELLWILLNOTGETENERGYBECAUSETHEWILLNOTGETTOTHESOLARCELLBECAUSETHEAREINGIT

2024-10-27 18:29:00,994 (asr_inference:509) INFO: speech length: 83457
2024-10-27 18:29:03,996 (beam_search:428) INFO: decoder input length: 64
2024-10-27 18:29:03,996 (beam_search:429) INFO: max output length: 64
2024-10-27 18:29:03,996 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:04,142 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:04,142 (beam_search:476) INFO:  -3.10 * 1.0 =  -3.10 for ctc
2024-10-27 18:29:04,142 (beam_search:479) INFO: total log probability: -3.10
2024-10-27 18:29:04,142 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:29:04,142 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:04,142 (beam_search:483) INFO: best hypo: ITHINKTHELARCELLWILLNOTBETOPRODUCEELECTRICITYONTHEDAY

2024-10-27 18:29:04,144 (asr_inference:509) INFO: speech length: 159461
2024-10-27 18:29:10,032 (beam_search:428) INFO: decoder input length: 124
2024-10-27 18:29:10,032 (beam_search:429) INFO: max output length: 124
2024-10-27 18:29:10,032 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:10,319 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:10,319 (beam_search:476) INFO:  -2.83 * 1.0 =  -2.83 for ctc
2024-10-27 18:29:10,319 (beam_search:479) INFO: total log probability: -2.83
2024-10-27 18:29:10,319 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:29:10,319 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:10,319 (beam_search:483) INFO: best hypo: WHENICLICKONTHETHETHETHEMOTORBUTONITAGAIN

2024-10-27 18:29:10,321 (asr_inference:509) INFO: speech length: 83258
2024-10-27 18:29:13,422 (beam_search:428) INFO: decoder input length: 64
2024-10-27 18:29:13,423 (beam_search:429) INFO: max output length: 64
2024-10-27 18:29:13,423 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:13,514 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:13,514 (beam_search:476) INFO:  -2.42 * 1.0 =  -2.42 for ctc
2024-10-27 18:29:13,514 (beam_search:479) INFO: total log probability: -2.42
2024-10-27 18:29:13,514 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:29:13,514 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:13,514 (beam_search:483) INFO: best hypo: WHENTHEISOUTTHETHEMOTORSTARTS

2024-10-27 18:29:13,516 (asr_inference:509) INFO: speech length: 158774
2024-10-27 18:29:19,432 (beam_search:428) INFO: decoder input length: 123
2024-10-27 18:29:19,433 (beam_search:429) INFO: max output length: 123
2024-10-27 18:29:19,433 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:19,838 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:19,838 (beam_search:476) INFO:  -3.62 * 1.0 =  -3.62 for ctc
2024-10-27 18:29:19,838 (beam_search:479) INFO: total log probability: -3.62
2024-10-27 18:29:19,838 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:29:19,838 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:19,838 (beam_search:483) INFO: best hypo: ASOLARCELLDOESNOTGETENERGYFROMTHEBECAUSEIT'SEDSOTHEMOTORWILLNOT

2024-10-27 18:29:19,840 (asr_inference:509) INFO: speech length: 65559
2024-10-27 18:29:22,265 (beam_search:428) INFO: decoder input length: 50
2024-10-27 18:29:22,265 (beam_search:429) INFO: max output length: 50
2024-10-27 18:29:22,265 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:22,361 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:22,361 (beam_search:476) INFO:  -1.58 * 1.0 =  -1.58 for ctc
2024-10-27 18:29:22,361 (beam_search:479) INFO: total log probability: -1.58
2024-10-27 18:29:22,361 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:29:22,361 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:22,361 (beam_search:483) INFO: best hypo: ITHATTHEMOTORSTARTSRUNNINGWHENTHESUNISOUT

2024-10-27 18:29:22,364 (asr_inference:509) INFO: speech length: 43129
2024-10-27 18:29:24,072 (beam_search:428) INFO: decoder input length: 33
2024-10-27 18:29:24,072 (beam_search:429) INFO: max output length: 33
2024-10-27 18:29:24,072 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:24,099 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:24,099 (beam_search:476) INFO:  -0.46 * 1.0 =  -0.46 for ctc
2024-10-27 18:29:24,099 (beam_search:479) INFO: total log probability: -0.46
2024-10-27 18:29:24,099 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:29:24,099 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:24,099 (beam_search:483) INFO: best hypo: THATMAKESTHEMOTOR

2024-10-27 18:29:24,101 (asr_inference:509) INFO: speech length: 410980
2024-10-27 18:29:43,351 (beam_search:428) INFO: decoder input length: 320
2024-10-27 18:29:43,351 (beam_search:429) INFO: max output length: 320
2024-10-27 18:29:43,351 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:46,076 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:46,076 (beam_search:476) INFO:  -8.77 * 1.0 =  -8.77 for ctc
2024-10-27 18:29:46,076 (beam_search:479) INFO: total log probability: -8.77
2024-10-27 18:29:46,076 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:29:46,076 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:46,076 (beam_search:483) INFO: best hypo: THISISSHOWINGTHATTHETHEBATTERYTHEBATTERYTOAWIREANDITGOESTOTHEMOTORANDTHENTHEMOTORHASATHATTOTHENEGATIVEANDTHENTHETHEENERGYORELECTRICITYTOTHEMOTORANDTHEMOTORSTARTSRUNNING

2024-10-27 18:29:46,080 (asr_inference:509) INFO: speech length: 32500
2024-10-27 18:29:47,403 (beam_search:428) INFO: decoder input length: 24
2024-10-27 18:29:47,404 (beam_search:429) INFO: max output length: 24
2024-10-27 18:29:47,404 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:47,423 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:47,423 (beam_search:476) INFO:  -0.14 * 1.0 =  -0.14 for ctc
2024-10-27 18:29:47,423 (beam_search:479) INFO: total log probability: -0.14
2024-10-27 18:29:47,423 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:29:47,423 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:47,423 (beam_search:483) INFO: best hypo: GOODHOWAREYOU

2024-10-27 18:29:47,425 (asr_inference:509) INFO: speech length: 152422
2024-10-27 18:29:52,908 (beam_search:428) INFO: decoder input length: 118
2024-10-27 18:29:52,908 (beam_search:429) INFO: max output length: 118
2024-10-27 18:29:52,908 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:53,104 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:53,104 (beam_search:476) INFO:  -1.96 * 1.0 =  -1.96 for ctc
2024-10-27 18:29:53,104 (beam_search:479) INFO: total log probability: -1.96
2024-10-27 18:29:53,104 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:29:53,104 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:53,104 (beam_search:483) INFO: best hypo: WE'VEBEENWITHANDANDSOLAR

2024-10-27 18:29:53,107 (asr_inference:509) INFO: speech length: 228009
2024-10-27 18:30:02,257 (beam_search:428) INFO: decoder input length: 177
2024-10-27 18:30:02,257 (beam_search:429) INFO: max output length: 177
2024-10-27 18:30:02,257 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:02,676 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:02,676 (beam_search:476) INFO:  -1.71 * 1.0 =  -1.71 for ctc
2024-10-27 18:30:02,676 (beam_search:479) INFO: total log probability: -1.71
2024-10-27 18:30:02,676 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:30:02,676 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:02,676 (beam_search:483) INFO: best hypo: WEANDWEWEUSEDSOLARTOTOMAKEARUN

2024-10-27 18:30:02,679 (asr_inference:509) INFO: speech length: 73578
2024-10-27 18:30:05,479 (beam_search:428) INFO: decoder input length: 56
2024-10-27 18:30:05,479 (beam_search:429) INFO: max output length: 56
2024-10-27 18:30:05,479 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:05,506 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:05,507 (beam_search:476) INFO:  -0.39 * 1.0 =  -0.39 for ctc
2024-10-27 18:30:05,507 (beam_search:479) INFO: total log probability: -0.39
2024-10-27 18:30:05,507 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:30:05,507 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:05,507 (beam_search:483) INFO: best hypo: THEIS

2024-10-27 18:30:05,509 (asr_inference:509) INFO: speech length: 354497
2024-10-27 18:30:21,465 (beam_search:428) INFO: decoder input length: 276
2024-10-27 18:30:21,465 (beam_search:429) INFO: max output length: 276
2024-10-27 18:30:21,465 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:22,355 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:22,355 (beam_search:476) INFO:  -4.63 * 1.0 =  -4.63 for ctc
2024-10-27 18:30:22,355 (beam_search:479) INFO: total log probability: -4.63
2024-10-27 18:30:22,355 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:30:22,355 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:22,355 (beam_search:483) INFO: best hypo: THERE'SANAILANDTWOWIRESTHERE'SANAILAANDAWIRE

2024-10-27 18:30:22,358 (asr_inference:509) INFO: speech length: 196085
2024-10-27 18:30:30,178 (beam_search:428) INFO: decoder input length: 152
2024-10-27 18:30:30,178 (beam_search:429) INFO: max output length: 152
2024-10-27 18:30:30,178 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:30,516 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:30,516 (beam_search:476) INFO:  -2.52 * 1.0 =  -2.52 for ctc
2024-10-27 18:30:30,516 (beam_search:479) INFO: total log probability: -2.52
2024-10-27 18:30:30,516 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:30:30,516 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:30,516 (beam_search:483) INFO: best hypo: THEREISAWIREASWITCHANAILAMOTORANDA

2024-10-27 18:30:30,520 (asr_inference:509) INFO: speech length: 141262
2024-10-27 18:30:35,832 (beam_search:428) INFO: decoder input length: 109
2024-10-27 18:30:35,832 (beam_search:429) INFO: max output length: 109
2024-10-27 18:30:35,832 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:35,975 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:35,976 (beam_search:476) INFO:  -2.64 * 1.0 =  -2.64 for ctc
2024-10-27 18:30:35,976 (beam_search:479) INFO: total log probability: -2.64
2024-10-27 18:30:35,976 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:30:35,976 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:35,976 (beam_search:483) INFO: best hypo: THEMOTORSWHENTHETHEOPEN

2024-10-27 18:30:35,978 (asr_inference:509) INFO: speech length: 88939
2024-10-27 18:30:39,254 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:30:39,255 (beam_search:429) INFO: max output length: 68
2024-10-27 18:30:39,255 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:39,360 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:39,360 (beam_search:476) INFO:  -1.75 * 1.0 =  -1.75 for ctc
2024-10-27 18:30:39,360 (beam_search:479) INFO: total log probability: -1.75
2024-10-27 18:30:39,360 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:30:39,360 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:39,360 (beam_search:483) INFO: best hypo: ITHINKTHATIT'SBECAUSETHEIS

2024-10-27 18:30:39,362 (asr_inference:509) INFO: speech length: 116617
2024-10-27 18:30:43,603 (beam_search:428) INFO: decoder input length: 90
2024-10-27 18:30:43,603 (beam_search:429) INFO: max output length: 90
2024-10-27 18:30:43,603 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:43,827 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:43,827 (beam_search:476) INFO:  -1.85 * 1.0 =  -1.85 for ctc
2024-10-27 18:30:43,827 (beam_search:479) INFO: total log probability: -1.85
2024-10-27 18:30:43,827 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:30:43,827 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:43,827 (beam_search:483) INFO: best hypo: THEISTHEREANDIT'SNOTMETALSOTHEMOTORISNOT

2024-10-27 18:30:43,830 (asr_inference:509) INFO: speech length: 263976
2024-10-27 18:30:54,699 (beam_search:428) INFO: decoder input length: 205
2024-10-27 18:30:54,699 (beam_search:429) INFO: max output length: 205
2024-10-27 18:30:54,699 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:55,234 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:55,234 (beam_search:476) INFO:  -3.99 * 1.0 =  -3.99 for ctc
2024-10-27 18:30:55,234 (beam_search:479) INFO: total log probability: -3.99
2024-10-27 18:30:55,234 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:30:55,234 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:55,234 (beam_search:483) INFO: best hypo: THEISANINSULATORISATHINGTHATCANLIKEATHATISWOOD

2024-10-27 18:30:55,236 (asr_inference:509) INFO: speech length: 134736
2024-10-27 18:31:00,386 (beam_search:428) INFO: decoder input length: 104
2024-10-27 18:31:00,386 (beam_search:429) INFO: max output length: 104
2024-10-27 18:31:00,386 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:00,525 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:00,525 (beam_search:476) INFO:  -2.64 * 1.0 =  -2.64 for ctc
2024-10-27 18:31:00,525 (beam_search:479) INFO: total log probability: -2.64
2024-10-27 18:31:00,525 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:31:00,525 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:00,525 (beam_search:483) INFO: best hypo: INSULATORSCANANDTHEYARENOTOF

2024-10-27 18:31:00,527 (asr_inference:509) INFO: speech length: 109390
2024-10-27 18:31:04,474 (beam_search:428) INFO: decoder input length: 84
2024-10-27 18:31:04,475 (beam_search:429) INFO: max output length: 84
2024-10-27 18:31:04,475 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:04,607 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:04,607 (beam_search:476) INFO:  -1.46 * 1.0 =  -1.46 for ctc
2024-10-27 18:31:04,607 (beam_search:479) INFO: total log probability: -1.46
2024-10-27 18:31:04,607 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:31:04,607 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:04,607 (beam_search:483) INFO: best hypo: THEINSULATORSARENOTMETALBUTTHEARE

2024-10-27 18:31:04,610 (asr_inference:509) INFO: speech length: 132973
2024-10-27 18:31:09,445 (beam_search:428) INFO: decoder input length: 103
2024-10-27 18:31:09,445 (beam_search:429) INFO: max output length: 103
2024-10-27 18:31:09,445 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:09,615 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:09,615 (beam_search:476) INFO:  -3.90 * 1.0 =  -3.90 for ctc
2024-10-27 18:31:09,615 (beam_search:479) INFO: total log probability: -3.90
2024-10-27 18:31:09,615 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:31:09,615 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:09,615 (beam_search:483) INFO: best hypo: THEARENOTANDTHEYTNOTCONDUCTELECTRICITY

2024-10-27 18:31:09,617 (asr_inference:509) INFO: speech length: 85115
2024-10-27 18:31:12,624 (beam_search:428) INFO: decoder input length: 65
2024-10-27 18:31:12,624 (beam_search:429) INFO: max output length: 65
2024-10-27 18:31:12,624 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:12,655 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:12,656 (beam_search:476) INFO:  -0.74 * 1.0 =  -0.74 for ctc
2024-10-27 18:31:12,656 (beam_search:479) INFO: total log probability: -0.74
2024-10-27 18:31:12,656 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:31:12,656 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:12,656 (beam_search:483) INFO: best hypo: THEMOTOR

2024-10-27 18:31:12,658 (asr_inference:509) INFO: speech length: 74507
2024-10-27 18:31:15,366 (beam_search:428) INFO: decoder input length: 57
2024-10-27 18:31:15,366 (beam_search:429) INFO: max output length: 57
2024-10-27 18:31:15,366 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:15,443 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:15,443 (beam_search:476) INFO:  -1.31 * 1.0 =  -1.31 for ctc
2024-10-27 18:31:15,443 (beam_search:479) INFO: total log probability: -1.31
2024-10-27 18:31:15,443 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:31:15,443 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:15,443 (beam_search:483) INFO: best hypo: IWOULDSAYITISIISOPEN

2024-10-27 18:31:15,446 (asr_inference:509) INFO: speech length: 59113
2024-10-27 18:31:17,792 (beam_search:428) INFO: decoder input length: 45
2024-10-27 18:31:17,792 (beam_search:429) INFO: max output length: 45
2024-10-27 18:31:17,792 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:17,816 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:17,817 (beam_search:476) INFO:  -0.10 * 1.0 =  -0.10 for ctc
2024-10-27 18:31:17,817 (beam_search:479) INFO: total log probability: -0.10
2024-10-27 18:31:17,817 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:31:17,817 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:17,817 (beam_search:483) INFO: best hypo: ITIS

2024-10-27 18:31:17,819 (asr_inference:509) INFO: speech length: 85173
2024-10-27 18:31:20,875 (beam_search:428) INFO: decoder input length: 66
2024-10-27 18:31:20,875 (beam_search:429) INFO: max output length: 66
2024-10-27 18:31:20,875 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:20,907 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:20,907 (beam_search:476) INFO:  -0.53 * 1.0 =  -0.53 for ctc
2024-10-27 18:31:20,907 (beam_search:479) INFO: total log probability: -0.53
2024-10-27 18:31:20,907 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:31:20,907 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:20,907 (beam_search:483) INFO: best hypo: AREMETAL

2024-10-27 18:31:20,909 (asr_inference:509) INFO: speech length: 39426
2024-10-27 18:31:22,414 (beam_search:428) INFO: decoder input length: 30
2024-10-27 18:31:22,414 (beam_search:429) INFO: max output length: 30
2024-10-27 18:31:22,414 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:22,430 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:22,430 (beam_search:476) INFO:  -1.07 * 1.0 =  -1.07 for ctc
2024-10-27 18:31:22,430 (beam_search:479) INFO: total log probability: -1.07
2024-10-27 18:31:22,430 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:31:22,430 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:22,431 (beam_search:483) INFO: best hypo: AREA

2024-10-27 18:31:22,433 (asr_inference:509) INFO: speech length: 51000
2024-10-27 18:31:24,395 (beam_search:428) INFO: decoder input length: 39
2024-10-27 18:31:24,395 (beam_search:429) INFO: max output length: 39
2024-10-27 18:31:24,395 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:24,421 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:24,421 (beam_search:476) INFO:  -0.61 * 1.0 =  -0.61 for ctc
2024-10-27 18:31:24,421 (beam_search:479) INFO: total log probability: -0.61
2024-10-27 18:31:24,421 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:31:24,421 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:24,421 (beam_search:483) INFO: best hypo: HOWAREYOU

2024-10-27 18:31:24,424 (asr_inference:509) INFO: speech length: 127242
2024-10-27 18:31:29,156 (beam_search:428) INFO: decoder input length: 98
2024-10-27 18:31:29,156 (beam_search:429) INFO: max output length: 98
2024-10-27 18:31:29,156 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:29,220 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:29,220 (beam_search:476) INFO:  -0.98 * 1.0 =  -0.98 for ctc
2024-10-27 18:31:29,220 (beam_search:479) INFO: total log probability: -0.98
2024-10-27 18:31:29,221 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:31:29,221 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:29,221 (beam_search:483) INFO: best hypo: WEABOUTCIRCUITS

2024-10-27 18:31:29,224 (asr_inference:509) INFO: speech length: 31397
2024-10-27 18:31:30,541 (beam_search:428) INFO: decoder input length: 24
2024-10-27 18:31:30,541 (beam_search:429) INFO: max output length: 24
2024-10-27 18:31:30,541 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:30,557 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:30,557 (beam_search:476) INFO:  -0.35 * 1.0 =  -0.35 for ctc
2024-10-27 18:31:30,557 (beam_search:479) INFO: total log probability: -0.35
2024-10-27 18:31:30,557 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:31:30,557 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:30,557 (beam_search:483) INFO: best hypo: ISEEA

2024-10-27 18:31:30,559 (asr_inference:509) INFO: speech length: 61938
2024-10-27 18:31:32,782 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:31:32,782 (beam_search:429) INFO: max output length: 47
2024-10-27 18:31:32,782 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:32,831 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:32,831 (beam_search:476) INFO:  -0.48 * 1.0 =  -0.48 for ctc
2024-10-27 18:31:32,831 (beam_search:479) INFO: total log probability: -0.48
2024-10-27 18:31:32,831 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:31:32,831 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:32,831 (beam_search:483) INFO: best hypo: THINKTHATTHELIGHTISIMPORTANT

2024-10-27 18:31:32,833 (asr_inference:509) INFO: speech length: 31735
2024-10-27 18:31:34,185 (beam_search:428) INFO: decoder input length: 24
2024-10-27 18:31:34,185 (beam_search:429) INFO: max output length: 24
2024-10-27 18:31:34,185 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:34,202 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:34,202 (beam_search:476) INFO:  -0.53 * 1.0 =  -0.53 for ctc
2024-10-27 18:31:34,202 (beam_search:479) INFO: total log probability: -0.53
2024-10-27 18:31:34,202 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:31:34,203 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:34,203 (beam_search:483) INFO: best hypo: BECAUSEWENEED

2024-10-27 18:31:34,205 (asr_inference:509) INFO: speech length: 63826
2024-10-27 18:31:36,646 (beam_search:428) INFO: decoder input length: 49
2024-10-27 18:31:36,646 (beam_search:429) INFO: max output length: 49
2024-10-27 18:31:36,646 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:36,688 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:36,688 (beam_search:476) INFO:  -0.95 * 1.0 =  -0.95 for ctc
2024-10-27 18:31:36,688 (beam_search:479) INFO: total log probability: -0.95
2024-10-27 18:31:36,688 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:31:36,688 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:36,688 (beam_search:483) INFO: best hypo: ITHATTHETHEIS

2024-10-27 18:31:36,690 (asr_inference:509) INFO: speech length: 82379
2024-10-27 18:31:39,604 (beam_search:428) INFO: decoder input length: 63
2024-10-27 18:31:39,604 (beam_search:429) INFO: max output length: 63
2024-10-27 18:31:39,604 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:39,667 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:39,667 (beam_search:476) INFO:  -1.05 * 1.0 =  -1.05 for ctc
2024-10-27 18:31:39,667 (beam_search:479) INFO: total log probability: -1.05
2024-10-27 18:31:39,667 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:31:39,667 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:39,667 (beam_search:483) INFO: best hypo: ISEETHATTHEIS

2024-10-27 18:31:39,671 (asr_inference:509) INFO: speech length: 47721
2024-10-27 18:31:41,514 (beam_search:428) INFO: decoder input length: 36
2024-10-27 18:31:41,514 (beam_search:429) INFO: max output length: 36
2024-10-27 18:31:41,514 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:41,548 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:41,548 (beam_search:476) INFO:  -0.69 * 1.0 =  -0.69 for ctc
2024-10-27 18:31:41,548 (beam_search:479) INFO: total log probability: -0.69
2024-10-27 18:31:41,548 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:31:41,548 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:41,548 (beam_search:483) INFO: best hypo: ISEETHATTHEIS

2024-10-27 18:31:41,551 (asr_inference:509) INFO: speech length: 54555
2024-10-27 18:31:43,683 (beam_search:428) INFO: decoder input length: 42
2024-10-27 18:31:43,683 (beam_search:429) INFO: max output length: 42
2024-10-27 18:31:43,683 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:43,710 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:43,711 (beam_search:476) INFO:  -0.05 * 1.0 =  -0.05 for ctc
2024-10-27 18:31:43,711 (beam_search:479) INFO: total log probability: -0.05
2024-10-27 18:31:43,711 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 18:31:43,711 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:43,711 (beam_search:483) INFO: best hypo: ISEEA

2024-10-27 18:31:43,714 (asr_inference:509) INFO: speech length: 57166
2024-10-27 18:31:45,856 (beam_search:428) INFO: decoder input length: 44
2024-10-27 18:31:45,857 (beam_search:429) INFO: max output length: 44
2024-10-27 18:31:45,857 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:45,884 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:45,884 (beam_search:476) INFO:  -0.75 * 1.0 =  -0.75 for ctc
2024-10-27 18:31:45,884 (beam_search:479) INFO: total log probability: -0.75
2024-10-27 18:31:45,884 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:31:45,884 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:45,884 (beam_search:483) INFO: best hypo: ITISTHE

2024-10-27 18:31:45,886 (asr_inference:509) INFO: speech length: 94002
2024-10-27 18:31:49,211 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:31:49,211 (beam_search:429) INFO: max output length: 72
2024-10-27 18:31:49,211 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:49,236 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:49,236 (beam_search:476) INFO:  -0.13 * 1.0 =  -0.13 for ctc
2024-10-27 18:31:49,236 (beam_search:479) INFO: total log probability: -0.13
2024-10-27 18:31:49,236 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 18:31:49,236 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:49,236 (beam_search:483) INFO: best hypo: IS

2024-10-27 18:31:49,239 (asr_inference:509) INFO: speech length: 41116
2024-10-27 18:31:50,841 (beam_search:428) INFO: decoder input length: 31
2024-10-27 18:31:50,841 (beam_search:429) INFO: max output length: 31
2024-10-27 18:31:50,841 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:50,858 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:50,858 (beam_search:476) INFO:  -0.49 * 1.0 =  -0.49 for ctc
2024-10-27 18:31:50,858 (beam_search:479) INFO: total log probability: -0.49
2024-10-27 18:31:50,858 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:31:50,858 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:50,858 (beam_search:483) INFO: best hypo: ISAND

2024-10-27 18:31:50,861 (asr_inference:509) INFO: speech length: 58059
2024-10-27 18:31:53,224 (beam_search:428) INFO: decoder input length: 44
2024-10-27 18:31:53,225 (beam_search:429) INFO: max output length: 44
2024-10-27 18:31:53,225 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:53,247 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:53,248 (beam_search:476) INFO:  -0.81 * 1.0 =  -0.81 for ctc
2024-10-27 18:31:53,248 (beam_search:479) INFO: total log probability: -0.81
2024-10-27 18:31:53,248 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:31:53,248 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:53,248 (beam_search:483) INFO: best hypo: ICAN

2024-10-27 18:31:53,250 (asr_inference:509) INFO: speech length: 133144
2024-10-27 18:31:58,148 (beam_search:428) INFO: decoder input length: 103
2024-10-27 18:31:58,148 (beam_search:429) INFO: max output length: 103
2024-10-27 18:31:58,148 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:58,386 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:58,386 (beam_search:476) INFO:  -3.43 * 1.0 =  -3.43 for ctc
2024-10-27 18:31:58,386 (beam_search:479) INFO: total log probability: -3.43
2024-10-27 18:31:58,386 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:31:58,386 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:58,386 (beam_search:483) INFO: best hypo: ITHINKTHATTHEISGETTINGENERGYFROMTHESHE'STO

2024-10-27 18:31:58,389 (asr_inference:509) INFO: speech length: 108261
2024-10-27 18:32:02,355 (beam_search:428) INFO: decoder input length: 84
2024-10-27 18:32:02,355 (beam_search:429) INFO: max output length: 84
2024-10-27 18:32:02,355 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:02,465 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:02,465 (beam_search:476) INFO:  -2.18 * 1.0 =  -2.18 for ctc
2024-10-27 18:32:02,465 (beam_search:479) INFO: total log probability: -2.18
2024-10-27 18:32:02,465 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:32:02,465 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:02,465 (beam_search:483) INFO: best hypo: THEISTHATISHASENERGYTO

2024-10-27 18:32:02,469 (asr_inference:509) INFO: speech length: 172272
2024-10-27 18:32:09,162 (beam_search:428) INFO: decoder input length: 134
2024-10-27 18:32:09,162 (beam_search:429) INFO: max output length: 134
2024-10-27 18:32:09,162 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:09,443 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:09,443 (beam_search:476) INFO:  -3.09 * 1.0 =  -3.09 for ctc
2024-10-27 18:32:09,443 (beam_search:479) INFO: total log probability: -3.09
2024-10-27 18:32:09,443 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:32:09,443 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:09,444 (beam_search:483) INFO: best hypo: IAAANDIDON'TKNOWWHATOTHER

2024-10-27 18:32:09,446 (asr_inference:509) INFO: speech length: 50067
2024-10-27 18:32:11,351 (beam_search:428) INFO: decoder input length: 38
2024-10-27 18:32:11,351 (beam_search:429) INFO: max output length: 38
2024-10-27 18:32:11,351 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:11,380 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:11,380 (beam_search:476) INFO:  -1.20 * 1.0 =  -1.20 for ctc
2024-10-27 18:32:11,380 (beam_search:479) INFO: total log probability: -1.20
2024-10-27 18:32:11,380 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:32:11,381 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:11,381 (beam_search:483) INFO: best hypo: ITHINKTHATIS

2024-10-27 18:32:11,383 (asr_inference:509) INFO: speech length: 53417
2024-10-27 18:32:13,373 (beam_search:428) INFO: decoder input length: 41
2024-10-27 18:32:13,373 (beam_search:429) INFO: max output length: 41
2024-10-27 18:32:13,373 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:13,389 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:13,389 (beam_search:476) INFO:  -0.55 * 1.0 =  -0.55 for ctc
2024-10-27 18:32:13,389 (beam_search:479) INFO: total log probability: -0.55
2024-10-27 18:32:13,389 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:32:13,389 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:13,389 (beam_search:483) INFO: best hypo: THE

2024-10-27 18:32:13,392 (asr_inference:509) INFO: speech length: 49328
2024-10-27 18:32:15,289 (beam_search:428) INFO: decoder input length: 38
2024-10-27 18:32:15,289 (beam_search:429) INFO: max output length: 38
2024-10-27 18:32:15,289 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:15,304 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:15,304 (beam_search:476) INFO:  -0.93 * 1.0 =  -0.93 for ctc
2024-10-27 18:32:15,304 (beam_search:479) INFO: total log probability: -0.93
2024-10-27 18:32:15,304 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 18:32:15,304 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:15,304 (beam_search:483) INFO: best hypo: IT

2024-10-27 18:32:15,307 (asr_inference:509) INFO: speech length: 23568
2024-10-27 18:32:16,314 (beam_search:428) INFO: decoder input length: 17
2024-10-27 18:32:16,314 (beam_search:429) INFO: max output length: 17
2024-10-27 18:32:16,314 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:16,332 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:16,332 (beam_search:476) INFO:  -0.02 * 1.0 =  -0.02 for ctc
2024-10-27 18:32:16,332 (beam_search:479) INFO: total log probability: -0.02
2024-10-27 18:32:16,332 (beam_search:480) INFO: normalized log probability: -0.00
2024-10-27 18:32:16,332 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:16,332 (beam_search:483) INFO: best hypo: IDON'TKNOW

2024-10-27 18:32:16,334 (asr_inference:509) INFO: speech length: 104561
2024-10-27 18:32:20,062 (beam_search:428) INFO: decoder input length: 81
2024-10-27 18:32:20,062 (beam_search:429) INFO: max output length: 81
2024-10-27 18:32:20,062 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:20,117 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:20,117 (beam_search:476) INFO:  -1.77 * 1.0 =  -1.77 for ctc
2024-10-27 18:32:20,117 (beam_search:479) INFO: total log probability: -1.77
2024-10-27 18:32:20,117 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:32:20,117 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:20,117 (beam_search:483) INFO: best hypo: THELARTHE

2024-10-27 18:32:20,120 (asr_inference:509) INFO: speech length: 61189
2024-10-27 18:32:22,427 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:32:22,428 (beam_search:429) INFO: max output length: 47
2024-10-27 18:32:22,428 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:22,454 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:22,454 (beam_search:476) INFO:  -0.55 * 1.0 =  -0.55 for ctc
2024-10-27 18:32:22,454 (beam_search:479) INFO: total log probability: -0.55
2024-10-27 18:32:22,454 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:32:22,454 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:22,454 (beam_search:483) INFO: best hypo: THEYARE

2024-10-27 18:32:22,457 (asr_inference:509) INFO: speech length: 27836
2024-10-27 18:32:23,771 (beam_search:428) INFO: decoder input length: 21
2024-10-27 18:32:23,771 (beam_search:429) INFO: max output length: 21
2024-10-27 18:32:23,771 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:23,795 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:23,795 (beam_search:476) INFO:  -0.04 * 1.0 =  -0.04 for ctc
2024-10-27 18:32:23,795 (beam_search:479) INFO: total log probability: -0.04
2024-10-27 18:32:23,795 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 18:32:23,795 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:23,795 (beam_search:483) INFO: best hypo: IDON'TKNOW

2024-10-27 18:32:23,797 (asr_inference:509) INFO: speech length: 61616
2024-10-27 18:32:26,046 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:32:26,046 (beam_search:429) INFO: max output length: 47
2024-10-27 18:32:26,046 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:26,082 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:26,083 (beam_search:476) INFO:  -2.38 * 1.0 =  -2.38 for ctc
2024-10-27 18:32:26,083 (beam_search:479) INFO: total log probability: -2.38
2024-10-27 18:32:26,083 (beam_search:480) INFO: normalized log probability: -0.40
2024-10-27 18:32:26,083 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:26,083 (beam_search:483) INFO: best hypo: ITHINKTHATBE

2024-10-27 18:32:26,085 (asr_inference:509) INFO: speech length: 68100
2024-10-27 18:32:28,549 (beam_search:428) INFO: decoder input length: 52
2024-10-27 18:32:28,549 (beam_search:429) INFO: max output length: 52
2024-10-27 18:32:28,549 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:28,584 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:28,584 (beam_search:476) INFO:  -0.55 * 1.0 =  -0.55 for ctc
2024-10-27 18:32:28,584 (beam_search:479) INFO: total log probability: -0.55
2024-10-27 18:32:28,584 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:32:28,584 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:28,584 (beam_search:483) INFO: best hypo: THETHEAIR

2024-10-27 18:32:28,586 (asr_inference:509) INFO: speech length: 47233
2024-10-27 18:32:30,428 (beam_search:428) INFO: decoder input length: 36
2024-10-27 18:32:30,428 (beam_search:429) INFO: max output length: 36
2024-10-27 18:32:30,428 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:30,451 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:30,451 (beam_search:476) INFO:  -1.09 * 1.0 =  -1.09 for ctc
2024-10-27 18:32:30,451 (beam_search:479) INFO: total log probability: -1.09
2024-10-27 18:32:30,451 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:32:30,451 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:30,451 (beam_search:483) INFO: best hypo: ANDTHEAIR

2024-10-27 18:32:30,453 (asr_inference:509) INFO: speech length: 40358
2024-10-27 18:32:31,948 (beam_search:428) INFO: decoder input length: 31
2024-10-27 18:32:31,948 (beam_search:429) INFO: max output length: 31
2024-10-27 18:32:31,948 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:31,976 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:31,977 (beam_search:476) INFO:  -0.02 * 1.0 =  -0.02 for ctc
2024-10-27 18:32:31,977 (beam_search:479) INFO: total log probability: -0.02
2024-10-27 18:32:31,977 (beam_search:480) INFO: normalized log probability: -0.00
2024-10-27 18:32:31,977 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:31,977 (beam_search:483) INFO: best hypo: IDON'TKNOW

2024-10-27 18:32:31,979 (asr_inference:509) INFO: speech length: 47000
2024-10-27 18:32:33,783 (beam_search:428) INFO: decoder input length: 36
2024-10-27 18:32:33,783 (beam_search:429) INFO: max output length: 36
2024-10-27 18:32:33,783 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:33,817 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:33,817 (beam_search:476) INFO:  -0.62 * 1.0 =  -0.62 for ctc
2024-10-27 18:32:33,817 (beam_search:479) INFO: total log probability: -0.62
2024-10-27 18:32:33,817 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:32:33,817 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:33,817 (beam_search:483) INFO: best hypo: IGOODHOWAREYOU

2024-10-27 18:32:33,819 (asr_inference:509) INFO: speech length: 139131
2024-10-27 18:32:38,924 (beam_search:428) INFO: decoder input length: 108
2024-10-27 18:32:38,924 (beam_search:429) INFO: max output length: 108
2024-10-27 18:32:38,924 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:39,123 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:39,123 (beam_search:476) INFO:  -4.01 * 1.0 =  -4.01 for ctc
2024-10-27 18:32:39,123 (beam_search:479) INFO: total log probability: -4.01
2024-10-27 18:32:39,123 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 18:32:39,123 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:39,123 (beam_search:483) INFO: best hypo: WE'VEBEENWEBEENLEARNINGABOUTANDPARALLEL

2024-10-27 18:32:39,125 (asr_inference:509) INFO: speech length: 353084
2024-10-27 18:32:55,317 (beam_search:428) INFO: decoder input length: 275
2024-10-27 18:32:55,317 (beam_search:429) INFO: max output length: 275
2024-10-27 18:32:55,317 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:55,501 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:55,501 (beam_search:476) INFO:  -1.25 * 1.0 =  -1.25 for ctc
2024-10-27 18:32:55,501 (beam_search:479) INFO: total log probability: -1.25
2024-10-27 18:32:55,501 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:32:55,501 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:55,501 (beam_search:483) INFO: best hypo: AISSOMETHING

2024-10-27 18:32:55,504 (asr_inference:509) INFO: speech length: 152622
2024-10-27 18:33:01,286 (beam_search:428) INFO: decoder input length: 118
2024-10-27 18:33:01,287 (beam_search:429) INFO: max output length: 118
2024-10-27 18:33:01,287 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:01,427 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:01,427 (beam_search:476) INFO:  -0.34 * 1.0 =  -0.34 for ctc
2024-10-27 18:33:01,427 (beam_search:479) INFO: total log probability: -0.34
2024-10-27 18:33:01,427 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 18:33:01,427 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:01,427 (beam_search:483) INFO: best hypo: THEELECTRICITYISANDITIS

2024-10-27 18:33:01,429 (asr_inference:509) INFO: speech length: 107084
2024-10-27 18:33:05,481 (beam_search:428) INFO: decoder input length: 83
2024-10-27 18:33:05,481 (beam_search:429) INFO: max output length: 83
2024-10-27 18:33:05,481 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:05,610 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:05,611 (beam_search:476) INFO:  -1.04 * 1.0 =  -1.04 for ctc
2024-10-27 18:33:05,611 (beam_search:479) INFO: total log probability: -1.04
2024-10-27 18:33:05,611 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:33:05,611 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:05,611 (beam_search:483) INFO: best hypo: THEELECTRICITYHASTOANDITCAN'T

2024-10-27 18:33:05,613 (asr_inference:509) INFO: speech length: 62858
2024-10-27 18:33:07,975 (beam_search:428) INFO: decoder input length: 48
2024-10-27 18:33:07,975 (beam_search:429) INFO: max output length: 48
2024-10-27 18:33:07,975 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:08,007 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:08,008 (beam_search:476) INFO:  -0.36 * 1.0 =  -0.36 for ctc
2024-10-27 18:33:08,008 (beam_search:479) INFO: total log probability: -0.36
2024-10-27 18:33:08,008 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:33:08,008 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:08,008 (beam_search:483) INFO: best hypo: ELECTRICITYINA

2024-10-27 18:33:08,011 (asr_inference:509) INFO: speech length: 198827
2024-10-27 18:33:15,765 (beam_search:428) INFO: decoder input length: 154
2024-10-27 18:33:15,765 (beam_search:429) INFO: max output length: 154
2024-10-27 18:33:15,765 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:16,194 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:16,194 (beam_search:476) INFO:  -4.07 * 1.0 =  -4.07 for ctc
2024-10-27 18:33:16,194 (beam_search:479) INFO: total log probability: -4.07
2024-10-27 18:33:16,194 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:33:16,194 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:16,195 (beam_search:483) INFO: best hypo: THEHASDENERGYTHATCANTHEBULBANDTHEWIRESTHETHETOTHEBULB

2024-10-27 18:33:16,197 (asr_inference:509) INFO: speech length: 230042
2024-10-27 18:33:25,840 (beam_search:428) INFO: decoder input length: 179
2024-10-27 18:33:25,840 (beam_search:429) INFO: max output length: 179
2024-10-27 18:33:25,840 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:26,351 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:26,351 (beam_search:476) INFO:  -3.10 * 1.0 =  -3.10 for ctc
2024-10-27 18:33:26,351 (beam_search:479) INFO: total log probability: -3.10
2024-10-27 18:33:26,351 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:33:26,351 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:26,351 (beam_search:483) INFO: best hypo: THETHEWIRESMAKEAFROMTHETHECIRCUITTHEWIRESMAKEATHETOTHE

2024-10-27 18:33:26,353 (asr_inference:509) INFO: speech length: 59610
2024-10-27 18:33:28,570 (beam_search:428) INFO: decoder input length: 46
2024-10-27 18:33:28,570 (beam_search:429) INFO: max output length: 46
2024-10-27 18:33:28,571 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:28,607 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:28,607 (beam_search:476) INFO:  -1.10 * 1.0 =  -1.10 for ctc
2024-10-27 18:33:28,607 (beam_search:479) INFO: total log probability: -1.10
2024-10-27 18:33:28,607 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:33:28,607 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:28,607 (beam_search:483) INFO: best hypo: OFELECTRICITYGOESTO

2024-10-27 18:33:28,610 (asr_inference:509) INFO: speech length: 133858
2024-10-27 18:33:33,583 (beam_search:428) INFO: decoder input length: 104
2024-10-27 18:33:33,583 (beam_search:429) INFO: max output length: 104
2024-10-27 18:33:33,583 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:33,790 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:33,790 (beam_search:476) INFO:  -1.65 * 1.0 =  -1.65 for ctc
2024-10-27 18:33:33,790 (beam_search:479) INFO: total log probability: -1.65
2024-10-27 18:33:33,790 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:33:33,790 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:33,790 (beam_search:483) INFO: best hypo: ELECTRICITYISFLOWINGTHROUGHTHEWIRESTHEANDTHELIGHTBULB

2024-10-27 18:33:33,793 (asr_inference:509) INFO: speech length: 387438
2024-10-27 18:33:52,002 (beam_search:428) INFO: decoder input length: 302
2024-10-27 18:33:52,003 (beam_search:429) INFO: max output length: 302
2024-10-27 18:33:52,003 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:54,523 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:54,523 (beam_search:476) INFO:  -5.95 * 1.0 =  -5.95 for ctc
2024-10-27 18:33:54,523 (beam_search:479) INFO: total log probability: -5.95
2024-10-27 18:33:54,523 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:33:54,523 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:54,523 (beam_search:483) INFO: best hypo: THETHEELECTRICITYGOESTHROUGHTHEDANDTHENTHROUGHTHEWIRESTOTHELIGHTTOTHETOTHELIGHTBULBANDGOESBACKTOTHEWIRESANDINTOTHESECONDLIGHTBULBANDGOESTOTHEANDTHENTOTHED

2024-10-27 18:33:54,526 (asr_inference:509) INFO: speech length: 120018
2024-10-27 18:33:58,868 (beam_search:428) INFO: decoder input length: 93
2024-10-27 18:33:58,868 (beam_search:429) INFO: max output length: 93
2024-10-27 18:33:58,868 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:59,052 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:59,053 (beam_search:476) INFO:  -1.91 * 1.0 =  -1.91 for ctc
2024-10-27 18:33:59,053 (beam_search:479) INFO: total log probability: -1.91
2024-10-27 18:33:59,053 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:33:59,053 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:59,053 (beam_search:483) INFO: best hypo: THEBLUEDOTSAREELECTRICITYIT'SHOWTHEELECTRICITY

2024-10-27 18:33:59,056 (asr_inference:509) INFO: speech length: 221138
2024-10-27 18:34:07,673 (beam_search:428) INFO: decoder input length: 172
2024-10-27 18:34:07,673 (beam_search:429) INFO: max output length: 172
2024-10-27 18:34:07,673 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:08,188 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:08,188 (beam_search:476) INFO:  -2.24 * 1.0 =  -2.24 for ctc
2024-10-27 18:34:08,188 (beam_search:479) INFO: total log probability: -2.24
2024-10-27 18:34:08,188 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:34:08,188 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:08,189 (beam_search:483) INFO: best hypo: ITHINKTHATTHETHEELECTRICITYANDTHEBLUEARELIKEELECTRICITYANDTHEYFLOWTHROUGH

2024-10-27 18:34:08,192 (asr_inference:509) INFO: speech length: 209069
2024-10-27 18:34:16,405 (beam_search:428) INFO: decoder input length: 162
2024-10-27 18:34:16,405 (beam_search:429) INFO: max output length: 162
2024-10-27 18:34:16,405 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:16,868 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:16,868 (beam_search:476) INFO:  -2.20 * 1.0 =  -2.20 for ctc
2024-10-27 18:34:16,868 (beam_search:479) INFO: total log probability: -2.20
2024-10-27 18:34:16,868 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:34:16,868 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:16,868 (beam_search:483) INFO: best hypo: THEELECTRICITYHASTOFLOWINTOTHEPOSITIVEANDANDOUTTHENEGATIVEANDTTHE

2024-10-27 18:34:16,870 (asr_inference:509) INFO: speech length: 60056
2024-10-27 18:34:19,029 (beam_search:428) INFO: decoder input length: 46
2024-10-27 18:34:19,029 (beam_search:429) INFO: max output length: 46
2024-10-27 18:34:19,029 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:19,053 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:19,053 (beam_search:476) INFO:  -1.05 * 1.0 =  -1.05 for ctc
2024-10-27 18:34:19,053 (beam_search:479) INFO: total log probability: -1.05
2024-10-27 18:34:19,053 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:34:19,053 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:19,053 (beam_search:483) INFO: best hypo: THEELECTRICITY

2024-10-27 18:34:19,056 (asr_inference:509) INFO: speech length: 31141
2024-10-27 18:34:20,433 (beam_search:428) INFO: decoder input length: 23
2024-10-27 18:34:20,433 (beam_search:429) INFO: max output length: 23
2024-10-27 18:34:20,433 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:20,443 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:20,443 (beam_search:476) INFO:  -0.54 * 1.0 =  -0.54 for ctc
2024-10-27 18:34:20,443 (beam_search:479) INFO: total log probability: -0.54
2024-10-27 18:34:20,443 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:34:20,443 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:20,443 (beam_search:483) INFO: best hypo: I

2024-10-27 18:34:20,445 (asr_inference:509) INFO: speech length: 113683
2024-10-27 18:34:24,696 (beam_search:428) INFO: decoder input length: 88
2024-10-27 18:34:24,697 (beam_search:429) INFO: max output length: 88
2024-10-27 18:34:24,697 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:24,903 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:24,904 (beam_search:476) INFO:  -2.25 * 1.0 =  -2.25 for ctc
2024-10-27 18:34:24,904 (beam_search:479) INFO: total log probability: -2.25
2024-10-27 18:34:24,904 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:34:24,904 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:24,904 (beam_search:483) INFO: best hypo: ITHINKTHATTHESWILLGETDIMMERANDWELLNEEDMOREWIRES

2024-10-27 18:34:24,906 (asr_inference:509) INFO: speech length: 83228
2024-10-27 18:34:28,039 (beam_search:428) INFO: decoder input length: 64
2024-10-27 18:34:28,039 (beam_search:429) INFO: max output length: 64
2024-10-27 18:34:28,039 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:28,128 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:28,128 (beam_search:476) INFO:  -1.27 * 1.0 =  -1.27 for ctc
2024-10-27 18:34:28,128 (beam_search:479) INFO: total log probability: -1.27
2024-10-27 18:34:28,128 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:34:28,128 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:28,128 (beam_search:483) INFO: best hypo: ITHINKTHATTHESWILLGETBRIGHTER

2024-10-27 18:34:28,130 (asr_inference:509) INFO: speech length: 66743
2024-10-27 18:34:30,535 (beam_search:428) INFO: decoder input length: 51
2024-10-27 18:34:30,536 (beam_search:429) INFO: max output length: 51
2024-10-27 18:34:30,536 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:30,561 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:30,561 (beam_search:476) INFO:  -0.80 * 1.0 =  -0.80 for ctc
2024-10-27 18:34:30,561 (beam_search:479) INFO: total log probability: -0.80
2024-10-27 18:34:30,561 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:34:30,561 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:30,561 (beam_search:483) INFO: best hypo: THES

2024-10-27 18:34:30,563 (asr_inference:509) INFO: speech length: 97116
2024-10-27 18:34:34,076 (beam_search:428) INFO: decoder input length: 75
2024-10-27 18:34:34,076 (beam_search:429) INFO: max output length: 75
2024-10-27 18:34:34,076 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:34,147 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:34,147 (beam_search:476) INFO:  -1.65 * 1.0 =  -1.65 for ctc
2024-10-27 18:34:34,147 (beam_search:479) INFO: total log probability: -1.65
2024-10-27 18:34:34,147 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:34:34,147 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:34,147 (beam_search:483) INFO: best hypo: THATTHELIGHTSIN

2024-10-27 18:34:34,149 (asr_inference:509) INFO: speech length: 88979
2024-10-27 18:34:37,364 (beam_search:428) INFO: decoder input length: 69
2024-10-27 18:34:37,364 (beam_search:429) INFO: max output length: 69
2024-10-27 18:34:37,364 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:37,410 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:37,410 (beam_search:476) INFO:  -0.09 * 1.0 =  -0.09 for ctc
2024-10-27 18:34:37,410 (beam_search:479) INFO: total log probability: -0.09
2024-10-27 18:34:37,410 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:34:37,410 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:37,410 (beam_search:483) INFO: best hypo: ISEETHAT

2024-10-27 18:34:37,412 (asr_inference:509) INFO: speech length: 195306
2024-10-27 18:34:45,104 (beam_search:428) INFO: decoder input length: 152
2024-10-27 18:34:45,104 (beam_search:429) INFO: max output length: 152
2024-10-27 18:34:45,104 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:45,643 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:45,643 (beam_search:476) INFO:  -3.17 * 1.0 =  -3.17 for ctc
2024-10-27 18:34:45,643 (beam_search:479) INFO: total log probability: -3.17
2024-10-27 18:34:45,643 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:34:45,643 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:45,643 (beam_search:483) INFO: best hypo: INTWODLIGHTBULBOUTOFLIGHTOUTREALLYBUTWITHONEDCELLTHEYDON'T

2024-10-27 18:34:45,646 (asr_inference:509) INFO: speech length: 304158
2024-10-27 18:34:59,214 (beam_search:428) INFO: decoder input length: 237
2024-10-27 18:34:59,214 (beam_search:429) INFO: max output length: 237
2024-10-27 18:34:59,214 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:59,984 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:59,984 (beam_search:476) INFO:  -7.73 * 1.0 =  -7.73 for ctc
2024-10-27 18:34:59,984 (beam_search:479) INFO: total log probability: -7.73
2024-10-27 18:34:59,984 (beam_search:480) INFO: normalized log probability: -0.39
2024-10-27 18:34:59,984 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:59,984 (beam_search:483) INFO: best hypo: ELECTRICITYISN'TBECAUSETHEONBULBTHEISGOINGTHETHEBECAUSETHETWOARETOGETHER

2024-10-27 18:34:59,987 (asr_inference:509) INFO: speech length: 71470
2024-10-27 18:35:02,721 (beam_search:428) INFO: decoder input length: 55
2024-10-27 18:35:02,721 (beam_search:429) INFO: max output length: 55
2024-10-27 18:35:02,721 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:02,771 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:02,771 (beam_search:476) INFO:  -0.39 * 1.0 =  -0.39 for ctc
2024-10-27 18:35:02,771 (beam_search:479) INFO: total log probability: -0.39
2024-10-27 18:35:02,771 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:35:02,771 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:02,771 (beam_search:483) INFO: best hypo: YOUARETHEINTHE

2024-10-27 18:35:02,773 (asr_inference:509) INFO: speech length: 52500
2024-10-27 18:35:04,782 (beam_search:428) INFO: decoder input length: 40
2024-10-27 18:35:04,782 (beam_search:429) INFO: max output length: 40
2024-10-27 18:35:04,782 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:04,830 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:04,830 (beam_search:476) INFO:  -0.63 * 1.0 =  -0.63 for ctc
2024-10-27 18:35:04,830 (beam_search:479) INFO: total log probability: -0.63
2024-10-27 18:35:04,830 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:35:04,830 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:04,830 (beam_search:483) INFO: best hypo: I'MGOODHOWAREYOU

2024-10-27 18:35:04,834 (asr_inference:509) INFO: speech length: 72780
2024-10-27 18:35:07,409 (beam_search:428) INFO: decoder input length: 56
2024-10-27 18:35:07,409 (beam_search:429) INFO: max output length: 56
2024-10-27 18:35:07,409 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:07,485 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:07,485 (beam_search:476) INFO:  -2.09 * 1.0 =  -2.09 for ctc
2024-10-27 18:35:07,485 (beam_search:479) INFO: total log probability: -2.09
2024-10-27 18:35:07,485 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:35:07,485 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:07,485 (beam_search:483) INFO: best hypo: WE'VEBEENABOUTANDPARALLELCIRCUITS

2024-10-27 18:35:07,487 (asr_inference:509) INFO: speech length: 150061
2024-10-27 18:35:12,989 (beam_search:428) INFO: decoder input length: 116
2024-10-27 18:35:12,989 (beam_search:429) INFO: max output length: 116
2024-10-27 18:35:12,989 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:13,253 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:13,253 (beam_search:476) INFO:  -1.82 * 1.0 =  -1.82 for ctc
2024-10-27 18:35:13,253 (beam_search:479) INFO: total log probability: -1.82
2024-10-27 18:35:13,253 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:35:13,253 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:13,253 (beam_search:483) INFO: best hypo: THETHEREARETWOSANDONEBATTERYANDTHELIGHTAREBRIGHT

2024-10-27 18:35:13,256 (asr_inference:509) INFO: speech length: 298948
2024-10-27 18:35:26,337 (beam_search:428) INFO: decoder input length: 233
2024-10-27 18:35:26,337 (beam_search:429) INFO: max output length: 233
2024-10-27 18:35:26,337 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:26,723 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:26,724 (beam_search:476) INFO:  -3.49 * 1.0 =  -3.49 for ctc
2024-10-27 18:35:26,724 (beam_search:479) INFO: total log probability: -3.49
2024-10-27 18:35:26,724 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 18:35:26,724 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:26,724 (beam_search:483) INFO: best hypo: THECIRCUITSITSANDTHELIGHTSARE

2024-10-27 18:35:26,726 (asr_inference:509) INFO: speech length: 26774
2024-10-27 18:35:27,914 (beam_search:428) INFO: decoder input length: 20
2024-10-27 18:35:27,915 (beam_search:429) INFO: max output length: 20
2024-10-27 18:35:27,915 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:27,929 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:27,929 (beam_search:476) INFO:  -0.44 * 1.0 =  -0.44 for ctc
2024-10-27 18:35:27,929 (beam_search:479) INFO: total log probability: -0.44
2024-10-27 18:35:27,929 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:35:27,929 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:27,929 (beam_search:483) INFO: best hypo: ITTURNS

2024-10-27 18:35:27,931 (asr_inference:509) INFO: speech length: 55089
2024-10-27 18:35:30,004 (beam_search:428) INFO: decoder input length: 42
2024-10-27 18:35:30,004 (beam_search:429) INFO: max output length: 42
2024-10-27 18:35:30,004 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:30,024 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:30,025 (beam_search:476) INFO:  -0.74 * 1.0 =  -0.74 for ctc
2024-10-27 18:35:30,025 (beam_search:479) INFO: total log probability: -0.74
2024-10-27 18:35:30,025 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:35:30,025 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:30,025 (beam_search:483) INFO: best hypo: THATARE

2024-10-27 18:35:30,027 (asr_inference:509) INFO: speech length: 245674
2024-10-27 18:35:40,016 (beam_search:428) INFO: decoder input length: 191
2024-10-27 18:35:40,016 (beam_search:429) INFO: max output length: 191
2024-10-27 18:35:40,016 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:40,507 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:40,507 (beam_search:476) INFO:  -4.09 * 1.0 =  -4.09 for ctc
2024-10-27 18:35:40,507 (beam_search:479) INFO: total log probability: -4.09
2024-10-27 18:35:40,507 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:35:40,507 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:40,507 (beam_search:483) INFO: best hypo: ITHINKTHATTHEBULBSWILLBEVERYVERYVERYANDANDTHEIN

2024-10-27 18:35:40,510 (asr_inference:509) INFO: speech length: 70764
2024-10-27 18:35:43,150 (beam_search:428) INFO: decoder input length: 54
2024-10-27 18:35:43,150 (beam_search:429) INFO: max output length: 54
2024-10-27 18:35:43,150 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:43,201 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:43,201 (beam_search:476) INFO:  -0.33 * 1.0 =  -0.33 for ctc
2024-10-27 18:35:43,201 (beam_search:479) INFO: total log probability: -0.33
2024-10-27 18:35:43,201 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:35:43,201 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:43,201 (beam_search:483) INFO: best hypo: ITHINKTHATTHEWILL

2024-10-27 18:35:43,203 (asr_inference:509) INFO: speech length: 98008
2024-10-27 18:35:46,713 (beam_search:428) INFO: decoder input length: 76
2024-10-27 18:35:46,713 (beam_search:429) INFO: max output length: 76
2024-10-27 18:35:46,713 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:46,811 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:46,811 (beam_search:476) INFO:  -2.08 * 1.0 =  -2.08 for ctc
2024-10-27 18:35:46,811 (beam_search:479) INFO: total log probability: -2.08
2024-10-27 18:35:46,811 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:35:46,811 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:46,811 (beam_search:483) INFO: best hypo: GOESTHROUGHTHEBATTERYTHROUGHTHEPOSITIVE

2024-10-27 18:35:46,814 (asr_inference:509) INFO: speech length: 199421
2024-10-27 18:35:54,769 (beam_search:428) INFO: decoder input length: 155
2024-10-27 18:35:54,769 (beam_search:429) INFO: max output length: 155
2024-10-27 18:35:54,769 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:55,263 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:55,263 (beam_search:476) INFO:  -3.78 * 1.0 =  -3.78 for ctc
2024-10-27 18:35:55,263 (beam_search:479) INFO: total log probability: -3.78
2024-10-27 18:35:55,263 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:35:55,263 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:55,263 (beam_search:483) INFO: best hypo: ITGOINGTHROUGHANDITANDIT'SGOINGINTOTHEPOSITIVEANDOUTTHROUGHTHE

2024-10-27 18:35:55,266 (asr_inference:509) INFO: speech length: 201024
2024-10-27 18:36:03,494 (beam_search:428) INFO: decoder input length: 156
2024-10-27 18:36:03,494 (beam_search:429) INFO: max output length: 156
2024-10-27 18:36:03,494 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:03,669 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:03,669 (beam_search:476) INFO:  -1.11 * 1.0 =  -1.11 for ctc
2024-10-27 18:36:03,669 (beam_search:479) INFO: total log probability: -1.11
2024-10-27 18:36:03,669 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:36:03,669 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:03,669 (beam_search:483) INFO: best hypo: THEENERGYGOESTOTOIT

2024-10-27 18:36:03,672 (asr_inference:509) INFO: speech length: 193133
2024-10-27 18:36:11,310 (beam_search:428) INFO: decoder input length: 150
2024-10-27 18:36:11,310 (beam_search:429) INFO: max output length: 150
2024-10-27 18:36:11,310 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:11,683 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:11,684 (beam_search:476) INFO:  -2.84 * 1.0 =  -2.84 for ctc
2024-10-27 18:36:11,684 (beam_search:479) INFO: total log probability: -2.84
2024-10-27 18:36:11,684 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:36:11,684 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:11,684 (beam_search:483) INFO: best hypo: THEITISGOINGINTHROUGHTHEPOSITIVEANDOUTTHROUGHTHENEGATIVE

2024-10-27 18:36:11,687 (asr_inference:509) INFO: speech length: 25123
2024-10-27 18:36:12,802 (beam_search:428) INFO: decoder input length: 19
2024-10-27 18:36:12,802 (beam_search:429) INFO: max output length: 19
2024-10-27 18:36:12,802 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:12,819 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:12,819 (beam_search:476) INFO:  -0.14 * 1.0 =  -0.14 for ctc
2024-10-27 18:36:12,819 (beam_search:479) INFO: total log probability: -0.14
2024-10-27 18:36:12,819 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:36:12,819 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:12,819 (beam_search:483) INFO: best hypo: WHATDIDYOUSAY

2024-10-27 18:36:12,821 (asr_inference:509) INFO: speech length: 121425
2024-10-27 18:36:17,247 (beam_search:428) INFO: decoder input length: 94
2024-10-27 18:36:17,247 (beam_search:429) INFO: max output length: 94
2024-10-27 18:36:17,247 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:17,479 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:17,479 (beam_search:476) INFO:  -2.29 * 1.0 =  -2.29 for ctc
2024-10-27 18:36:17,479 (beam_search:479) INFO: total log probability: -2.29
2024-10-27 18:36:17,479 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:36:17,480 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:17,480 (beam_search:483) INFO: best hypo: ITHATITGOESINTHROUGHTHEANDOUTTHROUGHTHEYOU'RETHE

2024-10-27 18:36:17,483 (asr_inference:509) INFO: speech length: 122257
2024-10-27 18:36:21,854 (beam_search:428) INFO: decoder input length: 95
2024-10-27 18:36:21,855 (beam_search:429) INFO: max output length: 95
2024-10-27 18:36:21,855 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:21,915 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:21,916 (beam_search:476) INFO:  -1.08 * 1.0 =  -1.08 for ctc
2024-10-27 18:36:21,916 (beam_search:479) INFO: total log probability: -1.08
2024-10-27 18:36:21,916 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:36:21,916 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:21,916 (beam_search:483) INFO: best hypo: THEREARETWO

2024-10-27 18:36:21,918 (asr_inference:509) INFO: speech length: 62439
2024-10-27 18:36:24,220 (beam_search:428) INFO: decoder input length: 48
2024-10-27 18:36:24,220 (beam_search:429) INFO: max output length: 48
2024-10-27 18:36:24,220 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:24,258 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:24,258 (beam_search:476) INFO:  -1.07 * 1.0 =  -1.07 for ctc
2024-10-27 18:36:24,258 (beam_search:479) INFO: total log probability: -1.07
2024-10-27 18:36:24,258 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:36:24,258 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:24,258 (beam_search:483) INFO: best hypo: ITHATTHEREARE

2024-10-27 18:36:24,260 (asr_inference:509) INFO: speech length: 168121
2024-10-27 18:36:30,649 (beam_search:428) INFO: decoder input length: 130
2024-10-27 18:36:30,649 (beam_search:429) INFO: max output length: 130
2024-10-27 18:36:30,649 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:30,838 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:30,838 (beam_search:476) INFO:  -1.79 * 1.0 =  -1.79 for ctc
2024-10-27 18:36:30,838 (beam_search:479) INFO: total log probability: -1.79
2024-10-27 18:36:30,838 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:36:30,838 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:30,838 (beam_search:483) INFO: best hypo: ITOLDYOUAATIMESARETWO

2024-10-27 18:36:30,840 (asr_inference:509) INFO: speech length: 68331
2024-10-27 18:36:33,392 (beam_search:428) INFO: decoder input length: 52
2024-10-27 18:36:33,392 (beam_search:429) INFO: max output length: 52
2024-10-27 18:36:33,392 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:33,418 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:33,418 (beam_search:476) INFO:  -0.38 * 1.0 =  -0.38 for ctc
2024-10-27 18:36:33,418 (beam_search:479) INFO: total log probability: -0.38
2024-10-27 18:36:33,418 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:36:33,418 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:33,418 (beam_search:483) INFO: best hypo: ITOUT

2024-10-27 18:36:33,420 (asr_inference:509) INFO: speech length: 80324
2024-10-27 18:36:36,374 (beam_search:428) INFO: decoder input length: 62
2024-10-27 18:36:36,374 (beam_search:429) INFO: max output length: 62
2024-10-27 18:36:36,375 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:36,407 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:36,408 (beam_search:476) INFO:  -0.09 * 1.0 =  -0.09 for ctc
2024-10-27 18:36:36,408 (beam_search:479) INFO: total log probability: -0.09
2024-10-27 18:36:36,408 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:36:36,408 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:36,408 (beam_search:483) INFO: best hypo: THEYARE

2024-10-27 18:36:36,410 (asr_inference:509) INFO: speech length: 139617
2024-10-27 18:36:41,493 (beam_search:428) INFO: decoder input length: 108
2024-10-27 18:36:41,494 (beam_search:429) INFO: max output length: 108
2024-10-27 18:36:41,494 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:41,845 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:41,846 (beam_search:476) INFO:  -2.05 * 1.0 =  -2.05 for ctc
2024-10-27 18:36:41,847 (beam_search:479) INFO: total log probability: -2.05
2024-10-27 18:36:41,848 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:36:41,848 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:41,848 (beam_search:483) INFO: best hypo: BECAUSEITHASTOWITHBOTHOFTHEMANDITDOESN'TGOITONLYONE

2024-10-27 18:36:41,850 (asr_inference:509) INFO: speech length: 93220
2024-10-27 18:36:45,292 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:36:45,292 (beam_search:429) INFO: max output length: 72
2024-10-27 18:36:45,292 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:45,361 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:45,361 (beam_search:476) INFO:  -1.84 * 1.0 =  -1.84 for ctc
2024-10-27 18:36:45,361 (beam_search:479) INFO: total log probability: -1.84
2024-10-27 18:36:45,361 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:36:45,361 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:45,361 (beam_search:483) INFO: best hypo: THEYDONOTHAVEA

2024-10-27 18:36:45,363 (asr_inference:509) INFO: speech length: 70136
2024-10-27 18:36:47,980 (beam_search:428) INFO: decoder input length: 54
2024-10-27 18:36:47,981 (beam_search:429) INFO: max output length: 54
2024-10-27 18:36:47,981 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:48,001 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:48,001 (beam_search:476) INFO:  -0.43 * 1.0 =  -0.43 for ctc
2024-10-27 18:36:48,001 (beam_search:479) INFO: total log probability: -0.43
2024-10-27 18:36:48,001 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:36:48,001 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:48,001 (beam_search:483) INFO: best hypo: THE

2024-10-27 18:36:48,004 (asr_inference:509) INFO: speech length: 32500
2024-10-27 18:36:49,265 (beam_search:428) INFO: decoder input length: 24
2024-10-27 18:36:49,265 (beam_search:429) INFO: max output length: 24
2024-10-27 18:36:49,265 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:49,288 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:49,288 (beam_search:476) INFO:  -0.63 * 1.0 =  -0.63 for ctc
2024-10-27 18:36:49,289 (beam_search:479) INFO: total log probability: -0.63
2024-10-27 18:36:49,289 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:36:49,289 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:49,289 (beam_search:483) INFO: best hypo: IGOODHOWAREYOU

2024-10-27 18:36:49,291 (asr_inference:509) INFO: speech length: 43423
2024-10-27 18:36:51,019 (beam_search:428) INFO: decoder input length: 33
2024-10-27 18:36:51,019 (beam_search:429) INFO: max output length: 33
2024-10-27 18:36:51,019 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:51,062 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:51,062 (beam_search:476) INFO:  -2.92 * 1.0 =  -2.92 for ctc
2024-10-27 18:36:51,062 (beam_search:479) INFO: total log probability: -2.92
2024-10-27 18:36:51,062 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 18:36:51,062 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:51,062 (beam_search:483) INFO: best hypo: THEYTHEYARETWOSANDTHEY

2024-10-27 18:36:51,065 (asr_inference:509) INFO: speech length: 64245
2024-10-27 18:36:53,393 (beam_search:428) INFO: decoder input length: 49
2024-10-27 18:36:53,394 (beam_search:429) INFO: max output length: 49
2024-10-27 18:36:53,394 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:53,469 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:53,470 (beam_search:476) INFO:  -2.94 * 1.0 =  -2.94 for ctc
2024-10-27 18:36:53,470 (beam_search:479) INFO: total log probability: -2.94
2024-10-27 18:36:53,470 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:36:53,470 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:53,470 (beam_search:483) INFO: best hypo: THEREISONETHEREISONEDANDTWO

2024-10-27 18:36:53,472 (asr_inference:509) INFO: speech length: 51616
2024-10-27 18:36:55,490 (beam_search:428) INFO: decoder input length: 39
2024-10-27 18:36:55,490 (beam_search:429) INFO: max output length: 39
2024-10-27 18:36:55,490 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:55,544 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:55,545 (beam_search:476) INFO:  -2.73 * 1.0 =  -2.73 for ctc
2024-10-27 18:36:55,545 (beam_search:479) INFO: total log probability: -2.73
2024-10-27 18:36:55,545 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:36:55,545 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:55,545 (beam_search:483) INFO: best hypo: EACHLIGHTBULBHASITSPATHWAYARE

2024-10-27 18:36:55,547 (asr_inference:509) INFO: speech length: 67303
2024-10-27 18:36:58,110 (beam_search:428) INFO: decoder input length: 52
2024-10-27 18:36:58,110 (beam_search:429) INFO: max output length: 52
2024-10-27 18:36:58,110 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:58,146 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:58,146 (beam_search:476) INFO:  -0.93 * 1.0 =  -0.93 for ctc
2024-10-27 18:36:58,146 (beam_search:479) INFO: total log probability: -0.93
2024-10-27 18:36:58,146 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:36:58,146 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:58,146 (beam_search:483) INFO: best hypo: THEYHAVETHEIR

2024-10-27 18:36:58,148 (asr_inference:509) INFO: speech length: 149585
2024-10-27 18:37:03,611 (beam_search:428) INFO: decoder input length: 116
2024-10-27 18:37:03,611 (beam_search:429) INFO: max output length: 116
2024-10-27 18:37:03,611 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:03,685 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:03,685 (beam_search:476) INFO:  -2.08 * 1.0 =  -2.08 for ctc
2024-10-27 18:37:03,685 (beam_search:479) INFO: total log probability: -2.08
2024-10-27 18:37:03,685 (beam_search:480) INFO: normalized log probability: -0.42
2024-10-27 18:37:03,685 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:03,685 (beam_search:483) INFO: best hypo: THEYAREALL

2024-10-27 18:37:03,687 (asr_inference:509) INFO: speech length: 65098
2024-10-27 18:37:06,060 (beam_search:428) INFO: decoder input length: 50
2024-10-27 18:37:06,060 (beam_search:429) INFO: max output length: 50
2024-10-27 18:37:06,060 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:06,082 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:06,082 (beam_search:476) INFO:  -0.82 * 1.0 =  -0.82 for ctc
2024-10-27 18:37:06,082 (beam_search:479) INFO: total log probability: -0.82
2024-10-27 18:37:06,082 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:37:06,082 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:06,082 (beam_search:483) INFO: best hypo: IS

2024-10-27 18:37:06,085 (asr_inference:509) INFO: speech length: 182669
2024-10-27 18:37:13,459 (beam_search:428) INFO: decoder input length: 142
2024-10-27 18:37:13,460 (beam_search:429) INFO: max output length: 142
2024-10-27 18:37:13,460 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:13,929 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:13,929 (beam_search:476) INFO:  -3.79 * 1.0 =  -3.79 for ctc
2024-10-27 18:37:13,929 (beam_search:479) INFO: total log probability: -3.79
2024-10-27 18:37:13,929 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:37:13,929 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:13,929 (beam_search:483) INFO: best hypo: WE'VEBEENWITHANDCIRCUITSI'VETOLDYOULIKEATIMESYOUARETHEMOST

2024-10-27 18:37:13,931 (asr_inference:509) INFO: speech length: 61371
2024-10-27 18:37:16,304 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:37:16,305 (beam_search:429) INFO: max output length: 47
2024-10-27 18:37:16,305 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:16,362 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:16,362 (beam_search:476) INFO:  -1.54 * 1.0 =  -1.54 for ctc
2024-10-27 18:37:16,362 (beam_search:479) INFO: total log probability: -1.54
2024-10-27 18:37:16,362 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:37:16,362 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:16,362 (beam_search:483) INFO: best hypo: ITHINKTHATITWILLBECOMEBRIGHTER

2024-10-27 18:37:16,364 (asr_inference:509) INFO: speech length: 56894
2024-10-27 18:37:18,701 (beam_search:428) INFO: decoder input length: 43
2024-10-27 18:37:18,701 (beam_search:429) INFO: max output length: 43
2024-10-27 18:37:18,701 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:18,751 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:18,751 (beam_search:476) INFO:  -1.12 * 1.0 =  -1.12 for ctc
2024-10-27 18:37:18,751 (beam_search:479) INFO: total log probability: -1.12
2024-10-27 18:37:18,751 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:37:18,751 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:18,751 (beam_search:483) INFO: best hypo: PARALLELCIRCUITSAREYOUARETHE

2024-10-27 18:37:18,754 (asr_inference:509) INFO: speech length: 66717
2024-10-27 18:37:21,423 (beam_search:428) INFO: decoder input length: 51
2024-10-27 18:37:21,423 (beam_search:429) INFO: max output length: 51
2024-10-27 18:37:21,423 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:21,463 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:21,463 (beam_search:476) INFO:  -0.76 * 1.0 =  -0.76 for ctc
2024-10-27 18:37:21,463 (beam_search:479) INFO: total log probability: -0.76
2024-10-27 18:37:21,463 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:37:21,463 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:21,463 (beam_search:483) INFO: best hypo: THEYAREGOODTHEY

2024-10-27 18:37:21,466 (asr_inference:509) INFO: speech length: 71637
2024-10-27 18:37:24,260 (beam_search:428) INFO: decoder input length: 55
2024-10-27 18:37:24,260 (beam_search:429) INFO: max output length: 55
2024-10-27 18:37:24,260 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:24,331 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:24,331 (beam_search:476) INFO:  -0.84 * 1.0 =  -0.84 for ctc
2024-10-27 18:37:24,331 (beam_search:479) INFO: total log probability: -0.84
2024-10-27 18:37:24,331 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:37:24,331 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:24,331 (beam_search:483) INFO: best hypo: THEYARELIKETHEFLOWOFELECTRICITY

2024-10-27 18:37:24,334 (asr_inference:509) INFO: speech length: 112694
2024-10-27 18:37:28,579 (beam_search:428) INFO: decoder input length: 87
2024-10-27 18:37:28,579 (beam_search:429) INFO: max output length: 87
2024-10-27 18:37:28,579 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:28,623 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:28,623 (beam_search:476) INFO:  -0.18 * 1.0 =  -0.18 for ctc
2024-10-27 18:37:28,623 (beam_search:479) INFO: total log probability: -0.18
2024-10-27 18:37:28,623 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 18:37:28,623 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:28,623 (beam_search:483) INFO: best hypo: ITIS

2024-10-27 18:37:28,625 (asr_inference:509) INFO: speech length: 109478
2024-10-27 18:37:32,767 (beam_search:428) INFO: decoder input length: 85
2024-10-27 18:37:32,767 (beam_search:429) INFO: max output length: 85
2024-10-27 18:37:32,767 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:32,924 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:32,924 (beam_search:476) INFO:  -1.52 * 1.0 =  -1.52 for ctc
2024-10-27 18:37:32,924 (beam_search:479) INFO: total log probability: -1.52
2024-10-27 18:37:32,924 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:37:32,924 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:32,924 (beam_search:483) INFO: best hypo: THEISGOINGINTHROUGHTHEANDOUTTHROUGHTHE

2024-10-27 18:37:32,927 (asr_inference:509) INFO: speech length: 27513
2024-10-27 18:37:34,165 (beam_search:428) INFO: decoder input length: 20
2024-10-27 18:37:34,166 (beam_search:429) INFO: max output length: 20
2024-10-27 18:37:34,166 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:34,185 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:34,185 (beam_search:476) INFO:  -0.15 * 1.0 =  -0.15 for ctc
2024-10-27 18:37:34,185 (beam_search:479) INFO: total log probability: -0.15
2024-10-27 18:37:34,185 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:37:34,185 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:34,185 (beam_search:483) INFO: best hypo: WHATDIDYOUSAY

2024-10-27 18:37:34,187 (asr_inference:509) INFO: speech length: 117593
2024-10-27 18:37:38,625 (beam_search:428) INFO: decoder input length: 91
2024-10-27 18:37:38,626 (beam_search:429) INFO: max output length: 91
2024-10-27 18:37:38,626 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:38,831 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:38,831 (beam_search:476) INFO:  -2.18 * 1.0 =  -2.18 for ctc
2024-10-27 18:37:38,831 (beam_search:479) INFO: total log probability: -2.18
2024-10-27 18:37:38,831 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:37:38,831 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:38,831 (beam_search:483) INFO: best hypo: THEITISGOINGINTHROUGHTHEPOSITIVEANDOUTTHROUGHTHE

2024-10-27 18:37:38,834 (asr_inference:509) INFO: speech length: 59813
2024-10-27 18:37:41,270 (beam_search:428) INFO: decoder input length: 46
2024-10-27 18:37:41,270 (beam_search:429) INFO: max output length: 46
2024-10-27 18:37:41,270 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:41,336 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:41,336 (beam_search:476) INFO:  -0.71 * 1.0 =  -0.71 for ctc
2024-10-27 18:37:41,336 (beam_search:479) INFO: total log probability: -0.71
2024-10-27 18:37:41,336 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:37:41,336 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:41,336 (beam_search:483) INFO: best hypo: ITOLDYOUITGOESOUTTHROUGHTHE

2024-10-27 18:37:41,339 (asr_inference:509) INFO: speech length: 55747
2024-10-27 18:37:43,719 (beam_search:428) INFO: decoder input length: 43
2024-10-27 18:37:43,719 (beam_search:429) INFO: max output length: 43
2024-10-27 18:37:43,719 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:43,773 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:43,773 (beam_search:476) INFO:  -1.62 * 1.0 =  -1.62 for ctc
2024-10-27 18:37:43,774 (beam_search:479) INFO: total log probability: -1.62
2024-10-27 18:37:43,774 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:37:43,774 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:43,774 (beam_search:483) INFO: best hypo: INOTYOUSOPLEASEITAGAIN

2024-10-27 18:37:43,776 (asr_inference:509) INFO: speech length: 88394
2024-10-27 18:37:47,466 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:37:47,466 (beam_search:429) INFO: max output length: 68
2024-10-27 18:37:47,466 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:47,534 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:47,535 (beam_search:476) INFO:  -1.50 * 1.0 =  -1.50 for ctc
2024-10-27 18:37:47,535 (beam_search:479) INFO: total log probability: -1.50
2024-10-27 18:37:47,535 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:37:47,535 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:47,535 (beam_search:483) INFO: best hypo: ARETWOANDTHEYARE

2024-10-27 18:37:47,538 (asr_inference:509) INFO: speech length: 70048
2024-10-27 18:37:50,466 (beam_search:428) INFO: decoder input length: 54
2024-10-27 18:37:50,467 (beam_search:429) INFO: max output length: 54
2024-10-27 18:37:50,467 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:50,531 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:50,531 (beam_search:476) INFO:  -3.44 * 1.0 =  -3.44 for ctc
2024-10-27 18:37:50,531 (beam_search:479) INFO: total log probability: -3.44
2024-10-27 18:37:50,531 (beam_search:480) INFO: normalized log probability: -0.43
2024-10-27 18:37:50,531 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:50,531 (beam_search:483) INFO: best hypo: THEREAREPATHWAYSINAPARALLEL

2024-10-27 18:37:50,533 (asr_inference:509) INFO: speech length: 91043
2024-10-27 18:37:54,439 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:37:54,439 (beam_search:429) INFO: max output length: 70
2024-10-27 18:37:54,439 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:54,469 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:54,470 (beam_search:476) INFO:  -0.67 * 1.0 =  -0.67 for ctc
2024-10-27 18:37:54,470 (beam_search:479) INFO: total log probability: -0.67
2024-10-27 18:37:54,470 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:37:54,470 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:54,470 (beam_search:483) INFO: best hypo: IT

2024-10-27 18:37:54,472 (asr_inference:509) INFO: speech length: 93932
2024-10-27 18:37:58,322 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:37:58,323 (beam_search:429) INFO: max output length: 72
2024-10-27 18:37:58,323 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:58,363 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:58,363 (beam_search:476) INFO:  -0.61 * 1.0 =  -0.61 for ctc
2024-10-27 18:37:58,363 (beam_search:479) INFO: total log probability: -0.61
2024-10-27 18:37:58,363 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:37:58,363 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:58,363 (beam_search:483) INFO: best hypo: ARETWO

2024-10-27 18:37:58,366 (asr_inference:509) INFO: speech length: 92708
2024-10-27 18:38:02,876 (beam_search:428) INFO: decoder input length: 71
2024-10-27 18:38:02,876 (beam_search:429) INFO: max output length: 71
2024-10-27 18:38:02,876 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:02,976 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:02,976 (beam_search:476) INFO:  -2.29 * 1.0 =  -2.29 for ctc
2024-10-27 18:38:02,976 (beam_search:479) INFO: total log probability: -2.29
2024-10-27 18:38:02,976 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:38:02,976 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:02,976 (beam_search:483) INFO: best hypo: THEONTHEBOTTOMISOUT

2024-10-27 18:38:02,979 (asr_inference:509) INFO: speech length: 91472
2024-10-27 18:38:07,431 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:38:07,431 (beam_search:429) INFO: max output length: 70
2024-10-27 18:38:07,431 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:07,522 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:07,522 (beam_search:476) INFO:  -1.00 * 1.0 =  -1.00 for ctc
2024-10-27 18:38:07,522 (beam_search:479) INFO: total log probability: -1.00
2024-10-27 18:38:07,522 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:38:07,522 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:07,522 (beam_search:483) INFO: best hypo: THEAREBRIGHTERTHANTHEBOTTOM

2024-10-27 18:38:07,525 (asr_inference:509) INFO: speech length: 79537
2024-10-27 18:38:10,807 (beam_search:428) INFO: decoder input length: 61
2024-10-27 18:38:10,808 (beam_search:429) INFO: max output length: 61
2024-10-27 18:38:10,808 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:10,872 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:10,872 (beam_search:476) INFO:  -0.49 * 1.0 =  -0.49 for ctc
2024-10-27 18:38:10,872 (beam_search:479) INFO: total log probability: -0.49
2024-10-27 18:38:10,872 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:38:10,872 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:10,872 (beam_search:483) INFO: best hypo: BECAUSEITHASTOTHE

2024-10-27 18:38:10,874 (asr_inference:509) INFO: speech length: 69142
2024-10-27 18:38:13,659 (beam_search:428) INFO: decoder input length: 53
2024-10-27 18:38:13,659 (beam_search:429) INFO: max output length: 53
2024-10-27 18:38:13,659 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:13,733 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:13,733 (beam_search:476) INFO:  -0.62 * 1.0 =  -0.62 for ctc
2024-10-27 18:38:13,733 (beam_search:479) INFO: total log probability: -0.62
2024-10-27 18:38:13,733 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:38:13,733 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:13,734 (beam_search:483) INFO: best hypo: THEYHAVETOAPATHWAYANDA

2024-10-27 18:38:13,736 (asr_inference:509) INFO: speech length: 57462
2024-10-27 18:38:16,291 (beam_search:428) INFO: decoder input length: 44
2024-10-27 18:38:16,291 (beam_search:429) INFO: max output length: 44
2024-10-27 18:38:16,291 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:16,356 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:16,356 (beam_search:476) INFO:  -2.59 * 1.0 =  -2.59 for ctc
2024-10-27 18:38:16,356 (beam_search:479) INFO: total log probability: -2.59
2024-10-27 18:38:16,356 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:38:16,356 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:16,356 (beam_search:483) INFO: best hypo: THEREARETWOANDBOTHOFTHEM

2024-10-27 18:38:16,359 (asr_inference:509) INFO: speech length: 189147
2024-10-27 18:38:25,965 (beam_search:428) INFO: decoder input length: 147
2024-10-27 18:38:25,965 (beam_search:429) INFO: max output length: 147
2024-10-27 18:38:25,965 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:26,268 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:26,268 (beam_search:476) INFO:  -2.18 * 1.0 =  -2.18 for ctc
2024-10-27 18:38:26,270 (beam_search:479) INFO: total log probability: -2.18
2024-10-27 18:38:26,270 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:38:26,270 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:26,270 (beam_search:483) INFO: best hypo: ITHINKTHATTHEBETHEOFORTHECIRCUITOF

2024-10-27 18:38:26,283 (asr_inference:509) INFO: speech length: 97688
2024-10-27 18:38:30,165 (beam_search:428) INFO: decoder input length: 75
2024-10-27 18:38:30,165 (beam_search:429) INFO: max output length: 75
2024-10-27 18:38:30,165 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:30,367 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:30,367 (beam_search:476) INFO:  -2.19 * 1.0 =  -2.19 for ctc
2024-10-27 18:38:30,367 (beam_search:479) INFO: total log probability: -2.19
2024-10-27 18:38:30,367 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:38:30,367 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:30,367 (beam_search:483) INFO: best hypo: THINKTHATWENEEDTOMAKETHATTHESARESOWENEEDTHECIRCUIT

2024-10-27 18:38:30,380 (asr_inference:509) INFO: speech length: 44251
2024-10-27 18:38:32,350 (beam_search:428) INFO: decoder input length: 34
2024-10-27 18:38:32,350 (beam_search:429) INFO: max output length: 34
2024-10-27 18:38:32,350 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:32,366 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:32,366 (beam_search:476) INFO:  -0.27 * 1.0 =  -0.27 for ctc
2024-10-27 18:38:32,366 (beam_search:479) INFO: total log probability: -0.27
2024-10-27 18:38:32,366 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:38:32,366 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:32,367 (beam_search:483) INFO: best hypo: THE

2024-10-27 18:38:32,369 (asr_inference:509) INFO: speech length: 86000
2024-10-27 18:38:35,758 (beam_search:428) INFO: decoder input length: 66
2024-10-27 18:38:35,758 (beam_search:429) INFO: max output length: 66
2024-10-27 18:38:35,758 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:35,810 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:35,810 (beam_search:476) INFO:  -1.63 * 1.0 =  -1.63 for ctc
2024-10-27 18:38:35,810 (beam_search:479) INFO: total log probability: -1.63
2024-10-27 18:38:35,810 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:38:35,810 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:35,810 (beam_search:483) INFO: best hypo: GOODAREYOUMY

2024-10-27 18:38:35,813 (asr_inference:509) INFO: speech length: 162552
2024-10-27 18:38:42,645 (beam_search:428) INFO: decoder input length: 126
2024-10-27 18:38:42,645 (beam_search:429) INFO: max output length: 126
2024-10-27 18:38:42,645 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:42,798 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:42,798 (beam_search:476) INFO:  -2.47 * 1.0 =  -2.47 for ctc
2024-10-27 18:38:42,798 (beam_search:479) INFO: total log probability: -2.47
2024-10-27 18:38:42,798 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 18:38:42,798 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:42,798 (beam_search:483) INFO: best hypo: WEVEDOINGSTUFFLIKEAND

2024-10-27 18:38:42,800 (asr_inference:509) INFO: speech length: 124856
2024-10-27 18:38:47,900 (beam_search:428) INFO: decoder input length: 97
2024-10-27 18:38:47,900 (beam_search:429) INFO: max output length: 97
2024-10-27 18:38:47,900 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:47,988 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:47,989 (beam_search:476) INFO:  -1.59 * 1.0 =  -1.59 for ctc
2024-10-27 18:38:47,989 (beam_search:479) INFO: total log probability: -1.59
2024-10-27 18:38:47,989 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:38:47,989 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:47,989 (beam_search:483) INFO: best hypo: IOUTTHATSTICK

2024-10-27 18:38:47,991 (asr_inference:509) INFO: speech length: 78215
2024-10-27 18:38:50,983 (beam_search:428) INFO: decoder input length: 60
2024-10-27 18:38:50,983 (beam_search:429) INFO: max output length: 60
2024-10-27 18:38:50,983 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:51,021 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:51,022 (beam_search:476) INFO:  -0.46 * 1.0 =  -0.46 for ctc
2024-10-27 18:38:51,022 (beam_search:479) INFO: total log probability: -0.46
2024-10-27 18:38:51,022 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:38:51,022 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:51,022 (beam_search:483) INFO: best hypo: ANDTHEAND

2024-10-27 18:38:51,024 (asr_inference:509) INFO: speech length: 91632
2024-10-27 18:38:54,562 (beam_search:428) INFO: decoder input length: 71
2024-10-27 18:38:54,562 (beam_search:429) INFO: max output length: 71
2024-10-27 18:38:54,562 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:54,650 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:54,650 (beam_search:476) INFO:  -2.12 * 1.0 =  -2.12 for ctc
2024-10-27 18:38:54,651 (beam_search:479) INFO: total log probability: -2.12
2024-10-27 18:38:54,651 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:38:54,651 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:54,651 (beam_search:483) INFO: best hypo: THEMAGNETSARESTICKINGTOTHE

2024-10-27 18:38:54,664 (asr_inference:509) INFO: speech length: 66184
2024-10-27 18:38:57,493 (beam_search:428) INFO: decoder input length: 51
2024-10-27 18:38:57,493 (beam_search:429) INFO: max output length: 51
2024-10-27 18:38:57,493 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:57,519 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:57,519 (beam_search:476) INFO:  -0.37 * 1.0 =  -0.37 for ctc
2024-10-27 18:38:57,519 (beam_search:479) INFO: total log probability: -0.37
2024-10-27 18:38:57,519 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:38:57,520 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:57,520 (beam_search:483) INFO: best hypo: THEIS

2024-10-27 18:38:57,522 (asr_inference:509) INFO: speech length: 114592
2024-10-27 18:39:02,426 (beam_search:428) INFO: decoder input length: 89
2024-10-27 18:39:02,426 (beam_search:429) INFO: max output length: 89
2024-10-27 18:39:02,426 (beam_search:430) INFO: min output length: 0
2024-10-27 18:39:02,491 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:39:02,491 (beam_search:476) INFO:  -1.50 * 1.0 =  -1.50 for ctc
2024-10-27 18:39:02,491 (beam_search:479) INFO: total log probability: -1.50
2024-10-27 18:39:02,491 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 18:39:02,491 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:39:02,492 (beam_search:483) INFO: best hypo: ISSTICKINGTHE

2024-10-27 18:39:02,494 (asr_inference:509) INFO: speech length: 313636
2024-10-27 18:58:01,685 (beam_search:428) INFO: decoder input length: 244
2024-10-27 18:58:01,730 (beam_search:429) INFO: max output length: 244
2024-10-27 18:58:01,730 (beam_search:430) INFO: min output length: 0
2024-10-27 18:58:02,503 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:58:02,503 (beam_search:476) INFO:  -9.74 * 1.0 =  -9.74 for ctc
2024-10-27 18:58:02,503 (beam_search:479) INFO: total log probability: -9.74
2024-10-27 18:58:02,503 (beam_search:480) INFO: normalized log probability: -0.54
2024-10-27 18:58:02,503 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:58:02,503 (beam_search:483) INFO: best hypo: BECAUSEITTHEISANDTHETHEY'REOFTHEYOFANDTHEYTOEACH

2024-10-27 18:58:02,508 (asr_inference:509) INFO: speech length: 118898
2024-10-27 18:58:07,236 (beam_search:428) INFO: decoder input length: 92
2024-10-27 18:58:07,237 (beam_search:429) INFO: max output length: 92
2024-10-27 18:58:07,237 (beam_search:430) INFO: min output length: 0
2024-10-27 18:58:07,337 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:58:07,337 (beam_search:476) INFO:  -0.19 * 1.0 =  -0.19 for ctc
2024-10-27 18:58:07,337 (beam_search:479) INFO: total log probability: -0.19
2024-10-27 18:58:07,337 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:58:07,338 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:58:07,338 (beam_search:483) INFO: best hypo: ITISMADEOFNOT

2024-10-27 18:58:07,341 (asr_inference:509) INFO: speech length: 156670
2024-10-27 18:58:13,933 (beam_search:428) INFO: decoder input length: 121
2024-10-27 18:58:13,933 (beam_search:429) INFO: max output length: 121
2024-10-27 18:58:13,933 (beam_search:430) INFO: min output length: 0
2024-10-27 18:58:14,014 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:58:14,014 (beam_search:476) INFO:  -1.50 * 1.0 =  -1.50 for ctc
2024-10-27 18:58:14,014 (beam_search:479) INFO: total log probability: -1.50
2024-10-27 18:58:14,014 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 18:58:14,014 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:58:14,014 (beam_search:483) INFO: best hypo: WOODPLASTICYEAH

2024-10-27 18:58:14,017 (asr_inference:509) INFO: speech length: 117056
2024-10-27 18:58:18,561 (beam_search:428) INFO: decoder input length: 90
2024-10-27 18:58:18,562 (beam_search:429) INFO: max output length: 90
2024-10-27 18:58:18,562 (beam_search:430) INFO: min output length: 0
2024-10-27 18:58:18,692 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:58:18,693 (beam_search:476) INFO:  -2.54 * 1.0 =  -2.54 for ctc
2024-10-27 18:58:18,693 (beam_search:479) INFO: total log probability: -2.54
2024-10-27 18:58:18,693 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:58:18,693 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:58:18,693 (beam_search:483) INFO: best hypo: THATAREEITHERMADEOFIRONOR

2024-10-27 18:58:18,705 (asr_inference:509) INFO: speech length: 91500
2024-10-27 18:58:22,405 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:58:22,405 (beam_search:429) INFO: max output length: 70
2024-10-27 18:58:22,405 (beam_search:430) INFO: min output length: 0
2024-10-27 18:58:22,512 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:58:22,512 (beam_search:476) INFO:  -4.17 * 1.0 =  -4.17 for ctc
2024-10-27 18:58:22,512 (beam_search:479) INFO: total log probability: -4.17
2024-10-27 18:58:22,512 (beam_search:480) INFO: normalized log probability: -0.38
2024-10-27 18:58:22,512 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:58:22,512 (beam_search:483) INFO: best hypo: THEWOOD'HAVESOSTICKTOTHEMAGNET

2024-10-27 18:58:22,525 (asr_inference:509) INFO: speech length: 187278
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 843, in inference
    results = speech2text(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 515, in __call__
    enc, enc_olens = self.asr_model.encode(**batch)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 378, in encode
    feats, feats_lengths = self._extract_feats(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 445, in _extract_feats
    feats, feats_lengths = self.frontend(speech, speech_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/frontend/s3prl.py", line 99, in forward
    feats, feats_lens = self.upstream(input, input_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/nn/upstream.py", line 209, in forward
    hidden_states = self.upstream(wavs_list)["hidden_states"]
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/interfaces.py", line 103, in __call__
    result = super().__call__(wavs, *args, **kwargs) or {}
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/expert.py", line 83, in forward
    features, feat_padding_mask = self.model.extract_features(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 389, in extract_features
    x, layer_results = self.encoder(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 592, in forward
    x, layer_results = self.extract_features(x, padding_mask, streaming_mask, layer)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 626, in extract_features
    x, z, pos_bias = layer(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 742, in forward
    x = self.activation_fn(self.fc1(x))
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1498924) is killed by signal: Killed. 
# Accounting: time=5276 threads=1
# Ended (code 1) at Sun Oct 27 18:58:53 EDT 2024, elapsed time 5276 seconds
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/keys.8.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.cer_ctc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/output.8 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:30:57 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/keys.8.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.cer_ctc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/output.8 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:31:05,875 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:31:06,030 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:31:06,098 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:31:06,099 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:31:06,982 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:31:16,348 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
2024-10-27 17:31:18,834 (asr_inference:372) INFO: BatchBeamSearch implementation is selected.
2024-10-27 17:31:18,834 (asr_inference:383) INFO: Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict()
)
2024-10-27 17:31:18,834 (asr_inference:384) INFO: Decoding device=cpu, dtype=float32
2024-10-27 17:31:18,836 (asr_inference:462) INFO: Text tokenizer: SentencepiecesTokenizer(model="data/en_token_list/bpe_unigram5000/bpe.model")
2024-10-27 17:31:18,838 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
2024-10-27 17:31:20,135 (asr_inference:509) INFO: speech length: 211248
2024-10-27 17:31:28,809 (beam_search:428) INFO: decoder input length: 164
2024-10-27 17:31:28,809 (beam_search:429) INFO: max output length: 164
2024-10-27 17:31:28,809 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:29,886 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:29,887 (beam_search:476) INFO:  -4.83 * 1.0 =  -4.83 for ctc
2024-10-27 17:31:29,887 (beam_search:479) INFO: total log probability: -4.83
2024-10-27 17:31:29,887 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:31:29,887 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:29,887 (beam_search:483) INFO: best hypo: BECAUSETHEMAGNETHASENOUGHENERGYTHATITSENERGYISTHANTHEPAPERSOWHENYOUPUTTHEMAGNETONYOUPUTTHEPAPERIT'STILLSTICKBECAUSEOFTHEMAGNET'S

2024-10-27 17:31:29,895 (asr_inference:509) INFO: speech length: 17152
2024-10-27 17:31:30,809 (beam_search:428) INFO: decoder input length: 12
2024-10-27 17:31:30,809 (beam_search:429) INFO: max output length: 12
2024-10-27 17:31:30,809 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:30,822 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:30,822 (beam_search:476) INFO:  -0.06 * 1.0 =  -0.06 for ctc
2024-10-27 17:31:30,822 (beam_search:479) INFO: total log probability: -0.06
2024-10-27 17:31:30,822 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 17:31:30,822 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:30,822 (beam_search:483) INFO: best hypo: IDON'TKNOW

2024-10-27 17:31:30,824 (asr_inference:509) INFO: speech length: 138560
2024-10-27 17:31:36,142 (beam_search:428) INFO: decoder input length: 107
2024-10-27 17:31:36,143 (beam_search:429) INFO: max output length: 107
2024-10-27 17:31:36,143 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:36,498 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:36,498 (beam_search:476) INFO:  -3.28 * 1.0 =  -3.28 for ctc
2024-10-27 17:31:36,498 (beam_search:479) INFO: total log probability: -3.28
2024-10-27 17:31:36,498 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:31:36,498 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:36,498 (beam_search:483) INFO: best hypo: YESWETRIEDTOPAPERCLIPSONUMTHEMAGNETTOSEEIFITWOULDSTICKTOOTHERPAPER

2024-10-27 17:31:36,500 (asr_inference:509) INFO: speech length: 96144
2024-10-27 17:31:39,929 (beam_search:428) INFO: decoder input length: 74
2024-10-27 17:31:39,929 (beam_search:429) INFO: max output length: 74
2024-10-27 17:31:39,929 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:40,152 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:40,152 (beam_search:476) INFO:  -2.79 * 1.0 =  -2.79 for ctc
2024-10-27 17:31:40,152 (beam_search:479) INFO: total log probability: -2.79
2024-10-27 17:31:40,152 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:31:40,152 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:40,152 (beam_search:483) INFO: best hypo: ITWASTOGETENOUGHENERGYINTHEINTHEPAPERCLIPTHATITWOULDSTICKTOANOTHERONE

2024-10-27 17:31:40,154 (asr_inference:509) INFO: speech length: 45072
2024-10-27 17:31:41,776 (beam_search:428) INFO: decoder input length: 34
2024-10-27 17:31:41,776 (beam_search:429) INFO: max output length: 34
2024-10-27 17:31:41,776 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:41,815 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:41,816 (beam_search:476) INFO:  -1.07 * 1.0 =  -1.07 for ctc
2024-10-27 17:31:41,816 (beam_search:479) INFO: total log probability: -1.07
2024-10-27 17:31:41,816 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:31:41,816 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:41,816 (beam_search:483) INFO: best hypo: YESITDIDBECAUSEOFTHEING

2024-10-27 17:31:41,818 (asr_inference:509) INFO: speech length: 322384
2024-10-27 17:31:56,347 (beam_search:428) INFO: decoder input length: 251
2024-10-27 17:31:56,347 (beam_search:429) INFO: max output length: 251
2024-10-27 17:31:56,348 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:58,754 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:58,754 (beam_search:476) INFO:  -5.31 * 1.0 =  -5.31 for ctc
2024-10-27 17:31:58,754 (beam_search:479) INFO: total log probability: -5.31
2024-10-27 17:31:58,754 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:31:58,754 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:58,754 (beam_search:483) INFO: best hypo: ITUSMOREABOUTMAGNETSBECAUSEITITHELPEDTHATIFYOUSOMETHINGTHATCOULDSTICKTOAMAGNETANDIFYOUITWITHTHATTHESAMETHINGLIKEANAILIFYOUITINTHEMAGNETANDIFYOUITWITHANOTHERNAILITWOULDSTILLUMITWOULD

2024-10-27 17:31:58,756 (asr_inference:509) INFO: speech length: 332416
2024-10-27 17:32:13,361 (beam_search:428) INFO: decoder input length: 259
2024-10-27 17:32:13,362 (beam_search:429) INFO: max output length: 259
2024-10-27 17:32:13,362 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:15,971 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:15,972 (beam_search:476) INFO:  -6.12 * 1.0 =  -6.12 for ctc
2024-10-27 17:32:15,972 (beam_search:479) INFO: total log probability: -6.12
2024-10-27 17:32:15,972 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:32:15,972 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:15,972 (beam_search:483) INFO: best hypo: THEMAGNETTHETHEMAGNETTHAT'SISMOVINGTHETHAT'SONSOSOIFITWASMORETHENTHEMAGNET'TMOVEBECAUSEIT'STOOANDIFITIT'SLIKEYOUCOULDTHEMAGNETCOULDSTILLHAVEENOUGHENERGYTHATIT'LLGO

2024-10-27 17:32:15,976 (asr_inference:509) INFO: speech length: 111024
2024-10-27 17:32:20,100 (beam_search:428) INFO: decoder input length: 86
2024-10-27 17:32:20,100 (beam_search:429) INFO: max output length: 86
2024-10-27 17:32:20,100 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:20,357 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:20,357 (beam_search:476) INFO:  -2.72 * 1.0 =  -2.72 for ctc
2024-10-27 17:32:20,357 (beam_search:479) INFO: total log probability: -2.72
2024-10-27 17:32:20,357 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:32:20,357 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:20,357 (beam_search:483) INFO: best hypo: TODAYUMWELEARNEDABOUTHOWIT'STOHAVEANDNOANDALLTHOSEOTHERSTUFF

2024-10-27 17:32:20,360 (asr_inference:509) INFO: speech length: 138832
2024-10-27 17:32:25,466 (beam_search:428) INFO: decoder input length: 107
2024-10-27 17:32:25,466 (beam_search:429) INFO: max output length: 107
2024-10-27 17:32:25,466 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:25,670 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:25,670 (beam_search:476) INFO:  -2.33 * 1.0 =  -2.33 for ctc
2024-10-27 17:32:25,670 (beam_search:479) INFO: total log probability: -2.33
2024-10-27 17:32:25,670 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:32:25,670 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:25,670 (beam_search:483) INFO: best hypo: ANDISTOOBUTISREALLYGOODSOWEMADEFOR

2024-10-27 17:32:25,673 (asr_inference:509) INFO: speech length: 24544
2024-10-27 17:32:26,709 (beam_search:428) INFO: decoder input length: 18
2024-10-27 17:32:26,709 (beam_search:429) INFO: max output length: 18
2024-10-27 17:32:26,709 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:26,723 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:26,723 (beam_search:476) INFO:  -0.24 * 1.0 =  -0.24 for ctc
2024-10-27 17:32:26,723 (beam_search:479) INFO: total log probability: -0.24
2024-10-27 17:32:26,723 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:32:26,723 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:26,723 (beam_search:483) INFO: best hypo: IREALLYWAS

2024-10-27 17:32:26,726 (asr_inference:509) INFO: speech length: 13840
2024-10-27 17:32:27,513 (beam_search:428) INFO: decoder input length: 10
2024-10-27 17:32:27,513 (beam_search:429) INFO: max output length: 10
2024-10-27 17:32:27,513 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:27,523 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:27,523 (beam_search:476) INFO:  -0.21 * 1.0 =  -0.21 for ctc
2024-10-27 17:32:27,523 (beam_search:479) INFO: total log probability: -0.21
2024-10-27 17:32:27,523 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:32:27,523 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:27,523 (beam_search:483) INFO: best hypo: YOU'RE

2024-10-27 17:32:27,526 (asr_inference:509) INFO: speech length: 183392
2024-10-27 17:32:34,491 (beam_search:428) INFO: decoder input length: 142
2024-10-27 17:32:34,491 (beam_search:429) INFO: max output length: 142
2024-10-27 17:32:34,491 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:35,300 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:35,300 (beam_search:476) INFO:  -1.43 * 1.0 =  -1.43 for ctc
2024-10-27 17:32:35,300 (beam_search:479) INFO: total log probability: -1.43
2024-10-27 17:32:35,300 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:32:35,300 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:35,300 (beam_search:483) INFO: best hypo: WELLTHEBATTERYISABOUTIFYOUDIDN'THAVETHEBATTERYYOU'TGETTHEENERGYTOUMLIGHTUPTHELIGHTBULBANDITWOULDN'TWORK

2024-10-27 17:32:35,304 (asr_inference:509) INFO: speech length: 114688
2024-10-27 17:32:39,625 (beam_search:428) INFO: decoder input length: 89
2024-10-27 17:32:39,626 (beam_search:429) INFO: max output length: 89
2024-10-27 17:32:39,626 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:39,865 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:39,865 (beam_search:476) INFO:  -0.75 * 1.0 =  -0.75 for ctc
2024-10-27 17:32:39,865 (beam_search:479) INFO: total log probability: -0.75
2024-10-27 17:32:39,865 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:32:39,865 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:39,865 (beam_search:483) INFO: best hypo: WELLTHEISINGENERGYTOTHELIGHTBULBSOTHELIGHTBULBISLIGHTINGUP

2024-10-27 17:32:39,867 (asr_inference:509) INFO: speech length: 332176
2024-10-27 17:32:54,935 (beam_search:428) INFO: decoder input length: 259
2024-10-27 17:32:54,935 (beam_search:429) INFO: max output length: 259
2024-10-27 17:32:54,935 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:57,263 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:57,263 (beam_search:476) INFO:  -2.52 * 1.0 =  -2.52 for ctc
2024-10-27 17:32:57,263 (beam_search:479) INFO: total log probability: -2.52
2024-10-27 17:32:57,263 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:32:57,263 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:57,263 (beam_search:483) INFO: best hypo: ONEPARTOFTHEWIREISINTHEONTHEBOTTOMOFTHELIGHTBULBANDIT'SCONNECTEDTOTHEBATTERYANDTHEOTHERWIREISCONNECTEDTOTHEOTHERSIDEOFTHEBATTERYANDIT'SCONNECTEDTOTHELIGHTBULB'SMETAL

2024-10-27 17:32:57,266 (asr_inference:509) INFO: speech length: 127072
2024-10-27 17:33:02,011 (beam_search:428) INFO: decoder input length: 98
2024-10-27 17:33:02,011 (beam_search:429) INFO: max output length: 98
2024-10-27 17:33:02,011 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:02,384 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:02,384 (beam_search:476) INFO:  -1.78 * 1.0 =  -1.78 for ctc
2024-10-27 17:33:02,384 (beam_search:479) INFO: total log probability: -1.78
2024-10-27 17:33:02,384 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:33:02,384 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:02,384 (beam_search:483) INFO: best hypo: IFIFYOUDIDN'TPUTITINTHEMETALLIKEWHERETHEMETALWASTHEITWOULDN'TLIGHTUP

2024-10-27 17:33:02,386 (asr_inference:509) INFO: speech length: 103360
2024-10-27 17:33:06,072 (beam_search:428) INFO: decoder input length: 80
2024-10-27 17:33:06,072 (beam_search:429) INFO: max output length: 80
2024-10-27 17:33:06,072 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:06,310 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:06,310 (beam_search:476) INFO:  -4.17 * 1.0 =  -4.17 for ctc
2024-10-27 17:33:06,310 (beam_search:479) INFO: total log probability: -4.17
2024-10-27 17:33:06,310 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:33:06,310 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:06,310 (beam_search:483) INFO: best hypo: IFITWASN'TTHEITTHEENERGYWOULDN'TGOTHROUGHSOTHELIGHTBULB

2024-10-27 17:33:06,312 (asr_inference:509) INFO: speech length: 84272
2024-10-27 17:33:09,482 (beam_search:428) INFO: decoder input length: 65
2024-10-27 17:33:09,482 (beam_search:429) INFO: max output length: 65
2024-10-27 17:33:09,482 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:09,570 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:09,570 (beam_search:476) INFO:  -1.17 * 1.0 =  -1.17 for ctc
2024-10-27 17:33:09,570 (beam_search:479) INFO: total log probability: -1.17
2024-10-27 17:33:09,570 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:33:09,570 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:09,570 (beam_search:483) INFO: best hypo: THAT'STHETHEWOULDGOTHROUGH

2024-10-27 17:33:09,573 (asr_inference:509) INFO: speech length: 305520
2024-10-27 17:33:22,612 (beam_search:428) INFO: decoder input length: 238
2024-10-27 17:33:22,613 (beam_search:429) INFO: max output length: 238
2024-10-27 17:33:22,613 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:24,806 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:24,806 (beam_search:476) INFO:  -2.35 * 1.0 =  -2.35 for ctc
2024-10-27 17:33:24,806 (beam_search:479) INFO: total log probability: -2.35
2024-10-27 17:33:24,806 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:33:24,806 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:24,806 (beam_search:483) INFO: best hypo: WELLTHEWELLTHEBATTERY'SGIVINGENERGYTOTHELIGHTBULBWHEREIT'SINTHETHEMETALANDIT'SGOINGTHROUGHANDDOWNTHELIGHTBULBBACKTOTHEBATTERYANDIT'SGOINGBACKTOTHELIGHTBULB

2024-10-27 17:33:24,809 (asr_inference:509) INFO: speech length: 126944
2024-10-27 17:33:29,534 (beam_search:428) INFO: decoder input length: 98
2024-10-27 17:33:29,534 (beam_search:429) INFO: max output length: 98
2024-10-27 17:33:29,534 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:29,867 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:29,867 (beam_search:476) INFO:  -2.18 * 1.0 =  -2.18 for ctc
2024-10-27 17:33:29,867 (beam_search:479) INFO: total log probability: -2.18
2024-10-27 17:33:29,867 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:33:29,867 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:29,868 (beam_search:483) INFO: best hypo: IT'SGOINGTHESAMEWAYBECAUSEWHENITIT'SGOINGAROUNDANDBACKTOTHESAME

2024-10-27 17:33:29,871 (asr_inference:509) INFO: speech length: 275776
2024-10-27 17:33:41,426 (beam_search:428) INFO: decoder input length: 214
2024-10-27 17:33:41,426 (beam_search:429) INFO: max output length: 214
2024-10-27 17:33:41,426 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:43,568 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:43,568 (beam_search:476) INFO:  -3.05 * 1.0 =  -3.05 for ctc
2024-10-27 17:33:43,568 (beam_search:479) INFO: total log probability: -3.05
2024-10-27 17:33:43,568 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:33:43,568 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:43,568 (beam_search:483) INFO: best hypo: WELLFIRSTWHENITIT'SGOINGTHESAMEWAYTHENITTURNSANDTHENITGOESDIFFERENTANDTHENLIKEFIRSTWHENITCOMESOUTOFTHEBATTERYIT'SGOINGADIFFERENTWAYANDTHENWHENIT'SBACKIT'SGOINGADIFFERENTWAY

2024-10-27 17:33:43,571 (asr_inference:509) INFO: speech length: 196496
2024-10-27 17:33:51,479 (beam_search:428) INFO: decoder input length: 153
2024-10-27 17:33:51,480 (beam_search:429) INFO: max output length: 153
2024-10-27 17:33:51,480 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:52,271 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:52,271 (beam_search:476) INFO:  -3.24 * 1.0 =  -3.24 for ctc
2024-10-27 17:33:52,271 (beam_search:479) INFO: total log probability: -3.24
2024-10-27 17:33:52,271 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:33:52,271 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:52,271 (beam_search:483) INFO: best hypo: ONTHEBATTERYANDITGOESALLAROUNDTHEWIRESTOTHELIGHTBULBANDITCOMESBACKANDITANDITGOESTHROUGHTHESAMEWAYAGAINEVERYTIME

2024-10-27 17:33:52,274 (asr_inference:509) INFO: speech length: 112736
2024-10-27 17:33:56,356 (beam_search:428) INFO: decoder input length: 87
2024-10-27 17:33:56,356 (beam_search:429) INFO: max output length: 87
2024-10-27 17:33:56,356 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:56,689 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:56,689 (beam_search:476) INFO:  -1.99 * 1.0 =  -1.99 for ctc
2024-10-27 17:33:56,689 (beam_search:479) INFO: total log probability: -1.99
2024-10-27 17:33:56,689 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:33:56,689 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:56,689 (beam_search:483) INFO: best hypo: YESBECAUSEIFYOUPUTITTHEWRONGWAYWHENYOU'REPUTTINGONABATTERYTHEBATTERYWON'TWORK

2024-10-27 17:33:56,692 (asr_inference:509) INFO: speech length: 38960
2024-10-27 17:33:58,226 (beam_search:428) INFO: decoder input length: 29
2024-10-27 17:33:58,226 (beam_search:429) INFO: max output length: 29
2024-10-27 17:33:58,227 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:58,237 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:58,238 (beam_search:476) INFO:  -0.74 * 1.0 =  -0.74 for ctc
2024-10-27 17:33:58,238 (beam_search:479) INFO: total log probability: -0.74
2024-10-27 17:33:58,238 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:33:58,238 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:58,238 (beam_search:483) INFO: best hypo: THE

2024-10-27 17:33:58,240 (asr_inference:509) INFO: speech length: 131152
2024-10-27 17:34:03,035 (beam_search:428) INFO: decoder input length: 101
2024-10-27 17:34:03,036 (beam_search:429) INFO: max output length: 101
2024-10-27 17:34:03,036 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:03,556 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:03,556 (beam_search:476) INFO:  -2.73 * 1.0 =  -2.73 for ctc
2024-10-27 17:34:03,556 (beam_search:479) INFO: total log probability: -2.73
2024-10-27 17:34:03,556 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:34:03,557 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:03,557 (beam_search:483) INFO: best hypo: WELLIONTHEBATTERYIT'SGOINGTHEOPPOSITEWAYIFICLICKONTHEBATTERYAGAINIT'SGOINGTOGOADIFFERENTWAY

2024-10-27 17:34:03,559 (asr_inference:509) INFO: speech length: 108624
2024-10-27 17:34:07,801 (beam_search:428) INFO: decoder input length: 84
2024-10-27 17:34:07,801 (beam_search:429) INFO: max output length: 84
2024-10-27 17:34:07,801 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:08,051 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:08,051 (beam_search:476) INFO:  -2.28 * 1.0 =  -2.28 for ctc
2024-10-27 17:34:08,051 (beam_search:479) INFO: total log probability: -2.28
2024-10-27 17:34:08,051 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:34:08,051 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:08,051 (beam_search:483) INFO: best hypo: WELLITHINKIT'SABOUTLIKETHEBATTERYINGOUTENERGYIT'SLIKEA

2024-10-27 17:34:08,053 (asr_inference:509) INFO: speech length: 10864
2024-10-27 17:34:08,669 (beam_search:428) INFO: decoder input length: 7
2024-10-27 17:34:08,669 (beam_search:429) INFO: max output length: 7
2024-10-27 17:34:08,669 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:08,678 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:08,678 (beam_search:476) INFO:  -1.19 * 1.0 =  -1.19 for ctc
2024-10-27 17:34:08,678 (beam_search:479) INFO: total log probability: -1.19
2024-10-27 17:34:08,678 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:34:08,678 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:08,678 (beam_search:483) INFO: best hypo: I'MGOOD

2024-10-27 17:34:08,680 (asr_inference:509) INFO: speech length: 48384
2024-10-27 17:34:10,524 (beam_search:428) INFO: decoder input length: 37
2024-10-27 17:34:10,524 (beam_search:429) INFO: max output length: 37
2024-10-27 17:34:10,524 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:10,559 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:10,559 (beam_search:476) INFO:  -1.14 * 1.0 =  -1.14 for ctc
2024-10-27 17:34:10,559 (beam_search:479) INFO: total log probability: -1.14
2024-10-27 17:34:10,559 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:34:10,559 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:10,559 (beam_search:483) INFO: best hypo: ORABORD

2024-10-27 17:34:10,561 (asr_inference:509) INFO: speech length: 441824
2024-10-27 17:34:31,430 (beam_search:428) INFO: decoder input length: 344
2024-10-27 17:34:31,430 (beam_search:429) INFO: max output length: 344
2024-10-27 17:34:31,430 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:35,813 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:35,814 (beam_search:476) INFO:  -7.06 * 1.0 =  -7.06 for ctc
2024-10-27 17:34:35,814 (beam_search:479) INFO: total log probability: -7.06
2024-10-27 17:34:35,814 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:34:35,814 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:35,814 (beam_search:483) INFO: best hypo: WELLWHATITHATISLIKEWHENYOUIFYOUCONNECTLIKEWHENYOUCONNECTALIGHTBULBLIKEIT'SLIKECONNECTINGAMOTORTOTHEBATTERYANDTHEOTHERWIRETHEOTHERWIRETOTHEOTHEROFTHEBATTERYANDYOUCONNECTITTOBOTHOFIFYOUPUTBOTHOFTHEWIRESTOGETHERINSTEADOFJUSTITONAIT'LL

2024-10-27 17:34:35,816 (asr_inference:509) INFO: speech length: 77152
2024-10-27 17:34:38,730 (beam_search:428) INFO: decoder input length: 59
2024-10-27 17:34:38,730 (beam_search:429) INFO: max output length: 59
2024-10-27 17:34:38,730 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:38,858 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:38,858 (beam_search:476) INFO:  -1.22 * 1.0 =  -1.22 for ctc
2024-10-27 17:34:38,858 (beam_search:479) INFO: total log probability: -1.22
2024-10-27 17:34:38,858 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:34:38,858 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:38,858 (beam_search:483) INFO: best hypo: WEHAVEABATTERYWIREANDALIGHTBULBWITHTHELIGHTBULB

2024-10-27 17:34:38,860 (asr_inference:509) INFO: speech length: 259760
2024-10-27 17:34:49,733 (beam_search:428) INFO: decoder input length: 202
2024-10-27 17:34:49,734 (beam_search:429) INFO: max output length: 202
2024-10-27 17:34:49,734 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:51,218 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:51,218 (beam_search:476) INFO:  -4.21 * 1.0 =  -4.21 for ctc
2024-10-27 17:34:51,218 (beam_search:479) INFO: total log probability: -4.21
2024-10-27 17:34:51,218 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:34:51,218 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:51,218 (beam_search:483) INFO: best hypo: FIRSTWEPUTTHEBATTERYINWHERETHEBATTERYTHENWECANGETTHEWIREANDCONNECTITTOTHEANDTHENWECOULDPUTTHELIGHTBULBWHERETHELIGHTBULBGOESANDTHENWECOULDCONNECTTHEWIRE

2024-10-27 17:34:51,222 (asr_inference:509) INFO: speech length: 18112
2024-10-27 17:34:52,056 (beam_search:428) INFO: decoder input length: 13
2024-10-27 17:34:52,056 (beam_search:429) INFO: max output length: 13
2024-10-27 17:34:52,056 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:52,070 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:52,070 (beam_search:476) INFO:  -0.72 * 1.0 =  -0.72 for ctc
2024-10-27 17:34:52,070 (beam_search:479) INFO: total log probability: -0.72
2024-10-27 17:34:52,070 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:34:52,070 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:52,070 (beam_search:483) INFO: best hypo: YOUHADTOINLIKE

2024-10-27 17:34:52,073 (asr_inference:509) INFO: speech length: 156208
2024-10-27 17:34:58,058 (beam_search:428) INFO: decoder input length: 121
2024-10-27 17:34:58,059 (beam_search:429) INFO: max output length: 121
2024-10-27 17:34:58,059 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:58,529 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:58,529 (beam_search:476) INFO:  -8.04 * 1.0 =  -8.04 for ctc
2024-10-27 17:34:58,529 (beam_search:479) INFO: total log probability: -8.04
2024-10-27 17:34:58,529 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 17:34:58,529 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:58,529 (beam_search:483) INFO: best hypo: ANDAANDBANDAFTERYOUWEREDONEWITHTHATWEHAWEHADTODOTWOWEGOTTOWITHPEOPLE

2024-10-27 17:34:58,531 (asr_inference:509) INFO: speech length: 90352
2024-10-27 17:35:01,768 (beam_search:428) INFO: decoder input length: 70
2024-10-27 17:35:01,768 (beam_search:429) INFO: max output length: 70
2024-10-27 17:35:01,768 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:01,862 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:01,862 (beam_search:476) INFO:  -1.17 * 1.0 =  -1.17 for ctc
2024-10-27 17:35:01,862 (beam_search:479) INFO: total log probability: -1.17
2024-10-27 17:35:01,862 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:35:01,862 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:01,862 (beam_search:483) INFO: best hypo: WE'VEBEENDOINGELECTRICITYANDMAGNETISM

2024-10-27 17:35:01,864 (asr_inference:509) INFO: speech length: 22416
2024-10-27 17:35:02,850 (beam_search:428) INFO: decoder input length: 17
2024-10-27 17:35:02,851 (beam_search:429) INFO: max output length: 17
2024-10-27 17:35:02,851 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:02,858 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:02,859 (beam_search:476) INFO:  -0.11 * 1.0 =  -0.11 for ctc
2024-10-27 17:35:02,859 (beam_search:479) INFO: total log probability: -0.11
2024-10-27 17:35:02,859 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:35:02,859 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:02,859 (beam_search:483) INFO: best hypo: WE

2024-10-27 17:35:02,861 (asr_inference:509) INFO: speech length: 116048
2024-10-27 17:35:06,979 (beam_search:428) INFO: decoder input length: 90
2024-10-27 17:35:06,979 (beam_search:429) INFO: max output length: 90
2024-10-27 17:35:06,979 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:07,223 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:07,223 (beam_search:476) INFO:  -1.67 * 1.0 =  -1.67 for ctc
2024-10-27 17:35:07,223 (beam_search:479) INFO: total log probability: -1.67
2024-10-27 17:35:07,223 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:35:07,223 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:07,224 (beam_search:483) INFO: best hypo: WE'VEBEENTHELIGHTBULBLIGHTORTHETHETIMEWEHADWE

2024-10-27 17:35:07,226 (asr_inference:509) INFO: speech length: 53104
2024-10-27 17:35:09,262 (beam_search:428) INFO: decoder input length: 40
2024-10-27 17:35:09,262 (beam_search:429) INFO: max output length: 40
2024-10-27 17:35:09,262 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:09,304 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:09,305 (beam_search:476) INFO:  -1.67 * 1.0 =  -1.67 for ctc
2024-10-27 17:35:09,305 (beam_search:479) INFO: total log probability: -1.67
2024-10-27 17:35:09,305 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:35:09,305 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:09,305 (beam_search:483) INFO: best hypo: WEMADETHEUMAMOTOR

2024-10-27 17:35:09,307 (asr_inference:509) INFO: speech length: 214720
2024-10-27 17:35:18,079 (beam_search:428) INFO: decoder input length: 167
2024-10-27 17:35:18,079 (beam_search:429) INFO: max output length: 167
2024-10-27 17:35:18,079 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:19,116 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:19,116 (beam_search:476) INFO:  -3.52 * 1.0 =  -3.52 for ctc
2024-10-27 17:35:19,116 (beam_search:479) INFO: total log probability: -3.52
2024-10-27 17:35:19,116 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:35:19,116 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:19,116 (beam_search:483) INFO: best hypo: THEDCELLWEHOOKEDTWOWIRESUPTOTHEDCELLTHENWEUMPUTEMUPTOTHELIGHTBULBSOTHEELECTRICITYFLOWTHROUGHTHEWIRESTOTHELIGHTBULB

2024-10-27 17:35:19,118 (asr_inference:509) INFO: speech length: 57808
2024-10-27 17:35:21,220 (beam_search:428) INFO: decoder input length: 44
2024-10-27 17:35:21,220 (beam_search:429) INFO: max output length: 44
2024-10-27 17:35:21,220 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:21,292 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:21,292 (beam_search:476) INFO:  -1.25 * 1.0 =  -1.25 for ctc
2024-10-27 17:35:21,292 (beam_search:479) INFO: total log probability: -1.25
2024-10-27 17:35:21,292 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:35:21,292 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:21,292 (beam_search:483) INFO: best hypo: ALITTLEBITBUTWEONLYHADONELIGHTBULB

2024-10-27 17:35:21,295 (asr_inference:509) INFO: speech length: 341728
2024-10-27 17:35:36,228 (beam_search:428) INFO: decoder input length: 266
2024-10-27 17:35:36,229 (beam_search:429) INFO: max output length: 266
2024-10-27 17:35:36,229 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:38,465 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:38,466 (beam_search:476) INFO:  -4.58 * 1.0 =  -4.58 for ctc
2024-10-27 17:35:38,466 (beam_search:479) INFO: total log probability: -4.58
2024-10-27 17:35:38,466 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:35:38,466 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:38,466 (beam_search:483) INFO: best hypo: WELLISEETHETHEUMTHEFLOWUMGOINGTHROUGHTHELIGHTBULBTHENGOINGTHROUGHTHEMETALTHETHEBASEOFTHELIGHTBULBANDTHENFLOWINGTHROUGHINTOLIGHTBULBANDTHENTHEWIRESTHENEGATIVESIDEOFTHEBATTERY

2024-10-27 17:35:38,469 (asr_inference:509) INFO: speech length: 92288
2024-10-27 17:35:41,910 (beam_search:428) INFO: decoder input length: 71
2024-10-27 17:35:41,910 (beam_search:429) INFO: max output length: 71
2024-10-27 17:35:41,910 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:42,084 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:42,084 (beam_search:476) INFO:  -1.44 * 1.0 =  -1.44 for ctc
2024-10-27 17:35:42,084 (beam_search:479) INFO: total log probability: -1.44
2024-10-27 17:35:42,084 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:35:42,084 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:42,084 (beam_search:483) INFO: best hypo: ITISDIFFERENTBECAUSETHEYHAUMYOUHAVETWODCELLSINSTEADOFONE

2024-10-27 17:35:42,087 (asr_inference:509) INFO: speech length: 129168
2024-10-27 17:35:46,855 (beam_search:428) INFO: decoder input length: 100
2024-10-27 17:35:46,855 (beam_search:429) INFO: max output length: 100
2024-10-27 17:35:46,855 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:47,080 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:47,080 (beam_search:476) INFO:  -1.55 * 1.0 =  -1.55 for ctc
2024-10-27 17:35:47,080 (beam_search:479) INFO: total log probability: -1.55
2024-10-27 17:35:47,080 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:35:47,080 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:47,080 (beam_search:483) INFO: best hypo: THEUMTHELIGHTBULBSAREUPMORETHANTHEONEDCELL

2024-10-27 17:35:47,082 (asr_inference:509) INFO: speech length: 76256
2024-10-27 17:35:49,908 (beam_search:428) INFO: decoder input length: 59
2024-10-27 17:35:49,908 (beam_search:429) INFO: max output length: 59
2024-10-27 17:35:49,908 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:50,008 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:50,009 (beam_search:476) INFO:  -1.87 * 1.0 =  -1.87 for ctc
2024-10-27 17:35:50,009 (beam_search:479) INFO: total log probability: -1.87
2024-10-27 17:35:50,009 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:35:50,009 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:50,009 (beam_search:483) INFO: best hypo: BECAUSEYOUHAVETHEAMOUNTOFPOWERFROMTHED

2024-10-27 17:35:50,012 (asr_inference:509) INFO: speech length: 411024
2024-10-27 17:36:09,651 (beam_search:428) INFO: decoder input length: 320
2024-10-27 17:36:09,652 (beam_search:429) INFO: max output length: 320
2024-10-27 17:36:09,652 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:12,719 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:12,719 (beam_search:476) INFO:  -3.08 * 1.0 =  -3.08 for ctc
2024-10-27 17:36:12,719 (beam_search:479) INFO: total log probability: -3.08
2024-10-27 17:36:12,719 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:36:12,719 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:12,719 (beam_search:483) INFO: best hypo: WELLTHEDCELLBATTERYISUMISTOAMETALWHICHISTOAWIRETHENGOESTOALIGHTBULBANDTHETHEONELIGHTBULBANOTHERLIGHTBULBANDTHENITANDTHENTHEANDTHENANOTHERWIREGOESINTOTHENEGATIVESIDEOFTHEBATTERY

2024-10-27 17:36:12,742 (asr_inference:509) INFO: speech length: 164272
2024-10-27 17:36:19,049 (beam_search:428) INFO: decoder input length: 127
2024-10-27 17:36:19,049 (beam_search:429) INFO: max output length: 127
2024-10-27 17:36:19,049 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:19,461 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:19,461 (beam_search:476) INFO:  -2.52 * 1.0 =  -2.52 for ctc
2024-10-27 17:36:19,461 (beam_search:479) INFO: total log probability: -2.52
2024-10-27 17:36:19,461 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:36:19,461 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:19,461 (beam_search:483) INFO: best hypo: THEREISTWODCELLSANDTHELIGHTBULBSAREUPMOREBECAUSEITHASMOREELECTRICITYPOWER

2024-10-27 17:36:19,463 (asr_inference:509) INFO: speech length: 29088
2024-10-27 17:36:20,743 (beam_search:428) INFO: decoder input length: 22
2024-10-27 17:36:20,744 (beam_search:429) INFO: max output length: 22
2024-10-27 17:36:20,744 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:20,760 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:20,760 (beam_search:476) INFO:  -0.90 * 1.0 =  -0.90 for ctc
2024-10-27 17:36:20,760 (beam_search:479) INFO: total log probability: -0.90
2024-10-27 17:36:20,760 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:36:20,760 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:20,760 (beam_search:483) INFO: best hypo: YESALITTLE

2024-10-27 17:36:20,763 (asr_inference:509) INFO: speech length: 103664
2024-10-27 17:36:24,516 (beam_search:428) INFO: decoder input length: 80
2024-10-27 17:36:24,516 (beam_search:429) INFO: max output length: 80
2024-10-27 17:36:24,516 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:24,651 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:24,651 (beam_search:476) INFO:  -1.59 * 1.0 =  -1.59 for ctc
2024-10-27 17:36:24,651 (beam_search:479) INFO: total log probability: -1.59
2024-10-27 17:36:24,651 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:36:24,651 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:24,651 (beam_search:483) INFO: best hypo: WEHAVEMADEOFWHATWEHAVEUMINCLASS

2024-10-27 17:36:24,654 (asr_inference:509) INFO: speech length: 182336
2024-10-27 17:36:31,795 (beam_search:428) INFO: decoder input length: 141
2024-10-27 17:36:31,795 (beam_search:429) INFO: max output length: 141
2024-10-27 17:36:31,795 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:32,539 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:32,539 (beam_search:476) INFO:  -3.50 * 1.0 =  -3.50 for ctc
2024-10-27 17:36:32,539 (beam_search:479) INFO: total log probability: -3.50
2024-10-27 17:36:32,539 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:36:32,539 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:32,539 (beam_search:483) INFO: best hypo: WELLTHETWODTHEBOTTOMAREHOOKEDUPTOEACHOTHERANDTHENTHEYTOTWOLIGHTBULBSANDTHENTHERE'SAWIREINTHE

2024-10-27 17:36:32,542 (asr_inference:509) INFO: speech length: 63040
2024-10-27 17:36:34,874 (beam_search:428) INFO: decoder input length: 48
2024-10-27 17:36:34,874 (beam_search:429) INFO: max output length: 48
2024-10-27 17:36:34,874 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:34,934 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:34,934 (beam_search:476) INFO:  -1.26 * 1.0 =  -1.26 for ctc
2024-10-27 17:36:34,934 (beam_search:479) INFO: total log probability: -1.26
2024-10-27 17:36:34,934 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:36:34,934 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:34,934 (beam_search:483) INFO: best hypo: THATTHECURRENTAPLACETOGO

2024-10-27 17:36:34,936 (asr_inference:509) INFO: speech length: 81056
2024-10-27 17:36:37,810 (beam_search:428) INFO: decoder input length: 62
2024-10-27 17:36:37,810 (beam_search:429) INFO: max output length: 62
2024-10-27 17:36:37,811 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:37,910 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:37,910 (beam_search:476) INFO:  -2.03 * 1.0 =  -2.03 for ctc
2024-10-27 17:36:37,910 (beam_search:479) INFO: total log probability: -2.03
2024-10-27 17:36:37,910 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:36:37,910 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:37,910 (beam_search:483) INFO: best hypo: ISTHATALLTHETHETHEWIRESANDTHE

2024-10-27 17:36:37,912 (asr_inference:509) INFO: speech length: 121584
2024-10-27 17:36:42,388 (beam_search:428) INFO: decoder input length: 94
2024-10-27 17:36:42,388 (beam_search:429) INFO: max output length: 94
2024-10-27 17:36:42,388 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:42,637 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:42,638 (beam_search:476) INFO:  -1.18 * 1.0 =  -1.18 for ctc
2024-10-27 17:36:42,638 (beam_search:479) INFO: total log probability: -1.18
2024-10-27 17:36:42,638 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:36:42,638 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:42,638 (beam_search:483) INFO: best hypo: HAVETOBETOUCHINGORELSETHEORTHELIGHTBULBWILLNOTLIGHTUP

2024-10-27 17:36:42,640 (asr_inference:509) INFO: speech length: 97232
2024-10-27 17:36:46,204 (beam_search:428) INFO: decoder input length: 75
2024-10-27 17:36:46,204 (beam_search:429) INFO: max output length: 75
2024-10-27 17:36:46,204 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:46,358 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:46,358 (beam_search:476) INFO:  -1.70 * 1.0 =  -1.70 for ctc
2024-10-27 17:36:46,358 (beam_search:479) INFO: total log probability: -1.70
2024-10-27 17:36:46,358 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:36:46,358 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:46,358 (beam_search:483) INFO: best hypo: BECAUSETHEWILLNOTBEABLETOGETUMTHROUGHTOTHEM

2024-10-27 17:36:46,360 (asr_inference:509) INFO: speech length: 48544
2024-10-27 17:36:48,194 (beam_search:428) INFO: decoder input length: 37
2024-10-27 17:36:48,194 (beam_search:429) INFO: max output length: 37
2024-10-27 17:36:48,194 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:48,229 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:48,229 (beam_search:476) INFO:  -0.54 * 1.0 =  -0.54 for ctc
2024-10-27 17:36:48,229 (beam_search:479) INFO: total log probability: -0.54
2024-10-27 17:36:48,229 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:36:48,229 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:48,229 (beam_search:483) INFO: best hypo: THEISFLOWINGTOTHE

2024-10-27 17:36:48,231 (asr_inference:509) INFO: speech length: 106144
2024-10-27 17:36:52,047 (beam_search:428) INFO: decoder input length: 82
2024-10-27 17:36:52,047 (beam_search:429) INFO: max output length: 82
2024-10-27 17:36:52,047 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:52,279 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:52,279 (beam_search:476) INFO:  -2.00 * 1.0 =  -2.00 for ctc
2024-10-27 17:36:52,279 (beam_search:479) INFO: total log probability: -2.00
2024-10-27 17:36:52,279 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:36:52,279 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:52,279 (beam_search:483) INFO: best hypo: THEWAYTHATISGOINGOUTOFISTHENEGATIVESIDEANDISGOINGINTOTHEPOSITIVESIDE

2024-10-27 17:36:52,283 (asr_inference:509) INFO: speech length: 224880
2024-10-27 17:37:01,646 (beam_search:428) INFO: decoder input length: 175
2024-10-27 17:37:01,647 (beam_search:429) INFO: max output length: 175
2024-10-27 17:37:01,647 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:02,362 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:02,362 (beam_search:476) INFO:  -2.11 * 1.0 =  -2.11 for ctc
2024-10-27 17:37:02,362 (beam_search:479) INFO: total log probability: -2.11
2024-10-27 17:37:02,362 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:37:02,362 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:02,363 (beam_search:483) INFO: best hypo: WELLIFYOUONEITWOULDN'TBECAUSENEGATIVEANDNEGATIVEANDTHENTHETWOPOSITIVESWOULDBEONTHE

2024-10-27 17:37:02,365 (asr_inference:509) INFO: speech length: 83328
2024-10-27 17:37:05,347 (beam_search:428) INFO: decoder input length: 64
2024-10-27 17:37:05,347 (beam_search:429) INFO: max output length: 64
2024-10-27 17:37:05,347 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:05,417 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:05,417 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 17:37:05,417 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 17:37:05,417 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:37:05,417 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:05,417 (beam_search:483) INFO: best hypo: WOULDGOININTHEOPPOSITE

2024-10-27 17:37:05,419 (asr_inference:509) INFO: speech length: 109808
2024-10-27 17:37:09,420 (beam_search:428) INFO: decoder input length: 85
2024-10-27 17:37:09,420 (beam_search:429) INFO: max output length: 85
2024-10-27 17:37:09,420 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:09,543 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:09,543 (beam_search:476) INFO:  -2.23 * 1.0 =  -2.23 for ctc
2024-10-27 17:37:09,543 (beam_search:479) INFO: total log probability: -2.23
2024-10-27 17:37:09,543 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:37:09,543 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:09,543 (beam_search:483) INFO: best hypo: THATTHEWOULDTHROUGHTHENOTOTHE

2024-10-27 17:37:09,545 (asr_inference:509) INFO: speech length: 12112
2024-10-27 17:37:10,321 (beam_search:428) INFO: decoder input length: 8
2024-10-27 17:37:10,321 (beam_search:429) INFO: max output length: 8
2024-10-27 17:37:10,321 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:10,328 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:10,329 (beam_search:476) INFO:  -0.87 * 1.0 =  -0.87 for ctc
2024-10-27 17:37:10,329 (beam_search:479) INFO: total log probability: -0.87
2024-10-27 17:37:10,329 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 17:37:10,329 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:10,329 (beam_search:483) INFO: best hypo: RE

2024-10-27 17:37:10,331 (asr_inference:509) INFO: speech length: 25728
2024-10-27 17:37:11,413 (beam_search:428) INFO: decoder input length: 19
2024-10-27 17:37:11,414 (beam_search:429) INFO: max output length: 19
2024-10-27 17:37:11,414 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:11,430 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:11,430 (beam_search:476) INFO:  -0.93 * 1.0 =  -0.93 for ctc
2024-10-27 17:37:11,430 (beam_search:479) INFO: total log probability: -0.93
2024-10-27 17:37:11,430 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:37:11,430 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:11,430 (beam_search:483) INFO: best hypo: I'LLTO

2024-10-27 17:37:11,433 (asr_inference:509) INFO: speech length: 114528
2024-10-27 17:37:15,543 (beam_search:428) INFO: decoder input length: 88
2024-10-27 17:37:15,543 (beam_search:429) INFO: max output length: 88
2024-10-27 17:37:15,543 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:15,844 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:15,844 (beam_search:476) INFO:  -3.30 * 1.0 =  -3.30 for ctc
2024-10-27 17:37:15,844 (beam_search:479) INFO: total log probability: -3.30
2024-10-27 17:37:15,844 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:37:15,844 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:15,844 (beam_search:483) INFO: best hypo: WEPUTEMINTHECUPBYTHEMORWETHEMONTHEOFTHECUPORWEJUSTIN

2024-10-27 17:37:15,847 (asr_inference:509) INFO: speech length: 270128
2024-10-27 17:37:27,097 (beam_search:428) INFO: decoder input length: 210
2024-10-27 17:37:27,097 (beam_search:429) INFO: max output length: 210
2024-10-27 17:37:27,097 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:28,143 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:28,143 (beam_search:476) INFO:  -3.25 * 1.0 =  -3.25 for ctc
2024-10-27 17:37:28,143 (beam_search:479) INFO: total log probability: -3.25
2024-10-27 17:37:28,144 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:37:28,144 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:28,144 (beam_search:483) INFO: best hypo: UMWEWEWETOPUTTHEMETALWASHERSINTHEUMCUPTHATDIDN'TTHEMAGNETINITANDUNTILTHEUMMAGNETIC

2024-10-27 17:37:28,146 (asr_inference:509) INFO: speech length: 18064
2024-10-27 17:37:29,043 (beam_search:428) INFO: decoder input length: 13
2024-10-27 17:37:29,044 (beam_search:429) INFO: max output length: 13
2024-10-27 17:37:29,044 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:29,056 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:29,056 (beam_search:476) INFO:  -0.91 * 1.0 =  -0.91 for ctc
2024-10-27 17:37:29,056 (beam_search:479) INFO: total log probability: -0.91
2024-10-27 17:37:29,056 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:37:29,056 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:29,056 (beam_search:483) INFO: best hypo: YESWEDID

2024-10-27 17:37:29,058 (asr_inference:509) INFO: speech length: 272112
2024-10-27 17:37:40,225 (beam_search:428) INFO: decoder input length: 212
2024-10-27 17:37:40,225 (beam_search:429) INFO: max output length: 212
2024-10-27 17:37:40,225 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:41,529 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:41,529 (beam_search:476) INFO:  -2.77 * 1.0 =  -2.77 for ctc
2024-10-27 17:37:41,529 (beam_search:479) INFO: total log probability: -2.77
2024-10-27 17:37:41,529 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:37:41,529 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:41,529 (beam_search:483) INFO: best hypo: YEAHUMWEACTUALLYDIDTHEISTOPUTTHEWASHERSONONTHESIDEOFTHECUPUMALLUPTOGETHERANDTHENYOUCANGETMOREBEFORETHETHEUMMAGNETIC

2024-10-27 17:37:41,532 (asr_inference:509) INFO: speech length: 192416
2024-10-27 17:37:49,148 (beam_search:428) INFO: decoder input length: 149
2024-10-27 17:37:49,148 (beam_search:429) INFO: max output length: 149
2024-10-27 17:37:49,148 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:49,491 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:49,491 (beam_search:476) INFO:  -5.14 * 1.0 =  -5.14 for ctc
2024-10-27 17:37:49,491 (beam_search:479) INFO: total log probability: -5.14
2024-10-27 17:37:49,491 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 17:37:49,491 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:49,492 (beam_search:483) INFO: best hypo: UMITEDTHEWAYTHATISTHATTHEMAGNETICISUU

2024-10-27 17:37:49,494 (asr_inference:509) INFO: speech length: 139760
2024-10-27 17:37:54,821 (beam_search:428) INFO: decoder input length: 108
2024-10-27 17:37:54,822 (beam_search:429) INFO: max output length: 108
2024-10-27 17:37:54,822 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:55,029 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:55,029 (beam_search:476) INFO:  -2.20 * 1.0 =  -2.20 for ctc
2024-10-27 17:37:55,029 (beam_search:479) INFO: total log probability: -2.20
2024-10-27 17:37:55,029 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:37:55,029 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:55,029 (beam_search:483) INFO: best hypo: THEMAGNETICANDWEOUTTHATMOREPLASTICARETHEYCALLED

2024-10-27 17:37:55,031 (asr_inference:509) INFO: speech length: 390048
2024-10-27 17:38:13,017 (beam_search:428) INFO: decoder input length: 304
2024-10-27 17:38:13,017 (beam_search:429) INFO: max output length: 304
2024-10-27 17:38:13,017 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:15,314 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:15,314 (beam_search:476) INFO:  -7.12 * 1.0 =  -7.12 for ctc
2024-10-27 17:38:15,314 (beam_search:479) INFO: total log probability: -7.12
2024-10-27 17:38:15,314 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:38:15,314 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:15,315 (beam_search:483) INFO: best hypo: PLASTICYELLOWUMWECOULDPUTINANDTHENWEWEDTHEWEDLIKEONEWITHWEHADSIXTEENWITHONEUMPLASTICYELLOWINTHEMIDDLEWEHADSEVENANDTHENANDTHENFOURFOURANDFOUR

2024-10-27 17:38:15,318 (asr_inference:509) INFO: speech length: 101184
2024-10-27 17:38:19,008 (beam_search:428) INFO: decoder input length: 78
2024-10-27 17:38:19,008 (beam_search:429) INFO: max output length: 78
2024-10-27 17:38:19,008 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:19,201 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:19,201 (beam_search:476) INFO:  -2.22 * 1.0 =  -2.22 for ctc
2024-10-27 17:38:19,201 (beam_search:479) INFO: total log probability: -2.22
2024-10-27 17:38:19,201 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:38:19,201 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:19,201 (beam_search:483) INFO: best hypo: THEMAGNETICISBECAUSEDOESN'THAVEASMANYPLASTICYELLOWIN

2024-10-27 17:38:19,204 (asr_inference:509) INFO: speech length: 22240
2024-10-27 17:38:20,176 (beam_search:428) INFO: decoder input length: 16
2024-10-27 17:38:20,176 (beam_search:429) INFO: max output length: 16
2024-10-27 17:38:20,176 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:20,181 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:20,181 (beam_search:476) INFO:  -0.42 * 1.0 =  -0.42 for ctc
2024-10-27 17:38:20,181 (beam_search:479) INFO: total log probability: -0.42
2024-10-27 17:38:20,181 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:38:20,181 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:20,181 (beam_search:483) INFO: best hypo: 

2024-10-27 17:38:20,184 (asr_inference:509) INFO: speech length: 11440
2024-10-27 17:38:20,910 (beam_search:428) INFO: decoder input length: 8
2024-10-27 17:38:20,910 (beam_search:429) INFO: max output length: 8
2024-10-27 17:38:20,910 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:20,915 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:20,916 (beam_search:476) INFO:  -0.38 * 1.0 =  -0.38 for ctc
2024-10-27 17:38:20,916 (beam_search:479) INFO: total log probability: -0.38
2024-10-27 17:38:20,916 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:38:20,916 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:20,916 (beam_search:483) INFO: best hypo: IT

2024-10-27 17:38:20,918 (asr_inference:509) INFO: speech length: 54720
2024-10-27 17:38:22,976 (beam_search:428) INFO: decoder input length: 42
2024-10-27 17:38:22,976 (beam_search:429) INFO: max output length: 42
2024-10-27 17:38:22,976 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:23,018 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:23,018 (beam_search:476) INFO:  -0.94 * 1.0 =  -0.94 for ctc
2024-10-27 17:38:23,018 (beam_search:479) INFO: total log probability: -0.94
2024-10-27 17:38:23,018 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:38:23,018 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:23,018 (beam_search:483) INFO: best hypo: WEHAVEBEENWITHMAGNETSAND

2024-10-27 17:38:23,021 (asr_inference:509) INFO: speech length: 215968
2024-10-27 17:38:31,792 (beam_search:428) INFO: decoder input length: 168
2024-10-27 17:38:31,792 (beam_search:429) INFO: max output length: 168
2024-10-27 17:38:31,792 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:32,403 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:32,404 (beam_search:476) INFO:  -4.81 * 1.0 =  -4.81 for ctc
2024-10-27 17:38:32,404 (beam_search:479) INFO: total log probability: -4.81
2024-10-27 17:38:32,404 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:38:32,404 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:32,404 (beam_search:483) INFO: best hypo: WELLTODAYWEHADWEDIDSOIT'SKINDAATOGETHERBUTALLTHEOTHERWEUSEJUSTOR

2024-10-27 17:38:32,406 (asr_inference:509) INFO: speech length: 169120
2024-10-27 17:38:38,902 (beam_search:428) INFO: decoder input length: 131
2024-10-27 17:38:38,902 (beam_search:429) INFO: max output length: 131
2024-10-27 17:38:38,902 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:39,374 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:39,374 (beam_search:476) INFO:  -2.79 * 1.0 =  -2.79 for ctc
2024-10-27 17:38:39,374 (beam_search:479) INFO: total log probability: -2.79
2024-10-27 17:38:39,374 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:38:39,374 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:39,374 (beam_search:483) INFO: best hypo: MAGNETISAMAGNETBUTYOUCANTURNITONANDOFFANDIT'SMOREAREGULARMAGNET

2024-10-27 17:38:39,377 (asr_inference:509) INFO: speech length: 241680
2024-10-27 17:38:49,434 (beam_search:428) INFO: decoder input length: 188
2024-10-27 17:38:49,434 (beam_search:429) INFO: max output length: 188
2024-10-27 17:38:49,434 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:50,291 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:50,291 (beam_search:476) INFO:  -4.99 * 1.0 =  -4.99 for ctc
2024-10-27 17:38:50,292 (beam_search:479) INFO: total log probability: -4.99
2024-10-27 17:38:50,292 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:38:50,292 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:50,292 (beam_search:483) INFO: best hypo: ISEEAWIRESAANDARIVETANDWASHERSANDADCELLBATTERYTHOSEARETHETHATYOUNEEDTOMAKEANELECTROMAGNET

2024-10-27 17:38:50,294 (asr_inference:509) INFO: speech length: 172048
2024-10-27 17:38:56,892 (beam_search:428) INFO: decoder input length: 133
2024-10-27 17:38:56,893 (beam_search:429) INFO: max output length: 133
2024-10-27 17:38:56,893 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:57,314 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:57,315 (beam_search:476) INFO:  -2.74 * 1.0 =  -2.74 for ctc
2024-10-27 17:38:57,315 (beam_search:479) INFO: total log probability: -2.74
2024-10-27 17:38:57,315 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:38:57,315 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:57,315 (beam_search:483) INFO: best hypo: OFTHATISTHATTHETHEELECTRICITYTHROUGHTHEPIECEOFMETALWHICHMAKESITUHMAGNETELECTROMAGNET

2024-10-27 17:38:57,317 (asr_inference:509) INFO: speech length: 324304
2024-10-27 17:39:11,723 (beam_search:428) INFO: decoder input length: 252
2024-10-27 17:39:11,724 (beam_search:429) INFO: max output length: 252
2024-10-27 17:39:11,724 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:13,871 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:13,871 (beam_search:476) INFO:  -6.28 * 1.0 =  -6.28 for ctc
2024-10-27 17:39:13,871 (beam_search:479) INFO: total log probability: -6.28
2024-10-27 17:39:13,871 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:39:13,872 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:13,872 (beam_search:483) INFO: best hypo: THEISWHATYOUUSETOTHEELECTROMAGNETONANDOFFIFTHEIFTHEIFTHESWITCHIFTHEPIECEOFISTOUCHINGTHEOTHERPIECETHENTHEWILLWORKIFITIFITIFITDOESN'TTHENTHEWILLBE

2024-10-27 17:39:13,875 (asr_inference:509) INFO: speech length: 49792
2024-10-27 17:39:15,776 (beam_search:428) INFO: decoder input length: 38
2024-10-27 17:39:15,776 (beam_search:429) INFO: max output length: 38
2024-10-27 17:39:15,777 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:15,810 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:15,810 (beam_search:476) INFO:  -1.20 * 1.0 =  -1.20 for ctc
2024-10-27 17:39:15,810 (beam_search:479) INFO: total log probability: -1.20
2024-10-27 17:39:15,810 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:39:15,810 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:15,810 (beam_search:483) INFO: best hypo: THESTHEELECTROMAGNETON

2024-10-27 17:39:15,813 (asr_inference:509) INFO: speech length: 171104
2024-10-27 17:39:22,336 (beam_search:428) INFO: decoder input length: 133
2024-10-27 17:39:22,336 (beam_search:429) INFO: max output length: 133
2024-10-27 17:39:22,336 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:22,730 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:22,730 (beam_search:476) INFO:  -3.13 * 1.0 =  -3.13 for ctc
2024-10-27 17:39:22,730 (beam_search:479) INFO: total log probability: -3.13
2024-10-27 17:39:22,730 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:39:22,730 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:22,730 (beam_search:483) INFO: best hypo: YOUCOULDPUTINMAYBEATHATCANTHATCANBEEDUPBYAMAGNET

2024-10-27 17:39:22,734 (asr_inference:509) INFO: speech length: 161024
2024-10-27 17:39:28,693 (beam_search:428) INFO: decoder input length: 125
2024-10-27 17:39:28,694 (beam_search:429) INFO: max output length: 125
2024-10-27 17:39:28,694 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:28,940 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:28,941 (beam_search:476) INFO:  -1.47 * 1.0 =  -1.47 for ctc
2024-10-27 17:39:28,941 (beam_search:479) INFO: total log probability: -1.47
2024-10-27 17:39:28,941 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:39:28,941 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:28,941 (beam_search:483) INFO: best hypo: METALCANIT'SSTEELUMSTEELORITHINK

2024-10-27 17:39:28,943 (asr_inference:509) INFO: speech length: 434672
2024-10-27 17:39:51,084 (beam_search:428) INFO: decoder input length: 339
2024-10-27 17:39:51,084 (beam_search:429) INFO: max output length: 339
2024-10-27 17:39:51,084 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:54,566 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:54,566 (beam_search:476) INFO:  -8.54 * 1.0 =  -8.54 for ctc
2024-10-27 17:39:54,566 (beam_search:479) INFO: total log probability: -8.54
2024-10-27 17:39:54,566 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:39:54,566 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:54,566 (beam_search:483) INFO: best hypo: THEELECTRICITY'SFLOWINGFROMOFTHEBATTERYTHEYONEISTHEPOSITIVEAROUNDTHESTEELWHICHTHENISTOTHESWITCHANDTHESWITCHISTOTHENEGATIVESIDETHETHATSTHESWITCHANDITLOOKSLIKEIT'SMIGHTBEITABOUTTOPICKUPTHETHEWASHERS

2024-10-27 17:39:54,569 (asr_inference:509) INFO: speech length: 285376
2024-10-27 17:40:06,451 (beam_search:428) INFO: decoder input length: 222
2024-10-27 17:40:06,451 (beam_search:429) INFO: max output length: 222
2024-10-27 17:40:06,451 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:07,408 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:07,409 (beam_search:476) INFO:  -5.49 * 1.0 =  -5.49 for ctc
2024-10-27 17:40:07,409 (beam_search:479) INFO: total log probability: -5.49
2024-10-27 17:40:07,409 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:40:07,409 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:07,409 (beam_search:483) INFO: best hypo: WASHERSBECAUSETHEUMTHEISONAREATTRACTEDTOTHEBUTITTURNSOFFTHEELECTROMAGNETDOESNOTWORKANDTHEWASHERS

2024-10-27 17:40:07,411 (asr_inference:509) INFO: speech length: 385520
2024-10-27 17:40:25,058 (beam_search:428) INFO: decoder input length: 300
2024-10-27 17:40:25,059 (beam_search:429) INFO: max output length: 300
2024-10-27 17:40:25,059 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:28,073 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:28,073 (beam_search:476) INFO:  -7.53 * 1.0 =  -7.53 for ctc
2024-10-27 17:40:28,073 (beam_search:479) INFO: total log probability: -7.53
2024-10-27 17:40:28,073 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:40:28,073 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:28,073 (beam_search:483) INFO: best hypo: THESWITCHISTHEFLOWOFELECTRICITYTHENEGATIVEANDTHEPOSITIVEENDOFTHEBATTERYTHENCAUSEYOUCAN'TATTACHWELLYOUCOULDDOITONEWAYYOUPUTTHEAWIREONTHEPOSITIVEANDTHEANDYOUCANNOTTURNITOFFWITHOUTPULLINGTHESTUFFOFF

2024-10-27 17:40:28,075 (asr_inference:509) INFO: speech length: 124064
2024-10-27 17:40:32,724 (beam_search:428) INFO: decoder input length: 96
2024-10-27 17:40:32,724 (beam_search:429) INFO: max output length: 96
2024-10-27 17:40:32,724 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:33,015 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:33,016 (beam_search:476) INFO:  -1.58 * 1.0 =  -1.58 for ctc
2024-10-27 17:40:33,016 (beam_search:479) INFO: total log probability: -1.58
2024-10-27 17:40:33,016 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:40:33,016 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:33,016 (beam_search:483) INFO: best hypo: UMIWOULDTHATIFITHADTWOBATTERIESMAYBETHATWOULDMAKETHEMAGNETMOREPOWERFUL

2024-10-27 17:40:33,018 (asr_inference:509) INFO: speech length: 220256
2024-10-27 17:40:41,886 (beam_search:428) INFO: decoder input length: 171
2024-10-27 17:40:41,887 (beam_search:429) INFO: max output length: 171
2024-10-27 17:40:41,887 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:42,549 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:42,549 (beam_search:476) INFO:  -3.13 * 1.0 =  -3.13 for ctc
2024-10-27 17:40:42,550 (beam_search:479) INFO: total log probability: -3.13
2024-10-27 17:40:42,550 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:40:42,550 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:42,550 (beam_search:483) INFO: best hypo: BECAUSEIFYOUHAVETWODCELLITHASTWODCELLBATTERIESITHASMOREELECTRICITYINSIDETHEMTHANONE

2024-10-27 17:40:42,552 (asr_inference:509) INFO: speech length: 380416
2024-10-27 17:41:00,116 (beam_search:428) INFO: decoder input length: 296
2024-10-27 17:41:00,116 (beam_search:429) INFO: max output length: 296
2024-10-27 17:41:00,116 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:02,893 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:02,893 (beam_search:476) INFO:  -6.07 * 1.0 =  -6.07 for ctc
2024-10-27 17:41:02,893 (beam_search:479) INFO: total log probability: -6.07
2024-10-27 17:41:02,893 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:41:02,893 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:02,893 (beam_search:483) INFO: best hypo: WELLTHEMOREWINDSTHATWEHAVEHADTHEMOREWASHERSTHATWEWOULDGETBECAUSEIFWEPUTIFWEPUTUMUMTENWASHERSIITENWINDSTHENWEWOULDGETTHREEWASHERSSOEVERYTHATYOUITTHENUMBEROFWASHERSTHATYOUGOINGUP

2024-10-27 17:41:02,896 (asr_inference:509) INFO: speech length: 159616
2024-10-27 17:41:08,756 (beam_search:428) INFO: decoder input length: 124
2024-10-27 17:41:08,756 (beam_search:429) INFO: max output length: 124
2024-10-27 17:41:08,756 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:09,043 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:09,043 (beam_search:476) INFO:  -2.08 * 1.0 =  -2.08 for ctc
2024-10-27 17:41:09,043 (beam_search:479) INFO: total log probability: -2.08
2024-10-27 17:41:09,043 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:41:09,043 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:09,043 (beam_search:483) INFO: best hypo: THEITUMTHEMORETHEMORETHEELECTROMAGNETTHEMOREELECTROMAGNETIS

2024-10-27 17:41:09,046 (asr_inference:509) INFO: speech length: 404400
2024-10-27 17:41:28,175 (beam_search:428) INFO: decoder input length: 315
2024-10-27 17:41:28,175 (beam_search:429) INFO: max output length: 315
2024-10-27 17:41:28,175 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:31,073 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:31,074 (beam_search:476) INFO:  -5.53 * 1.0 =  -5.53 for ctc
2024-10-27 17:41:31,074 (beam_search:479) INFO: total log probability: -5.53
2024-10-27 17:41:31,074 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:41:31,074 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:31,074 (beam_search:483) INFO: best hypo: BECAUSETHEELECTRICITYISTHROUGHTHEWIRESSOIFITGOESAROUNDTHETHESTEELRIVETUMTHATAHUNDREDTIMESTHENIT'SMORELIKETOHAVEUMIT'SITHASFIFTEENWASHERSBUTIFIGOTOTOTENONLYGETTHREEWASHERS

2024-10-27 17:41:31,077 (asr_inference:509) INFO: speech length: 102992
2024-10-27 17:41:34,732 (beam_search:428) INFO: decoder input length: 79
2024-10-27 17:41:34,732 (beam_search:429) INFO: max output length: 79
2024-10-27 17:41:34,732 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:34,962 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:34,963 (beam_search:476) INFO:  -3.24 * 1.0 =  -3.24 for ctc
2024-10-27 17:41:34,963 (beam_search:479) INFO: total log probability: -3.24
2024-10-27 17:41:34,963 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:41:34,963 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:34,963 (beam_search:483) INFO: best hypo: BECAUSEITHASMOREELECTRICITYINTHEWIRESSOITCANMAKEITTHEELECTROMAGNETSSTRONGER

2024-10-27 17:41:34,965 (asr_inference:509) INFO: speech length: 148864
2024-10-27 17:41:40,559 (beam_search:428) INFO: decoder input length: 115
2024-10-27 17:41:40,560 (beam_search:429) INFO: max output length: 115
2024-10-27 17:41:40,560 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:40,713 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:40,713 (beam_search:476) INFO:  -1.32 * 1.0 =  -1.32 for ctc
2024-10-27 17:41:40,713 (beam_search:479) INFO: total log probability: -1.32
2024-10-27 17:41:40,713 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:41:40,713 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:40,713 (beam_search:483) INFO: best hypo: UMITITANDOFITIN

2024-10-27 17:41:40,716 (asr_inference:509) INFO: speech length: 126144
2024-10-27 17:41:45,299 (beam_search:428) INFO: decoder input length: 98
2024-10-27 17:41:45,299 (beam_search:429) INFO: max output length: 98
2024-10-27 17:41:45,299 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:45,684 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:45,684 (beam_search:476) INFO:  -2.77 * 1.0 =  -2.77 for ctc
2024-10-27 17:41:45,684 (beam_search:479) INFO: total log probability: -2.77
2024-10-27 17:41:45,684 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:41:45,684 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:45,684 (beam_search:483) INFO: best hypo: UMWHENYOUACIRCUITTHENITCANTURNOFFALIGHTANDWHENYOUOPENACIRCUITITCANTURNONALIGHT

2024-10-27 17:41:45,687 (asr_inference:509) INFO: speech length: 31088
2024-10-27 17:41:46,945 (beam_search:428) INFO: decoder input length: 23
2024-10-27 17:41:46,945 (beam_search:429) INFO: max output length: 23
2024-10-27 17:41:46,945 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:46,956 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:46,956 (beam_search:476) INFO:  -0.19 * 1.0 =  -0.19 for ctc
2024-10-27 17:41:46,956 (beam_search:479) INFO: total log probability: -0.19
2024-10-27 17:41:46,956 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:41:46,956 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:46,956 (beam_search:483) INFO: best hypo: IT

2024-10-27 17:41:46,960 (asr_inference:509) INFO: speech length: 231408
2024-10-27 17:41:56,413 (beam_search:428) INFO: decoder input length: 180
2024-10-27 17:41:56,413 (beam_search:429) INFO: max output length: 180
2024-10-27 17:41:56,413 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:57,585 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:57,585 (beam_search:476) INFO:  -3.96 * 1.0 =  -3.96 for ctc
2024-10-27 17:41:57,585 (beam_search:479) INFO: total log probability: -3.96
2024-10-27 17:41:57,585 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:41:57,585 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:57,585 (beam_search:483) INFO: best hypo: IFTHE'SWIRE'SCONNECTEDTOTHEENDOFTHEBATTERYTHEMOTOR'TRUNANDTHENTHESWITCHHASTOTHEWAYTHEBATTERYFORITTOMAKETHEMOTOR

2024-10-27 17:41:57,589 (asr_inference:509) INFO: speech length: 63216
2024-10-27 17:41:59,945 (beam_search:428) INFO: decoder input length: 48
2024-10-27 17:41:59,945 (beam_search:429) INFO: max output length: 48
2024-10-27 17:41:59,945 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:00,029 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:00,029 (beam_search:476) INFO:  -1.62 * 1.0 =  -1.62 for ctc
2024-10-27 17:42:00,029 (beam_search:479) INFO: total log probability: -1.62
2024-10-27 17:42:00,029 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:42:00,029 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:00,029 (beam_search:483) INFO: best hypo: ABATTERY'SCONNECTEDTOATOMAKETHERUN

2024-10-27 17:42:00,032 (asr_inference:509) INFO: speech length: 75936
2024-10-27 17:42:02,830 (beam_search:428) INFO: decoder input length: 58
2024-10-27 17:42:02,831 (beam_search:429) INFO: max output length: 58
2024-10-27 17:42:02,831 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:02,956 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:02,956 (beam_search:476) INFO:  -0.63 * 1.0 =  -0.63 for ctc
2024-10-27 17:42:02,956 (beam_search:479) INFO: total log probability: -0.63
2024-10-27 17:42:02,956 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:42:02,956 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:02,956 (beam_search:483) INFO: best hypo: THEREISASWITCHONITTOMAKETHEMOTORONANDOFF

2024-10-27 17:42:02,958 (asr_inference:509) INFO: speech length: 317856
2024-10-27 17:42:17,103 (beam_search:428) INFO: decoder input length: 247
2024-10-27 17:42:17,103 (beam_search:429) INFO: max output length: 247
2024-10-27 17:42:17,103 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:18,973 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:18,973 (beam_search:476) INFO:  -4.59 * 1.0 =  -4.59 for ctc
2024-10-27 17:42:18,973 (beam_search:479) INFO: total log probability: -4.59
2024-10-27 17:42:18,973 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:42:18,973 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:18,973 (beam_search:483) INFO: best hypo: WHENTHESWITCHISISOPENTHENTHEMOTORCAN'TRUNCAUSETHESWITCHISITCONNECTSTHESOTHEMOTORCANANDANDIFTHESWITCHTHENTHECANCAUSEIT'SCONNECTINGALLTHEWIRES

2024-10-27 17:42:18,976 (asr_inference:509) INFO: speech length: 347104
2024-10-27 17:42:34,293 (beam_search:428) INFO: decoder input length: 270
2024-10-27 17:42:34,293 (beam_search:429) INFO: max output length: 270
2024-10-27 17:42:34,293 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:35,566 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:35,567 (beam_search:476) INFO:  -4.38 * 1.0 =  -4.38 for ctc
2024-10-27 17:42:35,567 (beam_search:479) INFO: total log probability: -4.38
2024-10-27 17:42:35,567 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:42:35,567 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:35,567 (beam_search:483) INFO: best hypo: UMAITISISCOULDISWHENYOUHAVEAOFWIRESCONNECTEDTOEACHOTHERWITHASWITCHIFTHESWITCHOPEN

2024-10-27 17:42:35,570 (asr_inference:509) INFO: speech length: 164960
2024-10-27 17:42:41,853 (beam_search:428) INFO: decoder input length: 128
2024-10-27 17:42:41,853 (beam_search:429) INFO: max output length: 128
2024-10-27 17:42:41,853 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:42,368 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:42,368 (beam_search:476) INFO:  -1.81 * 1.0 =  -1.81 for ctc
2024-10-27 17:42:42,368 (beam_search:479) INFO: total log probability: -1.81
2024-10-27 17:42:42,368 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:42:42,368 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:42,368 (beam_search:483) INFO: best hypo: ANDITWILLBETHENWHATISTRYINGTOONWILLBEOFFTHENIFYOUTHESWITCHITITWILLTURNON

2024-10-27 17:42:42,371 (asr_inference:509) INFO: speech length: 116896
2024-10-27 17:42:46,542 (beam_search:428) INFO: decoder input length: 90
2024-10-27 17:42:46,542 (beam_search:429) INFO: max output length: 90
2024-10-27 17:42:46,542 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:46,722 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:46,723 (beam_search:476) INFO:  -2.50 * 1.0 =  -2.50 for ctc
2024-10-27 17:42:46,723 (beam_search:479) INFO: total log probability: -2.50
2024-10-27 17:42:46,723 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:42:46,723 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:46,723 (beam_search:483) INFO: best hypo: THEYOUCANMAKETHEMMAKEAMOTORORLIGHTRUN

2024-10-27 17:42:46,725 (asr_inference:509) INFO: speech length: 94448
2024-10-27 17:42:50,231 (beam_search:428) INFO: decoder input length: 73
2024-10-27 17:42:50,231 (beam_search:429) INFO: max output length: 73
2024-10-27 17:42:50,231 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:50,386 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:50,387 (beam_search:476) INFO:  -0.66 * 1.0 =  -0.66 for ctc
2024-10-27 17:42:50,387 (beam_search:479) INFO: total log probability: -0.66
2024-10-27 17:42:50,387 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:42:50,387 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:50,387 (beam_search:483) INFO: best hypo: ABATTERYISCONNECTEDTOWIRESTOMAKETWOLIGHTSON

2024-10-27 17:42:50,390 (asr_inference:509) INFO: speech length: 329664
2024-10-27 17:43:04,530 (beam_search:428) INFO: decoder input length: 257
2024-10-27 17:43:04,530 (beam_search:429) INFO: max output length: 257
2024-10-27 17:43:04,530 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:06,149 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:06,149 (beam_search:476) INFO:  -5.54 * 1.0 =  -5.54 for ctc
2024-10-27 17:43:06,149 (beam_search:479) INFO: total log probability: -5.54
2024-10-27 17:43:06,149 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:43:06,149 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:06,149 (beam_search:483) INFO: best hypo: UMTHETHEELECTRICITYFROMTHEBATTERYGOESTOONETOBOTHSANDTHENTHEYBOTHLIGHTSARECONNECTEDSOITIFONEISN'TFLOWINGTHENITCANGETTHE

2024-10-27 17:43:06,152 (asr_inference:509) INFO: speech length: 260336
2024-10-27 17:43:17,081 (beam_search:428) INFO: decoder input length: 202
2024-10-27 17:43:17,081 (beam_search:429) INFO: max output length: 202
2024-10-27 17:43:17,081 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:18,089 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:18,089 (beam_search:476) INFO:  -4.24 * 1.0 =  -4.24 for ctc
2024-10-27 17:43:18,089 (beam_search:479) INFO: total log probability: -4.24
2024-10-27 17:43:18,089 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:43:18,089 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:18,090 (beam_search:483) INFO: best hypo: ONEINTHEONEACHSIDEOFTHEBATTERYITTOSOTHEREELECTRICITYTOTHELIGHTBULBSANDTHENONETOTHEELECTRICITYINTHELIGHT

2024-10-27 17:43:18,092 (asr_inference:509) INFO: speech length: 134864
2024-10-27 17:43:23,127 (beam_search:428) INFO: decoder input length: 104
2024-10-27 17:43:23,127 (beam_search:429) INFO: max output length: 104
2024-10-27 17:43:23,127 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:23,446 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:23,446 (beam_search:476) INFO:  -1.64 * 1.0 =  -1.64 for ctc
2024-10-27 17:43:23,446 (beam_search:479) INFO: total log probability: -1.64
2024-10-27 17:43:23,446 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:43:23,446 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:23,446 (beam_search:483) INFO: best hypo: ITFLOWSFROMTHEBATTERYUPTHEWIREANDANDGOESINTOANDTHENGOESINTOTHEBULB

2024-10-27 17:43:23,449 (asr_inference:509) INFO: speech length: 150704
2024-10-27 17:43:29,107 (beam_search:428) INFO: decoder input length: 117
2024-10-27 17:43:29,107 (beam_search:429) INFO: max output length: 117
2024-10-27 17:43:29,107 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:29,529 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:29,530 (beam_search:476) INFO:  -0.75 * 1.0 =  -0.75 for ctc
2024-10-27 17:43:29,530 (beam_search:479) INFO: total log probability: -0.75
2024-10-27 17:43:29,530 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:43:29,530 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:29,530 (beam_search:483) INFO: best hypo: UMONETWOWIRESARECONNECTEDTOTHEBATTERYANDONEWIRE'SCONNECTEDTOTHELIGHTBULBS

2024-10-27 17:43:29,533 (asr_inference:509) INFO: speech length: 114816
2024-10-27 17:43:33,864 (beam_search:428) INFO: decoder input length: 89
2024-10-27 17:43:33,865 (beam_search:429) INFO: max output length: 89
2024-10-27 17:43:33,865 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:34,032 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:34,032 (beam_search:476) INFO:  -2.70 * 1.0 =  -2.70 for ctc
2024-10-27 17:43:34,032 (beam_search:479) INFO: total log probability: -2.70
2024-10-27 17:43:34,032 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:43:34,032 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:34,032 (beam_search:483) INFO: best hypo: UMITMAKESITSOTHATTHEREISENERGYFLOWINGTHE

2024-10-27 17:43:34,035 (asr_inference:509) INFO: speech length: 256640
2024-10-27 17:43:44,541 (beam_search:428) INFO: decoder input length: 199
2024-10-27 17:43:44,541 (beam_search:429) INFO: max output length: 199
2024-10-27 17:43:44,541 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:45,230 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:45,230 (beam_search:476) INFO:  -3.22 * 1.0 =  -3.22 for ctc
2024-10-27 17:43:45,230 (beam_search:479) INFO: total log probability: -3.22
2024-10-27 17:43:45,230 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:43:45,230 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:45,230 (beam_search:483) INFO: best hypo: THETHEELECTRICITYISFROMTHEBATTERYTOANDTHENITANDTHENITGOESINTOTHELIGHTBULB

2024-10-27 17:43:45,232 (asr_inference:509) INFO: speech length: 82256
2024-10-27 17:43:48,204 (beam_search:428) INFO: decoder input length: 63
2024-10-27 17:43:48,205 (beam_search:429) INFO: max output length: 63
2024-10-27 17:43:48,205 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:48,285 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:48,286 (beam_search:476) INFO:  -1.67 * 1.0 =  -1.67 for ctc
2024-10-27 17:43:48,286 (beam_search:479) INFO: total log probability: -1.67
2024-10-27 17:43:48,286 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:43:48,286 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:48,286 (beam_search:483) INFO: best hypo: ELECTRICITYISFLOWINGINAINA

2024-10-27 17:43:48,288 (asr_inference:509) INFO: speech length: 197072
2024-10-27 17:43:55,941 (beam_search:428) INFO: decoder input length: 153
2024-10-27 17:43:55,941 (beam_search:429) INFO: max output length: 153
2024-10-27 17:43:55,942 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:56,315 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:56,316 (beam_search:476) INFO:  -3.84 * 1.0 =  -3.84 for ctc
2024-10-27 17:43:56,316 (beam_search:479) INFO: total log probability: -3.84
2024-10-27 17:43:56,316 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:43:56,316 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:56,316 (beam_search:483) INFO: best hypo: UMUMTHATYOUCANSOMETHINGSBYINGTHEMONAMAGNET

2024-10-27 17:43:56,318 (asr_inference:509) INFO: speech length: 151744
2024-10-27 17:44:01,859 (beam_search:428) INFO: decoder input length: 118
2024-10-27 17:44:01,859 (beam_search:429) INFO: max output length: 118
2024-10-27 17:44:01,859 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:02,254 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:02,254 (beam_search:476) INFO:  -4.59 * 1.0 =  -4.59 for ctc
2024-10-27 17:44:02,254 (beam_search:479) INFO: total log probability: -4.59
2024-10-27 17:44:02,254 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:44:02,254 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:02,254 (beam_search:483) INFO: best hypo: UMWEEDAONAMAGNETANDTHENTRIEDTOCONNECTITTOANOTHERPAPERCLIPITTOGETHER

2024-10-27 17:44:02,256 (asr_inference:509) INFO: speech length: 207648
2024-10-27 17:44:10,525 (beam_search:428) INFO: decoder input length: 161
2024-10-27 17:44:10,525 (beam_search:429) INFO: max output length: 161
2024-10-27 17:44:10,525 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:11,054 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:11,055 (beam_search:476) INFO:  -2.91 * 1.0 =  -2.91 for ctc
2024-10-27 17:44:11,055 (beam_search:479) INFO: total log probability: -2.91
2024-10-27 17:44:11,055 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:44:11,055 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:11,055 (beam_search:483) INFO: best hypo: THATTHATYOUHAVETHATYOUHAVETOCONNECTASIDEOFTHEMAGNETTOGETITTO

2024-10-27 17:44:11,057 (asr_inference:509) INFO: speech length: 228912
2024-10-27 17:44:20,265 (beam_search:428) INFO: decoder input length: 178
2024-10-27 17:44:20,266 (beam_search:429) INFO: max output length: 178
2024-10-27 17:44:20,266 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:21,134 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:21,134 (beam_search:476) INFO:  -4.08 * 1.0 =  -4.08 for ctc
2024-10-27 17:44:21,134 (beam_search:479) INFO: total log probability: -4.08
2024-10-27 17:44:21,134 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:44:21,134 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:21,134 (beam_search:483) INFO: best hypo: YOUNEEDAYOUNEEDTOASIDETOTHEMAGNETANDTHENANDTHENTOUCHTHATSIDETOAOFTHEYOU'RETO

2024-10-27 17:44:21,137 (asr_inference:509) INFO: speech length: 87760
2024-10-27 17:44:24,296 (beam_search:428) INFO: decoder input length: 68
2024-10-27 17:44:24,297 (beam_search:429) INFO: max output length: 68
2024-10-27 17:44:24,297 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:24,447 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:24,447 (beam_search:476) INFO:  -2.57 * 1.0 =  -2.57 for ctc
2024-10-27 17:44:24,447 (beam_search:479) INFO: total log probability: -2.57
2024-10-27 17:44:24,447 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:44:24,447 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:24,447 (beam_search:483) INFO: best hypo: THE'STHENAILSOITCANPICKUPTHEPAPERCLIP

2024-10-27 17:44:24,450 (asr_inference:509) INFO: speech length: 271536
2024-10-27 17:44:35,432 (beam_search:428) INFO: decoder input length: 211
2024-10-27 17:44:35,432 (beam_search:429) INFO: max output length: 211
2024-10-27 17:44:35,432 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:36,592 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:36,592 (beam_search:476) INFO:  -4.57 * 1.0 =  -4.57 for ctc
2024-10-27 17:44:36,592 (beam_search:479) INFO: total log probability: -4.57
2024-10-27 17:44:36,592 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:44:36,592 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:36,592 (beam_search:483) INFO: best hypo: THEMAGNETHASSTEELTHEYOUHAVETOBEANDTHETHATYOU'RETOPICKUPWITHWHATYOU'REALSOHASTOBESTEEL

2024-10-27 17:44:36,595 (asr_inference:509) INFO: speech length: 307312
2024-10-27 17:44:49,766 (beam_search:428) INFO: decoder input length: 239
2024-10-27 17:44:49,766 (beam_search:429) INFO: max output length: 239
2024-10-27 17:44:49,766 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:50,565 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:50,565 (beam_search:476) INFO:  -4.21 * 1.0 =  -4.21 for ctc
2024-10-27 17:44:50,567 (beam_search:479) INFO: total log probability: -4.21
2024-10-27 17:44:50,567 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:44:50,567 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:50,567 (beam_search:483) INFO: best hypo: WE'VEWEVEHOWYOUCANHOWMUCHITITTOBREAKTHEMAGNETICINBETWEENTWO

2024-10-27 17:44:50,569 (asr_inference:509) INFO: speech length: 386000
2024-10-27 17:45:08,015 (beam_search:428) INFO: decoder input length: 301
2024-10-27 17:45:08,015 (beam_search:429) INFO: max output length: 301
2024-10-27 17:45:08,015 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:11,116 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:11,116 (beam_search:476) INFO:  -5.19 * 1.0 =  -5.19 for ctc
2024-10-27 17:45:11,116 (beam_search:479) INFO: total log probability: -5.19
2024-10-27 17:45:11,116 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:45:11,116 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:11,116 (beam_search:483) INFO: best hypo: WETOOKAANDPUTASTICKTHATAMAGNETONITINATHEHADANDTHENAMAGNETONTHEOTHERSIDESOTHEY'DTHENWEUSEDANDTHENWEUSEDTHENWEPUTINWASHERSTOSEEHOWHOWMANYWASHERSITTOOKTOMAKETHEMAGNETSGO

2024-10-27 17:45:11,119 (asr_inference:509) INFO: speech length: 410304
2024-10-27 17:45:30,191 (beam_search:428) INFO: decoder input length: 320
2024-10-27 17:45:30,191 (beam_search:429) INFO: max output length: 320
2024-10-27 17:45:30,191 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:32,636 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:32,636 (beam_search:476) INFO:  -7.15 * 1.0 =  -7.15 for ctc
2024-10-27 17:45:32,636 (beam_search:479) INFO: total log probability: -7.15
2024-10-27 17:45:32,636 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:45:32,636 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:32,636 (beam_search:483) INFO: best hypo: IOUTTHATMAGNETSIT'SIT'STOITGETTHEMAWHENTHEYJUSTTOGETHERWITHINTHEMBUTIT'STOGETTHEMWHENTHERE'SATHERE'SALITTLE

2024-10-27 17:45:32,639 (asr_inference:509) INFO: speech length: 302768
2024-10-27 17:45:45,325 (beam_search:428) INFO: decoder input length: 236
2024-10-27 17:45:45,325 (beam_search:429) INFO: max output length: 236
2024-10-27 17:45:45,325 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:46,547 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:46,547 (beam_search:476) INFO:  -3.29 * 1.0 =  -3.29 for ctc
2024-10-27 17:45:46,549 (beam_search:479) INFO: total log probability: -3.29
2024-10-27 17:45:46,549 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:45:46,549 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:46,549 (beam_search:483) INFO: best hypo: WECONNECTEDABATTERYTOASWITCHTHENWECONNECTEDTHEBATTERYSWITCHTOABIGMETALBYTHEAWIREAROUNDITTHENWEEDUPWASHERS

2024-10-27 17:45:46,552 (asr_inference:509) INFO: speech length: 84256
2024-10-27 17:45:49,755 (beam_search:428) INFO: decoder input length: 65
2024-10-27 17:45:49,755 (beam_search:429) INFO: max output length: 65
2024-10-27 17:45:49,755 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:49,839 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:49,840 (beam_search:476) INFO:  -0.96 * 1.0 =  -0.96 for ctc
2024-10-27 17:45:49,840 (beam_search:479) INFO: total log probability: -0.96
2024-10-27 17:45:49,840 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:45:49,840 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:49,840 (beam_search:483) INFO: best hypo: ITWASKINDOFLIKEACIRCUIT

2024-10-27 17:45:49,843 (asr_inference:509) INFO: speech length: 212720
2024-10-27 17:45:58,071 (beam_search:428) INFO: decoder input length: 165
2024-10-27 17:45:58,071 (beam_search:429) INFO: max output length: 165
2024-10-27 17:45:58,071 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:58,652 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:58,652 (beam_search:476) INFO:  -3.27 * 1.0 =  -3.27 for ctc
2024-10-27 17:45:58,652 (beam_search:479) INFO: total log probability: -3.27
2024-10-27 17:45:58,652 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:45:58,652 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:58,652 (beam_search:483) INFO: best hypo: UMTHESWITCHSTHEMAGNETONANDOFFCAUSEIT'SKINDOFLIKEINGA

2024-10-27 17:45:58,656 (asr_inference:509) INFO: speech length: 212368
2024-10-27 17:46:07,303 (beam_search:428) INFO: decoder input length: 165
2024-10-27 17:46:07,304 (beam_search:429) INFO: max output length: 165
2024-10-27 17:46:07,304 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:08,350 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:08,350 (beam_search:476) INFO:  -4.26 * 1.0 =  -4.26 for ctc
2024-10-27 17:46:08,352 (beam_search:479) INFO: total log probability: -4.26
2024-10-27 17:46:08,352 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:46:08,352 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:08,352 (beam_search:483) INFO: best hypo: WHENIFTHESWITCHISN'TTHENITCAN'TMAKETHENAILINTOAMAGNETCAUSEITALLHASTOBECONNECTEDFORTHENAILTOBEAMAGNET

2024-10-27 17:46:08,355 (asr_inference:509) INFO: speech length: 194784
2024-10-27 17:46:16,210 (beam_search:428) INFO: decoder input length: 151
2024-10-27 17:46:16,210 (beam_search:429) INFO: max output length: 151
2024-10-27 17:46:16,210 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:16,395 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:16,395 (beam_search:476) INFO:  -2.01 * 1.0 =  -2.01 for ctc
2024-10-27 17:46:16,395 (beam_search:479) INFO: total log probability: -2.01
2024-10-27 17:46:16,396 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:46:16,396 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:16,396 (beam_search:483) INFO: best hypo: UMTHEDIDANEXAMPLETHE

2024-10-27 17:46:16,399 (asr_inference:509) INFO: speech length: 371792
2024-10-27 17:46:33,471 (beam_search:428) INFO: decoder input length: 289
2024-10-27 17:46:33,471 (beam_search:429) INFO: max output length: 289
2024-10-27 17:46:33,471 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:35,700 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:35,701 (beam_search:476) INFO:  -5.89 * 1.0 =  -5.89 for ctc
2024-10-27 17:46:35,701 (beam_search:479) INFO: total log probability: -5.89
2024-10-27 17:46:35,701 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:46:35,701 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:35,701 (beam_search:483) INFO: best hypo: UMHEHECONNECTEDTHEBATTERYANDSWITCHTHENDITAROUNDANDTHENUHTHENDTHEWIREAROUNDTHENAILANDHADONEOFCOMEUPTOTHESWITCHANDSEEMANYWASHERSTHEYCOULDPICKUP

2024-10-27 17:46:35,705 (asr_inference:509) INFO: speech length: 330192
2024-10-27 17:46:50,206 (beam_search:428) INFO: decoder input length: 257
2024-10-27 17:46:50,206 (beam_search:429) INFO: max output length: 257
2024-10-27 17:46:50,206 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:51,268 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:51,268 (beam_search:476) INFO:  -3.95 * 1.0 =  -3.95 for ctc
2024-10-27 17:46:51,268 (beam_search:479) INFO: total log probability: -3.95
2024-10-27 17:46:51,268 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:46:51,268 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:51,268 (beam_search:483) INFO: best hypo: THEENERGYFROMTHEBATTERYGOESTHROUGHTHESWITCHANDTHENAILANDAITITSOIT'SANELECTROMAGNET

2024-10-27 17:46:51,270 (asr_inference:509) INFO: speech length: 131792
2024-10-27 17:46:56,037 (beam_search:428) INFO: decoder input length: 102
2024-10-27 17:46:56,037 (beam_search:429) INFO: max output length: 102
2024-10-27 17:46:56,037 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:56,339 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:56,340 (beam_search:476) INFO:  -2.34 * 1.0 =  -2.34 for ctc
2024-10-27 17:46:56,340 (beam_search:479) INFO: total log probability: -2.34
2024-10-27 17:46:56,340 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:46:56,340 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:56,340 (beam_search:483) INFO: best hypo: IFYOUCANINGUPTHEWASHERSWITHITTHENYOUCANIT'SMAGNETIC

2024-10-27 17:46:56,342 (asr_inference:509) INFO: speech length: 164224
2024-10-27 17:47:02,527 (beam_search:428) INFO: decoder input length: 127
2024-10-27 17:47:02,527 (beam_search:429) INFO: max output length: 127
2024-10-27 17:47:02,527 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:02,973 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:02,973 (beam_search:476) INFO:  -4.40 * 1.0 =  -4.40 for ctc
2024-10-27 17:47:02,973 (beam_search:479) INFO: total log probability: -4.40
2024-10-27 17:47:02,973 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:47:02,973 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:02,973 (beam_search:483) INFO: best hypo: IFITISN'TTOPICKUPANYITWASHERSANDTHESWITCHIT'SOPEN

2024-10-27 17:47:02,975 (asr_inference:509) INFO: speech length: 159536
2024-10-27 17:47:08,781 (beam_search:428) INFO: decoder input length: 124
2024-10-27 17:47:08,781 (beam_search:429) INFO: max output length: 124
2024-10-27 17:47:08,781 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:09,069 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:09,069 (beam_search:476) INFO:  -2.10 * 1.0 =  -2.10 for ctc
2024-10-27 17:47:09,069 (beam_search:479) INFO: total log probability: -2.10
2024-10-27 17:47:09,069 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:47:09,069 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:09,069 (beam_search:483) INFO: best hypo: UMIFYOUNEEDTOMOVESOMETHINGANDMETALTOADIFFERENTPLACE

2024-10-27 17:47:09,071 (asr_inference:509) INFO: speech length: 154496
2024-10-27 17:47:14,869 (beam_search:428) INFO: decoder input length: 120
2024-10-27 17:47:14,869 (beam_search:429) INFO: max output length: 120
2024-10-27 17:47:14,869 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:15,207 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:15,208 (beam_search:476) INFO:  -4.94 * 1.0 =  -4.94 for ctc
2024-10-27 17:47:15,208 (beam_search:479) INFO: total log probability: -4.94
2024-10-27 17:47:15,208 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 17:47:15,208 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:15,208 (beam_search:483) INFO: best hypo: WITHAMAGNETYOUCAN'TIFYOUWANTTOYOUHAVETOITOFF

2024-10-27 17:47:15,211 (asr_inference:509) INFO: speech length: 375296
2024-10-27 17:47:31,838 (beam_search:428) INFO: decoder input length: 292
2024-10-27 17:47:31,838 (beam_search:429) INFO: max output length: 292
2024-10-27 17:47:31,838 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:35,513 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:35,514 (beam_search:476) INFO:  -8.54 * 1.0 =  -8.54 for ctc
2024-10-27 17:47:35,514 (beam_search:479) INFO: total log probability: -8.54
2024-10-27 17:47:35,514 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:47:35,514 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:35,514 (beam_search:483) INFO: best hypo: WELLTODAYWEDIDSCIENCEANDWEUSEDABOARDANDWEDALIGHTBULBANDALIGHTBULBANDABATTERYINAANDTHEBATTERYABATTERYANDWEMADEASOTHEANDWEALSOHADAANDWEHADTOMAKETHEANDTHELIGHTTHELIGHTHADTOGOONANDTHEHADALITTLEPIECEOFTHATHADTO

2024-10-27 17:47:35,517 (asr_inference:509) INFO: speech length: 338640
2024-10-27 17:47:50,486 (beam_search:428) INFO: decoder input length: 264
2024-10-27 17:47:50,486 (beam_search:429) INFO: max output length: 264
2024-10-27 17:47:50,486 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:53,141 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:53,142 (beam_search:476) INFO:  -5.31 * 1.0 =  -5.31 for ctc
2024-10-27 17:47:53,142 (beam_search:479) INFO: total log probability: -5.31
2024-10-27 17:47:53,142 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:47:53,142 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:53,142 (beam_search:483) INFO: best hypo: WELLWEUMWESORTOFHADWIRESANDWEPUTTHEWIRESONTHEBATTERYENDSFROMTHEPOSITIVETOTHENEGATIVENONEGATIVETOTHEPOSITIVEANDSOTHENWEEDTHATUPTOTHEMOTORANDTHENWEHOOKEDONEOFTHESWIRESTOTHESWITCHTHATWEHAD

2024-10-27 17:47:53,145 (asr_inference:509) INFO: speech length: 79072
2024-10-27 17:47:56,049 (beam_search:428) INFO: decoder input length: 61
2024-10-27 17:47:56,049 (beam_search:429) INFO: max output length: 61
2024-10-27 17:47:56,049 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:56,200 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:56,200 (beam_search:476) INFO:  -3.68 * 1.0 =  -3.68 for ctc
2024-10-27 17:47:56,200 (beam_search:479) INFO: total log probability: -3.68
2024-10-27 17:47:56,200 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:47:56,200 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:56,200 (beam_search:483) INFO: best hypo: IDON'TREALLYKNOWIDON'TTHINKMYUSBUTUMYEAH

2024-10-27 17:47:56,202 (asr_inference:509) INFO: speech length: 158304
2024-10-27 17:48:02,284 (beam_search:428) INFO: decoder input length: 123
2024-10-27 17:48:02,284 (beam_search:429) INFO: max output length: 123
2024-10-27 17:48:02,284 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:02,972 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:02,972 (beam_search:476) INFO:  -0.92 * 1.0 =  -0.92 for ctc
2024-10-27 17:48:02,972 (beam_search:479) INFO: total log probability: -0.92
2024-10-27 17:48:02,972 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 17:48:02,972 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:02,972 (beam_search:483) INFO: best hypo: WELLTHERE'STWOWIRESCONNECTEDTOTHEBATTERYANDTOTHELIGHTBULBANDTHEENERGYTHEBATTERYHASITTOTHELIGHTBULBANDITMAKESITLIGHTUP

2024-10-27 17:48:02,974 (asr_inference:509) INFO: speech length: 213216
2024-10-27 17:48:11,398 (beam_search:428) INFO: decoder input length: 166
2024-10-27 17:48:11,398 (beam_search:429) INFO: max output length: 166
2024-10-27 17:48:11,398 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:12,272 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:12,272 (beam_search:476) INFO:  -2.33 * 1.0 =  -2.33 for ctc
2024-10-27 17:48:12,272 (beam_search:479) INFO: total log probability: -2.33
2024-10-27 17:48:12,272 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:48:12,272 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:12,272 (beam_search:483) INFO: best hypo: ICANSEETHATIT'SGOINGFROMTHETOTHERIGHTSANDITGOESTHROUGHTHELIGHTBULBLIKEIFITWASSTILLSORTOFUM

2024-10-27 17:48:12,275 (asr_inference:509) INFO: speech length: 53072
2024-10-27 17:48:14,272 (beam_search:428) INFO: decoder input length: 40
2024-10-27 17:48:14,272 (beam_search:429) INFO: max output length: 40
2024-10-27 17:48:14,272 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:14,346 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:14,346 (beam_search:476) INFO:  -0.59 * 1.0 =  -0.59 for ctc
2024-10-27 17:48:14,346 (beam_search:479) INFO: total log probability: -0.59
2024-10-27 17:48:14,346 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:48:14,346 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:14,346 (beam_search:483) INFO: best hypo: ITHINKTHATITISGOINGFROMTHENEGATIVETOTHEPOSITIVE

2024-10-27 17:48:14,348 (asr_inference:509) INFO: speech length: 183376
2024-10-27 17:48:21,488 (beam_search:428) INFO: decoder input length: 142
2024-10-27 17:48:21,488 (beam_search:429) INFO: max output length: 142
2024-10-27 17:48:21,488 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:22,047 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:22,047 (beam_search:476) INFO:  -2.02 * 1.0 =  -2.02 for ctc
2024-10-27 17:48:22,049 (beam_search:479) INFO: total log probability: -2.02
2024-10-27 17:48:22,049 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:48:22,049 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:22,049 (beam_search:483) INFO: best hypo: WELLTHEBATTERYHASTWOWIRESCONNECTEDTOITANDTHENIT'SCONNECTEDTOTHEMOTORWHICHMAKESTHELITTLE

2024-10-27 17:48:22,051 (asr_inference:509) INFO: speech length: 120464
2024-10-27 17:48:26,425 (beam_search:428) INFO: decoder input length: 93
2024-10-27 17:48:26,425 (beam_search:429) INFO: max output length: 93
2024-10-27 17:48:26,425 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:26,786 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:26,787 (beam_search:476) INFO:  -2.36 * 1.0 =  -2.36 for ctc
2024-10-27 17:48:26,787 (beam_search:479) INFO: total log probability: -2.36
2024-10-27 17:48:26,787 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:48:26,787 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:26,787 (beam_search:483) INFO: best hypo: UMITHINKITWON'TWORKASIIJUSTDON'TTHINKITIDON'TREALLYIT

2024-10-27 17:48:26,789 (asr_inference:509) INFO: speech length: 28128
2024-10-27 17:48:27,969 (beam_search:428) INFO: decoder input length: 21
2024-10-27 17:48:27,969 (beam_search:429) INFO: max output length: 21
2024-10-27 17:48:27,969 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:27,996 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:27,996 (beam_search:476) INFO:  -0.17 * 1.0 =  -0.17 for ctc
2024-10-27 17:48:27,996 (beam_search:479) INFO: total log probability: -0.17
2024-10-27 17:48:27,996 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 17:48:27,996 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:27,997 (beam_search:483) INFO: best hypo: NOI'MNOTREALLYSURE

2024-10-27 17:48:28,000 (asr_inference:509) INFO: speech length: 159984
2024-10-27 17:48:34,101 (beam_search:428) INFO: decoder input length: 124
2024-10-27 17:48:34,101 (beam_search:429) INFO: max output length: 124
2024-10-27 17:48:34,101 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:34,669 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:34,669 (beam_search:476) INFO:  -2.85 * 1.0 =  -2.85 for ctc
2024-10-27 17:48:34,669 (beam_search:479) INFO: total log probability: -2.85
2024-10-27 17:48:34,669 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:48:34,669 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:34,669 (beam_search:483) INFO: best hypo: WELLASIYOUBEFORETHEWIRESARECONNECTEDTOTHEMOTORANDTHEELECTRICITYFROMTHEBATTERYGOESTHROUGHTHEMOTORANDMAKESTHE

2024-10-27 17:48:34,672 (asr_inference:509) INFO: speech length: 62016
2024-10-27 17:48:36,994 (beam_search:428) INFO: decoder input length: 47
2024-10-27 17:48:36,994 (beam_search:429) INFO: max output length: 47
2024-10-27 17:48:36,994 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:37,112 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:37,112 (beam_search:476) INFO:  -2.11 * 1.0 =  -2.11 for ctc
2024-10-27 17:48:37,112 (beam_search:479) INFO: total log probability: -2.11
2024-10-27 17:48:37,112 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:48:37,112 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:37,112 (beam_search:483) INFO: best hypo: ITLOOKSLIKEIT'SSTILLBUTITDOESN'TLOOKASTOME

2024-10-27 17:48:37,114 (asr_inference:509) INFO: speech length: 15344
2024-10-27 17:48:37,885 (beam_search:428) INFO: decoder input length: 11
2024-10-27 17:48:37,885 (beam_search:429) INFO: max output length: 11
2024-10-27 17:48:37,885 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:37,894 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:37,894 (beam_search:476) INFO:  -2.04 * 1.0 =  -2.04 for ctc
2024-10-27 17:48:37,894 (beam_search:479) INFO: total log probability: -2.04
2024-10-27 17:48:37,894 (beam_search:480) INFO: normalized log probability: -0.41
2024-10-27 17:48:37,894 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:37,894 (beam_search:483) INFO: best hypo: 'DBE

2024-10-27 17:48:37,897 (asr_inference:509) INFO: speech length: 109200
2024-10-27 17:48:41,984 (beam_search:428) INFO: decoder input length: 84
2024-10-27 17:48:41,984 (beam_search:429) INFO: max output length: 84
2024-10-27 17:48:41,984 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:42,199 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:42,199 (beam_search:476) INFO:  -1.59 * 1.0 =  -1.59 for ctc
2024-10-27 17:48:42,199 (beam_search:479) INFO: total log probability: -1.59
2024-10-27 17:48:42,199 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:48:42,199 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:42,199 (beam_search:483) INFO: best hypo: YEAHIT'STHETHETHETHATITWASWHENTHEBATTERYWASTHE

2024-10-27 17:48:42,202 (asr_inference:509) INFO: speech length: 30048
2024-10-27 17:48:43,442 (beam_search:428) INFO: decoder input length: 22
2024-10-27 17:48:43,442 (beam_search:429) INFO: max output length: 22
2024-10-27 17:48:43,442 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:43,466 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:43,466 (beam_search:476) INFO:  -0.49 * 1.0 =  -0.49 for ctc
2024-10-27 17:48:43,466 (beam_search:479) INFO: total log probability: -0.49
2024-10-27 17:48:43,466 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:48:43,466 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:43,466 (beam_search:483) INFO: best hypo: IDON'TREALLYKNOW

2024-10-27 17:48:43,469 (asr_inference:509) INFO: speech length: 21296
2024-10-27 17:48:44,385 (beam_search:428) INFO: decoder input length: 16
2024-10-27 17:48:44,385 (beam_search:429) INFO: max output length: 16
2024-10-27 17:48:44,385 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:44,398 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:44,398 (beam_search:476) INFO:  -0.35 * 1.0 =  -0.35 for ctc
2024-10-27 17:48:44,398 (beam_search:479) INFO: total log probability: -0.35
2024-10-27 17:48:44,398 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:48:44,398 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:44,398 (beam_search:483) INFO: best hypo: UMIIT

2024-10-27 17:48:44,400 (asr_inference:509) INFO: speech length: 206144
2024-10-27 17:48:52,695 (beam_search:428) INFO: decoder input length: 160
2024-10-27 17:48:52,695 (beam_search:429) INFO: max output length: 160
2024-10-27 17:48:52,695 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:53,626 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:53,626 (beam_search:476) INFO:  -2.91 * 1.0 =  -2.91 for ctc
2024-10-27 17:48:53,626 (beam_search:479) INFO: total log probability: -2.91
2024-10-27 17:48:53,626 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:48:53,626 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:53,626 (beam_search:483) INFO: best hypo: WELLIT'SIFYOUTHEBATTERYAROUNDLIKEYOUTOLDMETODOITWILLGOTHEOPPOSITETHATITDOESWHENIT'STHEFROMNEGATIVETOPOSITIVE

2024-10-27 17:48:53,629 (asr_inference:509) INFO: speech length: 28112
2024-10-27 17:48:54,809 (beam_search:428) INFO: decoder input length: 21
2024-10-27 17:48:54,809 (beam_search:429) INFO: max output length: 21
2024-10-27 17:48:54,809 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:54,834 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:54,834 (beam_search:476) INFO:  -0.89 * 1.0 =  -0.89 for ctc
2024-10-27 17:48:54,834 (beam_search:479) INFO: total log probability: -0.89
2024-10-27 17:48:54,834 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:48:54,834 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:54,834 (beam_search:483) INFO: best hypo: IDON'TTHINKIT

2024-10-27 17:48:54,837 (asr_inference:509) INFO: speech length: 103872
2024-10-27 17:48:58,709 (beam_search:428) INFO: decoder input length: 80
2024-10-27 17:48:58,709 (beam_search:429) INFO: max output length: 80
2024-10-27 17:48:58,709 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:58,951 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:58,951 (beam_search:476) INFO:  -1.09 * 1.0 =  -1.09 for ctc
2024-10-27 17:48:58,951 (beam_search:479) INFO: total log probability: -1.09
2024-10-27 17:48:58,951 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:48:58,951 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:58,951 (beam_search:483) INFO: best hypo: WELLIDON'TTHINKITIFIT'SFROMTHEPOSITIVETOASASTHEENERGY

2024-10-27 17:48:58,954 (asr_inference:509) INFO: speech length: 122304
2024-10-27 17:49:03,497 (beam_search:428) INFO: decoder input length: 95
2024-10-27 17:49:03,497 (beam_search:429) INFO: max output length: 95
2024-10-27 17:49:03,497 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:03,715 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:03,715 (beam_search:476) INFO:  -2.23 * 1.0 =  -2.23 for ctc
2024-10-27 17:49:03,715 (beam_search:479) INFO: total log probability: -2.23
2024-10-27 17:49:03,715 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:49:03,715 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:03,715 (beam_search:483) INFO: best hypo: BECAUSEIT'SFLOWINGTHESAMEBUTIT'SJUSTTHE

2024-10-27 17:49:03,717 (asr_inference:509) INFO: speech length: 64496
2024-10-27 17:49:06,063 (beam_search:428) INFO: decoder input length: 49
2024-10-27 17:49:06,063 (beam_search:429) INFO: max output length: 49
2024-10-27 17:49:06,063 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:06,102 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:06,102 (beam_search:476) INFO:  -0.87 * 1.0 =  -0.87 for ctc
2024-10-27 17:49:06,102 (beam_search:479) INFO: total log probability: -0.87
2024-10-27 17:49:06,102 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:49:06,103 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:06,103 (beam_search:483) INFO: best hypo: UMWHATDOEXACTLY

2024-10-27 17:49:06,105 (asr_inference:509) INFO: speech length: 218528
2024-10-27 17:49:14,834 (beam_search:428) INFO: decoder input length: 170
2024-10-27 17:49:14,834 (beam_search:429) INFO: max output length: 170
2024-10-27 17:49:14,834 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:15,703 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:15,704 (beam_search:476) INFO:  -4.16 * 1.0 =  -4.16 for ctc
2024-10-27 17:49:15,704 (beam_search:479) INFO: total log probability: -4.16
2024-10-27 17:49:15,704 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:49:15,704 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:15,704 (beam_search:483) INFO: best hypo: UMIDON'TTHINKMUCHBECAUSEITJUSTSORTOFSINCEYOUTHEBATTERY'SSORTOFLIKEINGTHEWAYOFTHE

2024-10-27 17:49:15,707 (asr_inference:509) INFO: speech length: 121760
2024-10-27 17:49:20,369 (beam_search:428) INFO: decoder input length: 94
2024-10-27 17:49:20,369 (beam_search:429) INFO: max output length: 94
2024-10-27 17:49:20,369 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:20,535 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:20,536 (beam_search:476) INFO:  -0.81 * 1.0 =  -0.81 for ctc
2024-10-27 17:49:20,536 (beam_search:479) INFO: total log probability: -0.81
2024-10-27 17:49:20,536 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:49:20,536 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:20,536 (beam_search:483) INFO: best hypo: UMLIKEISAIDITJUSTTHEWAYTHETHE

2024-10-27 17:49:20,539 (asr_inference:509) INFO: speech length: 71712
2024-10-27 17:49:23,147 (beam_search:428) INFO: decoder input length: 55
2024-10-27 17:49:23,147 (beam_search:429) INFO: max output length: 55
2024-10-27 17:49:23,147 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:23,255 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:23,255 (beam_search:476) INFO:  -2.27 * 1.0 =  -2.27 for ctc
2024-10-27 17:49:23,255 (beam_search:479) INFO: total log probability: -2.27
2024-10-27 17:49:23,255 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:49:23,255 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:23,255 (beam_search:483) INFO: best hypo: OFGOINGFROMLEFTTORIGHTITGOFROMRIGHTTO

2024-10-27 17:49:23,257 (asr_inference:509) INFO: speech length: 188880
2024-10-27 17:49:30,656 (beam_search:428) INFO: decoder input length: 147
2024-10-27 17:49:30,656 (beam_search:429) INFO: max output length: 147
2024-10-27 17:49:30,656 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:31,297 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:31,297 (beam_search:476) INFO:  -3.23 * 1.0 =  -3.23 for ctc
2024-10-27 17:49:31,297 (beam_search:479) INFO: total log probability: -3.23
2024-10-27 17:49:31,297 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:49:31,297 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:31,297 (beam_search:483) INFO: best hypo: WELLMAYBEBECAUSEITTHEBATTERYTHE'SWAYOFIT'SLIKETHEFROMNEGATIVETOPOSITIVETOPOSITIVETONEGATIVE

2024-10-27 17:49:31,299 (asr_inference:509) INFO: speech length: 123664
2024-10-27 17:49:36,061 (beam_search:428) INFO: decoder input length: 96
2024-10-27 17:49:36,061 (beam_search:429) INFO: max output length: 96
2024-10-27 17:49:36,061 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:36,334 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:36,334 (beam_search:476) INFO:  -1.78 * 1.0 =  -1.78 for ctc
2024-10-27 17:49:36,334 (beam_search:479) INFO: total log probability: -1.78
2024-10-27 17:49:36,334 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:49:36,334 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:36,334 (beam_search:483) INFO: best hypo: BECAUSEIT'SJUSTLIKEINGTHEWAYTHEELECTRICITYLIKEYOUTHEANDTHEMOTOR

2024-10-27 17:49:36,337 (asr_inference:509) INFO: speech length: 10960
2024-10-27 17:49:37,005 (beam_search:428) INFO: decoder input length: 8
2024-10-27 17:49:37,005 (beam_search:429) INFO: max output length: 8
2024-10-27 17:49:37,005 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:37,013 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:37,013 (beam_search:476) INFO:  -0.08 * 1.0 =  -0.08 for ctc
2024-10-27 17:49:37,013 (beam_search:479) INFO: total log probability: -0.08
2024-10-27 17:49:37,013 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 17:49:37,013 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:37,013 (beam_search:483) INFO: best hypo: YOU'RE

2024-10-27 17:49:37,016 (asr_inference:509) INFO: speech length: 330992
2024-10-27 17:49:51,292 (beam_search:428) INFO: decoder input length: 258
2024-10-27 17:49:51,292 (beam_search:429) INFO: max output length: 258
2024-10-27 17:49:51,292 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:52,691 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:52,691 (beam_search:476) INFO:  -3.91 * 1.0 =  -3.91 for ctc
2024-10-27 17:49:52,691 (beam_search:479) INFO: total log probability: -3.91
2024-10-27 17:49:52,691 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:49:52,691 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:52,691 (beam_search:483) INFO: best hypo: WELLWEHAVEUMTHEYWASHERSWEHAVETWOANDWEHAVETWOMAGNETSANDWEUSETHEMTOSEEHOWTHEISUMHOWITWORKS

2024-10-27 17:49:52,693 (asr_inference:509) INFO: speech length: 224128
2024-10-27 17:50:01,633 (beam_search:428) INFO: decoder input length: 174
2024-10-27 17:50:01,633 (beam_search:429) INFO: max output length: 174
2024-10-27 17:50:01,633 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:02,744 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:02,744 (beam_search:476) INFO:  -4.31 * 1.0 =  -4.31 for ctc
2024-10-27 17:50:02,744 (beam_search:479) INFO: total log probability: -4.31
2024-10-27 17:50:02,744 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:50:02,744 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:02,744 (beam_search:483) INFO: best hypo: UMWELLWETOPUTITTOGETHERANDWEDIDITWITHAINTHEMIDDLEANDITWEHADTOSEEHOWMANYWASHERSITTOOKSOTHEITWOULDTAKETHEMAGNETSAPART

2024-10-27 17:50:02,746 (asr_inference:509) INFO: speech length: 120080
2024-10-27 17:50:07,147 (beam_search:428) INFO: decoder input length: 93
2024-10-27 17:50:07,147 (beam_search:429) INFO: max output length: 93
2024-10-27 17:50:07,147 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:07,335 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:07,335 (beam_search:476) INFO:  -0.78 * 1.0 =  -0.78 for ctc
2024-10-27 17:50:07,335 (beam_search:479) INFO: total log probability: -0.78
2024-10-27 17:50:07,335 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:50:07,335 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:07,335 (beam_search:483) INFO: best hypo: WEJUSTHADTOUSETHEMASLIKEATOTHEMAGNETS

2024-10-27 17:50:07,338 (asr_inference:509) INFO: speech length: 217248
2024-10-27 17:50:16,027 (beam_search:428) INFO: decoder input length: 169
2024-10-27 17:50:16,027 (beam_search:429) INFO: max output length: 169
2024-10-27 17:50:16,027 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:16,568 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:16,568 (beam_search:476) INFO:  -2.13 * 1.0 =  -2.13 for ctc
2024-10-27 17:50:16,568 (beam_search:479) INFO: total log probability: -2.13
2024-10-27 17:50:16,568 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:50:16,568 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:16,568 (beam_search:483) INFO: best hypo: WELLUSINGTHEWASHERSWEHADTOMAKETHEUMTOMAKETHEMAGNETSUMNOTTOBE

2024-10-27 17:50:16,571 (asr_inference:509) INFO: speech length: 9136
2024-10-27 17:50:17,201 (beam_search:428) INFO: decoder input length: 6
2024-10-27 17:50:17,202 (beam_search:429) INFO: max output length: 6
2024-10-27 17:50:17,202 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:17,205 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:17,205 (beam_search:476) INFO:  -0.08 * 1.0 =  -0.08 for ctc
2024-10-27 17:50:17,205 (beam_search:479) INFO: total log probability: -0.08
2024-10-27 17:50:17,205 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:50:17,205 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:17,205 (beam_search:483) INFO: best hypo: 

2024-10-27 17:50:17,207 (asr_inference:509) INFO: speech length: 36944
2024-10-27 17:50:18,705 (beam_search:428) INFO: decoder input length: 28
2024-10-27 17:50:18,705 (beam_search:429) INFO: max output length: 28
2024-10-27 17:50:18,705 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:18,725 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:18,725 (beam_search:476) INFO:  -1.39 * 1.0 =  -1.39 for ctc
2024-10-27 17:50:18,725 (beam_search:479) INFO: total log probability: -1.39
2024-10-27 17:50:18,725 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 17:50:18,725 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:18,725 (beam_search:483) INFO: best hypo: YOUDOIT

2024-10-27 17:50:18,727 (asr_inference:509) INFO: speech length: 93504
2024-10-27 17:50:22,113 (beam_search:428) INFO: decoder input length: 72
2024-10-27 17:50:22,113 (beam_search:429) INFO: max output length: 72
2024-10-27 17:50:22,113 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:22,189 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:22,189 (beam_search:476) INFO:  -1.76 * 1.0 =  -1.76 for ctc
2024-10-27 17:50:22,189 (beam_search:479) INFO: total log probability: -1.76
2024-10-27 17:50:22,189 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:50:22,189 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:22,189 (beam_search:483) INFO: best hypo: WELLWEWEREABOUTANDCIRCUITS

2024-10-27 17:50:22,191 (asr_inference:509) INFO: speech length: 86896
2024-10-27 17:50:25,279 (beam_search:428) INFO: decoder input length: 67
2024-10-27 17:50:25,279 (beam_search:429) INFO: max output length: 67
2024-10-27 17:50:25,279 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:25,433 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:25,434 (beam_search:476) INFO:  -0.75 * 1.0 =  -0.75 for ctc
2024-10-27 17:50:25,434 (beam_search:479) INFO: total log probability: -0.75
2024-10-27 17:50:25,434 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:50:25,434 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:25,434 (beam_search:483) INFO: best hypo: WELLIT'SLIKEACIRCUITTHATIT'SYOUCANTWOPATHWAYS

2024-10-27 17:50:25,437 (asr_inference:509) INFO: speech length: 178416
2024-10-27 17:50:32,518 (beam_search:428) INFO: decoder input length: 138
2024-10-27 17:50:32,518 (beam_search:429) INFO: max output length: 138
2024-10-27 17:50:32,518 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:33,096 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:33,096 (beam_search:476) INFO:  -4.06 * 1.0 =  -4.06 for ctc
2024-10-27 17:50:33,096 (beam_search:479) INFO: total log probability: -4.06
2024-10-27 17:50:33,096 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:50:33,096 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:33,096 (beam_search:483) INFO: best hypo: WELLIT'SIFYOUHAVEAAYOUCANONLYONEWITHYOURFINGERWHICHISAAWHICHISCIRCUITFROM

2024-10-27 17:50:33,098 (asr_inference:509) INFO: speech length: 27280
2024-10-27 17:50:34,189 (beam_search:428) INFO: decoder input length: 20
2024-10-27 17:50:34,189 (beam_search:429) INFO: max output length: 20
2024-10-27 17:50:34,189 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:34,215 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:34,215 (beam_search:476) INFO:  -0.09 * 1.0 =  -0.09 for ctc
2024-10-27 17:50:34,215 (beam_search:479) INFO: total log probability: -0.09
2024-10-27 17:50:34,215 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 17:50:34,215 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:34,215 (beam_search:483) INFO: best hypo: UMIDON'TREALLYKNOW

2024-10-27 17:50:34,218 (asr_inference:509) INFO: speech length: 199264
2024-10-27 17:50:42,236 (beam_search:428) INFO: decoder input length: 155
2024-10-27 17:50:42,236 (beam_search:429) INFO: max output length: 155
2024-10-27 17:50:42,236 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:43,113 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:43,113 (beam_search:476) INFO:  -5.05 * 1.0 =  -5.05 for ctc
2024-10-27 17:50:43,113 (beam_search:479) INFO: total log probability: -5.05
2024-10-27 17:50:43,113 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:50:43,113 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:43,113 (beam_search:483) INFO: best hypo: WELLTHERE'TWOLIGHTANDAONEBATTERYSOI'MGUESSINGCAUSEWEDIDTHISINCLASSIT'SNOTIT'SPROBABLYGONNABE

2024-10-27 17:50:43,115 (asr_inference:509) INFO: speech length: 20736
2024-10-27 17:50:44,073 (beam_search:428) INFO: decoder input length: 15
2024-10-27 17:50:44,073 (beam_search:429) INFO: max output length: 15
2024-10-27 17:50:44,073 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:44,083 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:44,083 (beam_search:476) INFO:  -0.50 * 1.0 =  -0.50 for ctc
2024-10-27 17:50:44,083 (beam_search:479) INFO: total log probability: -0.50
2024-10-27 17:50:44,083 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:50:44,083 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:44,083 (beam_search:483) INFO: best hypo: ITS

2024-10-27 17:50:44,085 (asr_inference:509) INFO: speech length: 102304
2024-10-27 17:50:47,785 (beam_search:428) INFO: decoder input length: 79
2024-10-27 17:50:47,785 (beam_search:429) INFO: max output length: 79
2024-10-27 17:50:47,786 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:47,900 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:47,900 (beam_search:476) INFO:  -2.67 * 1.0 =  -2.67 for ctc
2024-10-27 17:50:47,900 (beam_search:479) INFO: total log probability: -2.67
2024-10-27 17:50:47,901 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 17:50:47,901 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:47,901 (beam_search:483) INFO: best hypo: WELLTHISISASITCALLEDA

2024-10-27 17:50:47,904 (asr_inference:509) INFO: speech length: 78640
2024-10-27 17:50:50,736 (beam_search:428) INFO: decoder input length: 60
2024-10-27 17:50:50,736 (beam_search:429) INFO: max output length: 60
2024-10-27 17:50:50,736 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:50,774 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:50,775 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 17:50:50,775 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 17:50:50,775 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 17:50:50,775 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:50,775 (beam_search:483) INFO: best hypo: WHAT'S

2024-10-27 17:50:50,777 (asr_inference:509) INFO: speech length: 180752
2024-10-27 17:50:57,741 (beam_search:428) INFO: decoder input length: 140
2024-10-27 17:50:57,741 (beam_search:429) INFO: max output length: 140
2024-10-27 17:50:57,742 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:58,211 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:58,212 (beam_search:476) INFO:  -1.18 * 1.0 =  -1.18 for ctc
2024-10-27 17:50:58,212 (beam_search:479) INFO: total log probability: -1.18
2024-10-27 17:50:58,212 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:50:58,212 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:58,212 (beam_search:483) INFO: best hypo: WELLYOUCANTWOPATHWAYSUMWITHONELIGHTBULBANDANOTHERLIGHTBULBYOUCOULDANOTHERONE

2024-10-27 17:50:58,214 (asr_inference:509) INFO: speech length: 130480
2024-10-27 17:51:03,327 (beam_search:428) INFO: decoder input length: 101
2024-10-27 17:51:03,327 (beam_search:429) INFO: max output length: 101
2024-10-27 17:51:03,327 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:03,612 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:03,612 (beam_search:476) INFO:  -1.83 * 1.0 =  -1.83 for ctc
2024-10-27 17:51:03,612 (beam_search:479) INFO: total log probability: -1.83
2024-10-27 17:51:03,612 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:51:03,612 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:03,613 (beam_search:483) INFO: best hypo: WELLTHEBATTERYENERGYTOTHETHEWIRESTOTHELIGHTBULBMAKESITLIGHTUP

2024-10-27 17:51:03,615 (asr_inference:509) INFO: speech length: 182624
2024-10-27 17:51:10,521 (beam_search:428) INFO: decoder input length: 142
2024-10-27 17:51:10,522 (beam_search:429) INFO: max output length: 142
2024-10-27 17:51:10,522 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:11,476 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:11,476 (beam_search:476) INFO:  -1.96 * 1.0 =  -1.96 for ctc
2024-10-27 17:51:11,476 (beam_search:479) INFO: total log probability: -1.96
2024-10-27 17:51:11,476 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:51:11,476 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:11,476 (beam_search:483) INFO: best hypo: WELLONEISFROMTHEBATTERYTOTHELIGHTBULBANDTHENONEYOUCANI'MNOTSUREIFITISBUTUMFROMONELIGHTBULBTOANOTHERTHATLOOKSLIKEANOTHERONETOME

2024-10-27 17:51:11,479 (asr_inference:509) INFO: speech length: 82160
2024-10-27 17:51:14,520 (beam_search:428) INFO: decoder input length: 63
2024-10-27 17:51:14,520 (beam_search:429) INFO: max output length: 63
2024-10-27 17:51:14,520 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:14,624 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:14,624 (beam_search:476) INFO:  -0.75 * 1.0 =  -0.75 for ctc
2024-10-27 17:51:14,624 (beam_search:479) INFO: total log probability: -0.75
2024-10-27 17:51:14,624 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:51:14,624 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:14,624 (beam_search:483) INFO: best hypo: UMDOYOUMEANWITHTHETWOLIGHTBULBS

2024-10-27 17:51:14,627 (asr_inference:509) INFO: speech length: 169120
2024-10-27 17:51:21,098 (beam_search:428) INFO: decoder input length: 131
2024-10-27 17:51:21,099 (beam_search:429) INFO: max output length: 131
2024-10-27 17:51:21,099 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:21,858 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:21,859 (beam_search:476) INFO:  -1.96 * 1.0 =  -1.96 for ctc
2024-10-27 17:51:21,859 (beam_search:479) INFO: total log probability: -1.96
2024-10-27 17:51:21,859 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:51:21,859 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:21,859 (beam_search:483) INFO: best hypo: YOUCOULDDOITLIKEWITHALIGHTBULBANDTHENYOUCOULDGETAWIRETOITANDTHENTHEWIRETOTHELIGHTBULBWHICHAONEWIRETOTHEBATTERY

2024-10-27 17:51:21,862 (asr_inference:509) INFO: speech length: 41888
2024-10-27 17:51:23,502 (beam_search:428) INFO: decoder input length: 32
2024-10-27 17:51:23,502 (beam_search:429) INFO: max output length: 32
2024-10-27 17:51:23,502 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:23,541 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:23,541 (beam_search:476) INFO:  -1.30 * 1.0 =  -1.30 for ctc
2024-10-27 17:51:23,542 (beam_search:479) INFO: total log probability: -1.30
2024-10-27 17:51:23,542 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:51:23,542 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:23,542 (beam_search:483) INFO: best hypo: UMITHINKITALIKETHIS

2024-10-27 17:51:23,544 (asr_inference:509) INFO: speech length: 83392
2024-10-27 17:51:26,431 (beam_search:428) INFO: decoder input length: 64
2024-10-27 17:51:26,432 (beam_search:429) INFO: max output length: 64
2024-10-27 17:51:26,432 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:26,481 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:26,481 (beam_search:476) INFO:  -1.05 * 1.0 =  -1.05 for ctc
2024-10-27 17:51:26,481 (beam_search:479) INFO: total log probability: -1.05
2024-10-27 17:51:26,481 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:51:26,481 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:26,481 (beam_search:483) INFO: best hypo: THISISACIRCUIT

2024-10-27 17:51:26,484 (asr_inference:509) INFO: speech length: 293184
2024-10-27 17:51:39,363 (beam_search:428) INFO: decoder input length: 228
2024-10-27 17:51:39,364 (beam_search:429) INFO: max output length: 228
2024-10-27 17:51:39,364 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:41,634 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:41,634 (beam_search:476) INFO:  -5.72 * 1.0 =  -5.72 for ctc
2024-10-27 17:51:41,634 (beam_search:479) INFO: total log probability: -5.72
2024-10-27 17:51:41,634 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:51:41,634 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:41,634 (beam_search:483) INFO: best hypo: WELLTHEBATTERYHASAWIRETOTHEYLITTLETHAT'STOTHELIGHTBULBWHICHTHATLIGHTUPBUTTHENYOUNEEDTHEWIRETOTHETHEOTHERSIDEOFTHEBATTERYWHICHHASANOTHERWIRETHATTOTHELIGHTBULBANDTHENTHATHASANOTHERWIRETHATGOESTOTHEBATTERY

2024-10-27 17:51:41,638 (asr_inference:509) INFO: speech length: 185200
2024-10-27 17:51:48,709 (beam_search:428) INFO: decoder input length: 144
2024-10-27 17:51:48,709 (beam_search:429) INFO: max output length: 144
2024-10-27 17:51:48,709 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:49,591 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:49,591 (beam_search:476) INFO:  -3.48 * 1.0 =  -3.48 for ctc
2024-10-27 17:51:49,591 (beam_search:479) INFO: total log probability: -3.48
2024-10-27 17:51:49,591 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:51:49,591 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:49,591 (beam_search:483) INFO: best hypo: WELLTHETWOCIRCUITSISTHATYOUCANLIKETWOPATHWAYSLIKEYOUCANINTHISONEBECAUSEYOUCANONEFROMTHEBATTERYTOLIGHTBULBANDTHEBATTERYTOANOTHERLIGHTBULB

2024-10-27 17:51:49,594 (asr_inference:509) INFO: speech length: 82144
2024-10-27 17:51:52,490 (beam_search:428) INFO: decoder input length: 63
2024-10-27 17:51:52,491 (beam_search:429) INFO: max output length: 63
2024-10-27 17:51:52,491 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:52,629 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:52,629 (beam_search:476) INFO:  -2.32 * 1.0 =  -2.32 for ctc
2024-10-27 17:51:52,629 (beam_search:479) INFO: total log probability: -2.32
2024-10-27 17:51:52,629 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:51:52,629 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:52,629 (beam_search:483) INFO: best hypo: THEOTHERLIGHTBULBWILLSTILLKEEPWORKINGBECAUSEIT'SPARALLEL

2024-10-27 17:51:52,631 (asr_inference:509) INFO: speech length: 326480
2024-10-27 17:52:06,992 (beam_search:428) INFO: decoder input length: 254
2024-10-27 17:52:06,992 (beam_search:429) INFO: max output length: 254
2024-10-27 17:52:06,992 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:08,900 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:08,900 (beam_search:476) INFO:  -9.41 * 1.0 =  -9.41 for ctc
2024-10-27 17:52:08,900 (beam_search:479) INFO: total log probability: -9.41
2024-10-27 17:52:08,900 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:52:08,900 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:08,901 (beam_search:483) INFO: best hypo: BECAUSEWEDIDTHISEXPERIMENTTHEYITTOLDUSINTHESCIENCETHATUMPEOPLEWERETHESEBIGOFSANDTHEYWEREEMANDONEOUTTHETHEOUTANDSOWEHADTOTOTWOANDOUR

2024-10-27 17:52:08,903 (asr_inference:509) INFO: speech length: 70640
2024-10-27 17:52:11,630 (beam_search:428) INFO: decoder input length: 54
2024-10-27 17:52:11,630 (beam_search:429) INFO: max output length: 54
2024-10-27 17:52:11,630 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:11,714 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:11,715 (beam_search:476) INFO:  -2.26 * 1.0 =  -2.26 for ctc
2024-10-27 17:52:11,715 (beam_search:479) INFO: total log probability: -2.26
2024-10-27 17:52:11,715 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:52:11,715 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:11,715 (beam_search:483) INFO: best hypo: ALSOAPARALLELBECAUSEYOUCANTWOPATHWAYSAGAIN

2024-10-27 17:52:11,718 (asr_inference:509) INFO: speech length: 243632
2024-10-27 17:52:21,892 (beam_search:428) INFO: decoder input length: 189
2024-10-27 17:52:21,892 (beam_search:429) INFO: max output length: 189
2024-10-27 17:52:21,892 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:23,022 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:23,023 (beam_search:476) INFO:  -3.19 * 1.0 =  -3.19 for ctc
2024-10-27 17:52:23,023 (beam_search:479) INFO: total log probability: -3.19
2024-10-27 17:52:23,023 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:52:23,023 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:23,023 (beam_search:483) INFO: best hypo: UMWELLITHINKTHISWOULDBECAUSEITHASTWOTOUCHINGBUTITHINKTHEISTHATIT'SNOTITHINKTHEMINUSSIDESHOULDBEWHERETHEPLUSIS

2024-10-27 17:52:23,025 (asr_inference:509) INFO: speech length: 90240
2024-10-27 17:52:26,304 (beam_search:428) INFO: decoder input length: 69
2024-10-27 17:52:26,304 (beam_search:429) INFO: max output length: 69
2024-10-27 17:52:26,304 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:26,398 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:26,398 (beam_search:476) INFO:  -1.00 * 1.0 =  -1.00 for ctc
2024-10-27 17:52:26,399 (beam_search:479) INFO: total log probability: -1.00
2024-10-27 17:52:26,399 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:52:26,399 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:26,399 (beam_search:483) INFO: best hypo: UMTHERE'SATHATSAYS

2024-10-27 17:52:26,401 (asr_inference:509) INFO: speech length: 86784
2024-10-27 17:52:29,532 (beam_search:428) INFO: decoder input length: 67
2024-10-27 17:52:29,533 (beam_search:429) INFO: max output length: 67
2024-10-27 17:52:29,533 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:29,612 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:29,613 (beam_search:476) INFO:  -1.02 * 1.0 =  -1.02 for ctc
2024-10-27 17:52:29,613 (beam_search:479) INFO: total log probability: -1.02
2024-10-27 17:52:29,613 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:52:29,613 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:29,613 (beam_search:483) INFO: best hypo: WELLITHINKTHEYTHEYUPMORE

2024-10-27 17:52:29,615 (asr_inference:509) INFO: speech length: 67456
2024-10-27 17:52:32,098 (beam_search:428) INFO: decoder input length: 52
2024-10-27 17:52:32,098 (beam_search:429) INFO: max output length: 52
2024-10-27 17:52:32,098 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:32,164 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:32,164 (beam_search:476) INFO:  -1.59 * 1.0 =  -1.59 for ctc
2024-10-27 17:52:32,164 (beam_search:479) INFO: total log probability: -1.59
2024-10-27 17:52:32,164 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:52:32,164 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:32,164 (beam_search:483) INFO: best hypo: ITHEINOTICEDTHATTHEYMORE

2024-10-27 17:52:32,167 (asr_inference:509) INFO: speech length: 134960
2024-10-27 17:52:37,490 (beam_search:428) INFO: decoder input length: 104
2024-10-27 17:52:37,490 (beam_search:429) INFO: max output length: 104
2024-10-27 17:52:37,490 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:37,795 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:37,795 (beam_search:476) INFO:  -2.69 * 1.0 =  -2.69 for ctc
2024-10-27 17:52:37,795 (beam_search:479) INFO: total log probability: -2.69
2024-10-27 17:52:37,795 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:52:37,795 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:37,795 (beam_search:483) INFO: best hypo: THATWHENYOUTHETHEYYOUCANSEETHETHATITWASTHENITLIGHTSUP

2024-10-27 17:52:37,797 (asr_inference:509) INFO: speech length: 255744
2024-10-27 17:52:48,241 (beam_search:428) INFO: decoder input length: 199
2024-10-27 17:52:48,241 (beam_search:429) INFO: max output length: 199
2024-10-27 17:52:48,241 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:49,703 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:49,703 (beam_search:476) INFO:  -5.00 * 1.0 =  -5.00 for ctc
2024-10-27 17:52:49,703 (beam_search:479) INFO: total log probability: -5.00
2024-10-27 17:52:49,703 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:52:49,703 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:49,703 (beam_search:483) INFO: best hypo: WELLLIKEIWEDIDTHISONEINCLASSANDMANYTIMESANDITDOESN'TVERYWELLSOIWOULDANOTHERBATTERYAND'DMAKEITUMTOIMEANITITWOULDMAKEEMBRIGHTER

2024-10-27 17:52:49,705 (asr_inference:509) INFO: speech length: 33696
2024-10-27 17:52:51,041 (beam_search:428) INFO: decoder input length: 25
2024-10-27 17:52:51,041 (beam_search:429) INFO: max output length: 25
2024-10-27 17:52:51,041 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:51,062 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:51,062 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 17:52:51,062 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 17:52:51,062 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:52:51,062 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:51,062 (beam_search:483) INFO: best hypo: YOUCAN'T

2024-10-27 17:52:51,065 (asr_inference:509) INFO: speech length: 88400
2024-10-27 17:52:54,314 (beam_search:428) INFO: decoder input length: 68
2024-10-27 17:52:54,314 (beam_search:429) INFO: max output length: 68
2024-10-27 17:52:54,314 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:54,452 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:54,452 (beam_search:476) INFO:  -2.02 * 1.0 =  -2.02 for ctc
2024-10-27 17:52:54,452 (beam_search:479) INFO: total log probability: -2.02
2024-10-27 17:52:54,452 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:52:54,452 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:54,452 (beam_search:483) INFO: best hypo: UMWHATDOYOUMEANBYIFWEMADEITMAKEITMORE

2024-10-27 17:52:54,454 (asr_inference:509) INFO: speech length: 141696
2024-10-27 17:52:59,807 (beam_search:428) INFO: decoder input length: 110
2024-10-27 17:52:59,807 (beam_search:429) INFO: max output length: 110
2024-10-27 17:52:59,807 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:00,250 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:00,251 (beam_search:476) INFO:  -2.21 * 1.0 =  -2.21 for ctc
2024-10-27 17:53:00,251 (beam_search:479) INFO: total log probability: -2.21
2024-10-27 17:53:00,251 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:53:00,251 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:00,251 (beam_search:483) INFO: best hypo: UMIDON'TKNOWITHINKITPROBABLYBUTTHEGOODABOUTCIRCUITSISTHATWHENONEOUTTHEDON'T

2024-10-27 17:53:00,253 (asr_inference:509) INFO: speech length: 141552
2024-10-27 17:53:05,721 (beam_search:428) INFO: decoder input length: 110
2024-10-27 17:53:05,721 (beam_search:429) INFO: max output length: 110
2024-10-27 17:53:05,721 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:06,152 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:06,152 (beam_search:476) INFO:  -2.98 * 1.0 =  -2.98 for ctc
2024-10-27 17:53:06,152 (beam_search:479) INFO: total log probability: -2.98
2024-10-27 17:53:06,152 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:53:06,152 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:06,152 (beam_search:483) INFO: best hypo: WELLICANTSEETWOBUTTHEY'RESEPARATEANDONEITHINKISAPARALLELCIRCUITANDONEISA

2024-10-27 17:53:06,154 (asr_inference:509) INFO: speech length: 214336
2024-10-27 17:53:14,508 (beam_search:428) INFO: decoder input length: 166
2024-10-27 17:53:14,508 (beam_search:429) INFO: max output length: 166
2024-10-27 17:53:14,508 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:15,058 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:15,058 (beam_search:476) INFO:  -4.04 * 1.0 =  -4.04 for ctc
2024-10-27 17:53:15,058 (beam_search:479) INFO: total log probability: -4.04
2024-10-27 17:53:15,058 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:53:15,058 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:15,058 (beam_search:483) INFO: best hypo: WELLACIRCUITUMTHECIRCUITFROMSOYOUWOULDBEUSUALLYBETOWITHYOURAORNOT

2024-10-27 17:53:15,061 (asr_inference:509) INFO: speech length: 374960
2024-10-27 17:53:32,144 (beam_search:428) INFO: decoder input length: 292
2024-10-27 17:53:32,144 (beam_search:429) INFO: max output length: 292
2024-10-27 17:53:32,144 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:35,730 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:35,730 (beam_search:476) INFO:  -5.94 * 1.0 =  -5.94 for ctc
2024-10-27 17:53:35,730 (beam_search:479) INFO: total log probability: -5.94
2024-10-27 17:53:35,730 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:53:35,730 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:35,730 (beam_search:483) INFO: best hypo: ISEEISTWOANDONETHEONEONTHELEFTHASABATTERYTHENTWOLIGHTBULBSANDTHEONEONTHERIGHTHASABATTERYANDTHENHASALIGHTBULBANDTHENITHASANOTHERLIGHTBULBBUTIT'SSORTOFMORELIKEUMITITHINKTHAT'SPARALLELTHEONEONTHEISACIRCUIT

2024-10-27 17:53:35,733 (asr_inference:509) INFO: speech length: 51328
2024-10-27 17:53:37,683 (beam_search:428) INFO: decoder input length: 39
2024-10-27 17:53:37,684 (beam_search:429) INFO: max output length: 39
2024-10-27 17:53:37,684 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:37,713 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:37,713 (beam_search:476) INFO:  -0.80 * 1.0 =  -0.80 for ctc
2024-10-27 17:53:37,713 (beam_search:479) INFO: total log probability: -0.80
2024-10-27 17:53:37,713 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:53:37,713 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:37,713 (beam_search:483) INFO: best hypo: ALLTHEAND

2024-10-27 17:53:37,716 (asr_inference:509) INFO: speech length: 14464
2024-10-27 17:53:38,501 (beam_search:428) INFO: decoder input length: 10
2024-10-27 17:53:38,501 (beam_search:429) INFO: max output length: 10
2024-10-27 17:53:38,501 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:38,510 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:38,510 (beam_search:476) INFO:  -0.07 * 1.0 =  -0.07 for ctc
2024-10-27 17:53:38,510 (beam_search:479) INFO: total log probability: -0.07
2024-10-27 17:53:38,510 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 17:53:38,510 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:38,510 (beam_search:483) INFO: best hypo: IT'S

2024-10-27 17:53:38,513 (asr_inference:509) INFO: speech length: 72416
2024-10-27 17:53:41,115 (beam_search:428) INFO: decoder input length: 56
2024-10-27 17:53:41,115 (beam_search:429) INFO: max output length: 56
2024-10-27 17:53:41,115 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:41,250 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:41,250 (beam_search:476) INFO:  -1.17 * 1.0 =  -1.17 for ctc
2024-10-27 17:53:41,250 (beam_search:479) INFO: total log probability: -1.17
2024-10-27 17:53:41,250 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:53:41,250 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:41,250 (beam_search:483) INFO: best hypo: ITHINKTHEOTHERONEWOULDOUTASCAUSEIT'SNOTAPARALLELCIRCUIT

2024-10-27 17:53:41,253 (asr_inference:509) INFO: speech length: 312096
2024-10-27 17:53:54,949 (beam_search:428) INFO: decoder input length: 243
2024-10-27 17:53:54,950 (beam_search:429) INFO: max output length: 243
2024-10-27 17:53:54,950 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:57,237 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:57,238 (beam_search:476) INFO:  -5.52 * 1.0 =  -5.52 for ctc
2024-10-27 17:53:57,238 (beam_search:479) INFO: total log probability: -5.52
2024-10-27 17:53:57,238 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:53:57,238 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:57,238 (beam_search:483) INFO: best hypo: UMWELLYEAHWHENWEWERETHEEXPERIMENTOFTHEPEOPLEWTHEOFLIGHTSWEHADITANDTHENWEPUTLIKEALITTLEPIECEOFPAPERITTOSEEIFITWOULDELECTRICITYANDITDIDANDASASTHATTHEOTHERONEUMOUTASWELL

2024-10-27 17:53:57,241 (asr_inference:509) INFO: speech length: 134960
2024-10-27 17:54:02,274 (beam_search:428) INFO: decoder input length: 104
2024-10-27 17:54:02,275 (beam_search:429) INFO: max output length: 104
2024-10-27 17:54:02,275 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:02,586 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:02,586 (beam_search:476) INFO:  -3.37 * 1.0 =  -3.37 for ctc
2024-10-27 17:54:02,586 (beam_search:479) INFO: total log probability: -3.37
2024-10-27 17:54:02,586 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:54:02,586 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:02,586 (beam_search:483) INFO: best hypo: WELLSINCEUMELECTRICITYCANNOTGOTHROUGHPAPERITWOULDTHELIGHTBULBSTOOUT

2024-10-27 17:54:02,588 (asr_inference:509) INFO: speech length: 30320
2024-10-27 17:54:03,903 (beam_search:428) INFO: decoder input length: 23
2024-10-27 17:54:03,903 (beam_search:429) INFO: max output length: 23
2024-10-27 17:54:03,903 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:03,925 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:03,925 (beam_search:476) INFO:  -0.86 * 1.0 =  -0.86 for ctc
2024-10-27 17:54:03,925 (beam_search:479) INFO: total log probability: -0.86
2024-10-27 17:54:03,925 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:54:03,925 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:03,925 (beam_search:483) INFO: best hypo: ITCAN'TGO

2024-10-27 17:54:03,927 (asr_inference:509) INFO: speech length: 193200
2024-10-27 17:54:11,230 (beam_search:428) INFO: decoder input length: 150
2024-10-27 17:54:11,230 (beam_search:429) INFO: max output length: 150
2024-10-27 17:54:11,230 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:12,172 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:12,173 (beam_search:476) INFO:  -3.37 * 1.0 =  -3.37 for ctc
2024-10-27 17:54:12,173 (beam_search:479) INFO: total log probability: -3.37
2024-10-27 17:54:12,173 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:54:12,173 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:12,173 (beam_search:483) INFO: best hypo: ISEEABATTERYANDAANDIDON'TKNOWWHATTHEISITHINKITBEALIGHTBULBORSOMETHINGBUTIITLOOKSLIKETOMEAPARALLELCIRCUIT

2024-10-27 17:54:12,176 (asr_inference:509) INFO: speech length: 83504
2024-10-27 17:54:15,242 (beam_search:428) INFO: decoder input length: 64
2024-10-27 17:54:15,242 (beam_search:429) INFO: max output length: 64
2024-10-27 17:54:15,242 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:15,350 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:15,350 (beam_search:476) INFO:  -1.09 * 1.0 =  -1.09 for ctc
2024-10-27 17:54:15,350 (beam_search:479) INFO: total log probability: -1.09
2024-10-27 17:54:15,350 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:54:15,350 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:15,350 (beam_search:483) INFO: best hypo: BECAUSEYOUCANTHEPATHWAYFROMONETOTHEOTHER

2024-10-27 17:54:15,352 (asr_inference:509) INFO: speech length: 46560
2024-10-27 17:54:17,136 (beam_search:428) INFO: decoder input length: 35
2024-10-27 17:54:17,136 (beam_search:429) INFO: max output length: 35
2024-10-27 17:54:17,136 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:17,178 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:17,178 (beam_search:476) INFO:  -0.54 * 1.0 =  -0.54 for ctc
2024-10-27 17:54:17,178 (beam_search:479) INFO: total log probability: -0.54
2024-10-27 17:54:17,178 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:54:17,178 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:17,178 (beam_search:483) INFO: best hypo: ITHINKIT'SACIRCUIT

2024-10-27 17:54:17,181 (asr_inference:509) INFO: speech length: 9488
2024-10-27 17:54:17,779 (beam_search:428) INFO: decoder input length: 6
2024-10-27 17:54:17,780 (beam_search:429) INFO: max output length: 6
2024-10-27 17:54:17,780 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:17,783 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:17,783 (beam_search:476) INFO:  -0.14 * 1.0 =  -0.14 for ctc
2024-10-27 17:54:17,783 (beam_search:479) INFO: total log probability: -0.14
2024-10-27 17:54:17,783 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:54:17,783 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:17,783 (beam_search:483) INFO: best hypo: 

2024-10-27 17:54:17,785 (asr_inference:509) INFO: speech length: 28816
2024-10-27 17:54:18,958 (beam_search:428) INFO: decoder input length: 22
2024-10-27 17:54:18,958 (beam_search:429) INFO: max output length: 22
2024-10-27 17:54:18,958 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:18,964 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:18,964 (beam_search:476) INFO:  -0.00 * 1.0 =  -0.00 for ctc
2024-10-27 17:54:18,964 (beam_search:479) INFO: total log probability: -0.00
2024-10-27 17:54:18,964 (beam_search:480) INFO: normalized log probability: -0.00
2024-10-27 17:54:18,964 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:18,964 (beam_search:483) INFO: best hypo: 

2024-10-27 17:54:18,966 (asr_inference:509) INFO: speech length: 53040
2024-10-27 17:54:20,872 (beam_search:428) INFO: decoder input length: 40
2024-10-27 17:54:20,873 (beam_search:429) INFO: max output length: 40
2024-10-27 17:54:20,873 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:20,923 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:20,923 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 17:54:20,923 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 17:54:20,923 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:54:20,923 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:20,924 (beam_search:483) INFO: best hypo: WE'VEBEENDOINGTHINGSWITHMAGNETS

2024-10-27 17:54:20,926 (asr_inference:509) INFO: speech length: 121472
2024-10-27 17:54:25,561 (beam_search:428) INFO: decoder input length: 94
2024-10-27 17:54:25,561 (beam_search:429) INFO: max output length: 94
2024-10-27 17:54:25,563 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:25,831 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:25,831 (beam_search:476) INFO:  -2.73 * 1.0 =  -2.73 for ctc
2024-10-27 17:54:25,831 (beam_search:479) INFO: total log probability: -2.73
2024-10-27 17:54:25,831 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:54:25,831 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:25,831 (beam_search:483) INFO: best hypo: WEPUTMAGNETSINABOXANDITTHENUSEDDIFFERENTKINDSOFTOIT

2024-10-27 17:54:25,834 (asr_inference:509) INFO: speech length: 50016
2024-10-27 17:54:27,899 (beam_search:428) INFO: decoder input length: 38
2024-10-27 17:54:27,900 (beam_search:429) INFO: max output length: 38
2024-10-27 17:54:27,900 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:27,914 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:27,914 (beam_search:476) INFO:  -0.28 * 1.0 =  -0.28 for ctc
2024-10-27 17:54:27,914 (beam_search:479) INFO: total log probability: -0.28
2024-10-27 17:54:27,914 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:54:27,914 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:27,914 (beam_search:483) INFO: best hypo: IS

2024-10-27 17:54:27,916 (asr_inference:509) INFO: speech length: 111936
2024-10-27 17:54:32,017 (beam_search:428) INFO: decoder input length: 86
2024-10-27 17:54:32,017 (beam_search:429) INFO: max output length: 86
2024-10-27 17:54:32,017 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:32,065 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:32,065 (beam_search:476) INFO:  -0.38 * 1.0 =  -0.38 for ctc
2024-10-27 17:54:32,065 (beam_search:479) INFO: total log probability: -0.38
2024-10-27 17:54:32,065 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:54:32,065 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:32,065 (beam_search:483) INFO: best hypo: ANDUM

2024-10-27 17:54:32,067 (asr_inference:509) INFO: speech length: 235888
2024-10-27 17:54:41,516 (beam_search:428) INFO: decoder input length: 183
2024-10-27 17:54:41,516 (beam_search:429) INFO: max output length: 183
2024-10-27 17:54:41,516 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:42,098 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:42,099 (beam_search:476) INFO:  -2.36 * 1.0 =  -2.36 for ctc
2024-10-27 17:54:42,099 (beam_search:479) INFO: total log probability: -2.36
2024-10-27 17:54:42,099 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:54:42,099 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:42,099 (beam_search:483) INFO: best hypo: UMIUMMAGNETSAREIMPORTANTFORLIKEUMUMFORLIKETHINGSTHATUSEITHINK

2024-10-27 17:54:42,102 (asr_inference:509) INFO: speech length: 147504
2024-10-27 17:54:47,863 (beam_search:428) INFO: decoder input length: 114
2024-10-27 17:54:47,863 (beam_search:429) INFO: max output length: 114
2024-10-27 17:54:47,863 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:48,210 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:48,210 (beam_search:476) INFO:  -3.34 * 1.0 =  -3.34 for ctc
2024-10-27 17:54:48,210 (beam_search:479) INFO: total log probability: -3.34
2024-10-27 17:54:48,210 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:54:48,210 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:48,210 (beam_search:483) INFO: best hypo: WEWERETRYINGTOFINDTHEMCAUSEOTHERGROUPSEMTHEANDWETRIEDTOFINDIT

2024-10-27 17:54:48,213 (asr_inference:509) INFO: speech length: 236240
2024-10-27 17:54:57,997 (beam_search:428) INFO: decoder input length: 184
2024-10-27 17:54:57,998 (beam_search:429) INFO: max output length: 184
2024-10-27 17:54:57,998 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:58,988 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:58,989 (beam_search:476) INFO:  -3.27 * 1.0 =  -3.27 for ctc
2024-10-27 17:54:58,989 (beam_search:479) INFO: total log probability: -3.27
2024-10-27 17:54:58,989 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:54:58,989 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:58,989 (beam_search:483) INFO: best hypo: WEUMLIKEWEDIDN'TKNOWWHERETHEYWERESOWEUSEDUMMAGNETSLIKEIMEANMETALANDATOITCAUSETHEYITINTHEBOX

2024-10-27 17:54:58,992 (asr_inference:509) INFO: speech length: 129632
2024-10-27 17:55:03,745 (beam_search:428) INFO: decoder input length: 100
2024-10-27 17:55:03,745 (beam_search:429) INFO: max output length: 100
2024-10-27 17:55:03,745 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:04,079 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:04,079 (beam_search:476) INFO:  -1.63 * 1.0 =  -1.63 for ctc
2024-10-27 17:55:04,079 (beam_search:479) INFO: total log probability: -1.63
2024-10-27 17:55:04,079 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:55:04,079 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:04,079 (beam_search:483) INFO: best hypo: WHENWEUSEDITTHEKINDOFEDOUTWHEREITWASINTHEBOXSOTHENWETHAT

2024-10-27 17:55:04,082 (asr_inference:509) INFO: speech length: 80400
2024-10-27 17:55:07,128 (beam_search:428) INFO: decoder input length: 62
2024-10-27 17:55:07,128 (beam_search:429) INFO: max output length: 62
2024-10-27 17:55:07,128 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:07,218 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:07,218 (beam_search:476) INFO:  -1.03 * 1.0 =  -1.03 for ctc
2024-10-27 17:55:07,218 (beam_search:479) INFO: total log probability: -1.03
2024-10-27 17:55:07,218 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:55:07,218 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:07,218 (beam_search:483) INFO: best hypo: THEWHERETHEMETALISANDLIKEYEAH

2024-10-27 17:55:07,220 (asr_inference:509) INFO: speech length: 159952
2024-10-27 17:55:13,413 (beam_search:428) INFO: decoder input length: 124
2024-10-27 17:55:13,413 (beam_search:429) INFO: max output length: 124
2024-10-27 17:55:13,413 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:13,942 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:13,942 (beam_search:476) INFO:  -2.89 * 1.0 =  -2.89 for ctc
2024-10-27 17:55:13,942 (beam_search:479) INFO: total log probability: -2.89
2024-10-27 17:55:13,943 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:55:13,943 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:13,943 (beam_search:483) INFO: best hypo: THERE'SMETALORATHESOTHENIFTHERE'SAMETALORUMLIKEAROCKSOMEWHEREUMITITOUT

2024-10-27 17:55:13,945 (asr_inference:509) INFO: speech length: 80560
2024-10-27 17:55:16,864 (beam_search:428) INFO: decoder input length: 62
2024-10-27 17:55:16,864 (beam_search:429) INFO: max output length: 62
2024-10-27 17:55:16,864 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:16,962 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:16,962 (beam_search:476) INFO:  -0.73 * 1.0 =  -0.73 for ctc
2024-10-27 17:55:16,962 (beam_search:479) INFO: total log probability: -0.73
2024-10-27 17:55:16,962 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:55:16,962 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:16,962 (beam_search:483) INFO: best hypo: TOSOTHENITCANTHEOTHERAMAGNET

2024-10-27 17:55:16,964 (asr_inference:509) INFO: speech length: 136512
2024-10-27 17:55:22,065 (beam_search:428) INFO: decoder input length: 106
2024-10-27 17:55:22,065 (beam_search:429) INFO: max output length: 106
2024-10-27 17:55:22,065 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:22,229 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:22,229 (beam_search:476) INFO:  -1.79 * 1.0 =  -1.79 for ctc
2024-10-27 17:55:22,229 (beam_search:479) INFO: total log probability: -1.79
2024-10-27 17:55:22,229 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:55:22,229 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:22,229 (beam_search:483) INFO: best hypo: UMWELLITSTHEUMITSUM

2024-10-27 17:55:22,232 (asr_inference:509) INFO: speech length: 68240
2024-10-27 17:55:24,670 (beam_search:428) INFO: decoder input length: 52
2024-10-27 17:55:24,670 (beam_search:429) INFO: max output length: 52
2024-10-27 17:55:24,670 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:24,764 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:24,765 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 17:55:24,765 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 17:55:24,765 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:55:24,765 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:24,765 (beam_search:483) INFO: best hypo: ITN'TMETALITJUSTOUTWHEREITIS

2024-10-27 17:55:24,767 (asr_inference:509) INFO: speech length: 234528
2024-10-27 17:55:34,195 (beam_search:428) INFO: decoder input length: 182
2024-10-27 17:55:34,195 (beam_search:429) INFO: max output length: 182
2024-10-27 17:55:34,195 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:35,363 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:35,363 (beam_search:476) INFO:  -5.25 * 1.0 =  -5.25 for ctc
2024-10-27 17:55:35,363 (beam_search:479) INFO: total log probability: -5.25
2024-10-27 17:55:35,363 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:55:35,363 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:35,364 (beam_search:483) INFO: best hypo: WHENTHERE'STHISTHERE'SANDANDTHENTHERE'SUMALITTLESOTHATYOUPUTITINTHEANDTHENITWILLSSPINAROUNDANDPOINTOUTTHE

2024-10-27 17:55:35,366 (asr_inference:509) INFO: speech length: 79968
2024-10-27 17:55:38,358 (beam_search:428) INFO: decoder input length: 61
2024-10-27 17:55:38,358 (beam_search:429) INFO: max output length: 61
2024-10-27 17:55:38,358 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:38,440 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:38,440 (beam_search:476) INFO:  -1.26 * 1.0 =  -1.26 for ctc
2024-10-27 17:55:38,440 (beam_search:479) INFO: total log probability: -1.26
2024-10-27 17:55:38,440 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:55:38,440 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:38,440 (beam_search:483) INFO: best hypo: YEAHWITHOTHERUMINMYGROUP

2024-10-27 17:55:38,443 (asr_inference:509) INFO: speech length: 130848
2024-10-27 17:55:43,324 (beam_search:428) INFO: decoder input length: 101
2024-10-27 17:55:43,324 (beam_search:429) INFO: max output length: 101
2024-10-27 17:55:43,324 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:43,613 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:43,613 (beam_search:476) INFO:  -1.92 * 1.0 =  -1.92 for ctc
2024-10-27 17:55:43,613 (beam_search:479) INFO: total log probability: -1.92
2024-10-27 17:55:43,613 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:55:43,613 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:43,613 (beam_search:483) INFO: best hypo: USINGLIKEANAILORANOTHERMAGNETANDTHENLIKEITONTHEBOXANDITIT

2024-10-27 17:55:43,615 (asr_inference:509) INFO: speech length: 171808
2024-10-27 17:55:50,252 (beam_search:428) INFO: decoder input length: 133
2024-10-27 17:55:50,252 (beam_search:429) INFO: max output length: 133
2024-10-27 17:55:50,252 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:50,523 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:50,524 (beam_search:476) INFO:  -1.37 * 1.0 =  -1.37 for ctc
2024-10-27 17:55:50,524 (beam_search:479) INFO: total log probability: -1.37
2024-10-27 17:55:50,524 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:55:50,524 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:50,524 (beam_search:483) INFO: best hypo: WELLITONLYTOLIKEAKINDOFANDUMYEAH

2024-10-27 17:55:50,526 (asr_inference:509) INFO: speech length: 89200
2024-10-27 17:55:53,716 (beam_search:428) INFO: decoder input length: 69
2024-10-27 17:55:53,716 (beam_search:429) INFO: max output length: 69
2024-10-27 17:55:53,716 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:53,844 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:53,844 (beam_search:476) INFO:  -2.67 * 1.0 =  -2.67 for ctc
2024-10-27 17:55:53,844 (beam_search:479) INFO: total log probability: -2.67
2024-10-27 17:55:53,844 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:55:53,844 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:53,844 (beam_search:483) INFO: best hypo: THEREAREDIFFERENTOFANDTHEYONLYUMSTICKTOCERTAIN

2024-10-27 17:55:53,847 (asr_inference:509) INFO: speech length: 200544
2024-10-27 17:56:01,726 (beam_search:428) INFO: decoder input length: 156
2024-10-27 17:56:01,726 (beam_search:429) INFO: max output length: 156
2024-10-27 17:56:01,726 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:02,105 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:02,105 (beam_search:476) INFO:  -2.48 * 1.0 =  -2.48 for ctc
2024-10-27 17:56:02,105 (beam_search:479) INFO: total log probability: -2.48
2024-10-27 17:56:02,105 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:56:02,105 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:02,105 (beam_search:483) INFO: best hypo: ANDNOTSTEELANDUMUM'MNOTANDLIKELIKESTUFF

2024-10-27 17:56:02,108 (asr_inference:509) INFO: speech length: 25872
2024-10-27 17:56:03,241 (beam_search:428) INFO: decoder input length: 19
2024-10-27 17:56:03,241 (beam_search:429) INFO: max output length: 19
2024-10-27 17:56:03,241 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:03,253 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:03,253 (beam_search:476) INFO:  -0.16 * 1.0 =  -0.16 for ctc
2024-10-27 17:56:03,253 (beam_search:479) INFO: total log probability: -0.16
2024-10-27 17:56:03,253 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:56:03,253 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:03,253 (beam_search:483) INFO: best hypo: WEDID

2024-10-27 17:56:03,256 (asr_inference:509) INFO: speech length: 242656
2024-10-27 17:56:13,213 (beam_search:428) INFO: decoder input length: 189
2024-10-27 17:56:13,213 (beam_search:429) INFO: max output length: 189
2024-10-27 17:56:13,213 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:14,146 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:14,146 (beam_search:476) INFO:  -2.95 * 1.0 =  -2.95 for ctc
2024-10-27 17:56:14,146 (beam_search:479) INFO: total log probability: -2.95
2024-10-27 17:56:14,146 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:56:14,146 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:14,146 (beam_search:483) INFO: best hypo: WELLWEHADAUMANDTHENOURPUTSOMEOFITONANDWEHADAMAGNETANDPUTITANDSEEUMHOWITAND

2024-10-27 17:56:14,148 (asr_inference:509) INFO: speech length: 102240
2024-10-27 17:56:17,859 (beam_search:428) INFO: decoder input length: 79
2024-10-27 17:56:17,859 (beam_search:429) INFO: max output length: 79
2024-10-27 17:56:17,859 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:18,027 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:18,027 (beam_search:476) INFO:  -1.94 * 1.0 =  -1.94 for ctc
2024-10-27 17:56:18,027 (beam_search:479) INFO: total log probability: -1.94
2024-10-27 17:56:18,027 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:56:18,027 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:18,027 (beam_search:483) INFO: best hypo: ITKINDOFLIKEITUPANDITTHEMAGNETITWAS

2024-10-27 17:56:18,030 (asr_inference:509) INFO: speech length: 257040
2024-10-27 17:56:28,913 (beam_search:428) INFO: decoder input length: 200
2024-10-27 17:56:28,913 (beam_search:429) INFO: max output length: 200
2024-10-27 17:56:28,913 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:30,287 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:30,288 (beam_search:476) INFO:  -6.46 * 1.0 =  -6.46 for ctc
2024-10-27 17:56:30,288 (beam_search:479) INFO: total log probability: -6.46
2024-10-27 17:56:30,288 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:56:30,288 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:30,288 (beam_search:483) INFO: best hypo: THEIRONSTUCKTOTHEMAGNETCAUSEUMTHEMAGNETHASTHISINITTHATUMLIKEMAKESITGOUPSOITSITBUTITCAN'TTOITCAUSE'STHE

2024-10-27 17:56:30,290 (asr_inference:509) INFO: speech length: 123664
2024-10-27 17:56:34,856 (beam_search:428) INFO: decoder input length: 96
2024-10-27 17:56:34,856 (beam_search:429) INFO: max output length: 96
2024-10-27 17:56:34,856 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:35,062 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:35,062 (beam_search:476) INFO:  -1.47 * 1.0 =  -1.47 for ctc
2024-10-27 17:56:35,062 (beam_search:479) INFO: total log probability: -1.47
2024-10-27 17:56:35,062 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:56:35,062 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:35,062 (beam_search:483) INFO: best hypo: NOITDIDNOTITJUSTUMOFAROUNDTHEMAGNETWASTHE

2024-10-27 17:56:35,065 (asr_inference:509) INFO: speech length: 41152
2024-10-27 17:56:36,655 (beam_search:428) INFO: decoder input length: 31
2024-10-27 17:56:36,655 (beam_search:429) INFO: max output length: 31
2024-10-27 17:56:36,655 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:36,692 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:36,692 (beam_search:476) INFO:  -0.46 * 1.0 =  -0.46 for ctc
2024-10-27 17:56:36,692 (beam_search:479) INFO: total log probability: -0.46
2024-10-27 17:56:36,692 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:56:36,692 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:36,692 (beam_search:483) INFO: best hypo: UMI'MNOTREALLYSURE

2024-10-27 17:56:36,694 (asr_inference:509) INFO: speech length: 288880
2024-10-27 17:56:48,836 (beam_search:428) INFO: decoder input length: 225
2024-10-27 17:56:48,836 (beam_search:429) INFO: max output length: 225
2024-10-27 17:56:48,836 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:50,478 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:50,478 (beam_search:476) INFO:  -3.84 * 1.0 =  -3.84 for ctc
2024-10-27 17:56:50,478 (beam_search:479) INFO: total log probability: -3.84
2024-10-27 17:56:50,478 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:56:50,479 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:50,479 (beam_search:483) INFO: best hypo: THEMAGNETISUMINTOTHEUHUMLIKETOTHEPENCILBUTIT'SUMBECAUSETHERE'SINSIDETHEPENCILTHAT'SWHATWEUSEDTOANDTHEMAGNETUMTOTHEUM

2024-10-27 17:56:50,481 (asr_inference:509) INFO: speech length: 97968
2024-10-27 17:56:54,133 (beam_search:428) INFO: decoder input length: 76
2024-10-27 17:56:54,133 (beam_search:429) INFO: max output length: 76
2024-10-27 17:56:54,133 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:54,267 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:54,267 (beam_search:476) INFO:  -1.78 * 1.0 =  -1.78 for ctc
2024-10-27 17:56:54,267 (beam_search:479) INFO: total log probability: -1.78
2024-10-27 17:56:54,267 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:56:54,267 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:54,267 (beam_search:483) INFO: best hypo: THETHEMAGNETISTOTHEMAGNETWITHTHETHE

2024-10-27 17:56:54,270 (asr_inference:509) INFO: speech length: 164032
2024-10-27 17:57:00,625 (beam_search:428) INFO: decoder input length: 127
2024-10-27 17:57:00,625 (beam_search:429) INFO: max output length: 127
2024-10-27 17:57:00,625 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:01,204 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:01,204 (beam_search:476) INFO:  -2.30 * 1.0 =  -2.30 for ctc
2024-10-27 17:57:01,204 (beam_search:479) INFO: total log probability: -2.30
2024-10-27 17:57:01,204 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:57:01,204 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:01,204 (beam_search:483) INFO: best hypo: THEMAGNETISONSIDEANDTHEOTHERSOIT'SLIKETHEMAGNETPOWER'SMAKINGITONTOPOFTHEOTHERMAGNETS

2024-10-27 17:57:01,207 (asr_inference:509) INFO: speech length: 165088
2024-10-27 17:57:07,555 (beam_search:428) INFO: decoder input length: 128
2024-10-27 17:57:07,555 (beam_search:429) INFO: max output length: 128
2024-10-27 17:57:07,555 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:08,015 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:08,015 (beam_search:476) INFO:  -3.23 * 1.0 =  -3.23 for ctc
2024-10-27 17:57:08,015 (beam_search:479) INFO: total log probability: -3.23
2024-10-27 17:57:08,015 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:57:08,015 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:08,016 (beam_search:483) INFO: best hypo: THEPOWERWITHTWOGOTOGETHERUMSMOREPOWERSOTHENWHENMAGNETITITSONTOPOFIT

2024-10-27 17:57:08,018 (asr_inference:509) INFO: speech length: 29072
2024-10-27 17:57:09,194 (beam_search:428) INFO: decoder input length: 22
2024-10-27 17:57:09,194 (beam_search:429) INFO: max output length: 22
2024-10-27 17:57:09,194 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:09,212 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:09,212 (beam_search:476) INFO:  -1.61 * 1.0 =  -1.61 for ctc
2024-10-27 17:57:09,212 (beam_search:479) INFO: total log probability: -1.61
2024-10-27 17:57:09,212 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 17:57:09,212 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:09,212 (beam_search:483) INFO: best hypo: TWO'TTOGETHER

2024-10-27 17:57:09,214 (asr_inference:509) INFO: speech length: 32576
2024-10-27 17:57:10,514 (beam_search:428) INFO: decoder input length: 24
2024-10-27 17:57:10,514 (beam_search:429) INFO: max output length: 24
2024-10-27 17:57:10,514 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:10,540 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:10,540 (beam_search:476) INFO:  -0.21 * 1.0 =  -0.21 for ctc
2024-10-27 17:57:10,540 (beam_search:479) INFO: total log probability: -0.21
2024-10-27 17:57:10,540 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 17:57:10,540 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:10,540 (beam_search:483) INFO: best hypo: YEAHSOWHAT'STHE

2024-10-27 17:57:10,542 (asr_inference:509) INFO: speech length: 112752
2024-10-27 17:57:14,633 (beam_search:428) INFO: decoder input length: 87
2024-10-27 17:57:14,633 (beam_search:429) INFO: max output length: 87
2024-10-27 17:57:14,633 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:14,825 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:14,825 (beam_search:476) INFO:  -3.55 * 1.0 =  -3.55 for ctc
2024-10-27 17:57:14,825 (beam_search:479) INFO: total log probability: -3.55
2024-10-27 17:57:14,825 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:57:14,825 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:14,825 (beam_search:483) INFO: best hypo: THEISUMTHEY'REANDTHEY'RENOTASSTRONG

2024-10-27 17:57:14,828 (asr_inference:509) INFO: speech length: 180496
2024-10-27 17:57:21,876 (beam_search:428) INFO: decoder input length: 140
2024-10-27 17:57:21,876 (beam_search:429) INFO: max output length: 140
2024-10-27 17:57:21,876 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:22,476 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:22,476 (beam_search:476) INFO:  -2.15 * 1.0 =  -2.15 for ctc
2024-10-27 17:57:22,476 (beam_search:479) INFO: total log probability: -2.15
2024-10-27 17:57:22,476 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:57:22,476 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:22,476 (beam_search:483) INFO: best hypo: ARETWODIFFERENTKINDOFMAGNETSANDUMSOMEAREI'MNOTSUREBUTITHINKSOMEARETHANTHEOTHER

2024-10-27 17:57:22,479 (asr_inference:509) INFO: speech length: 144016
2024-10-27 17:57:28,044 (beam_search:428) INFO: decoder input length: 112
2024-10-27 17:57:28,044 (beam_search:429) INFO: max output length: 112
2024-10-27 17:57:28,044 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:28,422 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:28,422 (beam_search:476) INFO:  -2.58 * 1.0 =  -2.58 for ctc
2024-10-27 17:57:28,422 (beam_search:479) INFO: total log probability: -2.58
2024-10-27 17:57:28,422 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:57:28,422 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:28,423 (beam_search:483) INFO: best hypo: ISEETHETWODIFFERENTOFORMAGNETSARETOGETHERCAUSEONEISANDTHEOTHERISN'T

2024-10-27 17:57:28,425 (asr_inference:509) INFO: speech length: 229952
2024-10-27 17:57:37,832 (beam_search:428) INFO: decoder input length: 179
2024-10-27 17:57:37,832 (beam_search:429) INFO: max output length: 179
2024-10-27 17:57:37,832 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:38,926 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:38,926 (beam_search:476) INFO:  -4.72 * 1.0 =  -4.72 for ctc
2024-10-27 17:57:38,926 (beam_search:479) INFO: total log probability: -4.72
2024-10-27 17:57:38,926 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:57:38,926 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:38,926 (beam_search:483) INFO: best hypo: UMTHEOTHERSARETHESOTHEYCOULDBUTI'MNOTTHEPOWERSITHINKDIFFERENTSOTHENTHEYWON'TCOMETOGETHERCAUSETHEY'RETHESAME

2024-10-27 17:57:38,929 (asr_inference:509) INFO: speech length: 39392
2024-10-27 17:57:40,465 (beam_search:428) INFO: decoder input length: 30
2024-10-27 17:57:40,466 (beam_search:429) INFO: max output length: 30
2024-10-27 17:57:40,466 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:40,503 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:40,503 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 17:57:40,503 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 17:57:40,503 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:57:40,503 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:40,503 (beam_search:483) INFO: best hypo: I'MNOTQUITESURE

2024-10-27 17:57:40,506 (asr_inference:509) INFO: speech length: 219424
2024-10-27 17:57:49,367 (beam_search:428) INFO: decoder input length: 170
2024-10-27 17:57:49,367 (beam_search:429) INFO: max output length: 170
2024-10-27 17:57:49,367 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:50,518 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:50,518 (beam_search:476) INFO:  -5.17 * 1.0 =  -5.17 for ctc
2024-10-27 17:57:50,518 (beam_search:479) INFO: total log probability: -5.17
2024-10-27 17:57:50,518 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:57:50,518 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:50,518 (beam_search:483) INFO: best hypo: ITHINKTHATTHEYLIKETHESAMEBUTTHEPOWER'SREALLYUMSOTHENITWON'TLIKEANDTHEOTHER'TSOTHENUMITTHEDOESN'TWORK

2024-10-27 17:57:50,521 (asr_inference:509) INFO: speech length: 156944
2024-10-27 17:57:56,512 (beam_search:428) INFO: decoder input length: 122
2024-10-27 17:57:56,512 (beam_search:429) INFO: max output length: 122
2024-10-27 17:57:56,512 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:57,112 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:57,113 (beam_search:476) INFO:  -3.42 * 1.0 =  -3.42 for ctc
2024-10-27 17:57:57,113 (beam_search:479) INFO: total log probability: -3.42
2024-10-27 17:57:57,113 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:57:57,113 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:57,113 (beam_search:483) INFO: best hypo: ITHINKTHEMAGNETWILLUPTHEROCKTHEPAPERCLIPUMTHEANDITHINKIT'SLIKEAWASHERBUTI'MNOTSURE

2024-10-27 17:57:57,115 (asr_inference:509) INFO: speech length: 120960
2024-10-27 17:58:01,517 (beam_search:428) INFO: decoder input length: 93
2024-10-27 17:58:01,518 (beam_search:429) INFO: max output length: 93
2024-10-27 17:58:01,518 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:01,855 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:01,855 (beam_search:476) INFO:  -2.63 * 1.0 =  -2.63 for ctc
2024-10-27 17:58:01,855 (beam_search:479) INFO: total log probability: -2.63
2024-10-27 17:58:01,855 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:58:01,855 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:01,855 (beam_search:483) INFO: best hypo: BECAUSETHEOTHERSARENOTOFMETALANDITLOOKSLIKETHEKINDOFMETALWOULDSTICKTOMAAMAGNET

2024-10-27 17:58:01,857 (asr_inference:509) INFO: speech length: 67536
2024-10-27 17:58:04,251 (beam_search:428) INFO: decoder input length: 52
2024-10-27 17:58:04,251 (beam_search:429) INFO: max output length: 52
2024-10-27 17:58:04,251 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:04,297 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:04,297 (beam_search:476) INFO:  -1.38 * 1.0 =  -1.38 for ctc
2024-10-27 17:58:04,297 (beam_search:479) INFO: total log probability: -1.38
2024-10-27 17:58:04,297 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:58:04,297 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:04,297 (beam_search:483) INFO: best hypo: THEITHINKINTHE

2024-10-27 17:58:04,300 (asr_inference:509) INFO: speech length: 263696
2024-10-27 17:58:15,560 (beam_search:428) INFO: decoder input length: 205
2024-10-27 17:58:15,560 (beam_search:429) INFO: max output length: 205
2024-10-27 17:58:15,560 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:16,860 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:16,860 (beam_search:476) INFO:  -2.39 * 1.0 =  -2.39 for ctc
2024-10-27 17:58:16,862 (beam_search:479) INFO: total log probability: -2.39
2024-10-27 17:58:16,862 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:58:16,862 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:16,862 (beam_search:483) INFO: best hypo: WEHAVELIKEAANDUMTHATCONNECTSTOWIRESSOWEPUTITONABATTERYANDUMPUTTHEUMTHEWIRESTOTHENEGATIVEANDPOSITIVESOTHENITWILLSTART

2024-10-27 17:58:16,865 (asr_inference:509) INFO: speech length: 291584
2024-10-27 17:58:29,526 (beam_search:428) INFO: decoder input length: 227
2024-10-27 17:58:29,527 (beam_search:429) INFO: max output length: 227
2024-10-27 17:58:29,527 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:30,968 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:30,968 (beam_search:476) INFO:  -4.77 * 1.0 =  -4.77 for ctc
2024-10-27 17:58:30,968 (beam_search:479) INFO: total log probability: -4.77
2024-10-27 17:58:30,968 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:58:30,968 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:30,968 (beam_search:483) INFO: best hypo: WEUMSOWEHADASWITCHWITHITTOOSOWEHADANOTHERWIREANDPUTUMANOTHERONSOTHENIFWEPUTAMETALTHINGITITWILLSTOPANDYEAH

2024-10-27 17:58:30,971 (asr_inference:509) INFO: speech length: 82368
2024-10-27 17:58:33,908 (beam_search:428) INFO: decoder input length: 63
2024-10-27 17:58:33,908 (beam_search:429) INFO: max output length: 63
2024-10-27 17:58:33,908 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:34,044 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:34,044 (beam_search:476) INFO:  -1.23 * 1.0 =  -1.23 for ctc
2024-10-27 17:58:34,044 (beam_search:479) INFO: total log probability: -1.23
2024-10-27 17:58:34,044 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:58:34,044 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:34,044 (beam_search:483) INFO: best hypo: THESWITCHISUMSOITUMTURNSONANDSOFF

2024-10-27 17:58:34,047 (asr_inference:509) INFO: speech length: 161984
2024-10-27 17:58:40,096 (beam_search:428) INFO: decoder input length: 126
2024-10-27 17:58:40,096 (beam_search:429) INFO: max output length: 126
2024-10-27 17:58:40,096 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:40,688 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:40,688 (beam_search:476) INFO:  -3.15 * 1.0 =  -3.15 for ctc
2024-10-27 17:58:40,688 (beam_search:479) INFO: total log probability: -3.15
2024-10-27 17:58:40,688 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:58:40,688 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:40,688 (beam_search:483) INFO: best hypo: THERE'STHESETWOLITTLEANDYOUPUTTHEWIREONITANDTHEWIREANDTHENYOUHAVETHISMETALUMTHATYOUPUT

2024-10-27 17:58:40,691 (asr_inference:509) INFO: speech length: 162480
2024-10-27 17:58:47,043 (beam_search:428) INFO: decoder input length: 126
2024-10-27 17:58:47,043 (beam_search:429) INFO: max output length: 126
2024-10-27 17:58:47,043 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:47,509 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:47,509 (beam_search:476) INFO:  -1.97 * 1.0 =  -1.97 for ctc
2024-10-27 17:58:47,509 (beam_search:479) INFO: total log probability: -1.97
2024-10-27 17:58:47,509 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:58:47,509 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:47,509 (beam_search:483) INFO: best hypo: THEWIRESARECONNECTEDTOITSOTHENITHINKTHEMETALUMSONITSOITHINKUMIT

2024-10-27 17:58:47,512 (asr_inference:509) INFO: speech length: 79568
2024-10-27 17:58:50,426 (beam_search:428) INFO: decoder input length: 61
2024-10-27 17:58:50,426 (beam_search:429) INFO: max output length: 61
2024-10-27 17:58:50,426 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:50,512 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:50,512 (beam_search:476) INFO:  -0.30 * 1.0 =  -0.30 for ctc
2024-10-27 17:58:50,512 (beam_search:479) INFO: total log probability: -0.30
2024-10-27 17:58:50,513 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 17:58:50,513 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:50,513 (beam_search:483) INFO: best hypo: IT'SITTHEMOTORANDTHEUM

2024-10-27 17:58:50,515 (asr_inference:509) INFO: speech length: 178992
2024-10-27 17:58:57,508 (beam_search:428) INFO: decoder input length: 139
2024-10-27 17:58:57,508 (beam_search:429) INFO: max output length: 139
2024-10-27 17:58:57,508 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:57,835 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:57,835 (beam_search:476) INFO:  -0.66 * 1.0 =  -0.66 for ctc
2024-10-27 17:58:57,835 (beam_search:479) INFO: total log probability: -0.66
2024-10-27 17:58:57,835 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:58:57,835 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:57,836 (beam_search:483) INFO: best hypo: UHTHEENERGYTHEUMTHEDCELLBATTERYITTHROUGHTHEWIRE

2024-10-27 17:58:57,838 (asr_inference:509) INFO: speech length: 58256
2024-10-27 17:58:59,979 (beam_search:428) INFO: decoder input length: 45
2024-10-27 17:58:59,979 (beam_search:429) INFO: max output length: 45
2024-10-27 17:58:59,979 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:00,041 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:00,041 (beam_search:476) INFO:  -0.67 * 1.0 =  -0.67 for ctc
2024-10-27 17:59:00,041 (beam_search:479) INFO: total log probability: -0.67
2024-10-27 17:59:00,041 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:59:00,041 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:00,041 (beam_search:483) INFO: best hypo: UHWEALSOUMMADEALIGHTBULB

2024-10-27 17:59:00,044 (asr_inference:509) INFO: speech length: 154432
2024-10-27 17:59:05,776 (beam_search:428) INFO: decoder input length: 120
2024-10-27 17:59:05,776 (beam_search:429) INFO: max output length: 120
2024-10-27 17:59:05,776 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:06,178 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:06,179 (beam_search:476) INFO:  -2.66 * 1.0 =  -2.66 for ctc
2024-10-27 17:59:06,179 (beam_search:479) INFO: total log probability: -2.66
2024-10-27 17:59:06,179 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:59:06,179 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:06,179 (beam_search:483) INFO: best hypo: IT'SLIKETHEFIRSTBUTWEHADTHEUMANDASMALLLIGHTBULBINSTEADOFAA

2024-10-27 17:59:06,181 (asr_inference:509) INFO: speech length: 260048
2024-10-27 17:59:16,898 (beam_search:428) INFO: decoder input length: 202
2024-10-27 17:59:16,898 (beam_search:429) INFO: max output length: 202
2024-10-27 17:59:16,898 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:18,161 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:18,161 (beam_search:476) INFO:  -3.61 * 1.0 =  -3.61 for ctc
2024-10-27 17:59:18,161 (beam_search:479) INFO: total log probability: -3.61
2024-10-27 17:59:18,161 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:59:18,161 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:18,161 (beam_search:483) INFO: best hypo: WELLTHEENERGYFROMTHENEGATIVEUMITGOESUMOUTFROMTHENEGATIVEANDTHENITWILLLIKEBACKINTOTHEPOSITIVESOTHENTHEENERGYGOESUMLIKEUMINA

2024-10-27 17:59:18,164 (asr_inference:509) INFO: speech length: 60144
2024-10-27 17:59:20,389 (beam_search:428) INFO: decoder input length: 46
2024-10-27 17:59:20,389 (beam_search:429) INFO: max output length: 46
2024-10-27 17:59:20,389 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:20,493 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:20,494 (beam_search:476) INFO:  -1.24 * 1.0 =  -1.24 for ctc
2024-10-27 17:59:20,494 (beam_search:479) INFO: total log probability: -1.24
2024-10-27 17:59:20,494 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:59:20,494 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:20,494 (beam_search:483) INFO: best hypo: UMI'MNOTESURECAUSEWEDIDN'TREALLYDOTHAT

2024-10-27 17:59:20,497 (asr_inference:509) INFO: speech length: 53712
2024-10-27 17:59:22,547 (beam_search:428) INFO: decoder input length: 41
2024-10-27 17:59:22,547 (beam_search:429) INFO: max output length: 41
2024-10-27 17:59:22,547 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:22,580 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:22,581 (beam_search:476) INFO:  -1.97 * 1.0 =  -1.97 for ctc
2024-10-27 17:59:22,581 (beam_search:479) INFO: total log probability: -1.97
2024-10-27 17:59:22,581 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 17:59:22,581 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:22,581 (beam_search:483) INFO: best hypo: THEENERGYTHEOTHER

2024-10-27 17:59:22,583 (asr_inference:509) INFO: speech length: 116256
2024-10-27 17:59:26,909 (beam_search:428) INFO: decoder input length: 90
2024-10-27 17:59:26,909 (beam_search:429) INFO: max output length: 90
2024-10-27 17:59:26,909 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:27,077 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:27,077 (beam_search:476) INFO:  -2.62 * 1.0 =  -2.62 for ctc
2024-10-27 17:59:27,077 (beam_search:479) INFO: total log probability: -2.62
2024-10-27 17:59:27,077 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:59:27,077 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:27,077 (beam_search:483) INFO: best hypo: ACIRCUITISLIKEUMWHENENERGYGOESANDCOMESBACK

2024-10-27 17:59:27,079 (asr_inference:509) INFO: speech length: 190000
2024-10-27 17:59:34,392 (beam_search:428) INFO: decoder input length: 147
2024-10-27 17:59:34,392 (beam_search:429) INFO: max output length: 147
2024-10-27 17:59:34,392 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:35,171 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:35,171 (beam_search:476) INFO:  -3.27 * 1.0 =  -3.27 for ctc
2024-10-27 17:59:35,171 (beam_search:479) INFO: total log probability: -3.27
2024-10-27 17:59:35,171 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:59:35,171 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:35,171 (beam_search:483) INFO: best hypo: ONTHEBATTERYITHASTOBEONTHEUMFORTHEANDPOSITIVEANDTHEOTHERSIDEOFTHEWIREHASTOUMTOONTHELIGHTBULB

2024-10-27 17:59:35,174 (asr_inference:509) INFO: speech length: 47728
2024-10-27 17:59:36,872 (beam_search:428) INFO: decoder input length: 36
2024-10-27 17:59:36,872 (beam_search:429) INFO: max output length: 36
2024-10-27 17:59:36,872 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:36,896 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:36,896 (beam_search:476) INFO:  -0.71 * 1.0 =  -0.71 for ctc
2024-10-27 17:59:36,896 (beam_search:479) INFO: total log probability: -0.71
2024-10-27 17:59:36,896 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:59:36,896 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:36,896 (beam_search:483) INFO: best hypo: WEPUTIT

2024-10-27 17:59:36,899 (asr_inference:509) INFO: speech length: 68608
2024-10-27 17:59:39,468 (beam_search:428) INFO: decoder input length: 53
2024-10-27 17:59:39,468 (beam_search:429) INFO: max output length: 53
2024-10-27 17:59:39,468 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:39,549 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:39,549 (beam_search:476) INFO:  -0.79 * 1.0 =  -0.79 for ctc
2024-10-27 17:59:39,549 (beam_search:479) INFO: total log probability: -0.79
2024-10-27 17:59:39,549 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:59:39,549 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:39,549 (beam_search:483) INFO: best hypo: WELLINIIIDON'TSO

2024-10-27 17:59:39,551 (asr_inference:509) INFO: speech length: 135088
2024-10-27 17:59:44,697 (beam_search:428) INFO: decoder input length: 105
2024-10-27 17:59:44,697 (beam_search:429) INFO: max output length: 105
2024-10-27 17:59:44,697 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:44,964 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:44,964 (beam_search:476) INFO:  -0.42 * 1.0 =  -0.42 for ctc
2024-10-27 17:59:44,964 (beam_search:479) INFO: total log probability: -0.42
2024-10-27 17:59:44,964 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 17:59:44,964 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:44,964 (beam_search:483) INFO: best hypo: BECAUSETHEUMITHASTOGOTHROUGHLIKEUMTHISLITTLEOFIT

2024-10-27 17:59:44,967 (asr_inference:509) INFO: speech length: 25296
2024-10-27 17:59:46,015 (beam_search:428) INFO: decoder input length: 19
2024-10-27 17:59:46,016 (beam_search:429) INFO: max output length: 19
2024-10-27 17:59:46,016 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:46,021 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:46,021 (beam_search:476) INFO:  -0.03 * 1.0 =  -0.03 for ctc
2024-10-27 17:59:46,021 (beam_search:479) INFO: total log probability: -0.03
2024-10-27 17:59:46,021 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 17:59:46,021 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:46,021 (beam_search:483) INFO: best hypo: 

2024-10-27 17:59:46,024 (asr_inference:509) INFO: speech length: 151296
2024-10-27 17:59:51,643 (beam_search:428) INFO: decoder input length: 117
2024-10-27 17:59:51,643 (beam_search:429) INFO: max output length: 117
2024-10-27 17:59:51,643 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:51,977 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:51,977 (beam_search:476) INFO:  -2.79 * 1.0 =  -2.79 for ctc
2024-10-27 17:59:51,977 (beam_search:479) INFO: total log probability: -2.79
2024-10-27 17:59:51,977 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:59:51,977 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:51,977 (beam_search:483) INFO: best hypo: UHWEHADTHESELITTLESANDWEUMINAANDJUSTUMSAWTHETHINGS

2024-10-27 17:59:51,979 (asr_inference:509) INFO: speech length: 119616
2024-10-27 17:59:56,337 (beam_search:428) INFO: decoder input length: 92
2024-10-27 17:59:56,338 (beam_search:429) INFO: max output length: 92
2024-10-27 17:59:56,338 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:56,533 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:56,533 (beam_search:476) INFO:  -1.65 * 1.0 =  -1.65 for ctc
2024-10-27 17:59:56,533 (beam_search:479) INFO: total log probability: -1.65
2024-10-27 17:59:56,533 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:59:56,533 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:56,534 (beam_search:483) INFO: best hypo: UMWHATTHEWASANDLIGHTBULBANDANDLIKETHEWIRE

2024-10-27 17:59:56,536 (asr_inference:509) INFO: speech length: 53088
2024-10-27 17:59:58,567 (beam_search:428) INFO: decoder input length: 40
2024-10-27 17:59:58,567 (beam_search:429) INFO: max output length: 40
2024-10-27 17:59:58,567 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:58,620 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:58,621 (beam_search:476) INFO:  -0.77 * 1.0 =  -0.77 for ctc
2024-10-27 17:59:58,621 (beam_search:479) INFO: total log probability: -0.77
2024-10-27 17:59:58,621 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:59:58,621 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:58,621 (beam_search:483) INFO: best hypo: ISEETHELIGHTBULBTHEANDTHE

2024-10-27 17:59:58,623 (asr_inference:509) INFO: speech length: 48736
2024-10-27 18:00:00,418 (beam_search:428) INFO: decoder input length: 37
2024-10-27 18:00:00,418 (beam_search:429) INFO: max output length: 37
2024-10-27 18:00:00,418 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:00,469 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:00,469 (beam_search:476) INFO:  -1.66 * 1.0 =  -1.66 for ctc
2024-10-27 18:00:00,469 (beam_search:479) INFO: total log probability: -1.66
2024-10-27 18:00:00,469 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:00:00,469 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:00,469 (beam_search:483) INFO: best hypo: UMIDON'TIITBUT

2024-10-27 18:00:00,471 (asr_inference:509) INFO: speech length: 40240
2024-10-27 18:00:02,003 (beam_search:428) INFO: decoder input length: 30
2024-10-27 18:00:02,003 (beam_search:429) INFO: max output length: 30
2024-10-27 18:00:02,003 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:02,038 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:02,038 (beam_search:476) INFO:  -0.41 * 1.0 =  -0.41 for ctc
2024-10-27 18:00:02,038 (beam_search:479) INFO: total log probability: -0.41
2024-10-27 18:00:02,038 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:00:02,038 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:02,038 (beam_search:483) INFO: best hypo: UMI'MNOTESURE

2024-10-27 18:00:02,041 (asr_inference:509) INFO: speech length: 93856
2024-10-27 18:00:05,440 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:00:05,440 (beam_search:429) INFO: max output length: 72
2024-10-27 18:00:05,440 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:05,599 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:05,599 (beam_search:476) INFO:  -1.11 * 1.0 =  -1.11 for ctc
2024-10-27 18:00:05,599 (beam_search:479) INFO: total log probability: -1.11
2024-10-27 18:00:05,599 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:00:05,599 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:05,599 (beam_search:483) INFO: best hypo: ITHINKITISLIKETHEANDTHENTHESWITCHANDTHEBATTERY

2024-10-27 18:00:05,602 (asr_inference:509) INFO: speech length: 153808
2024-10-27 18:00:11,469 (beam_search:428) INFO: decoder input length: 119
2024-10-27 18:00:11,469 (beam_search:429) INFO: max output length: 119
2024-10-27 18:00:11,469 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:11,840 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:11,840 (beam_search:476) INFO:  -2.58 * 1.0 =  -2.58 for ctc
2024-10-27 18:00:11,840 (beam_search:479) INFO: total log probability: -2.58
2024-10-27 18:00:11,840 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:00:11,840 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:11,840 (beam_search:483) INFO: best hypo: UMTHEBECAUSEUMCOMESBACKTOBUTITGOESFORLIKEAANDITCOMESBACK

2024-10-27 18:00:11,843 (asr_inference:509) INFO: speech length: 54256
2024-10-27 18:00:13,811 (beam_search:428) INFO: decoder input length: 41
2024-10-27 18:00:13,812 (beam_search:429) INFO: max output length: 41
2024-10-27 18:00:13,812 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:13,849 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:13,849 (beam_search:476) INFO:  -1.19 * 1.0 =  -1.19 for ctc
2024-10-27 18:00:13,849 (beam_search:479) INFO: total log probability: -1.19
2024-10-27 18:00:13,849 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:00:13,849 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:13,849 (beam_search:483) INFO: best hypo: UMITOUTOFTHE

2024-10-27 18:00:13,852 (asr_inference:509) INFO: speech length: 83184
2024-10-27 18:00:16,958 (beam_search:428) INFO: decoder input length: 64
2024-10-27 18:00:16,958 (beam_search:429) INFO: max output length: 64
2024-10-27 18:00:16,958 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:17,048 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:17,048 (beam_search:476) INFO:  -2.11 * 1.0 =  -2.11 for ctc
2024-10-27 18:00:17,048 (beam_search:479) INFO: total log probability: -2.11
2024-10-27 18:00:17,048 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:00:17,048 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:17,048 (beam_search:483) INFO: best hypo: IHAVEBEENDOINGTHINGSWITHANDA

2024-10-27 18:00:17,050 (asr_inference:509) INFO: speech length: 138848
2024-10-27 18:00:22,166 (beam_search:428) INFO: decoder input length: 107
2024-10-27 18:00:22,166 (beam_search:429) INFO: max output length: 107
2024-10-27 18:00:22,166 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:22,619 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:22,619 (beam_search:476) INFO:  -3.24 * 1.0 =  -3.24 for ctc
2024-10-27 18:00:22,620 (beam_search:479) INFO: total log probability: -3.24
2024-10-27 18:00:22,620 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:00:22,620 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:22,620 (beam_search:483) INFO: best hypo: WEHAVEALIGHTANDWIRESANDWEPUTTHEINTHISANDTHENTHEWIRESTOITANDTHENTHETOTHELIGHTTOO

2024-10-27 18:00:22,622 (asr_inference:509) INFO: speech length: 249216
2024-10-27 18:00:32,674 (beam_search:428) INFO: decoder input length: 194
2024-10-27 18:00:32,674 (beam_search:429) INFO: max output length: 194
2024-10-27 18:00:32,674 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:34,004 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:34,005 (beam_search:476) INFO:  -3.27 * 1.0 =  -3.27 for ctc
2024-10-27 18:00:34,005 (beam_search:479) INFO: total log probability: -3.27
2024-10-27 18:00:34,005 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:00:34,005 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:34,005 (beam_search:483) INFO: best hypo: UMWEJUSTHADTHESELITTLETHATARELIKESANDONCEWEPUTEMINANDUMTHERE'SATHATGOESWITHTHELIGHTSOTHENITHASTOTOUCHTHEBOTTOMTHING

2024-10-27 18:00:34,008 (asr_inference:509) INFO: speech length: 162512
2024-10-27 18:00:40,254 (beam_search:428) INFO: decoder input length: 126
2024-10-27 18:00:40,254 (beam_search:429) INFO: max output length: 126
2024-10-27 18:00:40,254 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:40,674 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:40,674 (beam_search:476) INFO:  -2.18 * 1.0 =  -2.18 for ctc
2024-10-27 18:00:40,674 (beam_search:479) INFO: total log probability: -2.18
2024-10-27 18:00:40,674 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:00:40,674 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:40,674 (beam_search:483) INFO: best hypo: UMTHEWIRESHAVETOBEINANDTHETHESHAVETOBEFORTHELIGHTTOTOUCH

2024-10-27 18:00:40,677 (asr_inference:509) INFO: speech length: 92112
2024-10-27 18:00:44,039 (beam_search:428) INFO: decoder input length: 71
2024-10-27 18:00:44,039 (beam_search:429) INFO: max output length: 71
2024-10-27 18:00:44,039 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:44,179 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:44,180 (beam_search:476) INFO:  -1.83 * 1.0 =  -1.83 for ctc
2024-10-27 18:00:44,180 (beam_search:479) INFO: total log probability: -1.83
2024-10-27 18:00:44,180 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:00:44,180 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:44,180 (beam_search:483) INFO: best hypo: THEWIRESHAVETOSOTHEELECTRICITYCANFLOWINTHELIGHT

2024-10-27 18:00:44,182 (asr_inference:509) INFO: speech length: 180992
2024-10-27 18:00:51,281 (beam_search:428) INFO: decoder input length: 140
2024-10-27 18:00:51,281 (beam_search:429) INFO: max output length: 140
2024-10-27 18:00:51,281 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:52,260 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:52,260 (beam_search:476) INFO:  -1.88 * 1.0 =  -1.88 for ctc
2024-10-27 18:00:52,260 (beam_search:479) INFO: total log probability: -1.88
2024-10-27 18:00:52,260 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:00:52,260 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:52,260 (beam_search:483) INFO: best hypo: ONEHASTOBEONTHENEGATIVESIDEANDTHEOTHERHASTOBEONTHEPOSITIVEANDTHENTHEOTHERENDSOFTHEWIRESHAVETOBEINTHELITTLESSOITCONNECTSTOTHELIGHT

2024-10-27 18:00:52,263 (asr_inference:509) INFO: speech length: 137040
2024-10-27 18:00:57,303 (beam_search:428) INFO: decoder input length: 106
2024-10-27 18:00:57,303 (beam_search:429) INFO: max output length: 106
2024-10-27 18:00:57,303 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:57,614 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:57,615 (beam_search:476) INFO:  -1.19 * 1.0 =  -1.19 for ctc
2024-10-27 18:00:57,615 (beam_search:479) INFO: total log probability: -1.19
2024-10-27 18:00:57,615 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:00:57,615 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:57,615 (beam_search:483) INFO: best hypo: UMWELLTHETHEELECTRICITYCOMESOUTFROMTHENEGATIVEANDTHENCOMESBACKINTOTHEPOSITIVE

2024-10-27 18:00:57,617 (asr_inference:509) INFO: speech length: 119024
2024-10-27 18:01:01,977 (beam_search:428) INFO: decoder input length: 92
2024-10-27 18:01:01,977 (beam_search:429) INFO: max output length: 92
2024-10-27 18:01:01,977 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:02,187 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:02,187 (beam_search:476) INFO:  -1.62 * 1.0 =  -1.62 for ctc
2024-10-27 18:01:02,187 (beam_search:479) INFO: total log probability: -1.62
2024-10-27 18:01:02,187 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:01:02,187 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:02,187 (beam_search:483) INFO: best hypo: THEALLOFTHEWIRESAREATHOOKEDUPTOTHELITTLE

2024-10-27 18:01:02,189 (asr_inference:509) INFO: speech length: 155168
2024-10-27 18:01:07,953 (beam_search:428) INFO: decoder input length: 120
2024-10-27 18:01:07,953 (beam_search:429) INFO: max output length: 120
2024-10-27 18:01:07,953 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:08,527 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:08,527 (beam_search:476) INFO:  -1.82 * 1.0 =  -1.82 for ctc
2024-10-27 18:01:08,527 (beam_search:479) INFO: total log probability: -1.82
2024-10-27 18:01:08,527 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:01:08,527 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:08,527 (beam_search:483) INFO: best hypo: IFTHEY'RENOTEDUPTHEELECTRICITYCAN'TGOFROMTHEFROMTHEBATTERYANDITCAN'TGETTOTHELIGHT

2024-10-27 18:01:08,529 (asr_inference:509) INFO: speech length: 90560
2024-10-27 18:01:11,942 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:01:11,942 (beam_search:429) INFO: max output length: 70
2024-10-27 18:01:11,942 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:12,121 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:12,121 (beam_search:476) INFO:  -1.45 * 1.0 =  -1.45 for ctc
2024-10-27 18:01:12,121 (beam_search:479) INFO: total log probability: -1.45
2024-10-27 18:01:12,122 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:01:12,122 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:12,122 (beam_search:483) INFO: best hypo: THEYHELPHOLDITBECAUSEYOUHAVETOHOLDTHELIGHTYOUPUTTHEWIRESON

2024-10-27 18:01:12,124 (asr_inference:509) INFO: speech length: 248576
2024-10-27 18:01:22,170 (beam_search:428) INFO: decoder input length: 193
2024-10-27 18:01:22,171 (beam_search:429) INFO: max output length: 193
2024-10-27 18:01:22,171 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:23,410 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:23,411 (beam_search:476) INFO:  -2.68 * 1.0 =  -2.68 for ctc
2024-10-27 18:01:23,411 (beam_search:479) INFO: total log probability: -2.68
2024-10-27 18:01:23,411 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:01:23,411 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:23,411 (beam_search:483) INFO: best hypo: THELIGHTSINTOTHELITTLEALITTLESOTHENITSTHEBOTTOMTHESTHEBOTTOMSOWHENTHEWIRESAREUMINITTHETHEELECTRICITYGOESTOTHEBOTTOMANDUP

2024-10-27 18:01:23,414 (asr_inference:509) INFO: speech length: 130992
2024-10-27 18:01:28,230 (beam_search:428) INFO: decoder input length: 101
2024-10-27 18:01:28,230 (beam_search:429) INFO: max output length: 101
2024-10-27 18:01:28,230 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:28,437 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:28,438 (beam_search:476) INFO:  -2.24 * 1.0 =  -2.24 for ctc
2024-10-27 18:01:28,438 (beam_search:479) INFO: total log probability: -2.24
2024-10-27 18:01:28,438 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:01:28,438 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:28,438 (beam_search:483) INFO: best hypo: THEAREANDTHEBOTTOMISTOUCHINGTHEMETALONTOTHE

2024-10-27 18:01:28,440 (asr_inference:509) INFO: speech length: 22848
2024-10-27 18:01:29,520 (beam_search:428) INFO: decoder input length: 17
2024-10-27 18:01:29,520 (beam_search:429) INFO: max output length: 17
2024-10-27 18:01:29,520 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:29,534 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:29,534 (beam_search:476) INFO:  -0.29 * 1.0 =  -0.29 for ctc
2024-10-27 18:01:29,534 (beam_search:479) INFO: total log probability: -0.29
2024-10-27 18:01:29,534 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:01:29,534 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:29,534 (beam_search:483) INFO: best hypo: THEY'RE

2024-10-27 18:01:29,536 (asr_inference:509) INFO: speech length: 86400
2024-10-27 18:01:32,715 (beam_search:428) INFO: decoder input length: 66
2024-10-27 18:01:32,715 (beam_search:429) INFO: max output length: 66
2024-10-27 18:01:32,715 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:32,832 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:32,832 (beam_search:476) INFO:  -1.69 * 1.0 =  -1.69 for ctc
2024-10-27 18:01:32,832 (beam_search:479) INFO: total log probability: -1.69
2024-10-27 18:01:32,832 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:01:32,832 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:32,832 (beam_search:483) INFO: best hypo: UMWELLANDTHERE'STWOTHEISBRIGHTER

2024-10-27 18:01:32,835 (asr_inference:509) INFO: speech length: 59648
2024-10-27 18:01:35,076 (beam_search:428) INFO: decoder input length: 46
2024-10-27 18:01:35,076 (beam_search:429) INFO: max output length: 46
2024-10-27 18:01:35,076 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:35,126 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:35,127 (beam_search:476) INFO:  -1.51 * 1.0 =  -1.51 for ctc
2024-10-27 18:01:35,127 (beam_search:479) INFO: total log probability: -1.51
2024-10-27 18:01:35,127 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:01:35,127 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:35,127 (beam_search:483) INFO: best hypo: BECAUSETWOTHEREISMOREPOWER

2024-10-27 18:01:35,129 (asr_inference:509) INFO: speech length: 29408
2024-10-27 18:01:36,395 (beam_search:428) INFO: decoder input length: 22
2024-10-27 18:01:36,395 (beam_search:429) INFO: max output length: 22
2024-10-27 18:01:36,395 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:36,417 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:36,417 (beam_search:476) INFO:  -0.93 * 1.0 =  -0.93 for ctc
2024-10-27 18:01:36,417 (beam_search:479) INFO: total log probability: -0.93
2024-10-27 18:01:36,417 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:01:36,417 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:36,417 (beam_search:483) INFO: best hypo: THEY'RELESSLIGHT

2024-10-27 18:01:36,420 (asr_inference:509) INFO: speech length: 58688
2024-10-27 18:01:38,579 (beam_search:428) INFO: decoder input length: 45
2024-10-27 18:01:38,579 (beam_search:429) INFO: max output length: 45
2024-10-27 18:01:38,579 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:38,676 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:38,676 (beam_search:476) INFO:  -1.24 * 1.0 =  -1.24 for ctc
2024-10-27 18:01:38,676 (beam_search:479) INFO: total log probability: -1.24
2024-10-27 18:01:38,677 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:01:38,677 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:38,677 (beam_search:483) INFO: best hypo: THEY'RENOTTHEY'RENOTASBRIGHTASTHEOTHERONES

2024-10-27 18:01:38,680 (asr_inference:509) INFO: speech length: 53168
2024-10-27 18:01:40,669 (beam_search:428) INFO: decoder input length: 41
2024-10-27 18:01:40,669 (beam_search:429) INFO: max output length: 41
2024-10-27 18:01:40,669 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:40,718 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:40,719 (beam_search:476) INFO:  -0.98 * 1.0 =  -0.98 for ctc
2024-10-27 18:01:40,719 (beam_search:479) INFO: total log probability: -0.98
2024-10-27 18:01:40,719 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:01:40,719 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:40,719 (beam_search:483) INFO: best hypo: THEREARETWOSANDONEBATTERY

2024-10-27 18:01:40,722 (asr_inference:509) INFO: speech length: 68368
2024-10-27 18:01:43,199 (beam_search:428) INFO: decoder input length: 52
2024-10-27 18:01:43,199 (beam_search:429) INFO: max output length: 52
2024-10-27 18:01:43,199 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:43,262 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:43,262 (beam_search:476) INFO:  -0.56 * 1.0 =  -0.56 for ctc
2024-10-27 18:01:43,262 (beam_search:479) INFO: total log probability: -0.56
2024-10-27 18:01:43,262 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:01:43,262 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:43,262 (beam_search:483) INFO: best hypo: ITHEY'REALLCONNECTEDAND

2024-10-27 18:01:43,264 (asr_inference:509) INFO: speech length: 123888
2024-10-27 18:01:47,699 (beam_search:428) INFO: decoder input length: 96
2024-10-27 18:01:47,699 (beam_search:429) INFO: max output length: 96
2024-10-27 18:01:47,700 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:48,015 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:48,016 (beam_search:476) INFO:  -2.73 * 1.0 =  -2.73 for ctc
2024-10-27 18:01:48,016 (beam_search:479) INFO: total log probability: -2.73
2024-10-27 18:01:48,016 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:01:48,016 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:48,016 (beam_search:483) INFO: best hypo: IFTHEY'RENOTATTOUCHINGTHEBOTTOMPARTTHEELECTRICITYCAN'TGETTOTHELIGHTBULB

2024-10-27 18:01:48,019 (asr_inference:509) INFO: speech length: 19648
2024-10-27 18:01:48,993 (beam_search:428) INFO: decoder input length: 14
2024-10-27 18:01:48,993 (beam_search:429) INFO: max output length: 14
2024-10-27 18:01:48,993 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:49,001 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:49,001 (beam_search:476) INFO:  -0.59 * 1.0 =  -0.59 for ctc
2024-10-27 18:01:49,001 (beam_search:479) INFO: total log probability: -0.59
2024-10-27 18:01:49,001 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:01:49,001 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:49,001 (beam_search:483) INFO: best hypo: YEAH

2024-10-27 18:01:49,003 (asr_inference:509) INFO: speech length: 144112
2024-10-27 18:01:54,227 (beam_search:428) INFO: decoder input length: 112
2024-10-27 18:01:54,228 (beam_search:429) INFO: max output length: 112
2024-10-27 18:01:54,228 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:54,698 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:54,698 (beam_search:476) INFO:  -3.03 * 1.0 =  -3.03 for ctc
2024-10-27 18:01:54,698 (beam_search:479) INFO: total log probability: -3.03
2024-10-27 18:01:54,698 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:01:54,698 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:54,699 (beam_search:483) INFO: best hypo: THEELECTRICITYGOESFROMTHENEGATIVEANDITGOESTHROUGHTHELIGHTBULBTHENCOMESINTOTHEOTHERONEANDCOMESBACKTOTHEBATTERY

2024-10-27 18:01:54,702 (asr_inference:509) INFO: speech length: 81904
2024-10-27 18:01:58,000 (beam_search:428) INFO: decoder input length: 63
2024-10-27 18:01:58,000 (beam_search:429) INFO: max output length: 63
2024-10-27 18:01:58,000 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:58,098 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:58,098 (beam_search:476) INFO:  -1.66 * 1.0 =  -1.66 for ctc
2024-10-27 18:01:58,098 (beam_search:479) INFO: total log probability: -1.66
2024-10-27 18:01:58,098 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:01:58,098 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:58,098 (beam_search:483) INFO: best hypo: THEENERGYBECAUSETHENEGATIVEISONTHESIDE

2024-10-27 18:01:58,101 (asr_inference:509) INFO: speech length: 54656
2024-10-27 18:02:00,165 (beam_search:428) INFO: decoder input length: 42
2024-10-27 18:02:00,165 (beam_search:429) INFO: max output length: 42
2024-10-27 18:02:00,166 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:00,210 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:00,210 (beam_search:476) INFO:  -0.87 * 1.0 =  -0.87 for ctc
2024-10-27 18:02:00,210 (beam_search:479) INFO: total log probability: -0.87
2024-10-27 18:02:00,210 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:02:00,210 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:00,210 (beam_search:483) INFO: best hypo: THETHEENERGYTHEOTHERWAY

2024-10-27 18:02:00,212 (asr_inference:509) INFO: speech length: 148368
2024-10-27 18:02:05,739 (beam_search:428) INFO: decoder input length: 115
2024-10-27 18:02:05,739 (beam_search:429) INFO: max output length: 115
2024-10-27 18:02:05,739 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:06,001 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:06,001 (beam_search:476) INFO:  -1.00 * 1.0 =  -1.00 for ctc
2024-10-27 18:02:06,001 (beam_search:479) INFO: total log probability: -1.00
2024-10-27 18:02:06,001 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:02:06,001 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:06,001 (beam_search:483) INFO: best hypo: THEOTHERLIKETHEELECTRICITYGOESTOTHERIGHTINTOTHEOTHERSIDE

2024-10-27 18:02:06,004 (asr_inference:509) INFO: speech length: 138336
2024-10-27 18:02:11,049 (beam_search:428) INFO: decoder input length: 107
2024-10-27 18:02:11,049 (beam_search:429) INFO: max output length: 107
2024-10-27 18:02:11,049 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:11,299 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:11,300 (beam_search:476) INFO:  -1.04 * 1.0 =  -1.04 for ctc
2024-10-27 18:02:11,300 (beam_search:479) INFO: total log probability: -1.04
2024-10-27 18:02:11,300 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:02:11,300 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:11,300 (beam_search:483) INFO: best hypo: ITHINKTHERE'SUMIT'SABOUTUMLIGHTBULBSAND

2024-10-27 18:02:11,302 (asr_inference:509) INFO: speech length: 110608
2024-10-27 18:02:15,302 (beam_search:428) INFO: decoder input length: 85
2024-10-27 18:02:15,302 (beam_search:429) INFO: max output length: 85
2024-10-27 18:02:15,302 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:15,517 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:15,517 (beam_search:476) INFO:  -2.28 * 1.0 =  -2.28 for ctc
2024-10-27 18:02:15,517 (beam_search:479) INFO: total log probability: -2.28
2024-10-27 18:02:15,517 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:02:15,517 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:15,517 (beam_search:483) INFO: best hypo: INCLASSWEHAVEAPAPERTHATHASTHESESANDTHEY'REON

2024-10-27 18:02:15,520 (asr_inference:509) INFO: speech length: 231616
2024-10-27 18:02:25,429 (beam_search:428) INFO: decoder input length: 180
2024-10-27 18:02:25,429 (beam_search:429) INFO: max output length: 180
2024-10-27 18:02:25,429 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:26,074 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:26,074 (beam_search:476) INFO:  -4.01 * 1.0 =  -4.01 for ctc
2024-10-27 18:02:26,074 (beam_search:479) INFO: total log probability: -4.01
2024-10-27 18:02:26,074 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:02:26,074 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:26,074 (beam_search:483) INFO: best hypo: THEWITHARETHESANDTHEAREARETHEWIREANDTHEANDTHEONEISTHEBATTERY

2024-10-27 18:02:26,077 (asr_inference:509) INFO: speech length: 82240
2024-10-27 18:02:29,076 (beam_search:428) INFO: decoder input length: 63
2024-10-27 18:02:29,076 (beam_search:429) INFO: max output length: 63
2024-10-27 18:02:29,077 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:29,207 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:29,207 (beam_search:476) INFO:  -1.38 * 1.0 =  -1.38 for ctc
2024-10-27 18:02:29,207 (beam_search:479) INFO: total log probability: -1.38
2024-10-27 18:02:29,207 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:02:29,208 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:29,208 (beam_search:483) INFO: best hypo: UMI'MNOTSUREBUTITHINKIT'SANDTWO

2024-10-27 18:02:29,210 (asr_inference:509) INFO: speech length: 50128
2024-10-27 18:02:31,031 (beam_search:428) INFO: decoder input length: 38
2024-10-27 18:02:31,031 (beam_search:429) INFO: max output length: 38
2024-10-27 18:02:31,031 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:31,072 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:31,073 (beam_search:476) INFO:  -1.07 * 1.0 =  -1.07 for ctc
2024-10-27 18:02:31,073 (beam_search:479) INFO: total log probability: -1.07
2024-10-27 18:02:31,073 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:02:31,073 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:31,073 (beam_search:483) INFO: best hypo: UMI'MNOTE

2024-10-27 18:02:31,075 (asr_inference:509) INFO: speech length: 98256
2024-10-27 18:02:34,744 (beam_search:428) INFO: decoder input length: 76
2024-10-27 18:02:34,744 (beam_search:429) INFO: max output length: 76
2024-10-27 18:02:34,744 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:34,851 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:34,851 (beam_search:476) INFO:  -1.01 * 1.0 =  -1.01 for ctc
2024-10-27 18:02:34,851 (beam_search:479) INFO: total log probability: -1.01
2024-10-27 18:02:34,851 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:02:34,851 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:34,852 (beam_search:483) INFO: best hypo: WEAMAGNETTHATCANTURNANDON

2024-10-27 18:02:34,854 (asr_inference:509) INFO: speech length: 311088
2024-10-27 18:02:48,194 (beam_search:428) INFO: decoder input length: 242
2024-10-27 18:02:48,194 (beam_search:429) INFO: max output length: 242
2024-10-27 18:02:48,194 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:49,952 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:49,952 (beam_search:476) INFO:  -8.56 * 1.0 =  -8.56 for ctc
2024-10-27 18:02:49,952 (beam_search:479) INFO: total log probability: -8.56
2024-10-27 18:02:49,952 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:02:49,952 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:49,952 (beam_search:483) INFO: best hypo: WELLWEHADARIVETANDASHORTWIREANDALONGWIREWEALSOHADABATTERYANDWEFIRSTTRIEDTOALLOFEMTOGETHERANDWEHADTOWRAPAROUNDTHELIKEORTIMESTOTHE

2024-10-27 18:02:49,955 (asr_inference:509) INFO: speech length: 270608
2024-10-27 18:03:01,056 (beam_search:428) INFO: decoder input length: 210
2024-10-27 18:03:01,056 (beam_search:429) INFO: max output length: 210
2024-10-27 18:03:01,056 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:02,552 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:02,552 (beam_search:476) INFO:  -4.50 * 1.0 =  -4.50 for ctc
2024-10-27 18:03:02,552 (beam_search:479) INFO: total log probability: -4.50
2024-10-27 18:03:02,552 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:03:02,552 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:02,552 (beam_search:483) INFO: best hypo: WEUMHADASWITCHANDLITTLEWASHERSTOOANDWEJUSTPUTEMINANDWRAPPEDITAROUNDANDCONNECTEDALLOFTHEMTOGETHERTOTRYTOMAKEITWORKANDPICKUPTHEWASHERS

2024-10-27 18:03:02,555 (asr_inference:509) INFO: speech length: 52848
2024-10-27 18:03:04,483 (beam_search:428) INFO: decoder input length: 40
2024-10-27 18:03:04,484 (beam_search:429) INFO: max output length: 40
2024-10-27 18:03:04,484 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:04,510 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:04,510 (beam_search:476) INFO:  -0.52 * 1.0 =  -0.52 for ctc
2024-10-27 18:03:04,510 (beam_search:479) INFO: total log probability: -0.52
2024-10-27 18:03:04,510 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:03:04,511 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:04,511 (beam_search:483) INFO: best hypo: UHWAITWHAT

2024-10-27 18:03:04,513 (asr_inference:509) INFO: speech length: 164224
2024-10-27 18:03:10,909 (beam_search:428) INFO: decoder input length: 127
2024-10-27 18:03:10,909 (beam_search:429) INFO: max output length: 127
2024-10-27 18:03:10,909 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:11,465 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:11,465 (beam_search:476) INFO:  -2.45 * 1.0 =  -2.45 for ctc
2024-10-27 18:03:11,465 (beam_search:479) INFO: total log probability: -2.45
2024-10-27 18:03:11,465 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:03:11,465 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:11,465 (beam_search:483) INFO: best hypo: THEYPICKEDUPTHEWASHERSFROMTHELITTLERIVETANDWEHAVETOHAVEALOTOFPOWERSOWEDITA

2024-10-27 18:03:11,468 (asr_inference:509) INFO: speech length: 112544
2024-10-27 18:03:15,664 (beam_search:428) INFO: decoder input length: 87
2024-10-27 18:03:15,664 (beam_search:429) INFO: max output length: 87
2024-10-27 18:03:15,664 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:15,877 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:15,878 (beam_search:476) INFO:  -0.80 * 1.0 =  -0.80 for ctc
2024-10-27 18:03:15,878 (beam_search:479) INFO: total log probability: -0.80
2024-10-27 18:03:15,878 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:03:15,878 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:15,878 (beam_search:483) INFO: best hypo: WELLTHEBATTERYVETHEUMUMTHEPOWERGOTOTHETHERIVET

2024-10-27 18:03:15,880 (asr_inference:509) INFO: speech length: 143328
2024-10-27 18:03:21,404 (beam_search:428) INFO: decoder input length: 111
2024-10-27 18:03:21,405 (beam_search:429) INFO: max output length: 111
2024-10-27 18:03:21,405 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:21,972 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:21,972 (beam_search:476) INFO:  -2.83 * 1.0 =  -2.83 for ctc
2024-10-27 18:03:21,972 (beam_search:479) INFO: total log probability: -2.83
2024-10-27 18:03:21,972 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:03:21,972 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:21,973 (beam_search:483) INFO: best hypo: WEALSOHADTOCONNECTTHEWIRESTOTHETHAT'SHOWITCANTURNOFFANDONSOTHENITDOESN'TBACKTOTHEBATTERY

2024-10-27 18:03:21,975 (asr_inference:509) INFO: speech length: 36880
2024-10-27 18:03:23,459 (beam_search:428) INFO: decoder input length: 28
2024-10-27 18:03:23,459 (beam_search:429) INFO: max output length: 28
2024-10-27 18:03:23,459 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:23,474 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:23,474 (beam_search:476) INFO:  -0.54 * 1.0 =  -0.54 for ctc
2024-10-27 18:03:23,474 (beam_search:479) INFO: total log probability: -0.54
2024-10-27 18:03:23,474 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:03:23,474 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:23,474 (beam_search:483) INFO: best hypo: 'S

2024-10-27 18:03:23,476 (asr_inference:509) INFO: speech length: 101280
2024-10-27 18:03:27,035 (beam_search:428) INFO: decoder input length: 78
2024-10-27 18:03:27,036 (beam_search:429) INFO: max output length: 78
2024-10-27 18:03:27,036 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:27,220 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:27,220 (beam_search:476) INFO:  -3.36 * 1.0 =  -3.36 for ctc
2024-10-27 18:03:27,220 (beam_search:479) INFO: total log probability: -3.36
2024-10-27 18:03:27,220 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:03:27,220 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:27,220 (beam_search:483) INFO: best hypo: WHENTHESWITCHISCLOSEDTHETHELITTLERIVETPICKSUPTHEWASHERS

2024-10-27 18:03:27,223 (asr_inference:509) INFO: speech length: 101280
2024-10-27 18:03:30,948 (beam_search:428) INFO: decoder input length: 78
2024-10-27 18:03:30,948 (beam_search:429) INFO: max output length: 78
2024-10-27 18:03:30,948 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:31,166 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:31,166 (beam_search:476) INFO:  -1.22 * 1.0 =  -1.22 for ctc
2024-10-27 18:03:31,166 (beam_search:479) INFO: total log probability: -1.22
2024-10-27 18:03:31,166 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:03:31,166 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:31,166 (beam_search:483) INFO: best hypo: THERIVETISUMAMAGNETKINDOFSOTHENITPICKSUPTHEWASHERS

2024-10-27 18:03:31,169 (asr_inference:509) INFO: speech length: 122256
2024-10-27 18:03:35,578 (beam_search:428) INFO: decoder input length: 95
2024-10-27 18:03:35,578 (beam_search:429) INFO: max output length: 95
2024-10-27 18:03:35,578 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:35,915 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:35,915 (beam_search:476) INFO:  -1.82 * 1.0 =  -1.82 for ctc
2024-10-27 18:03:35,915 (beam_search:479) INFO: total log probability: -1.82
2024-10-27 18:03:35,915 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:03:35,915 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:35,915 (beam_search:483) INFO: best hypo: WELLI'MNOTESUREITHINKIT'SAMETALBUTI'MNOTESURE

2024-10-27 18:03:35,917 (asr_inference:509) INFO: speech length: 33648
2024-10-27 18:03:37,240 (beam_search:428) INFO: decoder input length: 25
2024-10-27 18:03:37,240 (beam_search:429) INFO: max output length: 25
2024-10-27 18:03:37,240 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:37,261 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:37,261 (beam_search:476) INFO:  -0.61 * 1.0 =  -0.61 for ctc
2024-10-27 18:03:37,261 (beam_search:479) INFO: total log probability: -0.61
2024-10-27 18:03:37,261 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:03:37,261 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:37,261 (beam_search:483) INFO: best hypo: THEYWON'T

2024-10-27 18:03:37,263 (asr_inference:509) INFO: speech length: 69168
2024-10-27 18:03:39,830 (beam_search:428) INFO: decoder input length: 53
2024-10-27 18:03:39,830 (beam_search:429) INFO: max output length: 53
2024-10-27 18:03:39,830 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:39,897 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:39,897 (beam_search:476) INFO:  -1.18 * 1.0 =  -1.18 for ctc
2024-10-27 18:03:39,897 (beam_search:479) INFO: total log probability: -1.18
2024-10-27 18:03:39,897 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:03:39,897 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:39,897 (beam_search:483) INFO: best hypo: THERIVETISINGUPTHEWASHERS

2024-10-27 18:03:39,900 (asr_inference:509) INFO: speech length: 86896
2024-10-27 18:03:43,105 (beam_search:428) INFO: decoder input length: 67
2024-10-27 18:03:43,105 (beam_search:429) INFO: max output length: 67
2024-10-27 18:03:43,105 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:43,247 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:43,247 (beam_search:476) INFO:  -1.94 * 1.0 =  -1.94 for ctc
2024-10-27 18:03:43,247 (beam_search:479) INFO: total log probability: -1.94
2024-10-27 18:03:43,248 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:03:43,248 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:43,248 (beam_search:483) INFO: best hypo: WETHATWOULDBECAUSETHERE'SNOPOWERGOINGINTOIT

2024-10-27 18:03:43,251 (asr_inference:509) INFO: speech length: 164224
2024-10-27 18:03:49,413 (beam_search:428) INFO: decoder input length: 127
2024-10-27 18:03:49,413 (beam_search:429) INFO: max output length: 127
2024-10-27 18:03:49,413 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:49,870 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:49,870 (beam_search:476) INFO:  -1.96 * 1.0 =  -1.96 for ctc
2024-10-27 18:03:49,870 (beam_search:479) INFO: total log probability: -1.96
2024-10-27 18:03:49,870 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:03:49,870 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:49,870 (beam_search:483) INFO: best hypo: ITHINKTHATWHENTHEPOWERGOESTOTHEMETALITITJUSTSOMETHINGTOGETTHETOGOUP

2024-10-27 18:03:49,872 (asr_inference:509) INFO: speech length: 41664
2024-10-27 18:03:51,496 (beam_search:428) INFO: decoder input length: 32
2024-10-27 18:03:51,496 (beam_search:429) INFO: max output length: 32
2024-10-27 18:03:51,497 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:51,521 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:51,521 (beam_search:476) INFO:  -1.02 * 1.0 =  -1.02 for ctc
2024-10-27 18:03:51,521 (beam_search:479) INFO: total log probability: -1.02
2024-10-27 18:03:51,521 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:03:51,521 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:51,521 (beam_search:483) INFO: best hypo: WEUSEJUSTMAGNETS

2024-10-27 18:03:51,523 (asr_inference:509) INFO: speech length: 107792
2024-10-27 18:03:55,338 (beam_search:428) INFO: decoder input length: 83
2024-10-27 18:03:55,338 (beam_search:429) INFO: max output length: 83
2024-10-27 18:03:55,338 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:55,615 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:55,616 (beam_search:476) INFO:  -2.19 * 1.0 =  -2.19 for ctc
2024-10-27 18:03:55,616 (beam_search:479) INFO: total log probability: -2.19
2024-10-27 18:03:55,616 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:03:55,616 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:55,616 (beam_search:483) INFO: best hypo: IDON'TTHINKSOBUTI'NOTSUREIFI'MIT'SJUSTIN

2024-10-27 18:03:55,618 (asr_inference:509) INFO: speech length: 164400
2024-10-27 18:04:01,845 (beam_search:428) INFO: decoder input length: 127
2024-10-27 18:04:01,845 (beam_search:429) INFO: max output length: 127
2024-10-27 18:04:01,845 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:02,371 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:02,371 (beam_search:476) INFO:  -2.37 * 1.0 =  -2.37 for ctc
2024-10-27 18:04:02,373 (beam_search:479) INFO: total log probability: -2.37
2024-10-27 18:04:02,374 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:04:02,374 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:02,374 (beam_search:483) INFO: best hypo: THEELECTRICITYALWAYSOUTTHENEGATIVESIDEANDCOMESBACKINTHEUMPOSITIVESOITGOESLIKEINABACKTOIT

2024-10-27 18:04:02,376 (asr_inference:509) INFO: speech length: 25504
2024-10-27 18:04:03,468 (beam_search:428) INFO: decoder input length: 19
2024-10-27 18:04:03,468 (beam_search:429) INFO: max output length: 19
2024-10-27 18:04:03,468 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:03,487 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:03,487 (beam_search:476) INFO:  -0.04 * 1.0 =  -0.04 for ctc
2024-10-27 18:04:03,487 (beam_search:479) INFO: total log probability: -0.04
2024-10-27 18:04:03,487 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 18:04:03,487 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:03,487 (beam_search:483) INFO: best hypo: UMWHATDIDYOUSAY

2024-10-27 18:04:03,489 (asr_inference:509) INFO: speech length: 133664
2024-10-27 18:04:08,535 (beam_search:428) INFO: decoder input length: 103
2024-10-27 18:04:08,535 (beam_search:429) INFO: max output length: 103
2024-10-27 18:04:08,535 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:08,944 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:08,944 (beam_search:476) INFO:  -2.95 * 1.0 =  -2.95 for ctc
2024-10-27 18:04:08,944 (beam_search:479) INFO: total log probability: -2.95
2024-10-27 18:04:08,944 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:04:08,944 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:08,944 (beam_search:483) INFO: best hypo: WELLIFUMTHEDOESN'TITWON'TANDIFASWITCHISN'TITWON'T

2024-10-27 18:04:08,946 (asr_inference:509) INFO: speech length: 96400
2024-10-27 18:04:12,494 (beam_search:428) INFO: decoder input length: 74
2024-10-27 18:04:12,494 (beam_search:429) INFO: max output length: 74
2024-10-27 18:04:12,494 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:12,572 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:12,572 (beam_search:476) INFO:  -0.72 * 1.0 =  -0.72 for ctc
2024-10-27 18:04:12,572 (beam_search:479) INFO: total log probability: -0.72
2024-10-27 18:04:12,572 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:04:12,572 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:12,572 (beam_search:483) INFO: best hypo: UMWEALSOJUSTI

2024-10-27 18:04:12,576 (asr_inference:509) INFO: speech length: 117488
2024-10-27 18:04:16,856 (beam_search:428) INFO: decoder input length: 91
2024-10-27 18:04:16,857 (beam_search:429) INFO: max output length: 91
2024-10-27 18:04:16,857 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:17,057 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:17,057 (beam_search:476) INFO:  -1.67 * 1.0 =  -1.67 for ctc
2024-10-27 18:04:17,057 (beam_search:479) INFO: total log probability: -1.67
2024-10-27 18:04:17,057 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:04:17,057 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:17,057 (beam_search:483) INFO: best hypo: WEWEJUSTTRIEDUMTHEWIRETOUCHINGINSTEADOFITAROUNDAND

2024-10-27 18:04:17,059 (asr_inference:509) INFO: speech length: 135264
2024-10-27 18:04:22,002 (beam_search:428) INFO: decoder input length: 105
2024-10-27 18:04:22,002 (beam_search:429) INFO: max output length: 105
2024-10-27 18:04:22,002 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:22,325 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:22,325 (beam_search:476) INFO:  -2.34 * 1.0 =  -2.34 for ctc
2024-10-27 18:04:22,325 (beam_search:479) INFO: total log probability: -2.34
2024-10-27 18:04:22,326 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:04:22,326 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:22,326 (beam_search:483) INFO: best hypo: 'SNOTENOUGHELECTRICITYGOINGTOTHERIVETSOTHENITCAN'TUPTHEWASHERS

2024-10-27 18:04:22,328 (asr_inference:509) INFO: speech length: 120576
2024-10-27 18:04:26,699 (beam_search:428) INFO: decoder input length: 93
2024-10-27 18:04:26,699 (beam_search:429) INFO: max output length: 93
2024-10-27 18:04:26,699 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:26,851 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:26,851 (beam_search:476) INFO:  -2.60 * 1.0 =  -2.60 for ctc
2024-10-27 18:04:26,851 (beam_search:479) INFO: total log probability: -2.60
2024-10-27 18:04:26,851 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:04:26,851 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:26,852 (beam_search:483) INFO: best hypo: WEFORFROMANDUSITAROUNDTHERIVET

2024-10-27 18:04:26,854 (asr_inference:509) INFO: speech length: 83504
2024-10-27 18:04:29,913 (beam_search:428) INFO: decoder input length: 64
2024-10-27 18:04:29,913 (beam_search:429) INFO: max output length: 64
2024-10-27 18:04:29,913 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:29,980 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:29,980 (beam_search:476) INFO:  -0.98 * 1.0 =  -0.98 for ctc
2024-10-27 18:04:29,980 (beam_search:479) INFO: total log probability: -0.98
2024-10-27 18:04:29,980 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:04:29,980 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:29,980 (beam_search:483) INFO: best hypo: THEUMTHEYTOTHERIVET

2024-10-27 18:04:29,982 (asr_inference:509) INFO: speech length: 68400
2024-10-27 18:04:32,495 (beam_search:428) INFO: decoder input length: 52
2024-10-27 18:04:32,495 (beam_search:429) INFO: max output length: 52
2024-10-27 18:04:32,495 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:32,565 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:32,566 (beam_search:476) INFO:  -1.39 * 1.0 =  -1.39 for ctc
2024-10-27 18:04:32,566 (beam_search:479) INFO: total log probability: -1.39
2024-10-27 18:04:32,566 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:04:32,566 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:32,566 (beam_search:483) INFO: best hypo: WE'VEBEENWITHUMONELECTRICITY

2024-10-27 18:04:32,568 (asr_inference:509) INFO: speech length: 233664
2024-10-27 18:04:42,269 (beam_search:428) INFO: decoder input length: 182
2024-10-27 18:04:42,269 (beam_search:429) INFO: max output length: 182
2024-10-27 18:04:42,269 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:42,906 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:42,907 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 18:04:42,907 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 18:04:42,907 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:04:42,907 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:42,907 (beam_search:483) INFO: best hypo: ELECTRICITYISWHATWEUSEDTOHAVETOHAVELIGHTWITHANDANDWEUSEALOTOFELECTRICITY

2024-10-27 18:04:42,909 (asr_inference:509) INFO: speech length: 22848
2024-10-27 18:04:44,017 (beam_search:428) INFO: decoder input length: 17
2024-10-27 18:04:44,017 (beam_search:429) INFO: max output length: 17
2024-10-27 18:04:44,017 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:44,027 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:44,027 (beam_search:476) INFO:  -1.05 * 1.0 =  -1.05 for ctc
2024-10-27 18:04:44,028 (beam_search:479) INFO: total log probability: -1.05
2024-10-27 18:04:44,028 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:04:44,028 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:44,028 (beam_search:483) INFO: best hypo: UMWE

2024-10-27 18:04:44,030 (asr_inference:509) INFO: speech length: 97504
2024-10-27 18:04:47,543 (beam_search:428) INFO: decoder input length: 75
2024-10-27 18:04:47,543 (beam_search:429) INFO: max output length: 75
2024-10-27 18:04:47,543 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:47,724 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:47,724 (beam_search:476) INFO:  -2.45 * 1.0 =  -2.45 for ctc
2024-10-27 18:04:47,724 (beam_search:479) INFO: total log probability: -2.45
2024-10-27 18:04:47,724 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:04:47,724 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:47,725 (beam_search:483) INFO: best hypo: WE'VEBEENANDTHECIRCUITSMAKETHEMAKETHELIGHTSON

2024-10-27 18:04:47,727 (asr_inference:509) INFO: speech length: 88976
2024-10-27 18:04:51,022 (beam_search:428) INFO: decoder input length: 69
2024-10-27 18:04:51,022 (beam_search:429) INFO: max output length: 69
2024-10-27 18:04:51,022 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:51,108 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:51,108 (beam_search:476) INFO:  -1.31 * 1.0 =  -1.31 for ctc
2024-10-27 18:04:51,108 (beam_search:479) INFO: total log probability: -1.31
2024-10-27 18:04:51,108 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:04:51,108 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:51,108 (beam_search:483) INFO: best hypo: YEAHACIRCUITISAOFELECTRICITY

2024-10-27 18:04:51,110 (asr_inference:509) INFO: speech length: 308512
2024-10-27 18:05:04,572 (beam_search:428) INFO: decoder input length: 240
2024-10-27 18:05:04,572 (beam_search:429) INFO: max output length: 240
2024-10-27 18:05:04,572 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:06,005 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:06,005 (beam_search:476) INFO:  -3.96 * 1.0 =  -3.96 for ctc
2024-10-27 18:05:06,005 (beam_search:479) INFO: total log probability: -3.96
2024-10-27 18:05:06,005 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:05:06,005 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:06,005 (beam_search:483) INFO: best hypo: WELLUMACIRCUITISISUMWELLIFYOUWANTTOLIGHTSOMETHINGORMAKESOMETHINGTHENACIRCUITISTHETHATTHEINANDIT'SLIKEA

2024-10-27 18:05:06,008 (asr_inference:509) INFO: speech length: 16000
2024-10-27 18:05:06,804 (beam_search:428) INFO: decoder input length: 11
2024-10-27 18:05:06,805 (beam_search:429) INFO: max output length: 11
2024-10-27 18:05:06,805 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:06,814 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:06,815 (beam_search:476) INFO:  -0.46 * 1.0 =  -0.46 for ctc
2024-10-27 18:05:06,815 (beam_search:479) INFO: total log probability: -0.46
2024-10-27 18:05:06,815 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:05:06,815 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:06,815 (beam_search:483) INFO: best hypo: ITIS

2024-10-27 18:05:06,817 (asr_inference:509) INFO: speech length: 205856
2024-10-27 18:05:14,779 (beam_search:428) INFO: decoder input length: 160
2024-10-27 18:05:14,779 (beam_search:429) INFO: max output length: 160
2024-10-27 18:05:14,779 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:15,329 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:15,329 (beam_search:476) INFO:  -1.18 * 1.0 =  -1.18 for ctc
2024-10-27 18:05:15,329 (beam_search:479) INFO: total log probability: -1.18
2024-10-27 18:05:15,329 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:05:15,329 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:15,329 (beam_search:483) INFO: best hypo: UMWELLTHEIT'STHEWAYTHATTHEIT'STHEWAYTHATTHETHATTHE

2024-10-27 18:05:15,332 (asr_inference:509) INFO: speech length: 24176
2024-10-27 18:05:16,493 (beam_search:428) INFO: decoder input length: 18
2024-10-27 18:05:16,493 (beam_search:429) INFO: max output length: 18
2024-10-27 18:05:16,493 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:16,502 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:16,502 (beam_search:476) INFO:  -1.46 * 1.0 =  -1.46 for ctc
2024-10-27 18:05:16,502 (beam_search:479) INFO: total log probability: -1.46
2024-10-27 18:05:16,502 (beam_search:480) INFO: normalized log probability: -0.49
2024-10-27 18:05:16,502 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:16,502 (beam_search:483) INFO: best hypo: THE

2024-10-27 18:05:16,505 (asr_inference:509) INFO: speech length: 213552
2024-10-27 18:05:25,004 (beam_search:428) INFO: decoder input length: 166
2024-10-27 18:05:25,005 (beam_search:429) INFO: max output length: 166
2024-10-27 18:05:25,005 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:25,960 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:25,960 (beam_search:476) INFO:  -2.20 * 1.0 =  -2.20 for ctc
2024-10-27 18:05:25,960 (beam_search:479) INFO: total log probability: -2.20
2024-10-27 18:05:25,960 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:05:25,960 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:25,960 (beam_search:483) INFO: best hypo: TOHAVEACIRCUITYOUHAVETOGETYOUHAVETOHAVEWIRESANDTHEYHAVETOBECONNECTEDTOABATTERYANDANELECTRICITYWHICHCOULDBEALIGHTBULB

2024-10-27 18:05:25,963 (asr_inference:509) INFO: speech length: 236224
2024-10-27 18:05:35,723 (beam_search:428) INFO: decoder input length: 184
2024-10-27 18:05:35,723 (beam_search:429) INFO: max output length: 184
2024-10-27 18:05:35,723 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:36,854 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:36,854 (beam_search:476) INFO:  -3.39 * 1.0 =  -3.39 for ctc
2024-10-27 18:05:36,854 (beam_search:479) INFO: total log probability: -3.39
2024-10-27 18:05:36,854 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:05:36,854 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:36,854 (beam_search:483) INFO: best hypo: WELLIFYOULIGHTSOMETHINGTHENYOUHAVETOHAVEWIRESTHATARECONNECTEDTOABATTERYANDTHEWIRESALSOHAVETOBECONNECTEDTOTHELIGHTBULBINITTOLIGHT

2024-10-27 18:05:36,857 (asr_inference:509) INFO: speech length: 12080
2024-10-27 18:05:37,502 (beam_search:428) INFO: decoder input length: 8
2024-10-27 18:05:37,502 (beam_search:429) INFO: max output length: 8
2024-10-27 18:05:37,502 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:37,509 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:37,509 (beam_search:476) INFO:  -0.26 * 1.0 =  -0.26 for ctc
2024-10-27 18:05:37,509 (beam_search:479) INFO: total log probability: -0.26
2024-10-27 18:05:37,509 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:05:37,509 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:37,509 (beam_search:483) INFO: best hypo: 'S

2024-10-27 18:05:37,511 (asr_inference:509) INFO: speech length: 97536
2024-10-27 18:05:40,894 (beam_search:428) INFO: decoder input length: 75
2024-10-27 18:05:40,894 (beam_search:429) INFO: max output length: 75
2024-10-27 18:05:40,894 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:41,013 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:41,013 (beam_search:476) INFO:  -0.17 * 1.0 =  -0.17 for ctc
2024-10-27 18:05:41,013 (beam_search:479) INFO: total log probability: -0.17
2024-10-27 18:05:41,013 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:05:41,013 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:41,014 (beam_search:483) INFO: best hypo: UMTHENTHETHENTHELIGHTBULBLIGHTS

2024-10-27 18:05:41,016 (asr_inference:509) INFO: speech length: 24560
2024-10-27 18:05:42,092 (beam_search:428) INFO: decoder input length: 18
2024-10-27 18:05:42,092 (beam_search:429) INFO: max output length: 18
2024-10-27 18:05:42,092 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:42,103 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:42,103 (beam_search:476) INFO:  -0.50 * 1.0 =  -0.50 for ctc
2024-10-27 18:05:42,103 (beam_search:479) INFO: total log probability: -0.50
2024-10-27 18:05:42,103 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:05:42,103 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:42,103 (beam_search:483) INFO: best hypo: S

2024-10-27 18:05:42,106 (asr_inference:509) INFO: speech length: 173936
2024-10-27 18:05:48,707 (beam_search:428) INFO: decoder input length: 135
2024-10-27 18:05:48,708 (beam_search:429) INFO: max output length: 135
2024-10-27 18:05:48,708 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:49,257 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:49,257 (beam_search:476) INFO:  -2.13 * 1.0 =  -2.13 for ctc
2024-10-27 18:05:49,257 (beam_search:479) INFO: total log probability: -2.13
2024-10-27 18:05:49,258 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:05:49,258 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:49,258 (beam_search:483) INFO: best hypo: SOYOUHAVEACIRCUITANDTHEWIRESARECONNECTEDTOTHEBATTERYANDTHELIGHTBULBTHENTHELIGHTBULBWILLLIGHT

2024-10-27 18:05:49,260 (asr_inference:509) INFO: speech length: 13392
2024-10-27 18:05:49,973 (beam_search:428) INFO: decoder input length: 9
2024-10-27 18:05:49,973 (beam_search:429) INFO: max output length: 9
2024-10-27 18:05:49,973 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:49,980 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:49,980 (beam_search:476) INFO:  -0.20 * 1.0 =  -0.20 for ctc
2024-10-27 18:05:49,981 (beam_search:479) INFO: total log probability: -0.20
2024-10-27 18:05:49,981 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:05:49,981 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:49,981 (beam_search:483) INFO: best hypo: YOUCAN

2024-10-27 18:05:49,983 (asr_inference:509) INFO: speech length: 160160
2024-10-27 18:05:56,123 (beam_search:428) INFO: decoder input length: 124
2024-10-27 18:05:56,123 (beam_search:429) INFO: max output length: 124
2024-10-27 18:05:56,123 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:56,505 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:56,505 (beam_search:476) INFO:  -1.30 * 1.0 =  -1.30 for ctc
2024-10-27 18:05:56,505 (beam_search:479) INFO: total log probability: -1.30
2024-10-27 18:05:56,505 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:05:56,505 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:56,506 (beam_search:483) INFO: best hypo: WELLYOUCANLIGHTALIGHTBULBYOUCANLIGHTMORELIGHTBULBSIFYOUMAKETHECIRCUIT

2024-10-27 18:05:56,508 (asr_inference:509) INFO: speech length: 88768
2024-10-27 18:05:59,809 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:05:59,809 (beam_search:429) INFO: max output length: 68
2024-10-27 18:05:59,809 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:59,890 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:59,891 (beam_search:476) INFO:  -1.34 * 1.0 =  -1.34 for ctc
2024-10-27 18:05:59,891 (beam_search:479) INFO: total log probability: -1.34
2024-10-27 18:05:59,891 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:05:59,891 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:59,891 (beam_search:483) INFO: best hypo: MAKETHETHENYOUWILLNEEDMORE

2024-10-27 18:05:59,894 (asr_inference:509) INFO: speech length: 212528
2024-10-27 18:06:08,463 (beam_search:428) INFO: decoder input length: 165
2024-10-27 18:06:08,463 (beam_search:429) INFO: max output length: 165
2024-10-27 18:06:08,463 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:08,854 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:08,854 (beam_search:476) INFO:  -2.77 * 1.0 =  -2.77 for ctc
2024-10-27 18:06:08,854 (beam_search:479) INFO: total log probability: -2.77
2024-10-27 18:06:08,854 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:06:08,854 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:08,854 (beam_search:483) INFO: best hypo: MOREYOUNEEDMOREWIRESANDMORELIGHTBULBSANDTHAT'S

2024-10-27 18:06:08,857 (asr_inference:509) INFO: speech length: 82560
2024-10-27 18:06:11,961 (beam_search:428) INFO: decoder input length: 63
2024-10-27 18:06:11,961 (beam_search:429) INFO: max output length: 63
2024-10-27 18:06:11,961 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:12,059 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:12,059 (beam_search:476) INFO:  -0.72 * 1.0 =  -0.72 for ctc
2024-10-27 18:06:12,059 (beam_search:479) INFO: total log probability: -0.72
2024-10-27 18:06:12,059 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:06:12,059 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:12,059 (beam_search:483) INFO: best hypo: NOBUTITHINKTHATITWILLLIKETHAT

2024-10-27 18:06:12,062 (asr_inference:509) INFO: speech length: 21216
2024-10-27 18:06:13,034 (beam_search:428) INFO: decoder input length: 16
2024-10-27 18:06:13,034 (beam_search:429) INFO: max output length: 16
2024-10-27 18:06:13,034 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:13,047 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:13,047 (beam_search:476) INFO:  -1.04 * 1.0 =  -1.04 for ctc
2024-10-27 18:06:13,047 (beam_search:479) INFO: total log probability: -1.04
2024-10-27 18:06:13,047 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:06:13,047 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:13,047 (beam_search:483) INFO: best hypo: ITWILLWORK

2024-10-27 18:06:13,050 (asr_inference:509) INFO: speech length: 25888
2024-10-27 18:06:14,129 (beam_search:428) INFO: decoder input length: 19
2024-10-27 18:06:14,130 (beam_search:429) INFO: max output length: 19
2024-10-27 18:06:14,130 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:14,149 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:14,150 (beam_search:476) INFO:  -2.36 * 1.0 =  -2.36 for ctc
2024-10-27 18:06:14,150 (beam_search:479) INFO: total log probability: -2.36
2024-10-27 18:06:14,150 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 18:06:14,150 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:14,150 (beam_search:483) INFO: best hypo: WEDIDWITHLIKETHIS

2024-10-27 18:06:14,153 (asr_inference:509) INFO: speech length: 17328
2024-10-27 18:06:14,973 (beam_search:428) INFO: decoder input length: 13
2024-10-27 18:06:14,973 (beam_search:429) INFO: max output length: 13
2024-10-27 18:06:14,973 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:14,984 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:14,984 (beam_search:476) INFO:  -0.55 * 1.0 =  -0.55 for ctc
2024-10-27 18:06:14,984 (beam_search:479) INFO: total log probability: -0.55
2024-10-27 18:06:14,984 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:06:14,984 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:14,984 (beam_search:483) INFO: best hypo: THAT'S

2024-10-27 18:06:14,987 (asr_inference:509) INFO: speech length: 131424
2024-10-27 18:06:19,830 (beam_search:428) INFO: decoder input length: 102
2024-10-27 18:06:19,831 (beam_search:429) INFO: max output length: 102
2024-10-27 18:06:19,831 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:20,041 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:20,041 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 18:06:20,041 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 18:06:20,041 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:06:20,042 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:20,042 (beam_search:483) INFO: best hypo: THEWAYBECAUSETHEENERGYISSTILLGOINGOUTOFTHENEGATIVE

2024-10-27 18:06:20,044 (asr_inference:509) INFO: speech length: 72544
2024-10-27 18:06:22,687 (beam_search:428) INFO: decoder input length: 56
2024-10-27 18:06:22,687 (beam_search:429) INFO: max output length: 56
2024-10-27 18:06:22,687 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:22,790 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:22,791 (beam_search:476) INFO:  -1.03 * 1.0 =  -1.03 for ctc
2024-10-27 18:06:22,791 (beam_search:479) INFO: total log probability: -1.03
2024-10-27 18:06:22,791 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:06:22,791 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:22,791 (beam_search:483) INFO: best hypo: YOUCANSTILLMAKEUMACIRCUITWITHTWOBATTERIES

2024-10-27 18:06:22,794 (asr_inference:509) INFO: speech length: 376896
2024-10-27 18:06:39,941 (beam_search:428) INFO: decoder input length: 293
2024-10-27 18:06:39,941 (beam_search:429) INFO: max output length: 293
2024-10-27 18:06:39,941 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:42,230 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:42,230 (beam_search:476) INFO:  -5.16 * 1.0 =  -5.16 for ctc
2024-10-27 18:06:42,230 (beam_search:479) INFO: total log probability: -5.16
2024-10-27 18:06:42,230 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:06:42,230 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:42,230 (beam_search:483) INFO: best hypo: YOUJUSTHAVETOGETMOREWIRESANDTHEWIRESTOTHEBATTERIESANDTHENYOUCANSTILLMAKEACIRCUITBUTIFYOUDOITLIKETHISTHENTHELIGHTBULBWILLBEWILLHAVEWILLBEEVENMORE

2024-10-27 18:06:42,233 (asr_inference:509) INFO: speech length: 49680
2024-10-27 18:06:44,146 (beam_search:428) INFO: decoder input length: 38
2024-10-27 18:06:44,146 (beam_search:429) INFO: max output length: 38
2024-10-27 18:06:44,146 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:44,203 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:44,204 (beam_search:476) INFO:  -1.91 * 1.0 =  -1.91 for ctc
2024-10-27 18:06:44,204 (beam_search:479) INFO: total log probability: -1.91
2024-10-27 18:06:44,204 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:06:44,204 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:44,204 (beam_search:483) INFO: best hypo: ITWILLBEEVENBRIGHTERTHANWITHONEBATTERY

2024-10-27 18:06:44,207 (asr_inference:509) INFO: speech length: 449008
2024-10-27 18:07:07,192 (beam_search:428) INFO: decoder input length: 350
2024-10-27 18:07:07,192 (beam_search:429) INFO: max output length: 350
2024-10-27 18:07:07,192 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:10,757 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:10,757 (beam_search:476) INFO:  -6.32 * 1.0 =  -6.32 for ctc
2024-10-27 18:07:10,757 (beam_search:479) INFO: total log probability: -6.32
2024-10-27 18:07:10,757 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:07:10,757 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:10,757 (beam_search:483) INFO: best hypo: WELLYOUJUSTAWIREINWELLYOUNEEDYOUNEEDATOWELLYOUTOHAVEAPLACETOTHEYOUNEEDTOHAVEABATTERYANDTHENYOUPUTTHEBATTERYINANDTHENYOUGETTHEOTHERWIRESANDTHENYOUTHEWIRESTOTOTHEBATTERY

2024-10-27 18:07:10,761 (asr_inference:509) INFO: speech length: 90368
2024-10-27 18:07:13,980 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:07:13,980 (beam_search:429) INFO: max output length: 70
2024-10-27 18:07:13,980 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:14,093 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:14,094 (beam_search:476) INFO:  -1.19 * 1.0 =  -1.19 for ctc
2024-10-27 18:07:14,094 (beam_search:479) INFO: total log probability: -1.19
2024-10-27 18:07:14,094 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:07:14,094 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:14,094 (beam_search:483) INFO: best hypo: THEBATTERYISBEINGUSEDTOLIGHTTOLIGHT

2024-10-27 18:07:14,096 (asr_inference:509) INFO: speech length: 171504
2024-10-27 18:07:20,822 (beam_search:428) INFO: decoder input length: 133
2024-10-27 18:07:20,822 (beam_search:429) INFO: max output length: 133
2024-10-27 18:07:20,822 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:21,181 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:21,181 (beam_search:476) INFO:  -1.64 * 1.0 =  -1.64 for ctc
2024-10-27 18:07:21,181 (beam_search:479) INFO: total log probability: -1.64
2024-10-27 18:07:21,181 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:07:21,181 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:21,181 (beam_search:483) INFO: best hypo: INTHISTHEREARETWOTHATAREBEINGUSEDTOUMLIGHTTWOLIGHTBULBS

2024-10-27 18:07:21,184 (asr_inference:509) INFO: speech length: 156192
2024-10-27 18:07:27,100 (beam_search:428) INFO: decoder input length: 121
2024-10-27 18:07:27,100 (beam_search:429) INFO: max output length: 121
2024-10-27 18:07:27,100 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:27,630 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:27,631 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 18:07:27,631 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 18:07:27,631 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:07:27,631 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:27,631 (beam_search:483) INFO: best hypo: YOUONTHEBATTERYANDTHELIGHTBULBSDON'TLIGHTTHAT'SBECAUSETHEBATTERYISININTHEWRONGPLACE

2024-10-27 18:07:27,633 (asr_inference:509) INFO: speech length: 78240
2024-10-27 18:07:30,493 (beam_search:428) INFO: decoder input length: 60
2024-10-27 18:07:30,493 (beam_search:429) INFO: max output length: 60
2024-10-27 18:07:30,493 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:30,547 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:30,547 (beam_search:476) INFO:  -1.79 * 1.0 =  -1.79 for ctc
2024-10-27 18:07:30,547 (beam_search:479) INFO: total log probability: -1.79
2024-10-27 18:07:30,547 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:07:30,547 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:30,547 (beam_search:483) INFO: best hypo: WELLTHEWILLBEIN

2024-10-27 18:07:30,550 (asr_inference:509) INFO: speech length: 100560
2024-10-27 18:07:34,156 (beam_search:428) INFO: decoder input length: 78
2024-10-27 18:07:34,157 (beam_search:429) INFO: max output length: 78
2024-10-27 18:07:34,157 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:34,325 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:34,325 (beam_search:476) INFO:  -2.44 * 1.0 =  -2.44 for ctc
2024-10-27 18:07:34,325 (beam_search:479) INFO: total log probability: -2.44
2024-10-27 18:07:34,325 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:07:34,325 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:34,325 (beam_search:483) INFO: best hypo: INTHEWRONGINTHEWRONGPLACEBECAUSEONESIDESHOULDBE

2024-10-27 18:07:34,328 (asr_inference:509) INFO: speech length: 113248
2024-10-27 18:07:38,528 (beam_search:428) INFO: decoder input length: 87
2024-10-27 18:07:38,528 (beam_search:429) INFO: max output length: 87
2024-10-27 18:07:38,528 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:38,663 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:38,663 (beam_search:476) INFO:  -1.48 * 1.0 =  -1.48 for ctc
2024-10-27 18:07:38,663 (beam_search:479) INFO: total log probability: -1.48
2024-10-27 18:07:38,663 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:07:38,663 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:38,663 (beam_search:483) INFO: best hypo: ANDTHEOTHERSIDEBEONTHEPOSITIVESIDE

2024-10-27 18:07:38,665 (asr_inference:509) INFO: speech length: 141136
2024-10-27 18:07:44,023 (beam_search:428) INFO: decoder input length: 109
2024-10-27 18:07:44,024 (beam_search:429) INFO: max output length: 109
2024-10-27 18:07:44,024 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:44,212 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:44,212 (beam_search:476) INFO:  -2.47 * 1.0 =  -2.47 for ctc
2024-10-27 18:07:44,212 (beam_search:479) INFO: total log probability: -2.47
2024-10-27 18:07:44,212 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:07:44,212 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:44,212 (beam_search:483) INFO: best hypo: THEYHAVETOBEINTHERIGHTFORITTO

2024-10-27 18:07:44,215 (asr_inference:509) INFO: speech length: 93376
2024-10-27 18:07:47,533 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:07:47,534 (beam_search:429) INFO: max output length: 72
2024-10-27 18:07:47,534 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:47,612 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:47,612 (beam_search:476) INFO:  -1.20 * 1.0 =  -1.20 for ctc
2024-10-27 18:07:47,612 (beam_search:479) INFO: total log probability: -1.20
2024-10-27 18:07:47,612 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:07:47,612 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:47,613 (beam_search:483) INFO: best hypo: UMITWOULDHAVENODIFFERENCE

2024-10-27 18:07:47,615 (asr_inference:509) INFO: speech length: 288512
2024-10-27 18:08:00,279 (beam_search:428) INFO: decoder input length: 224
2024-10-27 18:08:00,280 (beam_search:429) INFO: max output length: 224
2024-10-27 18:08:00,280 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:01,614 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:01,615 (beam_search:476) INFO:  -6.14 * 1.0 =  -6.14 for ctc
2024-10-27 18:08:01,615 (beam_search:479) INFO: total log probability: -6.14
2024-10-27 18:08:01,615 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:08:01,615 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:01,615 (beam_search:483) INFO: best hypo: INTHIS'SANOTHERCIRCUITANDALLTHEENERGYISSTILLRUNNINGOUTOFTHENEGATIVESIDEOFTHEBATTERYANDTHELIGHTSAREBRIGHTERBECAUSETHEY'RETWO

2024-10-27 18:08:01,617 (asr_inference:509) INFO: speech length: 229520
2024-10-27 18:08:10,937 (beam_search:428) INFO: decoder input length: 178
2024-10-27 18:08:10,937 (beam_search:429) INFO: max output length: 178
2024-10-27 18:08:10,937 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:11,745 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:11,745 (beam_search:476) INFO:  -3.04 * 1.0 =  -3.04 for ctc
2024-10-27 18:08:11,745 (beam_search:479) INFO: total log probability: -3.04
2024-10-27 18:08:11,745 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:08:11,745 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:11,745 (beam_search:483) INFO: best hypo: UMIFTHEPLUSSIDEISIMEANTHENEGATIVESIDEISTOUCHINGTHEOTHERNEGATIVESIDETHENUMTHENITWON'TBECAUSE

2024-10-27 18:08:11,748 (asr_inference:509) INFO: speech length: 193744
2024-10-27 18:08:19,399 (beam_search:428) INFO: decoder input length: 150
2024-10-27 18:08:19,399 (beam_search:429) INFO: max output length: 150
2024-10-27 18:08:19,399 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:19,845 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:19,845 (beam_search:476) INFO:  -3.26 * 1.0 =  -3.26 for ctc
2024-10-27 18:08:19,845 (beam_search:479) INFO: total log probability: -3.26
2024-10-27 18:08:19,845 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:08:19,845 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:19,845 (beam_search:483) INFO: best hypo: ITBECAUSETHECAUSETHEPOSITIVESIDEHASTOBEALWAYSHASTOBEINGENERGY

2024-10-27 18:08:19,848 (asr_inference:509) INFO: speech length: 14832
2024-10-27 18:08:20,629 (beam_search:428) INFO: decoder input length: 11
2024-10-27 18:08:20,629 (beam_search:429) INFO: max output length: 11
2024-10-27 18:08:20,631 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:20,641 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:20,641 (beam_search:476) INFO:  -0.41 * 1.0 =  -0.41 for ctc
2024-10-27 18:08:20,641 (beam_search:479) INFO: total log probability: -0.41
2024-10-27 18:08:20,641 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:08:20,641 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:20,641 (beam_search:483) INFO: best hypo: THAT'S

2024-10-27 18:08:20,643 (asr_inference:509) INFO: speech length: 19920
2024-10-27 18:08:21,586 (beam_search:428) INFO: decoder input length: 15
2024-10-27 18:08:21,586 (beam_search:429) INFO: max output length: 15
2024-10-27 18:08:21,586 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:21,600 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:21,600 (beam_search:476) INFO:  -0.24 * 1.0 =  -0.24 for ctc
2024-10-27 18:08:21,600 (beam_search:479) INFO: total log probability: -0.24
2024-10-27 18:08:21,601 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 18:08:21,601 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:21,601 (beam_search:483) INFO: best hypo: YEAHTHAT'S

2024-10-27 18:08:21,603 (asr_inference:509) INFO: speech length: 161792
2024-10-27 18:08:27,835 (beam_search:428) INFO: decoder input length: 125
2024-10-27 18:08:27,835 (beam_search:429) INFO: max output length: 125
2024-10-27 18:08:27,835 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:28,253 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:28,253 (beam_search:476) INFO:  -1.26 * 1.0 =  -1.26 for ctc
2024-10-27 18:08:28,253 (beam_search:479) INFO: total log probability: -1.26
2024-10-27 18:08:28,253 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:08:28,253 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:28,253 (beam_search:483) INFO: best hypo: UMNOITN'TSHOWTHEENERGYGOINGTOTHELIGHTBUTUMITOSAYTHATTHE

2024-10-27 18:08:28,255 (asr_inference:509) INFO: speech length: 43392
2024-10-27 18:08:30,060 (beam_search:428) INFO: decoder input length: 33
2024-10-27 18:08:30,061 (beam_search:429) INFO: max output length: 33
2024-10-27 18:08:30,061 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:30,088 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:30,088 (beam_search:476) INFO:  -1.86 * 1.0 =  -1.86 for ctc
2024-10-27 18:08:30,088 (beam_search:479) INFO: total log probability: -1.86
2024-10-27 18:08:30,088 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 18:08:30,088 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:30,088 (beam_search:483) INFO: best hypo: YESTHISPROBABLYWOULD

2024-10-27 18:08:30,091 (asr_inference:509) INFO: speech length: 58720
2024-10-27 18:08:32,395 (beam_search:428) INFO: decoder input length: 45
2024-10-27 18:08:32,395 (beam_search:429) INFO: max output length: 45
2024-10-27 18:08:32,395 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:32,422 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:32,422 (beam_search:476) INFO:  -1.12 * 1.0 =  -1.12 for ctc
2024-10-27 18:08:32,422 (beam_search:479) INFO: total log probability: -1.12
2024-10-27 18:08:32,422 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:08:32,422 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:32,422 (beam_search:483) INFO: best hypo: THISWOULDWORK

2024-10-27 18:08:32,424 (asr_inference:509) INFO: speech length: 10448
2024-10-27 18:08:33,055 (beam_search:428) INFO: decoder input length: 7
2024-10-27 18:08:33,055 (beam_search:429) INFO: max output length: 7
2024-10-27 18:08:33,055 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:33,063 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:33,063 (beam_search:476) INFO:  -0.93 * 1.0 =  -0.93 for ctc
2024-10-27 18:08:33,063 (beam_search:479) INFO: total log probability: -0.93
2024-10-27 18:08:33,063 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:08:33,063 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:33,063 (beam_search:483) INFO: best hypo: YOU'RE

2024-10-27 18:08:33,065 (asr_inference:509) INFO: speech length: 195072
2024-10-27 18:08:40,949 (beam_search:428) INFO: decoder input length: 151
2024-10-27 18:08:40,949 (beam_search:429) INFO: max output length: 151
2024-10-27 18:08:40,949 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:41,616 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:41,617 (beam_search:476) INFO:  -4.00 * 1.0 =  -4.00 for ctc
2024-10-27 18:08:41,617 (beam_search:479) INFO: total log probability: -4.00
2024-10-27 18:08:41,617 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:08:41,617 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:41,617 (beam_search:483) INFO: best hypo: UMTHATISTWOUMUMTWOMAGNETSIFYOUHAVEAPOLEANDATHENTHISMEANSTHATTHETWOMAGNETSWILLTOEACH

2024-10-27 18:08:41,620 (asr_inference:509) INFO: speech length: 200304
2024-10-27 18:08:49,362 (beam_search:428) INFO: decoder input length: 155
2024-10-27 18:08:49,362 (beam_search:429) INFO: max output length: 155
2024-10-27 18:08:49,362 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:50,065 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:50,065 (beam_search:476) INFO:  -5.85 * 1.0 =  -5.85 for ctc
2024-10-27 18:08:50,065 (beam_search:479) INFO: total log probability: -5.85
2024-10-27 18:08:50,065 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:08:50,065 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:50,065 (beam_search:483) INFO: best hypo: THEMAGNETSONTHERIGHTSIDEOFTHISAREWILLSTICKTOEACHOTHERORTOEACHOTHERBECAUSETHENORTHANDAREEACHOTHER

2024-10-27 18:08:50,068 (asr_inference:509) INFO: speech length: 178864
2024-10-27 18:08:57,195 (beam_search:428) INFO: decoder input length: 139
2024-10-27 18:08:57,195 (beam_search:429) INFO: max output length: 139
2024-10-27 18:08:57,195 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:57,841 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:57,842 (beam_search:476) INFO:  -5.38 * 1.0 =  -5.38 for ctc
2024-10-27 18:08:57,842 (beam_search:479) INFO: total log probability: -5.38
2024-10-27 18:08:57,842 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:08:57,842 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:57,842 (beam_search:483) INFO: best hypo: WELLTHETWOANDSOUTHWEREEACHOTHERSOTHATMEANSASITOLDYOUTHATANDSOUTHTOEACHOTHERORSOITWILLATTRACT

2024-10-27 18:08:57,844 (asr_inference:509) INFO: speech length: 172560
2024-10-27 18:09:04,724 (beam_search:428) INFO: decoder input length: 134
2024-10-27 18:09:04,724 (beam_search:429) INFO: max output length: 134
2024-10-27 18:09:04,724 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:05,361 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:05,361 (beam_search:476) INFO:  -2.58 * 1.0 =  -2.58 for ctc
2024-10-27 18:09:05,361 (beam_search:479) INFO: total log probability: -2.58
2024-10-27 18:09:05,362 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:09:05,362 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:05,362 (beam_search:483) INFO: best hypo: WELLINTHETHEREALOTOFTHEREAREALOTOFOBJECTSANDTHEMAGNETWILLONLYSTICKTOAOFTHETOAOFTHE

2024-10-27 18:09:05,364 (asr_inference:509) INFO: speech length: 13696
2024-10-27 18:09:06,082 (beam_search:428) INFO: decoder input length: 10
2024-10-27 18:09:06,082 (beam_search:429) INFO: max output length: 10
2024-10-27 18:09:06,082 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:06,088 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:06,088 (beam_search:476) INFO:  -1.65 * 1.0 =  -1.65 for ctc
2024-10-27 18:09:06,088 (beam_search:479) INFO: total log probability: -1.65
2024-10-27 18:09:06,088 (beam_search:480) INFO: normalized log probability: -0.55
2024-10-27 18:09:06,088 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:06,088 (beam_search:483) INFO: best hypo: AND

2024-10-27 18:09:06,091 (asr_inference:509) INFO: speech length: 196160
2024-10-27 18:09:13,650 (beam_search:428) INFO: decoder input length: 152
2024-10-27 18:09:13,650 (beam_search:429) INFO: max output length: 152
2024-10-27 18:09:13,650 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:14,340 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:14,340 (beam_search:476) INFO:  -4.17 * 1.0 =  -4.17 for ctc
2024-10-27 18:09:14,340 (beam_search:479) INFO: total log probability: -4.17
2024-10-27 18:09:14,340 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:09:14,340 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:14,340 (beam_search:483) INFO: best hypo: WELLTHEMAITHINKTHATTHEMAGNETWILLSTICKTOTHEWIRESCREENTHETHENAILTHEPAPERTHEWASHERANDTHAT'SALL

2024-10-27 18:09:14,343 (asr_inference:509) INFO: speech length: 146272
2024-10-27 18:09:19,932 (beam_search:428) INFO: decoder input length: 113
2024-10-27 18:09:19,932 (beam_search:429) INFO: max output length: 113
2024-10-27 18:09:19,932 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:20,261 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:20,261 (beam_search:476) INFO:  -1.58 * 1.0 =  -1.58 for ctc
2024-10-27 18:09:20,261 (beam_search:479) INFO: total log probability: -1.58
2024-10-27 18:09:20,261 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:09:20,261 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:20,261 (beam_search:483) INFO: best hypo: WELLTHESTEELWIRESCREENISTOTHEMAGNETANDTHAT'SWHAT'S

2024-10-27 18:09:20,263 (asr_inference:509) INFO: speech length: 22656
2024-10-27 18:09:21,313 (beam_search:428) INFO: decoder input length: 17
2024-10-27 18:09:21,313 (beam_search:429) INFO: max output length: 17
2024-10-27 18:09:21,313 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:21,325 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:21,325 (beam_search:476) INFO:  -0.47 * 1.0 =  -0.47 for ctc
2024-10-27 18:09:21,325 (beam_search:479) INFO: total log probability: -0.47
2024-10-27 18:09:21,325 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:09:21,325 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:21,325 (beam_search:483) INFO: best hypo: THATIS

2024-10-27 18:09:21,327 (asr_inference:509) INFO: speech length: 91376
2024-10-27 18:09:24,659 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:09:24,659 (beam_search:429) INFO: max output length: 70
2024-10-27 18:09:24,659 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:24,767 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:24,767 (beam_search:476) INFO:  -2.50 * 1.0 =  -2.50 for ctc
2024-10-27 18:09:24,767 (beam_search:479) INFO: total log probability: -2.50
2024-10-27 18:09:24,767 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:09:24,767 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:24,768 (beam_search:483) INFO: best hypo: ITTOANYSTEELTOANYSTEELORMADE

2024-10-27 18:09:24,770 (asr_inference:509) INFO: speech length: 15040
2024-10-27 18:09:25,520 (beam_search:428) INFO: decoder input length: 11
2024-10-27 18:09:25,520 (beam_search:429) INFO: max output length: 11
2024-10-27 18:09:25,520 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:25,526 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:25,526 (beam_search:476) INFO:  -0.14 * 1.0 =  -0.14 for ctc
2024-10-27 18:09:25,526 (beam_search:479) INFO: total log probability: -0.14
2024-10-27 18:09:25,526 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:09:25,526 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:25,526 (beam_search:483) INFO: best hypo: IT

2024-10-27 18:09:25,528 (asr_inference:509) INFO: speech length: 6864
2024-10-27 18:09:26,067 (beam_search:428) INFO: decoder input length: 4
2024-10-27 18:09:26,067 (beam_search:429) INFO: max output length: 4
2024-10-27 18:09:26,067 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:26,071 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:26,071 (beam_search:476) INFO:  -0.82 * 1.0 =  -0.82 for ctc
2024-10-27 18:09:26,071 (beam_search:479) INFO: total log probability: -0.82
2024-10-27 18:09:26,072 (beam_search:480) INFO: normalized log probability: -0.41
2024-10-27 18:09:26,072 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:26,072 (beam_search:483) INFO: best hypo: 

2024-10-27 18:09:26,074 (asr_inference:509) INFO: speech length: 319792
2024-10-27 18:09:40,388 (beam_search:428) INFO: decoder input length: 249
2024-10-27 18:09:40,389 (beam_search:429) INFO: max output length: 249
2024-10-27 18:09:40,389 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:41,892 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:41,892 (beam_search:476) INFO:  -6.91 * 1.0 =  -6.91 for ctc
2024-10-27 18:09:41,892 (beam_search:479) INFO: total log probability: -6.91
2024-10-27 18:09:41,892 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:09:41,892 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:41,892 (beam_search:483) INFO: best hypo: UMWELLBEOTHERTHINGSINTHEPICTUREAREASPONGEAAAPAPERUMTWOASTICKTWONAILSUMAOFASTRAWASPONGEANDTHAT'S

2024-10-27 18:09:41,895 (asr_inference:509) INFO: speech length: 46912
2024-10-27 18:09:43,705 (beam_search:428) INFO: decoder input length: 36
2024-10-27 18:09:43,705 (beam_search:429) INFO: max output length: 36
2024-10-27 18:09:43,705 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:43,768 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:43,768 (beam_search:476) INFO:  -1.82 * 1.0 =  -1.82 for ctc
2024-10-27 18:09:43,768 (beam_search:479) INFO: total log probability: -1.82
2024-10-27 18:09:43,768 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:09:43,768 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:43,768 (beam_search:483) INFO: best hypo: NOBECAUSETHESPONGEISNOTMADEOFSTEELORIRON

2024-10-27 18:09:43,770 (asr_inference:509) INFO: speech length: 156512
2024-10-27 18:09:49,712 (beam_search:428) INFO: decoder input length: 121
2024-10-27 18:09:49,712 (beam_search:429) INFO: max output length: 121
2024-10-27 18:09:49,712 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:50,051 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:50,051 (beam_search:476) INFO:  -1.79 * 1.0 =  -1.79 for ctc
2024-10-27 18:09:50,051 (beam_search:479) INFO: total log probability: -1.79
2024-10-27 18:09:50,051 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:09:50,051 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:50,051 (beam_search:483) INFO: best hypo: UMSOTHESOTHEMAGNETISTOANAILANDTHENAILISTOA

2024-10-27 18:09:50,054 (asr_inference:509) INFO: speech length: 57344
2024-10-27 18:09:52,230 (beam_search:428) INFO: decoder input length: 44
2024-10-27 18:09:52,230 (beam_search:429) INFO: max output length: 44
2024-10-27 18:09:52,230 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:52,290 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:52,290 (beam_search:476) INFO:  -1.38 * 1.0 =  -1.38 for ctc
2024-10-27 18:09:52,290 (beam_search:479) INFO: total log probability: -1.38
2024-10-27 18:09:52,290 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:09:52,290 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:52,290 (beam_search:483) INFO: best hypo: UMIFORGOTTHEOFWHAT'S

2024-10-27 18:09:52,292 (asr_inference:509) INFO: speech length: 167024
2024-10-27 18:09:58,703 (beam_search:428) INFO: decoder input length: 129
2024-10-27 18:09:58,704 (beam_search:429) INFO: max output length: 129
2024-10-27 18:09:58,704 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:59,024 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:59,025 (beam_search:476) INFO:  -1.83 * 1.0 =  -1.83 for ctc
2024-10-27 18:09:59,025 (beam_search:479) INFO: total log probability: -1.83
2024-10-27 18:09:59,025 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:09:59,025 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:59,025 (beam_search:483) INFO: best hypo: UMTHENAILTHEMAGNETANDTHEANDTHEPAPERCLIPAREFORCEOF

2024-10-27 18:09:59,027 (asr_inference:509) INFO: speech length: 110000
2024-10-27 18:10:03,023 (beam_search:428) INFO: decoder input length: 85
2024-10-27 18:10:03,023 (beam_search:429) INFO: max output length: 85
2024-10-27 18:10:03,023 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:03,262 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:03,262 (beam_search:476) INFO:  -1.91 * 1.0 =  -1.91 for ctc
2024-10-27 18:10:03,262 (beam_search:479) INFO: total log probability: -1.91
2024-10-27 18:10:03,262 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:10:03,262 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:03,262 (beam_search:483) INFO: best hypo: UMYESITISBECAUSEIBELIEVETHATAPAPERISMADEOFIRONORSTEEL

2024-10-27 18:10:03,265 (asr_inference:509) INFO: speech length: 47280
2024-10-27 18:10:05,076 (beam_search:428) INFO: decoder input length: 36
2024-10-27 18:10:05,076 (beam_search:429) INFO: max output length: 36
2024-10-27 18:10:05,076 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:05,133 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:05,133 (beam_search:476) INFO:  -0.82 * 1.0 =  -0.82 for ctc
2024-10-27 18:10:05,133 (beam_search:479) INFO: total log probability: -0.82
2024-10-27 18:10:05,133 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:10:05,133 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:05,133 (beam_search:483) INFO: best hypo: ITISBECAUSEITISMADEOFIRONORSTEEL

2024-10-27 18:10:05,135 (asr_inference:509) INFO: speech length: 410800
2024-10-27 18:10:26,754 (beam_search:428) INFO: decoder input length: 320
2024-10-27 18:10:26,754 (beam_search:429) INFO: max output length: 320
2024-10-27 18:10:26,754 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:29,010 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:29,011 (beam_search:476) INFO:  -6.47 * 1.0 =  -6.47 for ctc
2024-10-27 18:10:29,011 (beam_search:479) INFO: total log probability: -6.47
2024-10-27 18:10:29,011 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:10:29,011 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:29,011 (beam_search:483) INFO: best hypo: ASYOUCANSEETHETHEFORCEOFTHEOFISUMISINTHISPICTUREBECAUSEUMTHEFORCEOFOFTWOISGOINGTHROUGHTHEUMTHETHINPIECEOFPAPERORTHEOBJECT

2024-10-27 18:10:29,014 (asr_inference:509) INFO: speech length: 110576
2024-10-27 18:10:33,030 (beam_search:428) INFO: decoder input length: 85
2024-10-27 18:10:33,031 (beam_search:429) INFO: max output length: 85
2024-10-27 18:10:33,031 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:33,201 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:33,201 (beam_search:476) INFO:  -1.86 * 1.0 =  -1.86 for ctc
2024-10-27 18:10:33,201 (beam_search:479) INFO: total log probability: -1.86
2024-10-27 18:10:33,201 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:10:33,201 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:33,201 (beam_search:483) INFO: best hypo: WELLAMAGNETISMADEOFANYTHINGISMADEOFORSTEEL

2024-10-27 18:10:33,204 (asr_inference:509) INFO: speech length: 167264
2024-10-27 18:10:39,746 (beam_search:428) INFO: decoder input length: 130
2024-10-27 18:10:39,746 (beam_search:429) INFO: max output length: 130
2024-10-27 18:10:39,746 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:40,272 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:40,272 (beam_search:476) INFO:  -3.93 * 1.0 =  -3.93 for ctc
2024-10-27 18:10:40,272 (beam_search:479) INFO: total log probability: -3.93
2024-10-27 18:10:40,272 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:10:40,272 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:40,272 (beam_search:483) INFO: best hypo: ANDITIMEANAMAGNETONLYTOAMAGNETONLYSTICKSTOUMANYTHINGTHAT'SMADEOFIRONORSTEEL

2024-10-27 18:10:40,275 (asr_inference:509) INFO: speech length: 191232
2024-10-27 18:10:48,029 (beam_search:428) INFO: decoder input length: 148
2024-10-27 18:10:48,029 (beam_search:429) INFO: max output length: 148
2024-10-27 18:10:48,029 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:48,519 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:48,519 (beam_search:476) INFO:  -2.90 * 1.0 =  -2.90 for ctc
2024-10-27 18:10:48,519 (beam_search:479) INFO: total log probability: -2.90
2024-10-27 18:10:48,519 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:10:48,519 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:48,519 (beam_search:483) INFO: best hypo: WELLUMITCANSTICKTOSTEELANDBUTSOMEITCANNOTSTICKTOSOITIS

2024-10-27 18:10:48,522 (asr_inference:509) INFO: speech length: 15424
2024-10-27 18:10:49,406 (beam_search:428) INFO: decoder input length: 11
2024-10-27 18:10:49,406 (beam_search:429) INFO: max output length: 11
2024-10-27 18:10:49,406 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:49,414 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:49,414 (beam_search:476) INFO:  -0.18 * 1.0 =  -0.18 for ctc
2024-10-27 18:10:49,414 (beam_search:479) INFO: total log probability: -0.18
2024-10-27 18:10:49,415 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 18:10:49,415 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:49,415 (beam_search:483) INFO: best hypo: THEYWOULD

2024-10-27 18:10:49,417 (asr_inference:509) INFO: speech length: 107152
2024-10-27 18:10:53,310 (beam_search:428) INFO: decoder input length: 83
2024-10-27 18:10:53,310 (beam_search:429) INFO: max output length: 83
2024-10-27 18:10:53,310 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:53,444 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:53,444 (beam_search:476) INFO:  -1.70 * 1.0 =  -1.70 for ctc
2024-10-27 18:10:53,444 (beam_search:479) INFO: total log probability: -1.70
2024-10-27 18:10:53,444 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:10:53,444 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:53,444 (beam_search:483) INFO: best hypo: NONOTEXACTLYBUTWEDIDTOTHIS

2024-10-27 18:10:53,446 (asr_inference:509) INFO: speech length: 249120
2024-10-27 18:11:03,700 (beam_search:428) INFO: decoder input length: 194
2024-10-27 18:11:03,701 (beam_search:429) INFO: max output length: 194
2024-10-27 18:11:03,701 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:04,363 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:04,363 (beam_search:476) INFO:  -3.86 * 1.0 =  -3.86 for ctc
2024-10-27 18:11:04,363 (beam_search:479) INFO: total log probability: -3.86
2024-10-27 18:11:04,363 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:11:04,363 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:04,364 (beam_search:483) INFO: best hypo: THEMAGNETISTHESETWOMAGNETSHAVEAOFANDITHASTOBEOFFORTHETWO

2024-10-27 18:11:04,367 (asr_inference:509) INFO: speech length: 17984
2024-10-27 18:11:05,337 (beam_search:428) INFO: decoder input length: 13
2024-10-27 18:11:05,337 (beam_search:429) INFO: max output length: 13
2024-10-27 18:11:05,337 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:05,346 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:05,346 (beam_search:476) INFO:  -0.06 * 1.0 =  -0.06 for ctc
2024-10-27 18:11:05,346 (beam_search:479) INFO: total log probability: -0.06
2024-10-27 18:11:05,346 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 18:11:05,346 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:05,346 (beam_search:483) INFO: best hypo: THEYARE

2024-10-27 18:11:05,349 (asr_inference:509) INFO: speech length: 85056
2024-10-27 18:11:08,512 (beam_search:428) INFO: decoder input length: 65
2024-10-27 18:11:08,512 (beam_search:429) INFO: max output length: 65
2024-10-27 18:11:08,512 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:08,588 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:08,588 (beam_search:476) INFO:  -1.37 * 1.0 =  -1.37 for ctc
2024-10-27 18:11:08,588 (beam_search:479) INFO: total log probability: -1.37
2024-10-27 18:11:08,588 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:11:08,588 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:08,588 (beam_search:483) INFO: best hypo: UMTHEMAGNETSBOTHHAVEAOF

2024-10-27 18:11:08,590 (asr_inference:509) INFO: speech length: 21472
2024-10-27 18:11:09,531 (beam_search:428) INFO: decoder input length: 16
2024-10-27 18:11:09,531 (beam_search:429) INFO: max output length: 16
2024-10-27 18:11:09,531 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:09,540 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:09,540 (beam_search:476) INFO:  -0.71 * 1.0 =  -0.71 for ctc
2024-10-27 18:11:09,540 (beam_search:479) INFO: total log probability: -0.71
2024-10-27 18:11:09,540 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:11:09,540 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:09,541 (beam_search:483) INFO: best hypo: OKAYYOU

2024-10-27 18:11:09,542 (asr_inference:509) INFO: speech length: 56784
2024-10-27 18:11:11,768 (beam_search:428) INFO: decoder input length: 43
2024-10-27 18:11:11,768 (beam_search:429) INFO: max output length: 43
2024-10-27 18:11:11,768 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:11,806 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:11,806 (beam_search:476) INFO:  -1.10 * 1.0 =  -1.10 for ctc
2024-10-27 18:11:11,806 (beam_search:479) INFO: total log probability: -1.10
2024-10-27 18:11:11,806 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:11:11,806 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:11,806 (beam_search:483) INFO: best hypo: WEHAVEBEENONMAGNETISM

2024-10-27 18:11:11,809 (asr_inference:509) INFO: speech length: 291664
2024-10-27 18:11:24,218 (beam_search:428) INFO: decoder input length: 227
2024-10-27 18:11:24,218 (beam_search:429) INFO: max output length: 227
2024-10-27 18:11:24,218 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:25,229 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:25,230 (beam_search:476) INFO:  -4.69 * 1.0 =  -4.69 for ctc
2024-10-27 18:11:25,230 (beam_search:479) INFO: total log probability: -4.69
2024-10-27 18:11:25,230 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:11:25,230 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:25,230 (beam_search:483) INFO: best hypo: WEAREABOUTTHEOFANDOFANDTHEOFISTWOTWOMAGNETSUMSTICKTOGETHERANDTHEOFISTWOMAGNETSFROM

2024-10-27 18:11:25,233 (asr_inference:509) INFO: speech length: 167824
2024-10-27 18:11:31,706 (beam_search:428) INFO: decoder input length: 130
2024-10-27 18:11:31,706 (beam_search:429) INFO: max output length: 130
2024-10-27 18:11:31,706 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:32,037 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:32,037 (beam_search:476) INFO:  -2.56 * 1.0 =  -2.56 for ctc
2024-10-27 18:11:32,037 (beam_search:479) INFO: total log probability: -2.56
2024-10-27 18:11:32,037 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:11:32,037 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:32,037 (beam_search:483) INFO: best hypo: UMTHEISWHERETHETHETWOANDPOLESATTRACTTOEACHOTHER

2024-10-27 18:11:32,040 (asr_inference:509) INFO: speech length: 278848
2024-10-27 18:11:43,860 (beam_search:428) INFO: decoder input length: 217
2024-10-27 18:11:43,860 (beam_search:429) INFO: max output length: 217
2024-10-27 18:11:43,860 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:45,124 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:45,125 (beam_search:476) INFO:  -6.55 * 1.0 =  -6.55 for ctc
2024-10-27 18:11:45,125 (beam_search:479) INFO: total log probability: -6.55
2024-10-27 18:11:45,125 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:11:45,125 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:45,125 (beam_search:483) INFO: best hypo: THEREARETHINGSONAMAGNETCALLEDANDSOUTHANDWHENTHEREARETWOANDTHENTHEYTOEACHOTHERBUTWHENTHEREARETWOORTWOTHEYFROMEACHOTHER

2024-10-27 18:11:45,128 (asr_inference:509) INFO: speech length: 50736
2024-10-27 18:11:47,095 (beam_search:428) INFO: decoder input length: 39
2024-10-27 18:11:47,095 (beam_search:429) INFO: max output length: 39
2024-10-27 18:11:47,095 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:47,144 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:47,144 (beam_search:476) INFO:  -2.10 * 1.0 =  -2.10 for ctc
2024-10-27 18:11:47,144 (beam_search:479) INFO: total log probability: -2.10
2024-10-27 18:11:47,144 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:11:47,144 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:47,144 (beam_search:483) INFO: best hypo: UMNOWEHAVENOTONLIGHT

2024-10-27 18:11:47,146 (asr_inference:509) INFO: speech length: 21312
2024-10-27 18:11:48,096 (beam_search:428) INFO: decoder input length: 16
2024-10-27 18:11:48,096 (beam_search:429) INFO: max output length: 16
2024-10-27 18:11:48,096 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:48,110 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:48,110 (beam_search:476) INFO:  -0.11 * 1.0 =  -0.11 for ctc
2024-10-27 18:11:48,110 (beam_search:479) INFO: total log probability: -0.11
2024-10-27 18:11:48,111 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:11:48,111 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:48,111 (beam_search:483) INFO: best hypo: GOODHOWAREYOU

2024-10-27 18:11:48,114 (asr_inference:509) INFO: speech length: 261056
2024-10-27 18:11:58,905 (beam_search:428) INFO: decoder input length: 203
2024-10-27 18:11:58,905 (beam_search:429) INFO: max output length: 203
2024-10-27 18:11:58,905 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:59,795 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:59,795 (beam_search:476) INFO:  -4.29 * 1.0 =  -4.29 for ctc
2024-10-27 18:11:59,795 (beam_search:479) INFO: total log probability: -4.29
2024-10-27 18:11:59,795 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:11:59,795 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:59,795 (beam_search:483) INFO: best hypo: SOTODAYINWEHADTOWEHADTOTOUSEATOUPTOPICKUPUMTOPICKUPSOMELITTLE

2024-10-27 18:11:59,798 (asr_inference:509) INFO: speech length: 154496
2024-10-27 18:12:05,593 (beam_search:428) INFO: decoder input length: 120
2024-10-27 18:12:05,593 (beam_search:429) INFO: max output length: 120
2024-10-27 18:12:05,593 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:05,983 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:05,983 (beam_search:476) INFO:  -2.51 * 1.0 =  -2.51 for ctc
2024-10-27 18:12:05,983 (beam_search:479) INFO: total log probability: -2.51
2024-10-27 18:12:05,983 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:12:05,983 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:05,983 (beam_search:483) INFO: best hypo: UMYESTHEREWASTHEMORETIMESYOUTHEWIREAROUNDTHETHEMOREPOWERITWOULDHAVET

2024-10-27 18:12:05,986 (asr_inference:509) INFO: speech length: 26272
2024-10-27 18:12:07,123 (beam_search:428) INFO: decoder input length: 20
2024-10-27 18:12:07,123 (beam_search:429) INFO: max output length: 20
2024-10-27 18:12:07,123 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:07,132 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:07,132 (beam_search:476) INFO:  -0.66 * 1.0 =  -0.66 for ctc
2024-10-27 18:12:07,132 (beam_search:479) INFO: total log probability: -0.66
2024-10-27 18:12:07,132 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:12:07,132 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:07,132 (beam_search:483) INFO: best hypo: WASHERS

2024-10-27 18:12:07,135 (asr_inference:509) INFO: speech length: 275472
2024-10-27 18:12:18,656 (beam_search:428) INFO: decoder input length: 214
2024-10-27 18:12:18,657 (beam_search:429) INFO: max output length: 214
2024-10-27 18:12:18,657 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:20,181 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:20,182 (beam_search:476) INFO:  -4.22 * 1.0 =  -4.22 for ctc
2024-10-27 18:12:20,182 (beam_search:479) INFO: total log probability: -4.22
2024-10-27 18:12:20,182 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:12:20,182 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:20,182 (beam_search:483) INFO: best hypo: WELLIFYOUDWELLTHEMOREYOUDTHEWELLTHEMOREYOUDTHEWIREABOUTAROUNDTHERIVETTHENITWOULDTHENTHEREWOULDBESOMEMOREPOWEREACHTIMEYOUDITAROUND

2024-10-27 18:12:20,184 (asr_inference:509) INFO: speech length: 30384
2024-10-27 18:12:21,417 (beam_search:428) INFO: decoder input length: 23
2024-10-27 18:12:21,417 (beam_search:429) INFO: max output length: 23
2024-10-27 18:12:21,417 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:21,436 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:21,436 (beam_search:476) INFO:  -0.43 * 1.0 =  -0.43 for ctc
2024-10-27 18:12:21,436 (beam_search:479) INFO: total log probability: -0.43
2024-10-27 18:12:21,436 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:12:21,436 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:21,436 (beam_search:483) INFO: best hypo: YEAHTHAT'S

2024-10-27 18:12:21,438 (asr_inference:509) INFO: speech length: 322080
2024-10-27 18:12:35,812 (beam_search:428) INFO: decoder input length: 251
2024-10-27 18:12:35,812 (beam_search:429) INFO: max output length: 251
2024-10-27 18:12:35,812 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:36,824 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:36,824 (beam_search:476) INFO:  -6.25 * 1.0 =  -6.25 for ctc
2024-10-27 18:12:36,824 (beam_search:479) INFO: total log probability: -6.25
2024-10-27 18:12:36,824 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:12:36,824 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:36,825 (beam_search:483) INFO: best hypo: WELLWEWERELEARNINGABOUTANDELECTRICITYWERELEARNINGABOUTTHEMAGNETISMABOUTMAGNETISMANDWEABOUTMAGNETISMANDATTHETIME

2024-10-27 18:12:36,827 (asr_inference:509) INFO: speech length: 316960
2024-10-27 18:12:50,893 (beam_search:428) INFO: decoder input length: 247
2024-10-27 18:12:50,893 (beam_search:429) INFO: max output length: 247
2024-10-27 18:12:50,893 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:52,298 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:52,298 (beam_search:476) INFO:  -3.69 * 1.0 =  -3.69 for ctc
2024-10-27 18:12:52,298 (beam_search:479) INFO: total log probability: -3.69
2024-10-27 18:12:52,298 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:12:52,298 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:52,298 (beam_search:483) INFO: best hypo: WEUSEDELECTRICITYTOPOWERTHEWEUSEDTHEELECTRICITYTOPOWERTHESTEELRIVETANDANDWEALSOUSEDMAGNETISMTOMAKETHESTEELRIVETPICKUPTHEWASHERS

2024-10-27 18:12:52,300 (asr_inference:509) INFO: speech length: 103232
2024-10-27 18:12:56,041 (beam_search:428) INFO: decoder input length: 80
2024-10-27 18:12:56,041 (beam_search:429) INFO: max output length: 80
2024-10-27 18:12:56,041 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:56,180 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:56,180 (beam_search:476) INFO:  -3.00 * 1.0 =  -3.00 for ctc
2024-10-27 18:12:56,180 (beam_search:479) INFO: total log probability: -3.00
2024-10-27 18:12:56,180 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:12:56,180 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:56,180 (beam_search:483) INFO: best hypo: WITHOUTTHERIVETCANNOTBEUSEDAMAGNET

2024-10-27 18:12:56,182 (asr_inference:509) INFO: speech length: 41744
2024-10-27 18:12:57,847 (beam_search:428) INFO: decoder input length: 32
2024-10-27 18:12:57,847 (beam_search:429) INFO: max output length: 32
2024-10-27 18:12:57,847 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:57,881 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:57,882 (beam_search:476) INFO:  -1.41 * 1.0 =  -1.41 for ctc
2024-10-27 18:12:57,882 (beam_search:479) INFO: total log probability: -1.41
2024-10-27 18:12:57,882 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:12:57,882 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:57,882 (beam_search:483) INFO: best hypo: ANDTHEYBOTHWORKEDWITHOTHER

2024-10-27 18:12:57,884 (asr_inference:509) INFO: speech length: 462336
2024-10-27 18:13:22,096 (beam_search:428) INFO: decoder input length: 360
2024-10-27 18:13:22,096 (beam_search:429) INFO: max output length: 360
2024-10-27 18:13:22,096 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:24,945 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:24,945 (beam_search:476) INFO:  -6.79 * 1.0 =  -6.79 for ctc
2024-10-27 18:13:24,945 (beam_search:479) INFO: total log probability: -6.79
2024-10-27 18:13:24,945 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:13:24,945 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:24,946 (beam_search:483) INFO: best hypo: WELLTHESWITCHONANDWHENTHEISTURNEDONTHECANFLOWANDANDTHEWHENTHEWASONTHENTHETHESWITCHWASONTHENTHESTEELRIVETCOULDCOULDANDPICKUPSOMEOFTHEMETALWASHERS

2024-10-27 18:13:24,949 (asr_inference:509) INFO: speech length: 36928
2024-10-27 18:13:26,425 (beam_search:428) INFO: decoder input length: 28
2024-10-27 18:13:26,425 (beam_search:429) INFO: max output length: 28
2024-10-27 18:13:26,425 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:26,444 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:26,444 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 18:13:26,444 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 18:13:26,444 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:13:26,444 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:26,444 (beam_search:483) INFO: best hypo: THATISRIGHT

2024-10-27 18:13:26,447 (asr_inference:509) INFO: speech length: 209504
2024-10-27 18:13:34,942 (beam_search:428) INFO: decoder input length: 163
2024-10-27 18:13:34,942 (beam_search:429) INFO: max output length: 163
2024-10-27 18:13:34,942 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:35,556 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:35,556 (beam_search:476) INFO:  -3.76 * 1.0 =  -3.76 for ctc
2024-10-27 18:13:35,556 (beam_search:479) INFO: total log probability: -3.76
2024-10-27 18:13:35,556 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:13:35,556 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:35,556 (beam_search:483) INFO: best hypo: BECAUSEITTOTOAMAGNETANDPICKUPTHINGSANDITHINKTHATTHAT'SWHYYOUAN

2024-10-27 18:13:35,559 (asr_inference:509) INFO: speech length: 40272
2024-10-27 18:13:37,078 (beam_search:428) INFO: decoder input length: 30
2024-10-27 18:13:37,078 (beam_search:429) INFO: max output length: 30
2024-10-27 18:13:37,078 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:37,091 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:37,091 (beam_search:476) INFO:  -0.52 * 1.0 =  -0.52 for ctc
2024-10-27 18:13:37,091 (beam_search:479) INFO: total log probability: -0.52
2024-10-27 18:13:37,091 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:13:37,091 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:37,091 (beam_search:483) INFO: best hypo: IS

2024-10-27 18:13:37,093 (asr_inference:509) INFO: speech length: 15952
2024-10-27 18:13:37,884 (beam_search:428) INFO: decoder input length: 11
2024-10-27 18:13:37,884 (beam_search:429) INFO: max output length: 11
2024-10-27 18:13:37,884 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:37,894 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:37,894 (beam_search:476) INFO:  -0.57 * 1.0 =  -0.57 for ctc
2024-10-27 18:13:37,894 (beam_search:479) INFO: total log probability: -0.57
2024-10-27 18:13:37,894 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:13:37,894 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:37,894 (beam_search:483) INFO: best hypo: YOU'RE

2024-10-27 18:13:37,896 (asr_inference:509) INFO: speech length: 11104
2024-10-27 18:13:38,557 (beam_search:428) INFO: decoder input length: 8
2024-10-27 18:13:38,557 (beam_search:429) INFO: max output length: 8
2024-10-27 18:13:38,557 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:38,567 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:38,567 (beam_search:476) INFO:  -0.46 * 1.0 =  -0.46 for ctc
2024-10-27 18:13:38,567 (beam_search:479) INFO: total log probability: -0.46
2024-10-27 18:13:38,567 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:13:38,567 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:38,567 (beam_search:483) INFO: best hypo: I'MGOOD

2024-10-27 18:13:38,569 (asr_inference:509) INFO: speech length: 67600
2024-10-27 18:13:41,139 (beam_search:428) INFO: decoder input length: 52
2024-10-27 18:13:41,140 (beam_search:429) INFO: max output length: 52
2024-10-27 18:13:41,140 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:41,202 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:41,202 (beam_search:476) INFO:  -0.49 * 1.0 =  -0.49 for ctc
2024-10-27 18:13:41,202 (beam_search:479) INFO: total log probability: -0.49
2024-10-27 18:13:41,202 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:13:41,202 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:41,202 (beam_search:483) INFO: best hypo: UMWE'RETHATHOWMANY

2024-10-27 18:13:41,205 (asr_inference:509) INFO: speech length: 74800
2024-10-27 18:13:44,044 (beam_search:428) INFO: decoder input length: 57
2024-10-27 18:13:44,044 (beam_search:429) INFO: max output length: 57
2024-10-27 18:13:44,044 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:44,080 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:44,081 (beam_search:476) INFO:  -0.73 * 1.0 =  -0.73 for ctc
2024-10-27 18:13:44,081 (beam_search:479) INFO: total log probability: -0.73
2024-10-27 18:13:44,081 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:13:44,081 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:44,081 (beam_search:483) INFO: best hypo: ITTOSEE

2024-10-27 18:13:44,084 (asr_inference:509) INFO: speech length: 61040
2024-10-27 18:13:46,378 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:13:46,378 (beam_search:429) INFO: max output length: 47
2024-10-27 18:13:46,378 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:46,445 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:46,446 (beam_search:476) INFO:  -1.82 * 1.0 =  -1.82 for ctc
2024-10-27 18:13:46,446 (beam_search:479) INFO: total log probability: -1.82
2024-10-27 18:13:46,446 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:13:46,446 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:46,446 (beam_search:483) INFO: best hypo: WHENTHEMAGNETSDON'TSTICKTOGETHERANYMORE

2024-10-27 18:13:46,448 (asr_inference:509) INFO: speech length: 191760
2024-10-27 18:13:53,978 (beam_search:428) INFO: decoder input length: 149
2024-10-27 18:13:53,979 (beam_search:429) INFO: max output length: 149
2024-10-27 18:13:53,979 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:54,515 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:54,515 (beam_search:476) INFO:  -4.54 * 1.0 =  -4.54 for ctc
2024-10-27 18:13:54,515 (beam_search:479) INFO: total log probability: -4.54
2024-10-27 18:13:54,515 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:13:54,515 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:54,515 (beam_search:483) INFO: best hypo: ONEONEANDTHETWOUMANDMAGNETSANDTHISOTHERTHINGTHATWEDIDN'TUSEIN

2024-10-27 18:13:54,519 (asr_inference:509) INFO: speech length: 108240
2024-10-27 18:13:58,765 (beam_search:428) INFO: decoder input length: 84
2024-10-27 18:13:58,765 (beam_search:429) INFO: max output length: 84
2024-10-27 18:13:58,765 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:58,978 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:58,978 (beam_search:476) INFO:  -2.82 * 1.0 =  -2.82 for ctc
2024-10-27 18:13:58,978 (beam_search:479) INFO: total log probability: -2.82
2024-10-27 18:13:58,978 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:13:58,978 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:58,978 (beam_search:483) INFO: best hypo: THETWOMAGNETSTOGETHERANDONEISGOINGINTOAITSTHETWOMAGNETS

2024-10-27 18:13:58,980 (asr_inference:509) INFO: speech length: 85584
2024-10-27 18:14:02,140 (beam_search:428) INFO: decoder input length: 66
2024-10-27 18:14:02,140 (beam_search:429) INFO: max output length: 66
2024-10-27 18:14:02,140 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:02,238 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:02,238 (beam_search:476) INFO:  -1.95 * 1.0 =  -1.95 for ctc
2024-10-27 18:14:02,238 (beam_search:479) INFO: total log probability: -1.95
2024-10-27 18:14:02,239 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:14:02,239 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:02,239 (beam_search:483) INFO: best hypo: IT'SHOWMANYITTOTWOMAGNETS

2024-10-27 18:14:02,241 (asr_inference:509) INFO: speech length: 182080
2024-10-27 18:14:09,338 (beam_search:428) INFO: decoder input length: 141
2024-10-27 18:14:09,338 (beam_search:429) INFO: max output length: 141
2024-10-27 18:14:09,338 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:09,907 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:09,907 (beam_search:476) INFO:  -2.62 * 1.0 =  -2.62 for ctc
2024-10-27 18:14:09,907 (beam_search:479) INFO: total log probability: -2.62
2024-10-27 18:14:09,907 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:14:09,907 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:09,907 (beam_search:483) INFO: best hypo: IT'SINGMETHATTHE'SALOTOFONBONBCUPBANDTHATIT'S

2024-10-27 18:14:09,909 (asr_inference:509) INFO: speech length: 98992
2024-10-27 18:14:13,523 (beam_search:428) INFO: decoder input length: 76
2024-10-27 18:14:13,523 (beam_search:429) INFO: max output length: 76
2024-10-27 18:14:13,523 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:13,732 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:13,732 (beam_search:476) INFO:  -1.93 * 1.0 =  -1.93 for ctc
2024-10-27 18:14:13,732 (beam_search:479) INFO: total log probability: -1.93
2024-10-27 18:14:13,732 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:14:13,732 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:13,732 (beam_search:483) INFO: best hypo: THATIT'STAKINGTHATMANYWASHERSHOWMANYAREINTHECUPTOTHE

2024-10-27 18:14:13,735 (asr_inference:509) INFO: speech length: 85088
2024-10-27 18:14:16,840 (beam_search:428) INFO: decoder input length: 65
2024-10-27 18:14:16,840 (beam_search:429) INFO: max output length: 65
2024-10-27 18:14:16,840 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:16,939 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:16,939 (beam_search:476) INFO:  -2.03 * 1.0 =  -2.03 for ctc
2024-10-27 18:14:16,939 (beam_search:479) INFO: total log probability: -2.03
2024-10-27 18:14:16,939 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:14:16,939 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:16,939 (beam_search:483) INFO: best hypo: THEY'REBEINGFROMOTHERBYTHEWASHERS

2024-10-27 18:14:16,942 (asr_inference:509) INFO: speech length: 80992
2024-10-27 18:14:19,830 (beam_search:428) INFO: decoder input length: 62
2024-10-27 18:14:19,830 (beam_search:429) INFO: max output length: 62
2024-10-27 18:14:19,830 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:19,877 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:19,877 (beam_search:476) INFO:  -1.35 * 1.0 =  -1.35 for ctc
2024-10-27 18:14:19,878 (beam_search:479) INFO: total log probability: -1.35
2024-10-27 18:14:19,878 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:14:19,878 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:19,878 (beam_search:483) INFO: best hypo: THEYAREBEINGAND

2024-10-27 18:14:19,880 (asr_inference:509) INFO: speech length: 40336
2024-10-27 18:14:21,425 (beam_search:428) INFO: decoder input length: 31
2024-10-27 18:14:21,425 (beam_search:429) INFO: max output length: 31
2024-10-27 18:14:21,425 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:21,449 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:21,449 (beam_search:476) INFO:  -0.81 * 1.0 =  -0.81 for ctc
2024-10-27 18:14:21,449 (beam_search:479) INFO: total log probability: -0.81
2024-10-27 18:14:21,449 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:14:21,449 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:21,449 (beam_search:483) INFO: best hypo: WASTOTHEMAGNETS

2024-10-27 18:14:21,451 (asr_inference:509) INFO: speech length: 92720
2024-10-27 18:14:24,864 (beam_search:428) INFO: decoder input length: 71
2024-10-27 18:14:24,864 (beam_search:429) INFO: max output length: 71
2024-10-27 18:14:24,864 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:24,997 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:24,997 (beam_search:476) INFO:  -2.43 * 1.0 =  -2.43 for ctc
2024-10-27 18:14:24,997 (beam_search:479) INFO: total log probability: -2.43
2024-10-27 18:14:24,997 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:14:24,997 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:24,997 (beam_search:483) INFO: best hypo: WHENYOUTHEWASHERTHEONEWASHERSTHETWOMAGNETS

2024-10-27 18:14:25,000 (asr_inference:509) INFO: speech length: 30576
2024-10-27 18:14:26,317 (beam_search:428) INFO: decoder input length: 23
2024-10-27 18:14:26,317 (beam_search:429) INFO: max output length: 23
2024-10-27 18:14:26,317 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:26,327 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:26,327 (beam_search:476) INFO:  -0.14 * 1.0 =  -0.14 for ctc
2024-10-27 18:14:26,327 (beam_search:479) INFO: total log probability: -0.14
2024-10-27 18:14:26,327 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:14:26,327 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:26,327 (beam_search:483) INFO: best hypo: AND

2024-10-27 18:14:26,329 (asr_inference:509) INFO: speech length: 62576
2024-10-27 18:14:28,663 (beam_search:428) INFO: decoder input length: 48
2024-10-27 18:14:28,663 (beam_search:429) INFO: max output length: 48
2024-10-27 18:14:28,663 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:28,722 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:28,722 (beam_search:476) INFO:  -1.17 * 1.0 =  -1.17 for ctc
2024-10-27 18:14:28,722 (beam_search:479) INFO: total log probability: -1.17
2024-10-27 18:14:28,722 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:14:28,722 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:28,722 (beam_search:483) INFO: best hypo: WEDIDANDWEDIDTHEWITH

2024-10-27 18:14:28,724 (asr_inference:509) INFO: speech length: 29840
2024-10-27 18:14:30,036 (beam_search:428) INFO: decoder input length: 22
2024-10-27 18:14:30,036 (beam_search:429) INFO: max output length: 22
2024-10-27 18:14:30,036 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:30,053 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:30,053 (beam_search:476) INFO:  -0.16 * 1.0 =  -0.16 for ctc
2024-10-27 18:14:30,053 (beam_search:479) INFO: total log probability: -0.16
2024-10-27 18:14:30,053 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:14:30,053 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:30,053 (beam_search:483) INFO: best hypo: ICANTHE

2024-10-27 18:14:30,056 (asr_inference:509) INFO: speech length: 43184
2024-10-27 18:14:31,687 (beam_search:428) INFO: decoder input length: 33
2024-10-27 18:14:31,687 (beam_search:429) INFO: max output length: 33
2024-10-27 18:14:31,687 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:31,699 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:31,699 (beam_search:476) INFO:  -0.55 * 1.0 =  -0.55 for ctc
2024-10-27 18:14:31,699 (beam_search:479) INFO: total log probability: -0.55
2024-10-27 18:14:31,699 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:14:31,699 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:31,700 (beam_search:483) INFO: best hypo: IT

2024-10-27 18:14:31,702 (asr_inference:509) INFO: speech length: 45584
2024-10-27 18:14:33,518 (beam_search:428) INFO: decoder input length: 35
2024-10-27 18:14:33,518 (beam_search:429) INFO: max output length: 35
2024-10-27 18:14:33,518 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:33,541 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:33,541 (beam_search:476) INFO:  -0.38 * 1.0 =  -0.38 for ctc
2024-10-27 18:14:33,541 (beam_search:479) INFO: total log probability: -0.38
2024-10-27 18:14:33,541 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:14:33,541 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:33,541 (beam_search:483) INFO: best hypo: ITTOTHE

2024-10-27 18:14:33,544 (asr_inference:509) INFO: speech length: 107120
2024-10-27 18:14:37,353 (beam_search:428) INFO: decoder input length: 83
2024-10-27 18:14:37,353 (beam_search:429) INFO: max output length: 83
2024-10-27 18:14:37,353 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:37,456 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:37,456 (beam_search:476) INFO:  -1.64 * 1.0 =  -1.64 for ctc
2024-10-27 18:14:37,456 (beam_search:479) INFO: total log probability: -1.64
2024-10-27 18:14:37,456 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:14:37,456 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:37,456 (beam_search:483) INFO: best hypo: HOWMANYMETALWASHERSITTOTHE

2024-10-27 18:14:37,459 (asr_inference:509) INFO: speech length: 109152
2024-10-27 18:14:41,404 (beam_search:428) INFO: decoder input length: 84
2024-10-27 18:14:41,405 (beam_search:429) INFO: max output length: 84
2024-10-27 18:14:41,405 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:41,565 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:41,565 (beam_search:476) INFO:  -2.68 * 1.0 =  -2.68 for ctc
2024-10-27 18:14:41,565 (beam_search:479) INFO: total log probability: -2.68
2024-10-27 18:14:41,565 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:14:41,565 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:41,565 (beam_search:483) INFO: best hypo: THETHEWASHERSTHEMORELIKELYITTOTHEMAGNETIC

2024-10-27 18:14:41,568 (asr_inference:509) INFO: speech length: 43600
2024-10-27 18:14:43,226 (beam_search:428) INFO: decoder input length: 33
2024-10-27 18:14:43,226 (beam_search:429) INFO: max output length: 33
2024-10-27 18:14:43,226 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:43,259 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:43,259 (beam_search:476) INFO:  -1.33 * 1.0 =  -1.33 for ctc
2024-10-27 18:14:43,259 (beam_search:479) INFO: total log probability: -1.33
2024-10-27 18:14:43,259 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:14:43,259 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:43,259 (beam_search:483) INFO: best hypo: IWHATWASIMPORTANTWAS

2024-10-27 18:14:43,261 (asr_inference:509) INFO: speech length: 448320
2024-10-27 18:15:06,698 (beam_search:428) INFO: decoder input length: 349
2024-10-27 18:15:06,698 (beam_search:429) INFO: max output length: 349
2024-10-27 18:15:06,698 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:10,763 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:10,763 (beam_search:476) INFO:  -8.26 * 1.0 =  -8.26 for ctc
2024-10-27 18:15:10,763 (beam_search:479) INFO: total log probability: -8.26
2024-10-27 18:15:10,763 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:15:10,763 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:10,763 (beam_search:483) INFO: best hypo: THATTHEMETALTHATTHEIRONWASEDUMBYTHEFROMTHEMAGNETITSOITATEMPORARYMAGNETTHATJUSTEDITANDUMALSOSIFYOUUSEONAPENCILYOUIFYOUPUTANEGATIVESIDESTAPOSITIVESIDEIT'LLSOTHERE'SANOTHERMAGNETICTHATYOUCANUSE

2024-10-27 18:15:10,766 (asr_inference:509) INFO: speech length: 261840
2024-10-27 18:15:21,545 (beam_search:428) INFO: decoder input length: 204
2024-10-27 18:15:21,545 (beam_search:429) INFO: max output length: 204
2024-10-27 18:15:21,545 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:22,193 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:22,193 (beam_search:476) INFO:  -2.65 * 1.0 =  -2.65 for ctc
2024-10-27 18:15:22,193 (beam_search:479) INFO: total log probability: -2.65
2024-10-27 18:15:22,193 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:15:22,193 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:22,193 (beam_search:483) INFO: best hypo: UMITITITUSESUMITTOMAKETHEAMAGNETSTITANDUM

2024-10-27 18:15:22,196 (asr_inference:509) INFO: speech length: 442560
2024-10-27 18:15:45,008 (beam_search:428) INFO: decoder input length: 345
2024-10-27 18:15:45,008 (beam_search:429) INFO: max output length: 345
2024-10-27 18:15:45,008 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:50,065 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:50,065 (beam_search:476) INFO:  -8.46 * 1.0 =  -8.46 for ctc
2024-10-27 18:15:50,065 (beam_search:479) INFO: total log probability: -8.46
2024-10-27 18:15:50,065 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:15:50,065 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:50,065 (beam_search:483) INFO: best hypo: WELLWEDIDN'TUSEMAGNETSSOTHEY'TPUSHEACHOTHERBUTUMITWASKINDOFBUTWEDIDN'TUSEANOTHERMAGNETNOTREALLYBECAUSEWELLWEWEUSEDMOREIRONUMTOFINDTHEMAGNETSMORETHANOTHERMAGNETSCAUSEWEWEREN'TTOUSEMAGNETCAUSEITWOULDMAKEITAEASIERANDWEWOULDN'TOUTWHATINTHATTHATWEHADHADIRON

2024-10-27 18:15:50,069 (asr_inference:509) INFO: speech length: 18960
2024-10-27 18:15:51,034 (beam_search:428) INFO: decoder input length: 14
2024-10-27 18:15:51,034 (beam_search:429) INFO: max output length: 14
2024-10-27 18:15:51,034 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:51,043 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:51,043 (beam_search:476) INFO:  -1.25 * 1.0 =  -1.25 for ctc
2024-10-27 18:15:51,043 (beam_search:479) INFO: total log probability: -1.25
2024-10-27 18:15:51,043 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 18:15:51,043 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:51,043 (beam_search:483) INFO: best hypo: HADA

2024-10-27 18:15:51,045 (asr_inference:509) INFO: speech length: 16976
2024-10-27 18:15:51,908 (beam_search:428) INFO: decoder input length: 12
2024-10-27 18:15:51,908 (beam_search:429) INFO: max output length: 12
2024-10-27 18:15:51,908 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:51,920 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:51,920 (beam_search:476) INFO:  -0.55 * 1.0 =  -0.55 for ctc
2024-10-27 18:15:51,920 (beam_search:479) INFO: total log probability: -0.55
2024-10-27 18:15:51,920 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:15:51,920 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:51,920 (beam_search:483) INFO: best hypo: 'MGOOD

2024-10-27 18:15:51,922 (asr_inference:509) INFO: speech length: 95200
2024-10-27 18:15:55,513 (beam_search:428) INFO: decoder input length: 73
2024-10-27 18:15:55,514 (beam_search:429) INFO: max output length: 73
2024-10-27 18:15:55,514 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:55,594 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:55,594 (beam_search:476) INFO:  -1.26 * 1.0 =  -1.26 for ctc
2024-10-27 18:15:55,594 (beam_search:479) INFO: total log probability: -1.26
2024-10-27 18:15:55,595 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:15:55,595 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:55,595 (beam_search:483) INFO: best hypo: THEMOTORUMHADTHETHE

2024-10-27 18:15:55,597 (asr_inference:509) INFO: speech length: 27824
2024-10-27 18:15:56,828 (beam_search:428) INFO: decoder input length: 21
2024-10-27 18:15:56,828 (beam_search:429) INFO: max output length: 21
2024-10-27 18:15:56,828 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:56,851 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:56,851 (beam_search:476) INFO:  -2.32 * 1.0 =  -2.32 for ctc
2024-10-27 18:15:56,851 (beam_search:479) INFO: total log probability: -2.32
2024-10-27 18:15:56,851 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:15:56,851 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:56,851 (beam_search:483) INFO: best hypo: IT'SSTARTWITHIT

2024-10-27 18:15:56,854 (asr_inference:509) INFO: speech length: 126912
2024-10-27 18:16:01,424 (beam_search:428) INFO: decoder input length: 98
2024-10-27 18:16:01,424 (beam_search:429) INFO: max output length: 98
2024-10-27 18:16:01,424 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:01,689 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:01,689 (beam_search:476) INFO:  -3.03 * 1.0 =  -3.03 for ctc
2024-10-27 18:16:01,689 (beam_search:479) INFO: total log probability: -3.03
2024-10-27 18:16:01,689 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:16:01,689 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:01,689 (beam_search:483) INFO: best hypo: THEHADALITTLEFLAGONITANDUMWHENWHENYOUHADITCONNECTEDIT

2024-10-27 18:16:01,692 (asr_inference:509) INFO: speech length: 70176
2024-10-27 18:16:04,235 (beam_search:428) INFO: decoder input length: 54
2024-10-27 18:16:04,235 (beam_search:429) INFO: max output length: 54
2024-10-27 18:16:04,235 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:04,323 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:04,323 (beam_search:476) INFO:  -0.41 * 1.0 =  -0.41 for ctc
2024-10-27 18:16:04,323 (beam_search:479) INFO: total log probability: -0.41
2024-10-27 18:16:04,323 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:16:04,323 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:04,323 (beam_search:483) INFO: best hypo: WEHADTOHAVETHEWIREUMONTHEBATTERY

2024-10-27 18:16:04,325 (asr_inference:509) INFO: speech length: 475104
2024-10-27 18:16:29,275 (beam_search:428) INFO: decoder input length: 370
2024-10-27 18:16:29,276 (beam_search:429) INFO: max output length: 370
2024-10-27 18:16:29,276 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:34,476 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:34,476 (beam_search:476) INFO:  -8.18 * 1.0 =  -8.18 for ctc
2024-10-27 18:16:34,476 (beam_search:479) INFO: total log probability: -8.18
2024-10-27 18:16:34,476 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:16:34,476 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:34,476 (beam_search:483) INFO: best hypo: ONSIDEOFTHEBATTERYWEHADTOPUTATHEOFTHEWIREWHEREIT'SNOTONTHEOTHERBECAUSEIFITWASONTHEOTHERTHENITWOULDN'TITHADTOBETHEENERGYOUTTHEENERGYFROMTHEWIRESWASELECTRICITYANDITCAMETHROUGHTHEWIRESANDTOTHEMOTORANDANDWHENITINSIDETHEMOTORITAROUNDTHEFLAGREALLY

2024-10-27 18:16:34,479 (asr_inference:509) INFO: speech length: 296080
2024-10-27 18:16:47,187 (beam_search:428) INFO: decoder input length: 230
2024-10-27 18:16:47,188 (beam_search:429) INFO: max output length: 230
2024-10-27 18:16:47,188 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:49,267 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:49,267 (beam_search:476) INFO:  -6.46 * 1.0 =  -6.46 for ctc
2024-10-27 18:16:49,267 (beam_search:479) INFO: total log probability: -6.46
2024-10-27 18:16:49,267 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:16:49,267 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:49,267 (beam_search:483) INFO: best hypo: THERETHEREWASI'MNOTSURECAUSETHEWIRESWASCONNECTEDITEITHERWENTTHROUGHCAUSEWASTAPEONTHESIDESSOYOU'TREALLYSEEORUMITEITHERWASTWOWIRESAROUNDORJUSTONEWIREGOINGALLTHEWAYTHROUGHAROUND

2024-10-27 18:16:49,270 (asr_inference:509) INFO: speech length: 140384
2024-10-27 18:16:54,706 (beam_search:428) INFO: decoder input length: 109
2024-10-27 18:16:54,706 (beam_search:429) INFO: max output length: 109
2024-10-27 18:16:54,706 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:54,980 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:54,980 (beam_search:476) INFO:  -1.87 * 1.0 =  -1.87 for ctc
2024-10-27 18:16:54,980 (beam_search:479) INFO: total log probability: -1.87
2024-10-27 18:16:54,980 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:16:54,980 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:54,980 (beam_search:483) INFO: best hypo: THEBATTERYABATTERYANDITWELLWHATDOYOUMEANBYTHAT

2024-10-27 18:16:54,983 (asr_inference:509) INFO: speech length: 415024
2024-10-27 18:17:14,207 (beam_search:428) INFO: decoder input length: 323
2024-10-27 18:17:14,207 (beam_search:429) INFO: max output length: 323
2024-10-27 18:17:14,207 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:17,510 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:17,510 (beam_search:476) INFO:  -5.98 * 1.0 =  -5.98 for ctc
2024-10-27 18:17:17,511 (beam_search:479) INFO: total log probability: -5.98
2024-10-27 18:17:17,511 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:17:17,511 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:17,511 (beam_search:483) INFO: best hypo: ITGOESFROMTHEOFTHEBATTERYWHEREITTHEELECTRICITYGOUMOUTOFTHEBATTERYANDTOTHEWIRESANDTHENITGOESAROUNDANDINTOTHEMOTORSOITUMTHEELECTRICITYINACIRCUITANDUMITFLOWSTHROUGHTHEWIRESANDONTOTHEMOTORWHICHMAKESIT

2024-10-27 18:17:17,513 (asr_inference:509) INFO: speech length: 15680
2024-10-27 18:17:18,265 (beam_search:428) INFO: decoder input length: 11
2024-10-27 18:17:18,265 (beam_search:429) INFO: max output length: 11
2024-10-27 18:17:18,265 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:18,275 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:18,275 (beam_search:476) INFO:  -1.37 * 1.0 =  -1.37 for ctc
2024-10-27 18:17:18,275 (beam_search:479) INFO: total log probability: -1.37
2024-10-27 18:17:18,275 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:17:18,275 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:18,275 (beam_search:483) INFO: best hypo: ITHINKSO

2024-10-27 18:17:18,277 (asr_inference:509) INFO: speech length: 123504
2024-10-27 18:17:22,715 (beam_search:428) INFO: decoder input length: 95
2024-10-27 18:17:22,715 (beam_search:429) INFO: max output length: 95
2024-10-27 18:17:22,715 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:22,978 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:22,978 (beam_search:476) INFO:  -2.60 * 1.0 =  -2.60 for ctc
2024-10-27 18:17:22,978 (beam_search:479) INFO: total log probability: -2.60
2024-10-27 18:17:22,978 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:17:22,978 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:22,978 (beam_search:483) INFO: best hypo: THEELECTRICITYISFROMTHEBATTERYTOTHEMOTORANDANDLETTHEMOTORRUN

2024-10-27 18:17:22,982 (asr_inference:509) INFO: speech length: 150560
2024-10-27 18:17:28,715 (beam_search:428) INFO: decoder input length: 117
2024-10-27 18:17:28,716 (beam_search:429) INFO: max output length: 117
2024-10-27 18:17:28,716 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:29,258 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:29,258 (beam_search:476) INFO:  -2.62 * 1.0 =  -2.62 for ctc
2024-10-27 18:17:29,258 (beam_search:479) INFO: total log probability: -2.62
2024-10-27 18:17:29,259 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:17:29,259 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:29,259 (beam_search:483) INFO: best hypo: THATISTHEWEDIDINWEUSEDUMTHEBATTERYTOGOLIKETHATINTOTHELIGHTBULBANDLIGHTUPTHELIGHTBULB

2024-10-27 18:17:29,261 (asr_inference:509) INFO: speech length: 13392
2024-10-27 18:17:30,145 (beam_search:428) INFO: decoder input length: 9
2024-10-27 18:17:30,146 (beam_search:429) INFO: max output length: 9
2024-10-27 18:17:30,146 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:30,152 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:30,152 (beam_search:476) INFO:  -0.23 * 1.0 =  -0.23 for ctc
2024-10-27 18:17:30,152 (beam_search:479) INFO: total log probability: -0.23
2024-10-27 18:17:30,152 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:17:30,152 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:30,152 (beam_search:483) INFO: best hypo: THE

2024-10-27 18:17:30,155 (asr_inference:509) INFO: speech length: 479040
2024-10-27 18:17:56,176 (beam_search:428) INFO: decoder input length: 373
2024-10-27 18:17:56,176 (beam_search:429) INFO: max output length: 373
2024-10-27 18:17:56,176 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:01,815 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:01,816 (beam_search:476) INFO:  -8.28 * 1.0 =  -8.28 for ctc
2024-10-27 18:18:01,816 (beam_search:479) INFO: total log probability: -8.28
2024-10-27 18:18:01,816 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:18:01,816 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:01,816 (beam_search:483) INFO: best hypo: THATISUMTHEONANDOFFTHETHINGTHEISCONNECTEDBECAUSETHEELECTRICITYIFTHATMOTORISNOTIFTHATSWITCHISNOTCONNECTEDTOTHETHINGTHEELECTRICITYCANNOTFLOWTHROUGHITWILLNOTUMGOTHROUGHTHEANDCONNECTTOTHEMOTORBECAUSEITITISITWILLGOOUTANDITWILLNOTTHEOTHERSOTHAT'SKINDOFLIKEANONANDOFFSWITCHSOIT'S

2024-10-27 18:18:01,820 (asr_inference:509) INFO: speech length: 353696
2024-10-27 18:18:17,689 (beam_search:428) INFO: decoder input length: 275
2024-10-27 18:18:17,689 (beam_search:429) INFO: max output length: 275
2024-10-27 18:18:17,689 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:20,705 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:20,705 (beam_search:476) INFO:  -7.62 * 1.0 =  -7.62 for ctc
2024-10-27 18:18:20,706 (beam_search:479) INFO: total log probability: -7.62
2024-10-27 18:18:20,706 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:18:20,706 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:20,706 (beam_search:483) INFO: best hypo: THEUMWILLTAKETHEANDITWILLTURNTHEMOFFBECAUSEIFWHENTHEELECTRICITYISFLOWINGTHROUGHITLLFLOWTHROUGHTHEMETALANDIFITDOESN'TTHEOTHERTHINGTHENITWILLNOTBETOGOTHROUGHANDMAKEANDLIGHTUPTHEANDMAKETHEMOTORGOANDROUND

2024-10-27 18:18:20,708 (asr_inference:509) INFO: speech length: 289216
2024-10-27 18:18:33,025 (beam_search:428) INFO: decoder input length: 225
2024-10-27 18:18:33,025 (beam_search:429) INFO: max output length: 225
2024-10-27 18:18:33,025 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:35,866 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:35,866 (beam_search:476) INFO:  -3.70 * 1.0 =  -3.70 for ctc
2024-10-27 18:18:35,866 (beam_search:479) INFO: total log probability: -3.70
2024-10-27 18:18:35,866 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:18:35,866 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:35,866 (beam_search:483) INFO: best hypo: THAT'SIT'SKINDALIKEANANDBECAUSEIFYOUCONNECTTHESWITCHTOTHEOTHERWIRETHENTHEELECTRICITYISTOGOTHROUGHANDITWILLTURNONTHEMOTORBUTWHENIT'SNOTTHENIT'SNOTCONNECTEDANDITWON'TBETOGOTHROUGHTOTHEOTHERSIDEOFTHEWIREWHEREITGETSCONNECTEDTOTHEMOTOR

2024-10-27 18:18:35,869 (asr_inference:509) INFO: speech length: 129792
2024-10-27 18:18:40,587 (beam_search:428) INFO: decoder input length: 100
2024-10-27 18:18:40,587 (beam_search:429) INFO: max output length: 100
2024-10-27 18:18:40,587 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:40,858 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:40,859 (beam_search:476) INFO:  -3.19 * 1.0 =  -3.19 for ctc
2024-10-27 18:18:40,859 (beam_search:479) INFO: total log probability: -3.19
2024-10-27 18:18:40,859 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:18:40,859 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:40,859 (beam_search:483) INFO: best hypo: YOUWOULDPUTTHEBATTERYINTOTHERIGHTATTHEBOTTOMANDTHENYOUWOULD

2024-10-27 18:18:40,861 (asr_inference:509) INFO: speech length: 46256
2024-10-27 18:18:42,712 (beam_search:428) INFO: decoder input length: 35
2024-10-27 18:18:42,712 (beam_search:429) INFO: max output length: 35
2024-10-27 18:18:42,712 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:42,734 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:42,734 (beam_search:476) INFO:  -0.08 * 1.0 =  -0.08 for ctc
2024-10-27 18:18:42,734 (beam_search:479) INFO: total log probability: -0.08
2024-10-27 18:18:42,734 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:18:42,734 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:42,734 (beam_search:483) INFO: best hypo: ISEETHAT

2024-10-27 18:18:42,737 (asr_inference:509) INFO: speech length: 67120
2024-10-27 18:18:45,238 (beam_search:428) INFO: decoder input length: 51
2024-10-27 18:18:45,238 (beam_search:429) INFO: max output length: 51
2024-10-27 18:18:45,238 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:45,307 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:45,307 (beam_search:476) INFO:  -2.04 * 1.0 =  -2.04 for ctc
2024-10-27 18:18:45,307 (beam_search:479) INFO: total log probability: -2.04
2024-10-27 18:18:45,307 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:18:45,307 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:45,307 (beam_search:483) INFO: best hypo: THE'SINISALIGHTBULB

2024-10-27 18:18:45,309 (asr_inference:509) INFO: speech length: 169696
2024-10-27 18:18:52,027 (beam_search:428) INFO: decoder input length: 132
2024-10-27 18:18:52,027 (beam_search:429) INFO: max output length: 132
2024-10-27 18:18:52,027 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:52,394 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:52,395 (beam_search:476) INFO:  -1.41 * 1.0 =  -1.41 for ctc
2024-10-27 18:18:52,395 (beam_search:479) INFO: total log probability: -1.41
2024-10-27 18:18:52,395 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:18:52,395 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:52,395 (beam_search:483) INFO: best hypo: THEBATTERYGETSINSIDETHETWOTHINGSANDTHEELECTRICITYTHROUGHINTOTHELIGHTBULB

2024-10-27 18:18:52,397 (asr_inference:509) INFO: speech length: 116240
2024-10-27 18:18:56,696 (beam_search:428) INFO: decoder input length: 90
2024-10-27 18:18:56,696 (beam_search:429) INFO: max output length: 90
2024-10-27 18:18:56,696 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:57,091 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:57,092 (beam_search:476) INFO:  -2.09 * 1.0 =  -2.09 for ctc
2024-10-27 18:18:57,092 (beam_search:479) INFO: total log probability: -2.09
2024-10-27 18:18:57,092 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:18:57,092 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:57,092 (beam_search:483) INFO: best hypo: IT'SACIRCUITBUTIT'SAAITIT'SACIRCUITBUTITLOOKSKINDOFLIKEARIGHTNOW

2024-10-27 18:18:57,094 (asr_inference:509) INFO: speech length: 339968
2024-10-27 18:19:11,719 (beam_search:428) INFO: decoder input length: 265
2024-10-27 18:19:11,719 (beam_search:429) INFO: max output length: 265
2024-10-27 18:19:11,719 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:13,700 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:13,700 (beam_search:476) INFO:  -6.94 * 1.0 =  -6.94 for ctc
2024-10-27 18:19:13,700 (beam_search:479) INFO: total log probability: -6.94
2024-10-27 18:19:13,700 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:19:13,700 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:13,700 (beam_search:483) INFO: best hypo: WE'VEUMWEHAVEBEENWELLWE'RETOLIGHTTWOLIGHTBULBSINONECIRCUITNOWSOWEARETOLIGHTTHEMWITHUMANDUSEONEBATTERYANDMAKEANDONECIRCUIT

2024-10-27 18:19:13,703 (asr_inference:509) INFO: speech length: 253136
2024-10-27 18:19:24,011 (beam_search:428) INFO: decoder input length: 197
2024-10-27 18:19:24,011 (beam_search:429) INFO: max output length: 197
2024-10-27 18:19:24,011 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:25,702 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:25,702 (beam_search:476) INFO:  -7.52 * 1.0 =  -7.52 for ctc
2024-10-27 18:19:25,702 (beam_search:479) INFO: total log probability: -7.52
2024-10-27 18:19:25,703 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:19:25,703 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:25,703 (beam_search:483) INFO: best hypo: THEHADTOBERIGHTBECAUSEIFITWASAPOSITIVESIDEGOINGTOANOTHERPOSITIVESIDEITWOULDN'TOUTIT'TTHEELECTRICITYFLOWTHROUGHBUTIFIT'SASIDETOAPOSITIVESIDEITYOUCANITFLOWTHROUGH

2024-10-27 18:19:25,705 (asr_inference:509) INFO: speech length: 286528
2024-10-27 18:19:37,779 (beam_search:428) INFO: decoder input length: 223
2024-10-27 18:19:37,779 (beam_search:429) INFO: max output length: 223
2024-10-27 18:19:37,779 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:39,584 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:39,584 (beam_search:476) INFO:  -5.36 * 1.0 =  -5.36 for ctc
2024-10-27 18:19:39,584 (beam_search:479) INFO: total log probability: -5.36
2024-10-27 18:19:39,584 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:19:39,584 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:39,584 (beam_search:483) INFO: best hypo: THEELECTRICITYISFLOWINGTHROUGHTHEWIRESANDINTOTHEUMANDTHELITTLEWIRETHEPARTOFTHEWIREISWITHTHEWIREINSIDETHELIGHTBULBUMANDIT'SGOINGUPANDLETTHETHEENERGYLIGHTITUP

2024-10-27 18:19:39,587 (asr_inference:509) INFO: speech length: 361040
2024-10-27 18:19:55,882 (beam_search:428) INFO: decoder input length: 281
2024-10-27 18:19:55,882 (beam_search:429) INFO: max output length: 281
2024-10-27 18:19:55,882 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:59,410 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:59,410 (beam_search:476) INFO:  -3.65 * 1.0 =  -3.65 for ctc
2024-10-27 18:19:59,410 (beam_search:479) INFO: total log probability: -3.65
2024-10-27 18:19:59,410 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:19:59,410 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:59,410 (beam_search:483) INFO: best hypo: THEAREASONEWITHUMTHEY'RECONNECTEDSOANDUMIT'SONONESIDEOFTHEBATTERYONTHENEGATIVESIDEOFTHEBATTERYIT'SGOINGUPTOINTOTHELIGHTBULBANDIT'SLIGHTINGTHELIGHTBULBUPONTHEOTHERSIDEIT'SJUSTGOINGTHROUGHTOGETTOTHEOTHEROFTHEBATTERY

2024-10-27 18:19:59,413 (asr_inference:509) INFO: speech length: 367312
2024-10-27 18:20:16,396 (beam_search:428) INFO: decoder input length: 286
2024-10-27 18:20:16,396 (beam_search:429) INFO: max output length: 286
2024-10-27 18:20:16,396 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:19,790 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:19,790 (beam_search:476) INFO:  -5.77 * 1.0 =  -5.77 for ctc
2024-10-27 18:20:19,790 (beam_search:479) INFO: total log probability: -5.77
2024-10-27 18:20:19,790 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:20:19,790 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:19,790 (beam_search:483) INFO: best hypo: THESIDEISTHENEGATIVESIDEANDTHEY'RETOUCHINGEACHOTHERTHENTHEY'REABLETOTOEACHOTHERSOIT'SKINDOFLIKEANOTHERWIRECONNECTINGTOTHEMTOGETHERUMBUTIT'SJUSTTHEY'RETOUCHINGOTHERANDTHENONTHEOTHERSIDESIT'STOUSEANDITMOREPOWER

2024-10-27 18:20:19,793 (asr_inference:509) INFO: speech length: 14528
2024-10-27 18:20:20,530 (beam_search:428) INFO: decoder input length: 10
2024-10-27 18:20:20,530 (beam_search:429) INFO: max output length: 10
2024-10-27 18:20:20,530 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:20,538 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:20,538 (beam_search:476) INFO:  -0.55 * 1.0 =  -0.55 for ctc
2024-10-27 18:20:20,538 (beam_search:479) INFO: total log probability: -0.55
2024-10-27 18:20:20,538 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:20:20,538 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:20,538 (beam_search:483) INFO: best hypo: I'

2024-10-27 18:20:20,540 (asr_inference:509) INFO: speech length: 232496
2024-10-27 18:20:29,781 (beam_search:428) INFO: decoder input length: 181
2024-10-27 18:20:29,782 (beam_search:429) INFO: max output length: 181
2024-10-27 18:20:29,782 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:31,069 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:31,069 (beam_search:476) INFO:  -5.09 * 1.0 =  -5.09 for ctc
2024-10-27 18:20:31,069 (beam_search:479) INFO: total log probability: -5.09
2024-10-27 18:20:31,070 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:20:31,070 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:31,070 (beam_search:483) INFO: best hypo: YEAHTHERECOULDBEAWIREINTHETWOTHINGSANDITWOULDITWOULDBETHERECOULDBEAWIREINTHETWOBATTERIESANDITWOULDTHESAMEASIFTHEYWERETOUCHINGEACHOTHER

2024-10-27 18:20:31,073 (asr_inference:509) INFO: speech length: 468256
2024-10-27 18:20:54,154 (beam_search:428) INFO: decoder input length: 365
2024-10-27 18:20:54,155 (beam_search:429) INFO: max output length: 365
2024-10-27 18:20:54,155 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:59,391 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:59,392 (beam_search:476) INFO:  -8.62 * 1.0 =  -8.62 for ctc
2024-10-27 18:20:59,392 (beam_search:479) INFO: total log probability: -8.62
2024-10-27 18:20:59,392 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:20:59,392 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:59,392 (beam_search:483) INFO: best hypo: 'SWHATWEDIDTODAYWEWOULDHAVETHEWIREINTHETWOWELLITTHEELECTRICITYISFLOWINGTHROUGHTHEWIRESOUMTHERE'SONEBATTERYANDTHEN'SAWIREBETWEENITAND'SANOTHERBATTERYTHATUMANDITLLGOTHROUGHJUSTLIKEUMIFTHEBATTERY'STOUCHINGEACHOTHERANDIT'LLGOAROUNDANDLIGHTUPEACHLIGHTBULB

2024-10-27 18:20:59,395 (asr_inference:509) INFO: speech length: 53184
2024-10-27 18:21:01,337 (beam_search:428) INFO: decoder input length: 41
2024-10-27 18:21:01,337 (beam_search:429) INFO: max output length: 41
2024-10-27 18:21:01,337 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:01,409 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:01,409 (beam_search:476) INFO:  -1.02 * 1.0 =  -1.02 for ctc
2024-10-27 18:21:01,409 (beam_search:479) INFO: total log probability: -1.02
2024-10-27 18:21:01,410 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:21:01,410 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:01,410 (beam_search:483) INFO: best hypo: THROUGHTHEWIRESANDLIGHTSUPTHELIGHTBULB

2024-10-27 18:21:01,412 (asr_inference:509) INFO: speech length: 17600
2024-10-27 18:21:02,287 (beam_search:428) INFO: decoder input length: 13
2024-10-27 18:21:02,287 (beam_search:429) INFO: max output length: 13
2024-10-27 18:21:02,287 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:02,296 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:02,296 (beam_search:476) INFO:  -0.74 * 1.0 =  -0.74 for ctc
2024-10-27 18:21:02,296 (beam_search:479) INFO: total log probability: -0.74
2024-10-27 18:21:02,296 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:21:02,296 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:02,296 (beam_search:483) INFO: best hypo: YEAHTHEY

2024-10-27 18:21:02,298 (asr_inference:509) INFO: speech length: 189056
2024-10-27 18:21:09,762 (beam_search:428) INFO: decoder input length: 147
2024-10-27 18:21:09,763 (beam_search:429) INFO: max output length: 147
2024-10-27 18:21:09,763 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:10,500 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:10,501 (beam_search:476) INFO:  -2.09 * 1.0 =  -2.09 for ctc
2024-10-27 18:21:10,501 (beam_search:479) INFO: total log probability: -2.09
2024-10-27 18:21:10,501 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:21:10,501 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:10,501 (beam_search:483) INFO: best hypo: THESAMEITDIDN'TLIKETAKEAWIREOUTORANDICAN'TSEEASWITCHSOIDON'TKNOW

2024-10-27 18:21:10,503 (asr_inference:509) INFO: speech length: 27744
2024-10-27 18:21:11,678 (beam_search:428) INFO: decoder input length: 21
2024-10-27 18:21:11,678 (beam_search:429) INFO: max output length: 21
2024-10-27 18:21:11,678 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:11,707 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:11,708 (beam_search:476) INFO:  -1.75 * 1.0 =  -1.75 for ctc
2024-10-27 18:21:11,708 (beam_search:479) INFO: total log probability: -1.75
2024-10-27 18:21:11,708 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:21:11,708 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:11,708 (beam_search:483) INFO: best hypo: ISEEITWHENTHEY'REON

2024-10-27 18:21:11,710 (asr_inference:509) INFO: speech length: 338336
2024-10-27 18:21:26,380 (beam_search:428) INFO: decoder input length: 263
2024-10-27 18:21:26,380 (beam_search:429) INFO: max output length: 263
2024-10-27 18:21:26,380 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:28,984 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:28,984 (beam_search:476) INFO:  -6.84 * 1.0 =  -6.84 for ctc
2024-10-27 18:21:28,984 (beam_search:479) INFO: total log probability: -6.84
2024-10-27 18:21:28,984 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:21:28,984 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:28,984 (beam_search:483) INFO: best hypo: OHUMIT'STHEANDTHENEGATIVESIDEAREUMSOTHENEGATIVECAN'TGOTHROUGHUMWITHANOTHERNEGATIVEJUSTLIKEAPOSITIVECAN'TGOTHROUGHWITHANOTHERPOSITIVESOUMITCAN'TTHEELECTRICITYGOTHROUGHITHASTOPLUSANDA

2024-10-27 18:21:28,986 (asr_inference:509) INFO: speech length: 22672
2024-10-27 18:21:29,966 (beam_search:428) INFO: decoder input length: 17
2024-10-27 18:21:29,966 (beam_search:429) INFO: max output length: 17
2024-10-27 18:21:29,966 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:29,977 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:29,977 (beam_search:476) INFO:  -0.52 * 1.0 =  -0.52 for ctc
2024-10-27 18:21:29,977 (beam_search:479) INFO: total log probability: -0.52
2024-10-27 18:21:29,977 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:21:29,977 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:29,977 (beam_search:483) INFO: best hypo: WASOFF

2024-10-27 18:21:29,980 (asr_inference:509) INFO: speech length: 276928
2024-10-27 18:21:41,483 (beam_search:428) INFO: decoder input length: 215
2024-10-27 18:21:41,483 (beam_search:429) INFO: max output length: 215
2024-10-27 18:21:41,483 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:43,036 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:43,036 (beam_search:476) INFO:  -1.13 * 1.0 =  -1.13 for ctc
2024-10-27 18:21:43,036 (beam_search:479) INFO: total log probability: -1.13
2024-10-27 18:21:43,036 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:21:43,036 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:43,036 (beam_search:483) INFO: best hypo: THEONELIGHTBULBWASOFFUMITITWASJUSTONEWIRETHATWASCONNECTINGFROMONEBATTERYUMSOITWASGOINGTHROUGHUMTOTHATONELIGHTBULBANDNOTTOTHEOTHER

2024-10-27 18:21:43,039 (asr_inference:509) INFO: speech length: 232016
2024-10-27 18:21:52,378 (beam_search:428) INFO: decoder input length: 180
2024-10-27 18:21:52,378 (beam_search:429) INFO: max output length: 180
2024-10-27 18:21:52,378 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:53,489 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:53,489 (beam_search:476) INFO:  -2.46 * 1.0 =  -2.46 for ctc
2024-10-27 18:21:53,489 (beam_search:479) INFO: total log probability: -2.46
2024-10-27 18:21:53,489 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:21:53,489 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:53,489 (beam_search:483) INFO: best hypo: ONITITWASONBUTWHENIONITINUMATTHEUHWHENTHEREWAS'TADCELLTHENUMITWASONEOFTHEMWASOFF

2024-10-27 18:21:53,492 (asr_inference:509) INFO: speech length: 264256
2024-10-27 18:22:04,707 (beam_search:428) INFO: decoder input length: 205
2024-10-27 18:22:04,707 (beam_search:429) INFO: max output length: 205
2024-10-27 18:22:04,707 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:06,636 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:06,636 (beam_search:476) INFO:  -5.74 * 1.0 =  -5.74 for ctc
2024-10-27 18:22:06,636 (beam_search:479) INFO: total log probability: -5.74
2024-10-27 18:22:06,636 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:22:06,636 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:06,636 (beam_search:483) INFO: best hypo: UMYEAHITHINKTHEYAREBOTHONBUTUMTHEBATTERYCAN'TASMUCHPOWERTOBOTHOFTHEMUMITCANONLYALITTLEOFPOWERSOTHEY'RETHEY'RENOTASBRIGHTASTHEONEWITHTWOBATTERIESTHEYGETA

2024-10-27 18:22:06,639 (asr_inference:509) INFO: speech length: 124400
2024-10-27 18:22:11,120 (beam_search:428) INFO: decoder input length: 96
2024-10-27 18:22:11,120 (beam_search:429) INFO: max output length: 96
2024-10-27 18:22:11,120 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:11,510 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:11,510 (beam_search:476) INFO:  -3.25 * 1.0 =  -3.25 for ctc
2024-10-27 18:22:11,510 (beam_search:479) INFO: total log probability: -3.25
2024-10-27 18:22:11,510 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:22:11,510 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:11,510 (beam_search:483) INFO: best hypo: THERETHREELIGHTSTHENUMTHEWOULDKINDOFBELIKEONEFORTWOLIGHTSTHEYWOULDN'TWORKASWELL

2024-10-27 18:22:11,513 (asr_inference:509) INFO: speech length: 67712
2024-10-27 18:22:14,019 (beam_search:428) INFO: decoder input length: 52
2024-10-27 18:22:14,019 (beam_search:429) INFO: max output length: 52
2024-10-27 18:22:14,019 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:14,089 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:14,089 (beam_search:476) INFO:  -1.20 * 1.0 =  -1.20 for ctc
2024-10-27 18:22:14,089 (beam_search:479) INFO: total log probability: -1.20
2024-10-27 18:22:14,089 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:22:14,089 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:14,089 (beam_search:483) INFO: best hypo: YEAHWEWOULDNEEDONEMOREBATTERYI

2024-10-27 18:22:14,092 (asr_inference:509) INFO: speech length: 15072
2024-10-27 18:22:14,817 (beam_search:428) INFO: decoder input length: 11
2024-10-27 18:22:14,817 (beam_search:429) INFO: max output length: 11
2024-10-27 18:22:14,817 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:14,820 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:14,821 (beam_search:476) INFO:  -0.77 * 1.0 =  -0.77 for ctc
2024-10-27 18:22:14,821 (beam_search:479) INFO: total log probability: -0.77
2024-10-27 18:22:14,821 (beam_search:480) INFO: normalized log probability: -0.39
2024-10-27 18:22:14,821 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:14,821 (beam_search:483) INFO: best hypo: 

2024-10-27 18:22:14,823 (asr_inference:509) INFO: speech length: 233008
2024-10-27 18:22:24,214 (beam_search:428) INFO: decoder input length: 181
2024-10-27 18:22:24,214 (beam_search:429) INFO: max output length: 181
2024-10-27 18:22:24,214 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:25,253 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:25,253 (beam_search:476) INFO:  -5.60 * 1.0 =  -5.60 for ctc
2024-10-27 18:22:25,253 (beam_search:479) INFO: total log probability: -5.60
2024-10-27 18:22:25,253 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:22:25,253 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:25,253 (beam_search:483) INFO: best hypo: INSCIENCEWEUSEDMAGNETSANDWEDINSIDEOURBOXTOUMMAKETHATCOULDSEEEMANDTHENWEUSEDMETALTOTRYTOOUTWHERETHEMAGNETSWERE

2024-10-27 18:22:25,256 (asr_inference:509) INFO: speech length: 420432
2024-10-27 18:22:44,898 (beam_search:428) INFO: decoder input length: 327
2024-10-27 18:22:44,898 (beam_search:429) INFO: max output length: 327
2024-10-27 18:22:44,898 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:48,534 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:48,534 (beam_search:476) INFO:  -7.42 * 1.0 =  -7.42 for ctc
2024-10-27 18:22:48,534 (beam_search:479) INFO: total log probability: -7.42
2024-10-27 18:22:48,534 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:22:48,534 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:48,534 (beam_search:483) INFO: best hypo: WELLSOMEUSEDAANDTHEYWOULDPUTITONANDTHENTHEEDTHEYWOULDUSEAMAGNETANDTRYTOFINDOUTTHEOTHERMAGNETSWEREANDTHENOTHERGROUPSJUSTTHEANDUMPUTTHEMOVERTHEOVERTHEBOXINDIFFERENTANDUMTHENTHEYOUTTHEYWOULDDRAWONAPIECEOF

2024-10-27 18:22:48,538 (asr_inference:509) INFO: speech length: 442176
2024-10-27 18:23:10,691 (beam_search:428) INFO: decoder input length: 344
2024-10-27 18:23:10,692 (beam_search:429) INFO: max output length: 344
2024-10-27 18:23:10,692 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:14,341 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:14,341 (beam_search:476) INFO:  -7.18 * 1.0 =  -7.18 for ctc
2024-10-27 18:23:14,341 (beam_search:479) INFO: total log probability: -7.18
2024-10-27 18:23:14,341 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:23:14,341 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:14,342 (beam_search:483) INFO: best hypo: UMYOUCOULDFEELWHERETHEYAREANDYOUWOULDUSEDIFFERENTMATERIALSYOUUSEWASHERSANDPAPERANDYOUWOULDSOMEHAVEATHEYWOULDTHEYWOULDHAVEAANDSOTHENYOUWOULDKNOWWHERETHEMAGNETISANDSOMEWEREANDSOYOUDIDN'TKNOWWHERETHEMAGNETWASAS

2024-10-27 18:23:14,344 (asr_inference:509) INFO: speech length: 42208
2024-10-27 18:23:15,963 (beam_search:428) INFO: decoder input length: 32
2024-10-27 18:23:15,964 (beam_search:429) INFO: max output length: 32
2024-10-27 18:23:15,964 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:15,989 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:15,989 (beam_search:476) INFO:  -0.84 * 1.0 =  -0.84 for ctc
2024-10-27 18:23:15,989 (beam_search:479) INFO: total log probability: -0.84
2024-10-27 18:23:15,989 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:23:15,989 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:15,989 (beam_search:483) INFO: best hypo: THEFORCEOFWAS

2024-10-27 18:23:15,991 (asr_inference:509) INFO: speech length: 11440
2024-10-27 18:23:16,659 (beam_search:428) INFO: decoder input length: 8
2024-10-27 18:23:16,659 (beam_search:429) INFO: max output length: 8
2024-10-27 18:23:16,659 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:16,668 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:16,668 (beam_search:476) INFO:  -0.17 * 1.0 =  -0.17 for ctc
2024-10-27 18:23:16,668 (beam_search:479) INFO: total log probability: -0.17
2024-10-27 18:23:16,668 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:23:16,668 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:16,668 (beam_search:483) INFO: best hypo: YOU'RE

2024-10-27 18:23:16,670 (asr_inference:509) INFO: speech length: 41232
2024-10-27 18:23:18,247 (beam_search:428) INFO: decoder input length: 31
2024-10-27 18:23:18,247 (beam_search:429) INFO: max output length: 31
2024-10-27 18:23:18,247 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:18,280 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:18,280 (beam_search:476) INFO:  -2.70 * 1.0 =  -2.70 for ctc
2024-10-27 18:23:18,280 (beam_search:479) INFO: total log probability: -2.70
2024-10-27 18:23:18,280 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 18:23:18,280 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:18,280 (beam_search:483) INFO: best hypo: NOWILLNOTATTRACTTHEMAGNET

2024-10-27 18:23:18,282 (asr_inference:509) INFO: speech length: 146416
2024-10-27 18:23:23,671 (beam_search:428) INFO: decoder input length: 113
2024-10-27 18:23:23,671 (beam_search:429) INFO: max output length: 113
2024-10-27 18:23:23,671 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:24,136 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:24,136 (beam_search:476) INFO:  -3.63 * 1.0 =  -3.63 for ctc
2024-10-27 18:23:24,136 (beam_search:479) INFO: total log probability: -3.63
2024-10-27 18:23:24,136 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:23:24,136 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:24,136 (beam_search:483) INFO: best hypo: THETHATTHETHEMAGNETTOHAVETOHAVEIRONORSTEELINTHEMANDAPLASTICCHIPNOTHAVEIRONORSTEEL

2024-10-27 18:23:24,139 (asr_inference:509) INFO: speech length: 80416
2024-10-27 18:23:27,077 (beam_search:428) INFO: decoder input length: 62
2024-10-27 18:23:27,077 (beam_search:429) INFO: max output length: 62
2024-10-27 18:23:27,077 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:27,146 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:27,146 (beam_search:476) INFO:  -2.82 * 1.0 =  -2.82 for ctc
2024-10-27 18:23:27,146 (beam_search:479) INFO: total log probability: -2.82
2024-10-27 18:23:27,146 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:23:27,146 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:27,146 (beam_search:483) INFO: best hypo: ISAREANDWHENAREAWAY

2024-10-27 18:23:27,149 (asr_inference:509) INFO: speech length: 60432
2024-10-27 18:23:29,332 (beam_search:428) INFO: decoder input length: 46
2024-10-27 18:23:29,333 (beam_search:429) INFO: max output length: 46
2024-10-27 18:23:29,333 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:29,418 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:29,419 (beam_search:476) INFO:  -1.85 * 1.0 =  -1.85 for ctc
2024-10-27 18:23:29,419 (beam_search:479) INFO: total log probability: -1.85
2024-10-27 18:23:29,419 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:23:29,419 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:29,419 (beam_search:483) INFO: best hypo: THEMAGNETCANGOTHROUGHTHINGSTHATIITCOULDGO

2024-10-27 18:23:29,421 (asr_inference:509) INFO: speech length: 442128
2024-10-27 18:23:50,520 (beam_search:428) INFO: decoder input length: 344
2024-10-27 18:23:50,520 (beam_search:429) INFO: max output length: 344
2024-10-27 18:23:50,520 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:54,773 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:54,773 (beam_search:476) INFO:  -9.72 * 1.0 =  -9.72 for ctc
2024-10-27 18:23:54,773 (beam_search:479) INFO: total log probability: -9.72
2024-10-27 18:23:54,773 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:23:54,773 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:54,773 (beam_search:483) INFO: best hypo: WEDIDTHATINANOTHERSCIENCEEXPERIMENTWEHADAUMAANDWEPUTPLASTICINTHEREANDTHENWEHADAMAGNETINONECUPANDAMAGNETTHECUPANDWEWOULDPUTPLASTICINBETWEENTHETWOMAGNETSANDWEHADWASHERSINTHEOTHERANDWESWOULDSEEHOWMANYWASHERSITTOOKTOSEPARATETHEFORCEOF

2024-10-27 18:23:54,776 (asr_inference:509) INFO: speech length: 303840
2024-10-27 18:24:07,679 (beam_search:428) INFO: decoder input length: 236
2024-10-27 18:24:07,679 (beam_search:429) INFO: max output length: 236
2024-10-27 18:24:07,679 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:09,473 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:09,473 (beam_search:476) INFO:  -3.98 * 1.0 =  -3.98 for ctc
2024-10-27 18:24:09,473 (beam_search:479) INFO: total log probability: -3.98
2024-10-27 18:24:09,473 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:24:09,473 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:09,473 (beam_search:483) INFO: best hypo: THETWOTHEMAGNETINAISATTRACTEDTOTHEMAGNETTHECUPANDSOTHEOTHERBISJUSTINTHEAIRBECAUSEOFTHEMAGNETANDAIT'SINGITDOWNANDTHAT'SWHAT

2024-10-27 18:24:09,477 (asr_inference:509) INFO: speech length: 238512
2024-10-27 18:24:19,076 (beam_search:428) INFO: decoder input length: 185
2024-10-27 18:24:19,076 (beam_search:429) INFO: max output length: 185
2024-10-27 18:24:19,076 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:20,244 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:20,245 (beam_search:476) INFO:  -4.62 * 1.0 =  -4.62 for ctc
2024-10-27 18:24:20,245 (beam_search:479) INFO: total log probability: -4.62
2024-10-27 18:24:20,245 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:24:20,245 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:20,245 (beam_search:483) INFO: best hypo: THEWASHERSWHENYOUPUTTHEMINTHEWEIGHTOFTHEWASHERSISTOBREAKTHEFORCEOFAANDTHEMAGNETANDSOTHEMOREWASHERSYOUPUTINTHETHETHEFORCEGETS

2024-10-27 18:24:20,248 (asr_inference:509) INFO: speech length: 166320
2024-10-27 18:24:26,638 (beam_search:428) INFO: decoder input length: 129
2024-10-27 18:24:26,638 (beam_search:429) INFO: max output length: 129
2024-10-27 18:24:26,638 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:27,270 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:27,270 (beam_search:476) INFO:  -3.40 * 1.0 =  -3.40 for ctc
2024-10-27 18:24:27,270 (beam_search:479) INFO: total log probability: -3.40
2024-10-27 18:24:27,270 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:24:27,270 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:27,270 (beam_search:483) INFO: best hypo: THEOFWASHERSWOULDGODOWNBECAUSEOFTHEWEIGHTWELLTHEFORCEWASEACHTIMESOTHEWEIGHTWOULDN'THAVETOBEASMUCH

2024-10-27 18:24:27,273 (asr_inference:509) INFO: speech length: 41440
2024-10-27 18:24:28,876 (beam_search:428) INFO: decoder input length: 31
2024-10-27 18:24:28,876 (beam_search:429) INFO: max output length: 31
2024-10-27 18:24:28,876 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:28,905 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:28,905 (beam_search:476) INFO:  -0.73 * 1.0 =  -0.73 for ctc
2024-10-27 18:24:28,905 (beam_search:479) INFO: total log probability: -0.73
2024-10-27 18:24:28,905 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:24:28,905 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:28,905 (beam_search:483) INFO: best hypo: IHAVEEDLEARNINGABOUT

2024-10-27 18:24:28,908 (asr_inference:509) INFO: speech length: 94896
2024-10-27 18:24:32,397 (beam_search:428) INFO: decoder input length: 73
2024-10-27 18:24:32,398 (beam_search:429) INFO: max output length: 73
2024-10-27 18:24:32,398 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:32,613 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:32,613 (beam_search:476) INFO:  -4.32 * 1.0 =  -4.32 for ctc
2024-10-27 18:24:32,614 (beam_search:479) INFO: total log probability: -4.32
2024-10-27 18:24:32,614 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:24:32,614 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:32,614 (beam_search:483) INFO: best hypo: UMWEABOXANDWEPUTMAGNETSSOMEWHEREDIDN'TTHINKPEOPLEWOULDFINDIT

2024-10-27 18:24:32,616 (asr_inference:509) INFO: speech length: 22336
2024-10-27 18:24:33,696 (beam_search:428) INFO: decoder input length: 16
2024-10-27 18:24:33,697 (beam_search:429) INFO: max output length: 16
2024-10-27 18:24:33,697 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:33,704 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:33,705 (beam_search:476) INFO:  -0.91 * 1.0 =  -0.91 for ctc
2024-10-27 18:24:33,705 (beam_search:479) INFO: total log probability: -0.91
2024-10-27 18:24:33,705 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 18:24:33,705 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:33,705 (beam_search:483) INFO: best hypo: A

2024-10-27 18:24:33,708 (asr_inference:509) INFO: speech length: 14192
2024-10-27 18:24:34,494 (beam_search:428) INFO: decoder input length: 10
2024-10-27 18:24:34,494 (beam_search:429) INFO: max output length: 10
2024-10-27 18:24:34,494 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:34,501 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:34,501 (beam_search:476) INFO:  -0.52 * 1.0 =  -0.52 for ctc
2024-10-27 18:24:34,501 (beam_search:479) INFO: total log probability: -0.52
2024-10-27 18:24:34,501 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:24:34,501 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:34,501 (beam_search:483) INFO: best hypo: A

2024-10-27 18:24:34,504 (asr_inference:509) INFO: speech length: 278784
2024-10-27 18:24:46,455 (beam_search:428) INFO: decoder input length: 217
2024-10-27 18:24:46,456 (beam_search:429) INFO: max output length: 217
2024-10-27 18:24:46,456 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:48,218 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:48,219 (beam_search:476) INFO:  -6.61 * 1.0 =  -6.61 for ctc
2024-10-27 18:24:48,219 (beam_search:479) INFO: total log probability: -6.61
2024-10-27 18:24:48,219 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:24:48,219 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:48,219 (beam_search:483) INFO: best hypo: UMWELLWEWITHPEOPLEANDWEUSEDMETALSORAANDWEITAROUNDTHEBOXANDTHENWEUMPUTITWEWHENWEWETHEMAGNETBEWEITONTHEPLACETHEONTHEBOXONTHEPAPER

2024-10-27 18:24:48,221 (asr_inference:509) INFO: speech length: 55840
2024-10-27 18:24:50,364 (beam_search:428) INFO: decoder input length: 43
2024-10-27 18:24:50,364 (beam_search:429) INFO: max output length: 43
2024-10-27 18:24:50,364 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:50,414 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:50,415 (beam_search:476) INFO:  -1.37 * 1.0 =  -1.37 for ctc
2024-10-27 18:24:50,415 (beam_search:479) INFO: total log probability: -1.37
2024-10-27 18:24:50,415 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:24:50,415 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:50,415 (beam_search:483) INFO: best hypo: THESTHEMAGNETTHEYWOULDUP

2024-10-27 18:24:50,417 (asr_inference:509) INFO: speech length: 164640
2024-10-27 18:24:57,092 (beam_search:428) INFO: decoder input length: 128
2024-10-27 18:24:57,092 (beam_search:429) INFO: max output length: 128
2024-10-27 18:24:57,092 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:57,448 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:57,448 (beam_search:476) INFO:  -1.46 * 1.0 =  -1.46 for ctc
2024-10-27 18:24:57,448 (beam_search:479) INFO: total log probability: -1.46
2024-10-27 18:24:57,448 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:24:57,448 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:57,448 (beam_search:483) INFO: best hypo: WELLTHEMAGNETISTOUMTHEYWOULDUPBECAUSEITWASGOINGINTOTHEUM

2024-10-27 18:24:57,451 (asr_inference:509) INFO: speech length: 60160
2024-10-27 18:24:59,617 (beam_search:428) INFO: decoder input length: 46
2024-10-27 18:24:59,617 (beam_search:429) INFO: max output length: 46
2024-10-27 18:24:59,617 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:59,679 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:59,679 (beam_search:476) INFO:  -3.34 * 1.0 =  -3.34 for ctc
2024-10-27 18:24:59,679 (beam_search:479) INFO: total log probability: -3.34
2024-10-27 18:24:59,679 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 18:24:59,679 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:59,679 (beam_search:483) INFO: best hypo: BECAUSEAMAGNET'STOANDSTEEL

2024-10-27 18:24:59,682 (asr_inference:509) INFO: speech length: 114624
2024-10-27 18:25:04,103 (beam_search:428) INFO: decoder input length: 89
2024-10-27 18:25:04,103 (beam_search:429) INFO: max output length: 89
2024-10-27 18:25:04,103 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:04,308 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:04,308 (beam_search:476) INFO:  -2.45 * 1.0 =  -2.45 for ctc
2024-10-27 18:25:04,308 (beam_search:479) INFO: total log probability: -2.45
2024-10-27 18:25:04,308 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:25:04,308 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:04,308 (beam_search:483) INFO: best hypo: UMNOITHASTOBEKINDOFALIKEPAPERORAVERY

2024-10-27 18:25:04,310 (asr_inference:509) INFO: speech length: 256352
2024-10-27 18:25:15,123 (beam_search:428) INFO: decoder input length: 199
2024-10-27 18:25:15,123 (beam_search:429) INFO: max output length: 199
2024-10-27 18:25:15,123 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:16,718 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:16,718 (beam_search:476) INFO:  -3.82 * 1.0 =  -3.82 for ctc
2024-10-27 18:25:16,718 (beam_search:479) INFO: total log probability: -3.82
2024-10-27 18:25:16,718 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:25:16,718 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:16,718 (beam_search:483) INFO: best hypo: UMLIKEIFITWASIT'TGOTHROUGHBECAUSEITWASTOOANDTHEMAUMMAGNETICCAN'TGOTHROUGHTHEBUTITCOULDGOTHROUGHAPIECEOFPAPERORBECAUSEITANDITWASN'TAS

2024-10-27 18:25:16,721 (asr_inference:509) INFO: speech length: 266800
2024-10-27 18:25:27,741 (beam_search:428) INFO: decoder input length: 207
2024-10-27 18:25:27,741 (beam_search:429) INFO: max output length: 207
2024-10-27 18:25:27,741 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:29,149 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:29,149 (beam_search:476) INFO:  -5.53 * 1.0 =  -5.53 for ctc
2024-10-27 18:25:29,149 (beam_search:479) INFO: total log probability: -5.53
2024-10-27 18:25:29,149 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:25:29,149 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:29,150 (beam_search:483) INFO: best hypo: ANDIFYOUINMOREWASHERSUMTHEMAGNETICTHEMAGNETTHEWASHERSWILLDOWNUMTHANTHEFORCEOFTHEMAGNETANDTHAT'LLCAUSETOMAKETHEMOVETHEBOTTOMBYTHEMAGNET

2024-10-27 18:25:29,153 (asr_inference:509) INFO: speech length: 467664
2024-10-27 18:25:51,592 (beam_search:428) INFO: decoder input length: 364
2024-10-27 18:25:51,592 (beam_search:429) INFO: max output length: 364
2024-10-27 18:25:51,592 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:54,999 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:54,999 (beam_search:476) INFO:  -7.13 * 1.0 =  -7.13 for ctc
2024-10-27 18:25:54,999 (beam_search:479) INFO: total log probability: -7.13
2024-10-27 18:25:54,999 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:25:54,999 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:54,999 (beam_search:483) INFO: best hypo: UMIT'SINGMETHATWITHZEROITWOULDGETUPTONINETEENWASHERSANDWITHONEIT'DUPTONINEWASHERSUMWITHTHREESPACERSITWOULDGETFIVEUMFOURITWOULDGETFIVEALSOANDWITHFIVEITWOULDGETTHREE

2024-10-27 18:25:55,003 (asr_inference:509) INFO: speech length: 109344
2024-10-27 18:25:59,090 (beam_search:428) INFO: decoder input length: 84
2024-10-27 18:25:59,090 (beam_search:429) INFO: max output length: 84
2024-10-27 18:25:59,090 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:59,313 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:59,313 (beam_search:476) INFO:  -1.80 * 1.0 =  -1.80 for ctc
2024-10-27 18:25:59,313 (beam_search:479) INFO: total log probability: -1.80
2024-10-27 18:25:59,313 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:25:59,313 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:59,313 (beam_search:483) INFO: best hypo: UMPROBABLYMAYBEABOUTORSEVENBECAUSEITWOULDHAVETOBETHANTHETHREE

2024-10-27 18:25:59,316 (asr_inference:509) INFO: speech length: 102832
2024-10-27 18:26:03,103 (beam_search:428) INFO: decoder input length: 79
2024-10-27 18:26:03,103 (beam_search:429) INFO: max output length: 79
2024-10-27 18:26:03,103 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:03,323 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:03,324 (beam_search:476) INFO:  -1.15 * 1.0 =  -1.15 for ctc
2024-10-27 18:26:03,324 (beam_search:479) INFO: total log probability: -1.15
2024-10-27 18:26:03,324 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:26:03,324 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:03,324 (beam_search:483) INFO: best hypo: ITWOULDBELESSWASHERSBUTLESSTHANTHEONEBECAUSETHERE'SMOREWASHERS

2024-10-27 18:26:03,327 (asr_inference:509) INFO: speech length: 229536
2024-10-27 18:26:12,781 (beam_search:428) INFO: decoder input length: 178
2024-10-27 18:26:12,781 (beam_search:429) INFO: max output length: 178
2024-10-27 18:26:12,781 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:13,963 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:13,963 (beam_search:476) INFO:  -7.04 * 1.0 =  -7.04 for ctc
2024-10-27 18:26:13,963 (beam_search:479) INFO: total log probability: -7.04
2024-10-27 18:26:13,963 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:26:13,964 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:13,964 (beam_search:483) INFO: best hypo: USINGTHEITUMKINDOFBECAUSEITWOULDTOTHEBUTTHERE'AOFTHINGSINTHEROOMANDITWASREALLYTOSOMETHINGMETALANDITWASN'TTOTHEUM

2024-10-27 18:26:13,966 (asr_inference:509) INFO: speech length: 305984
2024-10-27 18:26:26,827 (beam_search:428) INFO: decoder input length: 238
2024-10-27 18:26:26,827 (beam_search:429) INFO: max output length: 238
2024-10-27 18:26:26,828 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:28,471 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:28,472 (beam_search:476) INFO:  -4.41 * 1.0 =  -4.41 for ctc
2024-10-27 18:26:28,472 (beam_search:479) INFO: total log probability: -4.41
2024-10-27 18:26:28,472 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:26:28,472 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:28,472 (beam_search:483) INFO: best hypo: UMTHEISTOTHEORANDIT'SUSEDTOMAGNETICBUTTOIT'SIT'SMAGNETICANDUMITIT'SWHENIT'SUMIPUTWHEN

2024-10-27 18:26:28,475 (asr_inference:509) INFO: speech length: 134928
2024-10-27 18:26:33,666 (beam_search:428) INFO: decoder input length: 104
2024-10-27 18:26:33,666 (beam_search:429) INFO: max output length: 104
2024-10-27 18:26:33,666 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:34,072 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:34,072 (beam_search:476) INFO:  -3.03 * 1.0 =  -3.03 for ctc
2024-10-27 18:26:34,072 (beam_search:479) INFO: total log probability: -3.03
2024-10-27 18:26:34,072 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:26:34,072 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:34,072 (beam_search:483) INFO: best hypo: 'SBYTHANTHEMAGNETITWILLPOINTTOTHATINSTEADOFTHEMAGNETANDTHAT'SWHATWASME

2024-10-27 18:26:34,074 (asr_inference:509) INFO: speech length: 199184
2024-10-27 18:26:41,969 (beam_search:428) INFO: decoder input length: 155
2024-10-27 18:26:41,970 (beam_search:429) INFO: max output length: 155
2024-10-27 18:26:41,970 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:42,415 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:42,415 (beam_search:476) INFO:  -2.90 * 1.0 =  -2.90 for ctc
2024-10-27 18:26:42,415 (beam_search:479) INFO: total log probability: -2.90
2024-10-27 18:26:42,415 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:26:42,415 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:42,415 (beam_search:483) INFO: best hypo: UMTHESTHANTHEANDAOFSTEELMAYBEORSOMETHINGLIKEMAYBELIKEOR

2024-10-27 18:26:42,418 (asr_inference:509) INFO: speech length: 152832
2024-10-27 18:26:48,185 (beam_search:428) INFO: decoder input length: 118
2024-10-27 18:26:48,185 (beam_search:429) INFO: max output length: 118
2024-10-27 18:26:48,185 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:48,607 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:48,607 (beam_search:476) INFO:  -2.45 * 1.0 =  -2.45 for ctc
2024-10-27 18:26:48,607 (beam_search:479) INFO: total log probability: -2.45
2024-10-27 18:26:48,607 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:26:48,607 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:48,607 (beam_search:483) INFO: best hypo: APAPERBECAUSEIFITTOTHATWHERETHEMAGNETWASTHENYOU'DKNOWWHEREITWASYEAH

2024-10-27 18:26:48,610 (asr_inference:509) INFO: speech length: 176032
2024-10-27 18:26:55,452 (beam_search:428) INFO: decoder input length: 137
2024-10-27 18:26:55,452 (beam_search:429) INFO: max output length: 137
2024-10-27 18:26:55,452 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:55,976 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:55,976 (beam_search:476) INFO:  -1.74 * 1.0 =  -1.74 for ctc
2024-10-27 18:26:55,976 (beam_search:479) INFO: total log probability: -1.74
2024-10-27 18:26:55,977 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:26:55,977 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:55,977 (beam_search:483) INFO: best hypo: WELLUMIFTHEMAGNETCANATTRACTTOTHEPAPERCLIPITWILLSTICKTOTHEPAPERCLIPOFANDUM

2024-10-27 18:26:55,979 (asr_inference:509) INFO: speech length: 128800
2024-10-27 18:27:00,878 (beam_search:428) INFO: decoder input length: 100
2024-10-27 18:27:00,879 (beam_search:429) INFO: max output length: 100
2024-10-27 18:27:00,879 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:01,353 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:01,353 (beam_search:476) INFO:  -3.59 * 1.0 =  -3.59 for ctc
2024-10-27 18:27:01,353 (beam_search:479) INFO: total log probability: -3.59
2024-10-27 18:27:01,353 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:27:01,353 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:01,353 (beam_search:483) INFO: best hypo: ANDWHENITTOTHEPAPERCLIPYOUCANTHEFORCEWHENYOUTHEPAPERCLIPANDTHENYOU'LLKNOWTHATTHEMAGNET'STHERE

2024-10-27 18:27:01,355 (asr_inference:509) INFO: speech length: 285808
2024-10-27 18:27:13,146 (beam_search:428) INFO: decoder input length: 222
2024-10-27 18:27:13,147 (beam_search:429) INFO: max output length: 222
2024-10-27 18:27:13,147 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:13,886 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:13,886 (beam_search:476) INFO:  -5.03 * 1.0 =  -5.03 for ctc
2024-10-27 18:27:13,886 (beam_search:479) INFO: total log probability: -5.03
2024-10-27 18:27:13,886 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:27:13,886 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:13,886 (beam_search:483) INFO: best hypo: OFCAUSEOPPOSITEATTRACTANDTHESIDEANDSOIFUHIDON'TKNOWIFTHEUM

2024-10-27 18:27:13,889 (asr_inference:509) INFO: speech length: 439088
2024-10-27 18:27:34,607 (beam_search:428) INFO: decoder input length: 342
2024-10-27 18:27:34,608 (beam_search:429) INFO: max output length: 342
2024-10-27 18:27:34,608 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:38,233 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:38,233 (beam_search:476) INFO:  -8.72 * 1.0 =  -8.72 for ctc
2024-10-27 18:27:38,233 (beam_search:479) INFO: total log probability: -8.72
2024-10-27 18:27:38,234 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:27:38,234 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:38,234 (beam_search:483) INFO: best hypo: WELLAMAGNETHASTWODIFFERENTSIDESNORTHANDANDAANDANORTHORANDABUTAANDAANDAANDANORTHSIDEBECAUSETHEY'RETHESAMESIDEOFTHEMAGNETANDTHEYWON'TSTICKBECAUSETHEY'RETHESAMEBUTDIFFERENTWILLUMUHSTICK

2024-10-27 18:27:38,237 (asr_inference:509) INFO: speech length: 161504
2024-10-27 18:27:44,652 (beam_search:428) INFO: decoder input length: 125
2024-10-27 18:27:44,652 (beam_search:429) INFO: max output length: 125
2024-10-27 18:27:44,652 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:45,126 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:45,127 (beam_search:476) INFO:  -4.24 * 1.0 =  -4.24 for ctc
2024-10-27 18:27:45,127 (beam_search:479) INFO: total log probability: -4.24
2024-10-27 18:27:45,127 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:27:45,127 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:45,127 (beam_search:483) INFO: best hypo: THEMAGNETSAREANDORANDTHEY'LLUMATTRACTBUTIFTHEY'REANDORAND'LL

2024-10-27 18:27:45,129 (asr_inference:509) INFO: speech length: 10144
2024-10-27 18:27:45,832 (beam_search:428) INFO: decoder input length: 7
2024-10-27 18:27:45,832 (beam_search:429) INFO: max output length: 7
2024-10-27 18:27:45,832 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:45,837 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:45,837 (beam_search:476) INFO:  -0.31 * 1.0 =  -0.31 for ctc
2024-10-27 18:27:45,837 (beam_search:479) INFO: total log probability: -0.31
2024-10-27 18:27:45,837 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:27:45,837 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:45,837 (beam_search:483) INFO: best hypo: YOU

2024-10-27 18:27:45,839 (asr_inference:509) INFO: speech length: 168576
2024-10-27 18:27:52,656 (beam_search:428) INFO: decoder input length: 131
2024-10-27 18:27:52,656 (beam_search:429) INFO: max output length: 131
2024-10-27 18:27:52,656 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:53,257 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:53,257 (beam_search:476) INFO:  -2.29 * 1.0 =  -2.29 for ctc
2024-10-27 18:27:53,257 (beam_search:479) INFO: total log probability: -2.29
2024-10-27 18:27:53,257 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:27:53,257 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:53,257 (beam_search:483) INFO: best hypo: WELLINSCIENCEWE'VEBEENLEARNINGABOUTANDMAGNETISMANDSOWE'VEBEENOUTHOWWECANUSEABATTERYTOLIGHT

2024-10-27 18:27:53,260 (asr_inference:509) INFO: speech length: 455360
2024-10-27 18:28:15,190 (beam_search:428) INFO: decoder input length: 355
2024-10-27 18:28:15,190 (beam_search:429) INFO: max output length: 355
2024-10-27 18:28:15,190 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:20,057 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:20,058 (beam_search:476) INFO:  -6.41 * 1.0 =  -6.41 for ctc
2024-10-27 18:28:20,058 (beam_search:479) INFO: total log probability: -6.41
2024-10-27 18:28:20,058 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:28:20,058 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:20,058 (beam_search:483) INFO: best hypo: UMSOWEUSEDAMOTORANDWEEDUPTOACIRCUITANDTHENUMWEHADWIRESANDWEHADAUMAMOTORANDWEPUTTHEMOTORINTHEONTHEANDTHEBATTERYINTHEFORTHEBATTERYANDTHENUMWETHEWIRESTHROUGHTHEANDTHEUMONANDOFFSWITCHANDTHENWECOULDTURNTHELIGHTBULBONOROFF

2024-10-27 18:28:20,061 (asr_inference:509) INFO: speech length: 266176
2024-10-27 18:28:31,343 (beam_search:428) INFO: decoder input length: 207
2024-10-27 18:28:31,343 (beam_search:429) INFO: max output length: 207
2024-10-27 18:28:31,343 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:32,862 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:32,862 (beam_search:476) INFO:  -5.95 * 1.0 =  -5.95 for ctc
2024-10-27 18:28:32,862 (beam_search:479) INFO: total log probability: -5.95
2024-10-27 18:28:32,862 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:28:32,862 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:32,862 (beam_search:483) INFO: best hypo: WELLTHEREWASATHATHADINITWHERETHEBATTERYWOULDFITINANDTHEWOULDFITINANDANDWOULDFITANDTHENOTHERTHATWEDIDN'TUSEWEWHERETHEYWENTBECAUSETHEYWOULD

2024-10-27 18:28:32,865 (asr_inference:509) INFO: speech length: 473008
2024-10-27 18:28:56,030 (beam_search:428) INFO: decoder input length: 369
2024-10-27 18:28:56,030 (beam_search:429) INFO: max output length: 369
2024-10-27 18:28:56,030 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:01,537 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:01,538 (beam_search:476) INFO:  -8.30 * 1.0 =  -8.30 for ctc
2024-10-27 18:29:01,538 (beam_search:479) INFO: total log probability: -8.30
2024-10-27 18:29:01,538 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:29:01,538 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:01,538 (beam_search:483) INFO: best hypo: YOUWOULDN'TUSETHEMOTORYOUWOULDTAKETHEMOTORAWAYANDTHENPUTTHEUMLIGHTBULBONALIGHTBULBCIRCUITANDTHATCOULDGOINTHEINBETWEENTHEMOTORANDTHEUMBATTERYANDSOTHENYOUWOULDTAKETHEWIRESANDATTACHITTOTHEUMLIGHTBULBSUMLITTLEPLACETHATITWOULDGOONANDTHENITWOULDTURNONIFYOUHADTHEONANDOFFSWITCH

2024-10-27 18:29:01,540 (asr_inference:509) INFO: speech length: 162592
2024-10-27 18:29:07,976 (beam_search:428) INFO: decoder input length: 126
2024-10-27 18:29:07,976 (beam_search:429) INFO: max output length: 126
2024-10-27 18:29:07,976 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:08,463 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:08,464 (beam_search:476) INFO:  -2.82 * 1.0 =  -2.82 for ctc
2024-10-27 18:29:08,464 (beam_search:479) INFO: total log probability: -2.82
2024-10-27 18:29:08,464 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:29:08,464 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:08,464 (beam_search:483) INFO: best hypo: YOUWOULDTHEWIRESCONNECTEDTOTHETOTHEBATTERYANDTHENTHEBATTERYWOULDMAKEITRUNUMINACIRCLE

2024-10-27 18:29:08,467 (asr_inference:509) INFO: speech length: 110368
2024-10-27 18:29:12,599 (beam_search:428) INFO: decoder input length: 85
2024-10-27 18:29:12,599 (beam_search:429) INFO: max output length: 85
2024-10-27 18:29:12,599 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:12,845 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:12,845 (beam_search:476) INFO:  -0.71 * 1.0 =  -0.71 for ctc
2024-10-27 18:29:12,845 (beam_search:479) INFO: total log probability: -0.71
2024-10-27 18:29:12,845 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 18:29:12,845 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:12,845 (beam_search:483) INFO: best hypo: THEWIRESTOTHEBATTERYANDTHENTHEOTHERWIRESTOTHEONANDOFFSWITCH

2024-10-27 18:29:12,847 (asr_inference:509) INFO: speech length: 286272
2024-10-27 18:29:24,875 (beam_search:428) INFO: decoder input length: 223
2024-10-27 18:29:24,875 (beam_search:429) INFO: max output length: 223
2024-10-27 18:29:24,875 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:27,146 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:27,147 (beam_search:476) INFO:  -7.26 * 1.0 =  -7.26 for ctc
2024-10-27 18:29:27,147 (beam_search:479) INFO: total log probability: -7.26
2024-10-27 18:29:27,147 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:29:27,147 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:27,147 (beam_search:483) INFO: best hypo: THEANDSWITCHISIMPORTANTBECAUSEIFYOUDON'THAVETHEANDOFFSWITCHCONNECTEDTHEENERGY'SJUSTGONNAOFFTHEENDORITWOULDJUSTBUTTHENIFIT'SCONNECTEDITGOESBACKTHROUGHTHEWIRESANDONTOTHEMOTORWHICHMAKESTHEMOTORRUN

2024-10-27 18:29:27,149 (asr_inference:509) INFO: speech length: 137664
2024-10-27 18:29:32,172 (beam_search:428) INFO: decoder input length: 107
2024-10-27 18:29:32,172 (beam_search:429) INFO: max output length: 107
2024-10-27 18:29:32,172 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:32,624 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:32,624 (beam_search:476) INFO:  -2.74 * 1.0 =  -2.74 for ctc
2024-10-27 18:29:32,624 (beam_search:479) INFO: total log probability: -2.74
2024-10-27 18:29:32,624 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:29:32,624 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:32,624 (beam_search:483) INFO: best hypo: THATMEANSTHATTHEENERGYTHROUGHTHEONANDANDIFYOUDON'THAVETHEANDCONNECTEDITWON'TTHROUGH

2024-10-27 18:29:32,626 (asr_inference:509) INFO: speech length: 122416
2024-10-27 18:29:37,122 (beam_search:428) INFO: decoder input length: 95
2024-10-27 18:29:37,122 (beam_search:429) INFO: max output length: 95
2024-10-27 18:29:37,123 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:37,549 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:37,549 (beam_search:476) INFO:  -3.94 * 1.0 =  -3.94 for ctc
2024-10-27 18:29:37,549 (beam_search:479) INFO: total log probability: -3.94
2024-10-27 18:29:37,549 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:29:37,549 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:37,549 (beam_search:483) INFO: best hypo: WELLIFTHESWITCHISINTHEWRONGTHEMOTOR'SNOTGONNARUNANDIFIT'SINTHERIGHTTHEMOTORRUN

2024-10-27 18:29:37,552 (asr_inference:509) INFO: speech length: 123312
2024-10-27 18:29:42,148 (beam_search:428) INFO: decoder input length: 95
2024-10-27 18:29:42,148 (beam_search:429) INFO: max output length: 95
2024-10-27 18:29:42,148 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:42,510 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:42,510 (beam_search:476) INFO:  -2.25 * 1.0 =  -2.25 for ctc
2024-10-27 18:29:42,511 (beam_search:479) INFO: total log probability: -2.25
2024-10-27 18:29:42,511 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:29:42,511 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:42,511 (beam_search:483) INFO: best hypo: IFTHEMOTOR'SUMANDIFTHEWIRES'TCONNECTEDTOTHEBATTERYITALSOWON'TRUN

2024-10-27 18:29:42,513 (asr_inference:509) INFO: speech length: 32064
2024-10-27 18:29:43,882 (beam_search:428) INFO: decoder input length: 24
2024-10-27 18:29:43,882 (beam_search:429) INFO: max output length: 24
2024-10-27 18:29:43,882 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:43,913 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:43,913 (beam_search:476) INFO:  -0.91 * 1.0 =  -0.91 for ctc
2024-10-27 18:29:43,913 (beam_search:479) INFO: total log probability: -0.91
2024-10-27 18:29:43,913 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:29:43,913 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:43,913 (beam_search:483) INFO: best hypo: IT'SNOTINTHERIGHT

2024-10-27 18:29:43,916 (asr_inference:509) INFO: speech length: 385152
2024-10-27 18:30:01,116 (beam_search:428) INFO: decoder input length: 300
2024-10-27 18:30:01,116 (beam_search:429) INFO: max output length: 300
2024-10-27 18:30:01,116 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:04,409 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:04,409 (beam_search:476) INFO:  -5.13 * 1.0 =  -5.13 for ctc
2024-10-27 18:30:04,409 (beam_search:479) INFO: total log probability: -5.13
2024-10-27 18:30:04,409 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:30:04,409 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:04,409 (beam_search:483) INFO: best hypo: BECAUSEIT'SOUTANDNOTCONNECTEDTOTHEWIREANDWHENIT'SCONNECTEDTOTHEWIRETHEENERGYTHROUGHINTOTHEUMWIRETOTHEMOTORBUTIT'SNOTUMITTHEENERGYJUSTANDITCAN'TGOANYWHERESOITCAN'TMAKETHEMOTORRUN

2024-10-27 18:30:04,412 (asr_inference:509) INFO: speech length: 145504
2024-10-27 18:30:09,952 (beam_search:428) INFO: decoder input length: 113
2024-10-27 18:30:09,952 (beam_search:429) INFO: max output length: 113
2024-10-27 18:30:09,952 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:10,173 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:10,174 (beam_search:476) INFO:  -1.44 * 1.0 =  -1.44 for ctc
2024-10-27 18:30:10,174 (beam_search:479) INFO: total log probability: -1.44
2024-10-27 18:30:10,174 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:30:10,174 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:10,174 (beam_search:483) INFO: best hypo: UMALSOINSCIENCEWEHAVEAUMOHWEDO

2024-10-27 18:30:10,177 (asr_inference:509) INFO: speech length: 199504
2024-10-27 18:30:18,192 (beam_search:428) INFO: decoder input length: 155
2024-10-27 18:30:18,192 (beam_search:429) INFO: max output length: 155
2024-10-27 18:30:18,192 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:19,005 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:19,005 (beam_search:476) INFO:  -3.19 * 1.0 =  -3.19 for ctc
2024-10-27 18:30:19,005 (beam_search:479) INFO: total log probability: -3.19
2024-10-27 18:30:19,005 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:30:19,005 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:19,005 (beam_search:483) INFO: best hypo: WELLTHELIGHTBULBSINTHELIGHTBULBCIRCUITTHINGANDTHEREASONIT'SNOTLIGHTINGUPISBECAUSETHEONISISOFFANDNOTON

2024-10-27 18:30:19,009 (asr_inference:509) INFO: speech length: 304960
2024-10-27 18:30:32,253 (beam_search:428) INFO: decoder input length: 237
2024-10-27 18:30:32,253 (beam_search:429) INFO: max output length: 237
2024-10-27 18:30:32,253 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:34,156 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:34,156 (beam_search:476) INFO:  -5.60 * 1.0 =  -5.60 for ctc
2024-10-27 18:30:34,156 (beam_search:479) INFO: total log probability: -5.60
2024-10-27 18:30:34,156 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:30:34,156 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:34,156 (beam_search:483) INFO: best hypo: THEELECTRICITYFLOWSTHROUGHTHECIRCUITBECAUSEITCOMESFROMTHEBATTERYANDTHENITCOMESOUTTHROUGHTHEWIRESTHEANDTHENTOTHETHATTHELIGHTBULBSONANDTHENITUPINTOTHELIGHTBULBANDLIGHTSIT

2024-10-27 18:30:34,160 (asr_inference:509) INFO: speech length: 61536
2024-10-27 18:30:36,510 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:30:36,511 (beam_search:429) INFO: max output length: 47
2024-10-27 18:30:36,511 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:36,590 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:36,590 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 18:30:36,590 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 18:30:36,590 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:30:36,590 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:36,591 (beam_search:483) INFO: best hypo: ITFROMTHENEGATIVESIDETOTHEPOSITIVESIDE

2024-10-27 18:30:36,593 (asr_inference:509) INFO: speech length: 79424
2024-10-27 18:30:39,585 (beam_search:428) INFO: decoder input length: 61
2024-10-27 18:30:39,586 (beam_search:429) INFO: max output length: 61
2024-10-27 18:30:39,586 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:39,738 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:39,738 (beam_search:476) INFO:  -1.20 * 1.0 =  -1.20 for ctc
2024-10-27 18:30:39,738 (beam_search:479) INFO: total log probability: -1.20
2024-10-27 18:30:39,738 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:30:39,738 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:39,738 (beam_search:483) INFO: best hypo: IT'STHEWIRETHAT'STHEBULBTOMAKETHEBULBLIGHTUP

2024-10-27 18:30:39,740 (asr_inference:509) INFO: speech length: 195456
2024-10-27 18:30:47,370 (beam_search:428) INFO: decoder input length: 152
2024-10-27 18:30:47,370 (beam_search:429) INFO: max output length: 152
2024-10-27 18:30:47,370 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:48,322 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:48,322 (beam_search:476) INFO:  -1.67 * 1.0 =  -1.67 for ctc
2024-10-27 18:30:48,322 (beam_search:479) INFO: total log probability: -1.67
2024-10-27 18:30:48,322 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:30:48,322 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:48,322 (beam_search:483) INFO: best hypo: THATYOUCANSEETHEWIRETHAT'SCONNECTEDTOTHEOFTHEBULBANDTHENITCONNECTSTOTHEBOTTOMOFTHEBULBANDTHAT'SHOWITLIGHTSIT

2024-10-27 18:30:48,324 (asr_inference:509) INFO: speech length: 194448
2024-10-27 18:30:56,012 (beam_search:428) INFO: decoder input length: 151
2024-10-27 18:30:56,012 (beam_search:429) INFO: max output length: 151
2024-10-27 18:30:56,012 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:56,702 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:56,702 (beam_search:476) INFO:  -3.06 * 1.0 =  -3.06 for ctc
2024-10-27 18:30:56,702 (beam_search:479) INFO: total log probability: -3.06
2024-10-27 18:30:56,702 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:30:56,702 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:56,702 (beam_search:483) INFO: best hypo: UMTHEISACIRCLEANDTHEENERGYWOULDFLOWINACIRCUITWHICHMEANSTHEENERGYWOULDFLOWINATHROUGHTHELIGHTBULB

2024-10-27 18:30:56,705 (asr_inference:509) INFO: speech length: 357280
2024-10-27 18:31:12,540 (beam_search:428) INFO: decoder input length: 278
2024-10-27 18:31:12,540 (beam_search:429) INFO: max output length: 278
2024-10-27 18:31:12,540 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:15,061 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:15,061 (beam_search:476) INFO:  -4.79 * 1.0 =  -4.79 for ctc
2024-10-27 18:31:15,061 (beam_search:479) INFO: total log probability: -4.79
2024-10-27 18:31:15,061 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:31:15,061 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:15,062 (beam_search:483) INFO: best hypo: NOYOUCAN'TPUTTHEWIRESONTHEBULBYOUHAVETOPUTTHEMATTHEBOTTOMTHEUMWHERETHELITTLECYLINDERISORELSEBECAUSETHAT'SWHERETHEWIRECONNECTEDSOTHAT'SHOWITWILLLIGHTTHELIGHTBULB

2024-10-27 18:31:15,064 (asr_inference:509) INFO: speech length: 45616
2024-10-27 18:31:16,846 (beam_search:428) INFO: decoder input length: 35
2024-10-27 18:31:16,847 (beam_search:429) INFO: max output length: 35
2024-10-27 18:31:16,847 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:16,895 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:16,896 (beam_search:476) INFO:  -1.41 * 1.0 =  -1.41 for ctc
2024-10-27 18:31:16,896 (beam_search:479) INFO: total log probability: -1.41
2024-10-27 18:31:16,896 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:31:16,896 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:16,896 (beam_search:483) INFO: best hypo: YOUWOULDBOTHATTHEBOTTOMOFTHE

2024-10-27 18:31:16,899 (asr_inference:509) INFO: speech length: 142288
2024-10-27 18:31:22,276 (beam_search:428) INFO: decoder input length: 110
2024-10-27 18:31:22,276 (beam_search:429) INFO: max output length: 110
2024-10-27 18:31:22,276 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:22,648 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:22,648 (beam_search:476) INFO:  -3.34 * 1.0 =  -3.34 for ctc
2024-10-27 18:31:22,648 (beam_search:479) INFO: total log probability: -3.34
2024-10-27 18:31:22,648 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:31:22,648 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:22,648 (beam_search:483) INFO: best hypo: ITWOULDPROBABLYLIGHTITBUTIT'SNOTTOUCHINGUMYOUMIGHTHAVEATOLIGHTTHE

2024-10-27 18:31:22,650 (asr_inference:509) INFO: speech length: 73872
2024-10-27 18:31:25,393 (beam_search:428) INFO: decoder input length: 57
2024-10-27 18:31:25,393 (beam_search:429) INFO: max output length: 57
2024-10-27 18:31:25,393 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:25,533 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:25,533 (beam_search:476) INFO:  -0.89 * 1.0 =  -0.89 for ctc
2024-10-27 18:31:25,533 (beam_search:479) INFO: total log probability: -0.89
2024-10-27 18:31:25,533 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:31:25,533 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:25,533 (beam_search:483) INFO: best hypo: ONE'SCONNECTEDATTHEBOTTOMANDONEWIRE'SCONNECTEDATTHESIDE

2024-10-27 18:31:25,536 (asr_inference:509) INFO: speech length: 177760
2024-10-27 18:31:32,475 (beam_search:428) INFO: decoder input length: 138
2024-10-27 18:31:32,475 (beam_search:429) INFO: max output length: 138
2024-10-27 18:31:32,475 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:32,900 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:32,900 (beam_search:476) INFO:  -3.01 * 1.0 =  -3.01 for ctc
2024-10-27 18:31:32,900 (beam_search:479) INFO: total log probability: -3.01
2024-10-27 18:31:32,900 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:31:32,900 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:32,900 (beam_search:483) INFO: best hypo: UHWEMAKEUMWETOOKUMTHEONEIS'TDTOBEANOTHERWIRE

2024-10-27 18:31:32,902 (asr_inference:509) INFO: speech length: 204544
2024-10-27 18:31:41,009 (beam_search:428) INFO: decoder input length: 159
2024-10-27 18:31:41,009 (beam_search:429) INFO: max output length: 159
2024-10-27 18:31:41,009 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:41,967 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:41,967 (beam_search:476) INFO:  -7.11 * 1.0 =  -7.11 for ctc
2024-10-27 18:31:41,967 (beam_search:479) INFO: total log probability: -7.11
2024-10-27 18:31:41,967 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:31:41,967 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:41,967 (beam_search:483) INFO: best hypo: THEDAROUNDTHESITANDUMTHATKINDITOTHEREITKINDOFWOULDBEJUSTITWOULDN'TBEAMAGNETIFITWASAMAGNETAT

2024-10-27 18:31:41,970 (asr_inference:509) INFO: speech length: 117680
2024-10-27 18:31:46,283 (beam_search:428) INFO: decoder input length: 91
2024-10-27 18:31:46,283 (beam_search:429) INFO: max output length: 91
2024-10-27 18:31:46,283 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:46,541 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:46,541 (beam_search:476) INFO:  -1.75 * 1.0 =  -1.75 for ctc
2024-10-27 18:31:46,541 (beam_search:479) INFO: total log probability: -1.75
2024-10-27 18:31:46,541 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:31:46,541 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:46,541 (beam_search:483) INFO: best hypo: UMYEAHCAUSEWETRIEDTHATATIMESANDITDIDN'TREALLYOUTWELL

2024-10-27 18:31:46,543 (asr_inference:509) INFO: speech length: 52928
2024-10-27 18:31:48,498 (beam_search:428) INFO: decoder input length: 40
2024-10-27 18:31:48,498 (beam_search:429) INFO: max output length: 40
2024-10-27 18:31:48,498 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:48,548 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:48,548 (beam_search:476) INFO:  -1.91 * 1.0 =  -1.91 for ctc
2024-10-27 18:31:48,548 (beam_search:479) INFO: total log probability: -1.91
2024-10-27 18:31:48,548 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:31:48,548 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:48,548 (beam_search:483) INFO: best hypo: YESWEDIDMAKEITAMAGNET

2024-10-27 18:31:48,550 (asr_inference:509) INFO: speech length: 14496
2024-10-27 18:31:49,282 (beam_search:428) INFO: decoder input length: 10
2024-10-27 18:31:49,282 (beam_search:429) INFO: max output length: 10
2024-10-27 18:31:49,282 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:49,289 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:49,289 (beam_search:476) INFO:  -0.64 * 1.0 =  -0.64 for ctc
2024-10-27 18:31:49,289 (beam_search:479) INFO: total log probability: -0.64
2024-10-27 18:31:49,289 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:31:49,289 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:49,289 (beam_search:483) INFO: best hypo: THE

2024-10-27 18:31:49,291 (asr_inference:509) INFO: speech length: 401072
2024-10-27 18:32:09,470 (beam_search:428) INFO: decoder input length: 312
2024-10-27 18:32:09,470 (beam_search:429) INFO: max output length: 312
2024-10-27 18:32:09,470 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:12,107 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:12,108 (beam_search:476) INFO:  -6.87 * 1.0 =  -6.87 for ctc
2024-10-27 18:32:12,108 (beam_search:479) INFO: total log probability: -6.87
2024-10-27 18:32:12,108 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:32:12,108 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:12,108 (beam_search:483) INFO: best hypo: MAYBEELECTRICITYWEDIDN'TREALLYTALKABOUTTHISBUTITKINDOFINTOUMITLIKETHROUGHTHEWIREITALITTLEBITMAGNETISMWELLIFIT'SONAANAILOBJECTTHENWOULDITBETOMAGNETISMTHROUGHIT

2024-10-27 18:32:12,110 (asr_inference:509) INFO: speech length: 48208
2024-10-27 18:32:13,947 (beam_search:428) INFO: decoder input length: 37
2024-10-27 18:32:13,947 (beam_search:429) INFO: max output length: 37
2024-10-27 18:32:13,947 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:13,981 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:13,981 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 18:32:13,981 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 18:32:13,981 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:32:13,981 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:13,981 (beam_search:483) INFO: best hypo: SMALLTHEY'RECALLED

2024-10-27 18:32:13,984 (asr_inference:509) INFO: speech length: 272320
2024-10-27 18:32:25,612 (beam_search:428) INFO: decoder input length: 212
2024-10-27 18:32:25,613 (beam_search:429) INFO: max output length: 212
2024-10-27 18:32:25,613 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:26,961 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:26,961 (beam_search:476) INFO:  -3.62 * 1.0 =  -3.62 for ctc
2024-10-27 18:32:26,961 (beam_search:479) INFO: total log probability: -3.62
2024-10-27 18:32:26,961 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:32:26,961 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:26,961 (beam_search:483) INFO: best hypo: LIKEINAIT'LLUPWITHAELECTROMAGNETANDTHENWHENITNEEDSTODROPEMELSETHENTHEYUMTHEYLIKECONNECTASWITCHLIKEWEDIDONTHISANDITWILL

2024-10-27 18:32:26,964 (asr_inference:509) INFO: speech length: 41664
2024-10-27 18:32:28,586 (beam_search:428) INFO: decoder input length: 32
2024-10-27 18:32:28,587 (beam_search:429) INFO: max output length: 32
2024-10-27 18:32:28,587 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:28,609 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:28,609 (beam_search:476) INFO:  -1.12 * 1.0 =  -1.12 for ctc
2024-10-27 18:32:28,609 (beam_search:479) INFO: total log probability: -1.12
2024-10-27 18:32:28,609 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:32:28,609 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:28,609 (beam_search:483) INFO: best hypo: NOWOULDNOT

2024-10-27 18:32:28,612 (asr_inference:509) INFO: speech length: 180336
2024-10-27 18:32:35,448 (beam_search:428) INFO: decoder input length: 140
2024-10-27 18:32:35,449 (beam_search:429) INFO: max output length: 140
2024-10-27 18:32:35,449 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:35,985 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:35,986 (beam_search:476) INFO:  -2.43 * 1.0 =  -2.43 for ctc
2024-10-27 18:32:35,986 (beam_search:479) INFO: total log probability: -2.43
2024-10-27 18:32:35,986 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:32:35,986 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:35,986 (beam_search:483) INFO: best hypo: THATISTHEELECTRICITYWOULDGOTOTHEANDTHENITHASELECTRICITYINITANDITWILLPICKUPMETAL

2024-10-27 18:32:35,988 (asr_inference:509) INFO: speech length: 106240
2024-10-27 18:32:40,080 (beam_search:428) INFO: decoder input length: 82
2024-10-27 18:32:40,080 (beam_search:429) INFO: max output length: 82
2024-10-27 18:32:40,080 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:40,332 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:40,332 (beam_search:476) INFO:  -3.02 * 1.0 =  -3.02 for ctc
2024-10-27 18:32:40,332 (beam_search:479) INFO: total log probability: -3.02
2024-10-27 18:32:40,332 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:32:40,332 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:40,332 (beam_search:483) INFO: best hypo: THEWEDIDITISOURSWITCHWASALITTLEBITLONGERWELLYEAHUHYOUCOULD

2024-10-27 18:32:40,335 (asr_inference:509) INFO: speech length: 352320
2024-10-27 18:32:56,801 (beam_search:428) INFO: decoder input length: 274
2024-10-27 18:32:56,801 (beam_search:429) INFO: max output length: 274
2024-10-27 18:32:56,801 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:59,282 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:59,282 (beam_search:476) INFO:  -5.60 * 1.0 =  -5.60 for ctc
2024-10-27 18:32:59,282 (beam_search:479) INFO: total log probability: -5.60
2024-10-27 18:32:59,282 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:32:59,282 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:59,282 (beam_search:483) INFO: best hypo: YOUCOULDITALOTMOREANDUMYOUCOULDTHENYOUCANUMUHIFYOUWRAPITALOTMORETHENITWILLITMORESOYOUYOUCANMAKEITBUTIFIT'SLYTHENIT'SJUSTBEVERYIT

2024-10-27 18:32:59,285 (asr_inference:509) INFO: speech length: 321760
2024-10-27 18:33:14,221 (beam_search:428) INFO: decoder input length: 250
2024-10-27 18:33:14,221 (beam_search:429) INFO: max output length: 250
2024-10-27 18:33:14,221 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:17,038 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:17,038 (beam_search:476) INFO:  -5.08 * 1.0 =  -5.08 for ctc
2024-10-27 18:33:17,038 (beam_search:479) INFO: total log probability: -5.08
2024-10-27 18:33:17,038 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:33:17,038 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:17,038 (beam_search:483) INFO: best hypo: BECAUSEIT'SITMOREANDIT'SCAUSEWHENWIREDAROUNDABARITWON'TREALLYGOTHROUGHBUTIFTHERE'SSOMETHINGAROUNDITITWILLBEINGITSOTHATITJUSTSOIT'LLGOTHROUGHTHATANDNOTNOTBETOJUSTGETAWAYFROMIT

2024-10-27 18:33:17,043 (asr_inference:509) INFO: speech length: 262400
2024-10-27 18:33:28,337 (beam_search:428) INFO: decoder input length: 204
2024-10-27 18:33:28,338 (beam_search:429) INFO: max output length: 204
2024-10-27 18:33:28,338 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:30,232 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:30,232 (beam_search:476) INFO:  -9.26 * 1.0 =  -9.26 for ctc
2024-10-27 18:33:30,232 (beam_search:479) INFO: total log probability: -9.26
2024-10-27 18:33:30,232 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:33:30,232 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:30,232 (beam_search:483) INFO: best hypo: ITWOULDN'TASWELLITWOULDBEANDMAGNETISMBECAUSEIFYOUONLYDHALFOFITIT'SGONNAGOKINDITIT'SNOTGONNABETHROUGHANDIT'SJUSTGONNALIKEDROPALLTHEMAGNETSIT'SONLYA

2024-10-27 18:33:30,235 (asr_inference:509) INFO: speech length: 135248
2024-10-27 18:33:35,251 (beam_search:428) INFO: decoder input length: 105
2024-10-27 18:33:35,251 (beam_search:429) INFO: max output length: 105
2024-10-27 18:33:35,251 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:35,467 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:35,467 (beam_search:476) INFO:  -2.45 * 1.0 =  -2.45 for ctc
2024-10-27 18:33:35,467 (beam_search:479) INFO: total log probability: -2.45
2024-10-27 18:33:35,467 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:33:35,467 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:35,467 (beam_search:483) INFO: best hypo: UMCONNECTSITSOTHEMAGNETCANNOTSOTHE

2024-10-27 18:33:35,469 (asr_inference:509) INFO: speech length: 93440
2024-10-27 18:33:38,971 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:33:38,972 (beam_search:429) INFO: max output length: 72
2024-10-27 18:33:38,972 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:39,093 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:39,094 (beam_search:476) INFO:  -1.33 * 1.0 =  -1.33 for ctc
2024-10-27 18:33:39,094 (beam_search:479) INFO: total log probability: -1.33
2024-10-27 18:33:39,094 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:33:39,094 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:39,094 (beam_search:483) INFO: best hypo: GETTHROUGHTOTHEUMIT'LLGOTHROUGH

2024-10-27 18:33:39,096 (asr_inference:509) INFO: speech length: 283376
2024-10-27 18:33:51,689 (beam_search:428) INFO: decoder input length: 220
2024-10-27 18:33:51,689 (beam_search:429) INFO: max output length: 220
2024-10-27 18:33:51,689 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:53,219 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:53,219 (beam_search:476) INFO:  -3.30 * 1.0 =  -3.30 for ctc
2024-10-27 18:33:53,219 (beam_search:479) INFO: total log probability: -3.30
2024-10-27 18:33:53,219 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:33:53,219 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:53,219 (beam_search:483) INFO: best hypo: UMITBECAUSEYOUDTHATTHEELECTRICITYWOULDFLOWTHROUGHTHEWIRESONLYFROMONESIDEANDIFYOUHADPOSITIVEPOSITIVEIT'TWORKBECAUSEYOUHAVETOHAVEANEGATIVEAPOSITIVE

2024-10-27 18:33:53,222 (asr_inference:509) INFO: speech length: 17520
2024-10-27 18:33:54,102 (beam_search:428) INFO: decoder input length: 13
2024-10-27 18:33:54,102 (beam_search:429) INFO: max output length: 13
2024-10-27 18:33:54,102 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:54,116 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:54,117 (beam_search:476) INFO:  -1.49 * 1.0 =  -1.49 for ctc
2024-10-27 18:33:54,117 (beam_search:479) INFO: total log probability: -1.49
2024-10-27 18:33:54,117 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:33:54,117 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:54,117 (beam_search:483) INFO: best hypo: 'TGOIMEAN

2024-10-27 18:33:54,120 (asr_inference:509) INFO: speech length: 217280
2024-10-27 18:34:03,158 (beam_search:428) INFO: decoder input length: 169
2024-10-27 18:34:03,158 (beam_search:429) INFO: max output length: 169
2024-10-27 18:34:03,158 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:03,974 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:03,975 (beam_search:476) INFO:  -3.86 * 1.0 =  -3.86 for ctc
2024-10-27 18:34:03,975 (beam_search:479) INFO: total log probability: -3.86
2024-10-27 18:34:03,975 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:34:03,975 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:03,975 (beam_search:483) INFO: best hypo: UHIFSOMETHINGISCONNECTEDANDTHENYOUITLIKEWITHSIFTHETWOARECONNECTEDTOGETHERTHENANDYOUTAKEITIT'S

2024-10-27 18:34:03,977 (asr_inference:509) INFO: speech length: 83520
2024-10-27 18:34:06,969 (beam_search:428) INFO: decoder input length: 64
2024-10-27 18:34:06,969 (beam_search:429) INFO: max output length: 64
2024-10-27 18:34:06,969 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:07,087 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:07,087 (beam_search:476) INFO:  -2.47 * 1.0 =  -2.47 for ctc
2024-10-27 18:34:07,087 (beam_search:479) INFO: total log probability: -2.47
2024-10-27 18:34:07,087 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:34:07,087 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:07,087 (beam_search:483) INFO: best hypo: THEMOREWIREYOUTHEMORETHEMOREGETEDUP

2024-10-27 18:34:07,089 (asr_inference:509) INFO: speech length: 20688
2024-10-27 18:34:08,075 (beam_search:428) INFO: decoder input length: 15
2024-10-27 18:34:08,075 (beam_search:429) INFO: max output length: 15
2024-10-27 18:34:08,075 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:08,086 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:08,086 (beam_search:476) INFO:  -0.58 * 1.0 =  -0.58 for ctc
2024-10-27 18:34:08,086 (beam_search:479) INFO: total log probability: -0.58
2024-10-27 18:34:08,086 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:34:08,086 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:08,086 (beam_search:483) INFO: best hypo: YESI

2024-10-27 18:34:08,089 (asr_inference:509) INFO: speech length: 24128
2024-10-27 18:34:09,270 (beam_search:428) INFO: decoder input length: 18
2024-10-27 18:34:09,270 (beam_search:429) INFO: max output length: 18
2024-10-27 18:34:09,270 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:09,278 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:09,278 (beam_search:476) INFO:  -0.83 * 1.0 =  -0.83 for ctc
2024-10-27 18:34:09,278 (beam_search:479) INFO: total log probability: -0.83
2024-10-27 18:34:09,278 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:34:09,278 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:09,278 (beam_search:483) INFO: best hypo: WILL

2024-10-27 18:34:09,281 (asr_inference:509) INFO: speech length: 106064
2024-10-27 18:34:13,211 (beam_search:428) INFO: decoder input length: 82
2024-10-27 18:34:13,211 (beam_search:429) INFO: max output length: 82
2024-10-27 18:34:13,211 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:13,413 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:13,413 (beam_search:476) INFO:  -1.06 * 1.0 =  -1.06 for ctc
2024-10-27 18:34:13,413 (beam_search:479) INFO: total log probability: -1.06
2024-10-27 18:34:13,413 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:34:13,413 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:13,413 (beam_search:483) INFO: best hypo: 'SISINGUPTHEWASHERSBECAUSEIT'SATTRACTEDTOTHEM

2024-10-27 18:34:13,417 (asr_inference:509) INFO: speech length: 15984
2024-10-27 18:34:14,278 (beam_search:428) INFO: decoder input length: 11
2024-10-27 18:34:14,279 (beam_search:429) INFO: max output length: 11
2024-10-27 18:34:14,279 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:14,287 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:14,287 (beam_search:476) INFO:  -0.91 * 1.0 =  -0.91 for ctc
2024-10-27 18:34:14,287 (beam_search:479) INFO: total log probability: -0.91
2024-10-27 18:34:14,287 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:34:14,287 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:14,287 (beam_search:483) INFO: best hypo: ISGET

2024-10-27 18:34:14,290 (asr_inference:509) INFO: speech length: 35360
2024-10-27 18:34:15,672 (beam_search:428) INFO: decoder input length: 27
2024-10-27 18:34:15,672 (beam_search:429) INFO: max output length: 27
2024-10-27 18:34:15,672 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:15,695 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:15,695 (beam_search:476) INFO:  -1.48 * 1.0 =  -1.48 for ctc
2024-10-27 18:34:15,695 (beam_search:479) INFO: total log probability: -1.48
2024-10-27 18:34:15,695 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:34:15,695 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:15,695 (beam_search:483) INFO: best hypo: ISGETTINGEDTHROUGH

2024-10-27 18:34:15,698 (asr_inference:509) INFO: speech length: 30320
2024-10-27 18:34:16,916 (beam_search:428) INFO: decoder input length: 23
2024-10-27 18:34:16,916 (beam_search:429) INFO: max output length: 23
2024-10-27 18:34:16,916 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:16,927 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:16,927 (beam_search:476) INFO:  -0.93 * 1.0 =  -0.93 for ctc
2024-10-27 18:34:16,927 (beam_search:479) INFO: total log probability: -0.93
2024-10-27 18:34:16,927 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 18:34:16,927 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:16,927 (beam_search:483) INFO: best hypo: FOR

2024-10-27 18:34:16,929 (asr_inference:509) INFO: speech length: 77312
2024-10-27 18:34:19,757 (beam_search:428) INFO: decoder input length: 59
2024-10-27 18:34:19,757 (beam_search:429) INFO: max output length: 59
2024-10-27 18:34:19,757 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:19,849 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:19,850 (beam_search:476) INFO:  -1.12 * 1.0 =  -1.12 for ctc
2024-10-27 18:34:19,850 (beam_search:479) INFO: total log probability: -1.12
2024-10-27 18:34:19,850 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:34:19,850 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:19,850 (beam_search:483) INFO: best hypo: THATTHEELECTRICITYWASFLOWINGBECAUSEITGOTHOT

2024-10-27 18:34:19,852 (asr_inference:509) INFO: speech length: 151472
2024-10-27 18:34:25,714 (beam_search:428) INFO: decoder input length: 117
2024-10-27 18:34:25,714 (beam_search:429) INFO: max output length: 117
2024-10-27 18:34:25,714 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:26,169 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:26,169 (beam_search:476) INFO:  -3.20 * 1.0 =  -3.20 for ctc
2024-10-27 18:34:26,169 (beam_search:479) INFO: total log probability: -3.20
2024-10-27 18:34:26,169 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:34:26,169 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:26,169 (beam_search:483) INFO: best hypo: UMTHEITGOTHOTWASTHEELECTRICITYWASFLOWINGTHROUGHITANDSOITGOTSOSTRONGITTOGETHOT

2024-10-27 18:34:26,172 (asr_inference:509) INFO: speech length: 212704
2024-10-27 18:34:34,766 (beam_search:428) INFO: decoder input length: 165
2024-10-27 18:34:34,766 (beam_search:429) INFO: max output length: 165
2024-10-27 18:34:34,766 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:35,581 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:35,581 (beam_search:476) INFO:  -2.98 * 1.0 =  -2.98 for ctc
2024-10-27 18:34:35,581 (beam_search:479) INFO: total log probability: -2.98
2024-10-27 18:34:35,581 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:34:35,581 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:35,581 (beam_search:483) INFO: best hypo: UMTHEELECTRICITY'SBECAUSETHERE'SALOTOFWIREDAROUNDANDIT'SALLFLOWINGTHROUGHTHESOTHATITS

2024-10-27 18:34:35,584 (asr_inference:509) INFO: speech length: 79728
2024-10-27 18:34:38,545 (beam_search:428) INFO: decoder input length: 61
2024-10-27 18:34:38,546 (beam_search:429) INFO: max output length: 61
2024-10-27 18:34:38,546 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:38,603 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:38,603 (beam_search:476) INFO:  -0.66 * 1.0 =  -0.66 for ctc
2024-10-27 18:34:38,603 (beam_search:479) INFO: total log probability: -0.66
2024-10-27 18:34:38,603 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:34:38,603 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:38,603 (beam_search:483) INFO: best hypo: THEELECTRICITYFROMTHEBATTERY

2024-10-27 18:34:38,605 (asr_inference:509) INFO: speech length: 226768
2024-10-27 18:34:47,965 (beam_search:428) INFO: decoder input length: 176
2024-10-27 18:34:47,966 (beam_search:429) INFO: max output length: 176
2024-10-27 18:34:47,966 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:49,210 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:49,210 (beam_search:476) INFO:  -3.31 * 1.0 =  -3.31 for ctc
2024-10-27 18:34:49,210 (beam_search:479) INFO: total log probability: -3.31
2024-10-27 18:34:49,210 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:34:49,210 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:49,210 (beam_search:483) INFO: best hypo: THEBATTERYISOFELECTRICITYANDITMAKESRUNSOIFYOUHOOKAWIREUPTOITTHEELECTRICITY'SGOINGGOTHROUGHTHEWIREANDTURNONWHATEVERYOUWANTTOTURNON

2024-10-27 18:34:49,213 (asr_inference:509) INFO: speech length: 299712
2024-10-27 18:35:02,266 (beam_search:428) INFO: decoder input length: 233
2024-10-27 18:35:02,266 (beam_search:429) INFO: max output length: 233
2024-10-27 18:35:02,266 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:04,553 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:04,553 (beam_search:476) INFO:  -4.25 * 1.0 =  -4.25 for ctc
2024-10-27 18:35:04,553 (beam_search:479) INFO: total log probability: -4.25
2024-10-27 18:35:04,554 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:35:04,554 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:04,554 (beam_search:483) INFO: best hypo: INTHISPICTUREYOUHAVETHEWIRESEDUPTOTHEBATTERYANDHOOKEDUPTOTHESWITCHANDTHENYOUHAVETHEWIREHOOKEDUPTOTHEBATTERYANDDTHEANDTHENYOUHAVEITUMEDUPTOTHESWITCHANDTHEELECTRICITY'SONTHE

2024-10-27 18:35:04,556 (asr_inference:509) INFO: speech length: 127200
2024-10-27 18:35:09,288 (beam_search:428) INFO: decoder input length: 98
2024-10-27 18:35:09,288 (beam_search:429) INFO: max output length: 98
2024-10-27 18:35:09,288 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:09,556 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:09,556 (beam_search:476) INFO:  -0.87 * 1.0 =  -0.87 for ctc
2024-10-27 18:35:09,556 (beam_search:479) INFO: total log probability: -0.87
2024-10-27 18:35:09,556 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:35:09,556 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:09,556 (beam_search:483) INFO: best hypo: THETHEUMTHEELECTRICITY'SGETTINGANDIT'SMOREANDMOREWASHERS

2024-10-27 18:35:09,559 (asr_inference:509) INFO: speech length: 99712
2024-10-27 18:35:13,256 (beam_search:428) INFO: decoder input length: 77
2024-10-27 18:35:13,256 (beam_search:429) INFO: max output length: 77
2024-10-27 18:35:13,256 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:13,391 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:13,392 (beam_search:476) INFO:  -2.12 * 1.0 =  -2.12 for ctc
2024-10-27 18:35:13,392 (beam_search:479) INFO: total log probability: -2.12
2024-10-27 18:35:13,392 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:35:13,392 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:13,392 (beam_search:483) INFO: best hypo: ITINBECAUSEITWEDIDTHISJUSTIN

2024-10-27 18:35:13,395 (asr_inference:509) INFO: speech length: 188480
2024-10-27 18:35:21,039 (beam_search:428) INFO: decoder input length: 146
2024-10-27 18:35:21,039 (beam_search:429) INFO: max output length: 146
2024-10-27 18:35:21,039 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:21,581 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:21,581 (beam_search:476) INFO:  -2.59 * 1.0 =  -2.59 for ctc
2024-10-27 18:35:21,581 (beam_search:479) INFO: total log probability: -2.59
2024-10-27 18:35:21,582 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:35:21,582 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:21,582 (beam_search:483) INFO: best hypo: YOUCANTURNONANDOFFUMASWITCHANDTHENYOUCANTURNONANDOFFITJUSTISON

2024-10-27 18:35:21,584 (asr_inference:509) INFO: speech length: 178784
2024-10-27 18:35:28,375 (beam_search:428) INFO: decoder input length: 139
2024-10-27 18:35:28,375 (beam_search:429) INFO: max output length: 139
2024-10-27 18:35:28,376 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:29,129 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:29,130 (beam_search:476) INFO:  -3.85 * 1.0 =  -3.85 for ctc
2024-10-27 18:35:29,130 (beam_search:479) INFO: total log probability: -3.85
2024-10-27 18:35:29,130 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:35:29,130 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:29,130 (beam_search:483) INFO: best hypo: UMIT'SIMPORTANTBECAUSEYOUNEEDABATTERY'SIMEANAMAGNETTHAT'SGOINGBUTYOUNEEDTOBEABLETOABATTERYOFF

2024-10-27 18:35:29,132 (asr_inference:509) INFO: speech length: 67008
2024-10-27 18:35:31,611 (beam_search:428) INFO: decoder input length: 51
2024-10-27 18:35:31,611 (beam_search:429) INFO: max output length: 51
2024-10-27 18:35:31,611 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:31,676 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:31,676 (beam_search:476) INFO:  -0.56 * 1.0 =  -0.56 for ctc
2024-10-27 18:35:31,676 (beam_search:479) INFO: total log probability: -0.56
2024-10-27 18:35:31,676 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:35:31,676 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:31,676 (beam_search:483) INFO: best hypo: WE'VEBEENLEARNINGABOUTAND

2024-10-27 18:35:31,680 (asr_inference:509) INFO: speech length: 190208
2024-10-27 18:35:39,231 (beam_search:428) INFO: decoder input length: 148
2024-10-27 18:35:39,231 (beam_search:429) INFO: max output length: 148
2024-10-27 18:35:39,231 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:39,629 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:39,629 (beam_search:476) INFO:  -3.40 * 1.0 =  -3.40 for ctc
2024-10-27 18:35:39,629 (beam_search:479) INFO: total log probability: -3.40
2024-10-27 18:35:39,629 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:35:39,629 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:39,629 (beam_search:483) INFO: best hypo: THEYARETHEYARETHANAMAGNETBECAUSETHERE'SELECTRICITYANDMAKESMORE

2024-10-27 18:35:39,632 (asr_inference:509) INFO: speech length: 171824
2024-10-27 18:35:46,269 (beam_search:428) INFO: decoder input length: 133
2024-10-27 18:35:46,269 (beam_search:429) INFO: max output length: 133
2024-10-27 18:35:46,269 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:46,799 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:46,800 (beam_search:476) INFO:  -1.74 * 1.0 =  -1.74 for ctc
2024-10-27 18:35:46,800 (beam_search:479) INFO: total log probability: -1.74
2024-10-27 18:35:46,800 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:35:46,800 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:46,800 (beam_search:483) INFO: best hypo: THATTHEWIREISAROUNDTHEELECTROMAGNETTOSOTOMAKEITANELECTROMAGNETSOTHATITCANPICKUPTHEWASHERS

2024-10-27 18:35:46,802 (asr_inference:509) INFO: speech length: 191760
2024-10-27 18:35:54,489 (beam_search:428) INFO: decoder input length: 149
2024-10-27 18:35:54,490 (beam_search:429) INFO: max output length: 149
2024-10-27 18:35:54,490 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:55,155 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:55,155 (beam_search:476) INFO:  -3.88 * 1.0 =  -3.88 for ctc
2024-10-27 18:35:55,155 (beam_search:479) INFO: total log probability: -3.88
2024-10-27 18:35:55,155 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:35:55,155 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:55,155 (beam_search:483) INFO: best hypo: ITISDAROUNDACONNECTEDTOASWITCHANDABATTERYTOMAKEITUMANELECTROMAGNETSOTHATITCANPICKUPSTUFF

2024-10-27 18:35:55,157 (asr_inference:509) INFO: speech length: 300240
2024-10-27 18:36:08,532 (beam_search:428) INFO: decoder input length: 234
2024-10-27 18:36:08,532 (beam_search:429) INFO: max output length: 234
2024-10-27 18:36:08,532 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:10,576 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:10,576 (beam_search:476) INFO:  -4.59 * 1.0 =  -4.59 for ctc
2024-10-27 18:36:10,576 (beam_search:479) INFO: total log probability: -4.59
2024-10-27 18:36:10,576 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:36:10,576 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:10,577 (beam_search:483) INFO: best hypo: THEISANAILIFTHEWIRE'SNOTAROUNDITBECAUSEWHENIT'SAROUNDITITSTHEELECTRICITYTOGOTHROUGHANDTHENITSUPONTHESONTHENAILSOTHATITISTOPICKUPUMWASHERS

2024-10-27 18:36:10,579 (asr_inference:509) INFO: speech length: 24448
2024-10-27 18:36:11,665 (beam_search:428) INFO: decoder input length: 18
2024-10-27 18:36:11,665 (beam_search:429) INFO: max output length: 18
2024-10-27 18:36:11,665 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:11,675 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:11,675 (beam_search:476) INFO:  -1.08 * 1.0 =  -1.08 for ctc
2024-10-27 18:36:11,675 (beam_search:479) INFO: total log probability: -1.08
2024-10-27 18:36:11,675 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:36:11,675 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:11,675 (beam_search:483) INFO: best hypo: THATCORRECT

2024-10-27 18:36:11,678 (asr_inference:509) INFO: speech length: 118000
2024-10-27 18:36:15,990 (beam_search:428) INFO: decoder input length: 91
2024-10-27 18:36:15,991 (beam_search:429) INFO: max output length: 91
2024-10-27 18:36:15,991 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:16,277 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:16,277 (beam_search:476) INFO:  -3.15 * 1.0 =  -3.15 for ctc
2024-10-27 18:36:16,277 (beam_search:479) INFO: total log probability: -3.15
2024-10-27 18:36:16,277 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:36:16,277 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:16,277 (beam_search:483) INFO: best hypo: WHENITISOPENITDOESNOTPICKUPELECTRICITYBUTWHENITISCLOSEDITDOESUPELECTRICITY

2024-10-27 18:36:16,280 (asr_inference:509) INFO: speech length: 210912
2024-10-27 18:36:24,599 (beam_search:428) INFO: decoder input length: 164
2024-10-27 18:36:24,599 (beam_search:429) INFO: max output length: 164
2024-10-27 18:36:24,599 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:25,989 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:25,990 (beam_search:476) INFO:  -5.53 * 1.0 =  -5.53 for ctc
2024-10-27 18:36:25,990 (beam_search:479) INFO: total log probability: -5.53
2024-10-27 18:36:25,990 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:36:25,990 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:25,990 (beam_search:483) INFO: best hypo: YOUHAVETOOFHOWMANYYOUWRAPTHEWIREAROUNDTHENAILBECAUSEIFYOUDON'TAROUNDTHATMUCHIT'SIT'SNOTTHATBUTWHENYOUITAROUNDAIT'SREALLYANDCANPICKUPMORE

2024-10-27 18:36:25,992 (asr_inference:509) INFO: speech length: 230480
2024-10-27 18:36:35,308 (beam_search:428) INFO: decoder input length: 179
2024-10-27 18:36:35,308 (beam_search:429) INFO: max output length: 179
2024-10-27 18:36:35,308 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:36,645 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:36,645 (beam_search:476) INFO:  -4.51 * 1.0 =  -4.51 for ctc
2024-10-27 18:36:36,645 (beam_search:479) INFO: total log probability: -4.51
2024-10-27 18:36:36,645 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:36:36,645 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:36,645 (beam_search:483) INFO: best hypo: THATYOUITTOTHEANDTHESWITCHANDWHENYOUWANTTOPICKITUPYOUTHESWITCHANDWHENYOUANDTHENWHENYOUWANTTOITYOUNOTYOUDONOTOPENTHESWITCHYOUIT

2024-10-27 18:36:36,649 (asr_inference:509) INFO: speech length: 119248
2024-10-27 18:36:40,944 (beam_search:428) INFO: decoder input length: 92
2024-10-27 18:36:40,945 (beam_search:429) INFO: max output length: 92
2024-10-27 18:36:40,945 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:41,147 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:41,147 (beam_search:476) INFO:  -0.89 * 1.0 =  -0.89 for ctc
2024-10-27 18:36:41,147 (beam_search:479) INFO: total log probability: -0.89
2024-10-27 18:36:41,147 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:36:41,147 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:41,147 (beam_search:483) INFO: best hypo: THEELECTRICITYISTOANDITISTOPICKUPUMWASHERS

2024-10-27 18:36:41,149 (asr_inference:509) INFO: speech length: 23424
2024-10-27 18:36:42,206 (beam_search:428) INFO: decoder input length: 17
2024-10-27 18:36:42,206 (beam_search:429) INFO: max output length: 17
2024-10-27 18:36:42,206 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:42,217 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:42,217 (beam_search:476) INFO:  -0.57 * 1.0 =  -0.57 for ctc
2024-10-27 18:36:42,217 (beam_search:479) INFO: total log probability: -0.57
2024-10-27 18:36:42,218 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:36:42,218 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:42,218 (beam_search:483) INFO: best hypo: THATIS

2024-10-27 18:36:42,220 (asr_inference:509) INFO: speech length: 40096
2024-10-27 18:36:43,797 (beam_search:428) INFO: decoder input length: 30
2024-10-27 18:36:43,797 (beam_search:429) INFO: max output length: 30
2024-10-27 18:36:43,797 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:43,826 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:43,826 (beam_search:476) INFO:  -2.93 * 1.0 =  -2.93 for ctc
2024-10-27 18:36:43,826 (beam_search:479) INFO: total log probability: -2.93
2024-10-27 18:36:43,826 (beam_search:480) INFO: normalized log probability: -0.42
2024-10-27 18:36:43,826 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:43,826 (beam_search:483) INFO: best hypo: YOUCANMAKEANELECTROMAGNET

2024-10-27 18:36:43,829 (asr_inference:509) INFO: speech length: 170256
2024-10-27 18:36:50,283 (beam_search:428) INFO: decoder input length: 132
2024-10-27 18:36:50,283 (beam_search:429) INFO: max output length: 132
2024-10-27 18:36:50,283 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:50,747 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:50,747 (beam_search:476) INFO:  -4.01 * 1.0 =  -4.01 for ctc
2024-10-27 18:36:50,747 (beam_search:479) INFO: total log probability: -4.01
2024-10-27 18:36:50,747 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:36:50,747 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:50,747 (beam_search:483) INFO: best hypo: THEISWHATWHENYOUPUTELECTRICITYWIRESANDATOGETHERANDASWITCHYOUAREABLETOMAKEA

2024-10-27 18:36:50,750 (asr_inference:509) INFO: speech length: 146064
2024-10-27 18:36:56,443 (beam_search:428) INFO: decoder input length: 113
2024-10-27 18:36:56,443 (beam_search:429) INFO: max output length: 113
2024-10-27 18:36:56,443 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:56,679 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:56,679 (beam_search:476) INFO:  -2.66 * 1.0 =  -2.66 for ctc
2024-10-27 18:36:56,679 (beam_search:479) INFO: total log probability: -2.66
2024-10-27 18:36:56,679 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:36:56,679 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:56,679 (beam_search:483) INFO: best hypo: THEWIRESTHEELECTRICITYTOFLOWTHROUGHSOTHEELECTROMAGNET

2024-10-27 18:36:56,681 (asr_inference:509) INFO: speech length: 91152
2024-10-27 18:37:00,134 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:37:00,134 (beam_search:429) INFO: max output length: 70
2024-10-27 18:37:00,134 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:00,252 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:00,253 (beam_search:476) INFO:  -2.82 * 1.0 =  -2.82 for ctc
2024-10-27 18:37:00,253 (beam_search:479) INFO: total log probability: -2.82
2024-10-27 18:37:00,253 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:37:00,253 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:00,253 (beam_search:483) INFO: best hypo: MEANSLIKETHEWIREISINSIDEOFTHEGREEN

2024-10-27 18:37:00,255 (asr_inference:509) INFO: speech length: 63552
2024-10-27 18:37:02,536 (beam_search:428) INFO: decoder input length: 49
2024-10-27 18:37:02,536 (beam_search:429) INFO: max output length: 49
2024-10-27 18:37:02,536 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:02,619 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:02,620 (beam_search:476) INFO:  -0.97 * 1.0 =  -0.97 for ctc
2024-10-27 18:37:02,620 (beam_search:479) INFO: total log probability: -0.97
2024-10-27 18:37:02,620 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:37:02,620 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:02,620 (beam_search:483) INFO: best hypo: THATYOUITAROUNDANDYOUTURNONASWITCH

2024-10-27 18:37:02,622 (asr_inference:509) INFO: speech length: 76176
2024-10-27 18:37:05,394 (beam_search:428) INFO: decoder input length: 59
2024-10-27 18:37:05,394 (beam_search:429) INFO: max output length: 59
2024-10-27 18:37:05,394 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:05,473 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:05,473 (beam_search:476) INFO:  -1.30 * 1.0 =  -1.30 for ctc
2024-10-27 18:37:05,474 (beam_search:479) INFO: total log probability: -1.30
2024-10-27 18:37:05,474 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:37:05,474 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:05,474 (beam_search:483) INFO: best hypo: WHATDOYOUMEANBYTHERIVETMAGNETIC

2024-10-27 18:37:05,476 (asr_inference:509) INFO: speech length: 272608
2024-10-27 18:37:17,388 (beam_search:428) INFO: decoder input length: 212
2024-10-27 18:37:17,388 (beam_search:429) INFO: max output length: 212
2024-10-27 18:37:17,388 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:18,364 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:18,364 (beam_search:476) INFO:  -4.77 * 1.0 =  -4.77 for ctc
2024-10-27 18:37:18,364 (beam_search:479) INFO: total log probability: -4.77
2024-10-27 18:37:18,364 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:37:18,364 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:18,364 (beam_search:483) INFO: best hypo: NOTATHATWE'VEREALLYBEENISTHETHINGWEDIDINSCIENCEWASTHANTHATWEMADEAPARALLELCIRCUIT

2024-10-27 18:37:18,368 (asr_inference:509) INFO: speech length: 168176
2024-10-27 18:37:26,072 (beam_search:428) INFO: decoder input length: 130
2024-10-27 18:37:26,072 (beam_search:429) INFO: max output length: 130
2024-10-27 18:37:26,072 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:26,288 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:26,288 (beam_search:476) INFO:  -2.14 * 1.0 =  -2.14 for ctc
2024-10-27 18:37:26,288 (beam_search:479) INFO: total log probability: -2.14
2024-10-27 18:37:26,289 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:37:26,289 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:26,289 (beam_search:483) INFO: best hypo: WE'VEBEENWITHANDJUSTWEWITH

2024-10-27 18:37:26,291 (asr_inference:509) INFO: speech length: 120752
2024-10-27 18:37:31,394 (beam_search:428) INFO: decoder input length: 93
2024-10-27 18:37:31,394 (beam_search:429) INFO: max output length: 93
2024-10-27 18:37:31,394 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:31,623 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:31,623 (beam_search:476) INFO:  -0.83 * 1.0 =  -0.83 for ctc
2024-10-27 18:37:31,623 (beam_search:479) INFO: total log probability: -0.83
2024-10-27 18:37:31,623 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:37:31,623 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:31,623 (beam_search:483) INFO: best hypo: THELIGHTSTURNOFFANDYEAHIT'SABOUTIT

2024-10-27 18:37:31,626 (asr_inference:509) INFO: speech length: 86336
2024-10-27 18:37:35,387 (beam_search:428) INFO: decoder input length: 66
2024-10-27 18:37:35,387 (beam_search:429) INFO: max output length: 66
2024-10-27 18:37:35,387 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:35,486 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:35,487 (beam_search:476) INFO:  -1.55 * 1.0 =  -1.55 for ctc
2024-10-27 18:37:35,487 (beam_search:479) INFO: total log probability: -1.55
2024-10-27 18:37:35,487 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:37:35,487 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:35,487 (beam_search:483) INFO: best hypo: ISEETHEONANDOFFTIME

2024-10-27 18:37:35,490 (asr_inference:509) INFO: speech length: 79616
2024-10-27 18:37:38,879 (beam_search:428) INFO: decoder input length: 61
2024-10-27 18:37:38,879 (beam_search:429) INFO: max output length: 61
2024-10-27 18:37:38,879 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:39,040 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:39,041 (beam_search:476) INFO:  -0.98 * 1.0 =  -0.98 for ctc
2024-10-27 18:37:39,041 (beam_search:479) INFO: total log probability: -0.98
2024-10-27 18:37:39,041 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:37:39,041 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:39,041 (beam_search:483) INFO: best hypo: ITDOESN'TTHECIRCUITANDTHETHEOTHERBULBNOTLIGHT

2024-10-27 18:37:39,044 (asr_inference:509) INFO: speech length: 158160
2024-10-27 18:37:47,328 (beam_search:428) INFO: decoder input length: 123
2024-10-27 18:37:47,328 (beam_search:429) INFO: max output length: 123
2024-10-27 18:37:47,328 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:48,106 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:48,107 (beam_search:476) INFO:  -4.06 * 1.0 =  -4.06 for ctc
2024-10-27 18:37:48,107 (beam_search:479) INFO: total log probability: -4.06
2024-10-27 18:37:48,107 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:37:48,107 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:48,107 (beam_search:483) INFO: best hypo: THEELECTRICITYHASTOTHROUGHTHEBULBANDIFTHEBULBISN'TTHENITCAN'TTHROUGHANDITCAN'TTHEELECTRICITYCANNOTPOWERTHENEXTBULB

2024-10-27 18:37:48,109 (asr_inference:509) INFO: speech length: 188400
2024-10-27 18:37:56,216 (beam_search:428) INFO: decoder input length: 146
2024-10-27 18:37:56,216 (beam_search:429) INFO: max output length: 146
2024-10-27 18:37:56,216 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:57,171 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:57,172 (beam_search:476) INFO:  -3.61 * 1.0 =  -3.61 for ctc
2024-10-27 18:37:57,172 (beam_search:479) INFO: total log probability: -3.61
2024-10-27 18:37:57,172 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:37:57,172 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:57,172 (beam_search:483) INFO: best hypo: THEELECTRICITYDOESN'TTHROUGHTHATBULBITOUTANDTHEOTHERBULBCAN'TGETITSPOWERANDTHENTHENOTANDBULBSGOOUT

2024-10-27 18:37:57,191 (asr_inference:509) INFO: speech length: 222416
2024-10-27 18:38:09,927 (beam_search:428) INFO: decoder input length: 173
2024-10-27 18:38:09,927 (beam_search:429) INFO: max output length: 173
2024-10-27 18:38:09,927 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:11,422 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:11,422 (beam_search:476) INFO:  -6.33 * 1.0 =  -6.33 for ctc
2024-10-27 18:38:11,422 (beam_search:479) INFO: total log probability: -6.33
2024-10-27 18:38:11,422 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:38:11,422 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:11,422 (beam_search:483) INFO: best hypo: WHAT'SGOINGONISTHATINTHEBULBELECTRICITYHASTOGOALLTHETHROUGHANDWHENITOUTTHATMEANSOFTHEWIRESINSIDETHEBULBSOITDOESN'TGOALLTHETHROUGHTOTHEOTHERBULB

2024-10-27 18:38:11,472 (asr_inference:509) INFO: speech length: 298048
2024-10-27 18:38:27,353 (beam_search:428) INFO: decoder input length: 232
2024-10-27 18:38:27,354 (beam_search:429) INFO: max output length: 232
2024-10-27 18:38:27,354 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:29,011 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:29,012 (beam_search:476) INFO:  -4.83 * 1.0 =  -4.83 for ctc
2024-10-27 18:38:29,012 (beam_search:479) INFO: total log probability: -4.83
2024-10-27 18:38:29,012 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:38:29,012 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:29,012 (beam_search:483) INFO: best hypo: UMTHERE'SELECTRICITYFLOWINGINTWODIFFERENTPATHWAYSONEFOREACHBULBIFONEOUTBOTHONEWILLSTILLSTAYONASTOCIRCUITWHERETHEYBOTHGOOUTIFONEGOESOUT

2024-10-27 18:38:29,015 (asr_inference:509) INFO: speech length: 252640
2024-10-27 18:38:42,591 (beam_search:428) INFO: decoder input length: 196
2024-10-27 18:38:42,591 (beam_search:429) INFO: max output length: 196
2024-10-27 18:38:42,591 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:43,873 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:43,873 (beam_search:476) INFO:  -3.42 * 1.0 =  -3.42 for ctc
2024-10-27 18:38:43,873 (beam_search:479) INFO: total log probability: -3.42
2024-10-27 18:38:43,873 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:38:43,873 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:43,873 (beam_search:483) INFO: best hypo: IFIONABULBITOUTBUTSINCEIT'SAPARALLELCIRCUITINGTHATEACHBULBHASITSPATHWAYITCANONEBULBCANSTILLON

2024-10-27 18:38:43,887 (asr_inference:509) INFO: speech length: 100224
2024-10-27 18:38:48,637 (beam_search:428) INFO: decoder input length: 77
2024-10-27 18:38:48,637 (beam_search:429) INFO: max output length: 77
2024-10-27 18:38:48,637 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:48,745 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:48,746 (beam_search:476) INFO:  -1.63 * 1.0 =  -1.63 for ctc
2024-10-27 18:38:48,746 (beam_search:479) INFO: total log probability: -1.63
2024-10-27 18:38:48,746 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:38:48,746 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:48,746 (beam_search:483) INFO: best hypo: ITHEELECTRICITYISFLOWINGINSIDETHE

2024-10-27 18:38:48,759 (asr_inference:509) INFO: speech length: 168256
2024-10-27 18:38:57,236 (beam_search:428) INFO: decoder input length: 130
2024-10-27 18:38:57,236 (beam_search:429) INFO: max output length: 130
2024-10-27 18:38:57,236 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:57,968 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:57,968 (beam_search:476) INFO:  -4.52 * 1.0 =  -4.52 for ctc
2024-10-27 18:38:57,968 (beam_search:479) INFO: total log probability: -4.52
2024-10-27 18:38:57,968 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:38:57,968 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:57,968 (beam_search:483) INFO: best hypo: INITHINKAPARALLELCIRCUITWOULDBEBETTERALONGOFLIGHTSSOTHATONEGOESOUTTHERE'SSTILLTHATAREON

2024-10-27 18:38:57,981 (asr_inference:509) INFO: speech length: 141248
2024-10-27 18:39:05,720 (beam_search:428) INFO: decoder input length: 109
2024-10-27 18:39:05,720 (beam_search:429) INFO: max output length: 109
2024-10-27 18:39:05,720 (beam_search:430) INFO: min output length: 0
2024-10-27 18:39:06,198 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:39:06,198 (beam_search:476) INFO:  -2.43 * 1.0 =  -2.43 for ctc
2024-10-27 18:39:06,198 (beam_search:479) INFO: total log probability: -2.43
2024-10-27 18:39:06,198 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:39:06,198 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:39:06,198 (beam_search:483) INFO: best hypo: IFONEBULBOUTTHEPOWERDOESN'TFLOWTOTHEBULBINANDITCANNOTTHECIRCUIT

2024-10-27 18:39:06,213 (asr_inference:509) INFO: speech length: 275856
=======
FileNotFoundError: [Errno 2] No such file or directory: '/data/mohan/workdir/espnet/egs2/myst/asr1/exp/librispeech_100_ctc_e_branchformer/exp/asr_train_asr_ctc_e_branchformer_e12_raw_en_bpe5000_sp/valid.cer_ctc.ave_10best.pth'
[lambda-Lambda-Vector:0/2] 2024-10-31 20:45:26,341 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[lambda-Lambda-Vector:0/2] 2024-10-31 20:45:26,346 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 26.43 M
    Number of trainable parameters: 26.43 M (100.0%)
    Size: 105.74 MB
    Type: torch.float32
[lambda-Lambda-Vector:0/2] 2024-10-31 20:45:26,346 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 1.3333333333333336e-07
    maximize: False
    weight_decay: 1e-06
)
[lambda-Lambda-Vector:0/2] 2024-10-31 20:45:26,346 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[lambda-Lambda-Vector:0/2] 2024-10-31 20:45:26,346 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/config.yaml
[lambda-Lambda-Vector:0/2] 2024-10-31 20:45:26,409 (abs_task:1465) INFO: Loading pretrained params from /data/mohan/workdir/espnet/egs2/myst/asr1/exp/librispeech_100_ctc_e_branchformer/exp/asr_train_asr_ctc_e_branchformer_e12_raw_en_bpe5000_sp/valid.cer_ctc.ave_10best.pth:encoder:encoder
Process SpawnProcess-1:
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1466, in main_worker
    load_pretrained_model(
  File "/data/mohan/workdir/espnet/espnet2/torch_utils/load_pretrained_model.py", line 99, in load_pretrained_model
    src_state = torch.load(path, map_location=map_location)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/data/mohan/workdir/espnet/egs2/myst/asr1/exp/librispeech_100_ctc_e_branchformer/exp/asr_train_asr_ctc_e_branchformer_e12_raw_en_bpe5000_sp/valid.cer_ctc.ave_10best.pth'
W1031 20:45:27.053000 123363692857152 torch/multiprocessing/spawn.py:145] Terminating process 847750 via signal SIGTERM
>>>>>>> cea3af74175b4f1b718b2c4afeb0a5f028e5698c
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
<<<<<<< HEAD
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 843, in inference
    results = speech2text(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 515, in __call__
    enc, enc_olens = self.asr_model.encode(**batch)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 378, in encode
    feats, feats_lengths = self._extract_feats(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 445, in _extract_feats
    feats, feats_lengths = self.frontend(speech, speech_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/frontend/s3prl.py", line 99, in forward
    feats, feats_lens = self.upstream(input, input_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/nn/upstream.py", line 209, in forward
    hidden_states = self.upstream(wavs_list)["hidden_states"]
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/interfaces.py", line 103, in __call__
    result = super().__call__(wavs, *args, **kwargs) or {}
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/expert.py", line 83, in forward
    features, feat_padding_mask = self.model.extract_features(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 389, in extract_features
    x, layer_results = self.encoder(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 592, in forward
    x, layer_results = self.extract_features(x, padding_mask, streaming_mask, layer)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 626, in extract_features
    x, z, pos_bias = layer(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 742, in forward
    x = self.activation_fn(self.fc1(x))
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1498974) is killed by signal: Killed. 
# Accounting: time=5235 threads=1
# Ended (code 1) at Sun Oct 27 18:58:12 EDT 2024, elapsed time 5235 seconds
2024-10-28T00:15:06 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 4 --inference_asr_model valid.cer_ctc.ave.pth --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-28T00:15:07 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-28T00:15:07 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000
2024-10-28T00:15:07 (asr.sh:1508:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/run.sh'. You can resume the process from stage 12 using this script
2024-10-28T00:15:07 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/asr_inference.*.log'
2024-10-28T04:11:31 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/calculate_rtf.log'
2024-10-28T04:11:33 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/test/logdir/asr_inference.*.log'
2024-10-28T08:52:45 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/test/logdir/calculate_rtf.log'
2024-10-28T08:52:48 (asr.sh:1813:main) Successfully finished. [elapsed=31062s]
2024-10-28T16:43:32 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 4 --inference_asr_model valid.cer_ctc.best.pth --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-28T16:43:32 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-28T16:43:32 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000
2024-10-28T16:43:32 (asr.sh:1508:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 12 using this script
2024-10-28T16:43:32 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/asr_inference.*.log'
2024-10-28T20:51:20 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/calculate_rtf.log'
2024-10-28T20:51:23 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/test/logdir/asr_inference.*.log'
2024-10-29T01:42:48 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/test/logdir/calculate_rtf.log'
2024-10-29T01:42:51 (asr.sh:1813:main) Successfully finished. [elapsed=32359s]
=======
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1273, in main
    while not ProcessContext(processes, error_files).join():
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 177, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 1 terminated with exit code 1
# Accounting: time=14 threads=1
# Ended (code 1) at Thu Oct 31 20:45:27 PDT 2024, elapsed time 14 seconds

2024-10-31T20:46:59 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_onlyctc_finetune.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-31T20:46:59 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-31T20:46:59 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-31T20:46:59 (asr.sh:1407:main) Generate 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-31T20:46:59 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log'
2024-10-31 20:46:59,614 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log' --log exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000 --config conf/train_asr_onlyctc_finetune.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-31 20:46:59,624 (launch:237) INFO: single-node with 2gpu on distributed mode
2024-10-31 20:46:59,625 (launch:348) INFO: log file: exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log
2024-11-01T10:28:55 (asr.sh:1813:main) Successfully finished. [elapsed=49316s]
2024-11-01T10:41:58 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 2 --nj 16 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_onlyctc_finetune.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-11-01T10:41:58 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-11-01T10:41:58 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000
2024-11-01T10:41:58 (asr.sh:1508:main) Generate 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 12 using this script
2024-11-01T10:41:58 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/asr_inference.*.log'
2024-11-01T11:35:32 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/calculate_rtf.log'
2024-11-01T11:35:33 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/test/logdir/asr_inference.*.log'
2024-11-01T12:47:00 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/test/logdir/calculate_rtf.log'
2024-11-01T12:47:01 (asr.sh:1813:main) Successfully finished. [elapsed=7503s]
2024-11-09T00:06:48 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_onlyctc_nospecaug.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-11-09T00:06:48 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-11-09T00:06:48 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-11-09T00:06:48 (asr.sh:1407:main) Generate 'exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-11-09T00:06:48 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/train.log'
2024-11-09 00:06:48,818 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/train.log' --log exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000 --config conf/train_asr_onlyctc_nospecaug.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-11-09 00:06:48,831 (launch:237) INFO: single-node with 2gpu on distributed mode
2024-11-09 00:06:48,831 (launch:348) INFO: log file: exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/train.log
2024-11-09T13:13:52 (asr.sh:1813:main) Successfully finished. [elapsed=47224s]
2024-11-09T14:18:03 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 2 --nj 16 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_onlyctc_nospecaug.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-11-09T14:18:03 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-11-09T14:18:03 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000
2024-11-09T14:18:03 (asr.sh:1508:main) Generate 'exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 12 using this script
2024-11-09T14:18:03 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/asr_inference.*.log'
2024-11-09T15:11:34 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/calculate_rtf.log'
2024-11-09T15:11:36 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/test/logdir/asr_inference.*.log'
2024-11-09T16:31:08 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 2 --nj 16 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_onlyctc_nospecaug.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-11-09T16:31:08 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-11-09T16:31:08 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000
2024-11-09T16:31:08 (asr.sh:1508:main) Generate 'exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 12 using this script
2024-11-09T16:31:08 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/test/logdir/asr_inference.*.log'
2024-11-09T17:43:02 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/test/logdir/calculate_rtf.log'
2024-11-09T17:43:03 (asr.sh:1813:main) Successfully finished. [elapsed=4315s]
<<<<<<< HEAD
2025-01-07T23:56:22 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 4 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2025-01-07T23:56:23 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-01-07T23:56:23 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-01-07T23:56:23 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-01-07T23:56:23 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log'
2025-01-07 23:56:23,155 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-01-07 23:56:23,168 (launch:237) INFO: single-node with 4gpu on distributed mode
2025-01-07 23:56:23,169 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log', '--gpu', '4', 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '4', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
=======
>>>>>>> cea3af74175b4f1b718b2c4afeb0a5f028e5698c
2025-01-09T04:23:57 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 4 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-01-09T04:23:57 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-01-09T04:23:57 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-01-09T04:23:57 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-01-09T04:23:57 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-01-09 04:23:58,042 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-01-09 04:23:58,058 (launch:237) INFO: single-node with 4gpu on distributed mode
2025-01-09 04:23:58,059 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', '--gpu', '4', 'exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '4', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
>>>>>>> 5ebf8f1b6dc51bfa26a032f85e8e4b6ca01d6936
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
<<<<<<< HEAD
################### The last 1000 lines of exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 4 --multiprocessing_distributed True 
# Started at Tue Jan  7 23:56:23 PST 2025
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 4 --multiprocessing_distributed True
Process SpawnProcess-4:
=======
################### The last 1000 lines of exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 4 --multiprocessing_distributed True 
# Started at Thu Jan  9 04:23:58 EST 2025
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 4 --multiprocessing_distributed True
usage: asr_train.py [-h] [--config CONFIG] [--print_config]
                    [--log_level {ERROR,WARNING,INFO,DEBUG,NOTSET}]
                    [--drop_last_iter DROP_LAST_ITER] [--dry_run DRY_RUN]
                    [--iterator_type {sequence,category,chunk,task,none}]
                    [--valid_iterator_type {sequence,category,chunk,task,none}]
                    [--output_dir OUTPUT_DIR] [--ngpu NGPU] [--seed SEED]
                    [--num_workers NUM_WORKERS] [--num_att_plot NUM_ATT_PLOT]
                    [--dist_backend DIST_BACKEND]
                    [--dist_init_method DIST_INIT_METHOD]
                    [--dist_world_size DIST_WORLD_SIZE]
                    [--dist_rank DIST_RANK] [--local_rank LOCAL_RANK]
                    [--dist_master_addr DIST_MASTER_ADDR]
                    [--dist_master_port DIST_MASTER_PORT]
                    [--dist_launcher {slurm,mpi,None}]
                    [--multiprocessing_distributed MULTIPROCESSING_DISTRIBUTED]
                    [--unused_parameters UNUSED_PARAMETERS]
                    [--sharded_ddp SHARDED_DDP]
                    [--use_deepspeed USE_DEEPSPEED]
                    [--deepspeed_config DEEPSPEED_CONFIG]
                    [--cudnn_enabled CUDNN_ENABLED]
                    [--cudnn_benchmark CUDNN_BENCHMARK]
                    [--cudnn_deterministic CUDNN_DETERMINISTIC]
                    [--use_tf32 USE_TF32] [--collect_stats COLLECT_STATS]
                    [--write_collected_feats WRITE_COLLECTED_FEATS]
                    [--max_epoch MAX_EPOCH] [--patience PATIENCE]
                    [--val_scheduler_criterion VAL_SCHEDULER_CRITERION VAL_SCHEDULER_CRITERION]
                    [--early_stopping_criterion EARLY_STOPPING_CRITERION EARLY_STOPPING_CRITERION EARLY_STOPPING_CRITERION]
                    [--best_model_criterion BEST_MODEL_CRITERION [BEST_MODEL_CRITERION ...]]
                    [--keep_nbest_models KEEP_NBEST_MODELS [KEEP_NBEST_MODELS ...]]
                    [--nbest_averaging_interval NBEST_AVERAGING_INTERVAL]
                    [--grad_clip GRAD_CLIP] [--grad_clip_type GRAD_CLIP_TYPE]
                    [--grad_noise GRAD_NOISE] [--accum_grad ACCUM_GRAD]
                    [--no_forward_run NO_FORWARD_RUN] [--resume RESUME]
                    [--train_dtype {float16,float32,float64}]
                    [--use_amp USE_AMP] [--log_interval LOG_INTERVAL]
                    [--use_matplotlib USE_MATPLOTLIB]
                    [--use_tensorboard USE_TENSORBOARD]
                    [--create_graph_in_tensorboard CREATE_GRAPH_IN_TENSORBOARD]
                    [--use_wandb USE_WANDB] [--wandb_project WANDB_PROJECT]
                    [--wandb_id WANDB_ID] [--wandb_entity WANDB_ENTITY]
                    [--wandb_name WANDB_NAME]
                    [--wandb_model_log_interval WANDB_MODEL_LOG_INTERVAL]
                    [--detect_anomaly DETECT_ANOMALY]
                    [--use_adapter USE_ADAPTER] [--adapter {lora,houlsby}]
                    [--save_strategy {all,adapter_only,required_grad_only}]
                    [--adapter_conf ADAPTER_CONF]
                    [--pretrain_path PRETRAIN_PATH]
                    [--init_param [INIT_PARAM ...]]
                    [--ignore_init_mismatch IGNORE_INIT_MISMATCH]
                    [--freeze_param [FREEZE_PARAM ...]]
                    [--num_iters_per_epoch NUM_ITERS_PER_EPOCH]
                    [--batch_size BATCH_SIZE]
                    [--valid_batch_size VALID_BATCH_SIZE]
                    [--batch_bins BATCH_BINS]
                    [--valid_batch_bins VALID_BATCH_BINS]
                    [--category_sample_size CATEGORY_SAMPLE_SIZE]
                    [--train_shape_file TRAIN_SHAPE_FILE]
                    [--valid_shape_file VALID_SHAPE_FILE]
                    [--batch_type {unsorted,sorted,folded,length,numel}]
                    [--valid_batch_type {unsorted,sorted,folded,length,numel,None}]
                    [--fold_length FOLD_LENGTH]
                    [--sort_in_batch {descending,ascending}]
                    [--shuffle_within_batch SHUFFLE_WITHIN_BATCH]
                    [--sort_batch {descending,ascending}]
                    [--multiple_iterator MULTIPLE_ITERATOR]
                    [--chunk_length CHUNK_LENGTH]
                    [--chunk_shift_ratio CHUNK_SHIFT_RATIO]
                    [--num_cache_chunks NUM_CACHE_CHUNKS]
                    [--chunk_excluded_key_prefixes CHUNK_EXCLUDED_KEY_PREFIXES [CHUNK_EXCLUDED_KEY_PREFIXES ...]]
                    [--chunk_default_fs CHUNK_DEFAULT_FS]
                    [--chunk_max_abs_length CHUNK_MAX_ABS_LENGTH]
                    [--chunk_discard_short_samples CHUNK_DISCARD_SHORT_SAMPLES]
                    [--train_data_path_and_name_and_type TRAIN_DATA_PATH_AND_NAME_AND_TYPE]
                    [--valid_data_path_and_name_and_type VALID_DATA_PATH_AND_NAME_AND_TYPE]
                    [--multi_task_dataset MULTI_TASK_DATASET]
                    [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS]
                    [--max_cache_size MAX_CACHE_SIZE]
                    [--max_cache_fd MAX_CACHE_FD]
                    [--allow_multi_rates ALLOW_MULTI_RATES]
                    [--valid_max_cache_size VALID_MAX_CACHE_SIZE]
                    [--exclude_weight_decay EXCLUDE_WEIGHT_DECAY]
                    [--exclude_weight_decay_conf EXCLUDE_WEIGHT_DECAY_CONF]
                    [--optim {adam,adamw,sgd,adadelta,adagrad,adamax,asgd,lbfgs,rmsprop,rprop,radam}]
                    [--optim_conf OPTIM_CONF]
                    [--scheduler {reducelronplateau,lambdalr,steplr,multisteplr,exponentiallr,cosineannealinglr,noamlr,warmuplr,piecewiselinearwarmuplr,warmupsteplr,warmupreducelronplateau,cycliclr,onecyclelr,cosineannealingwarmrestarts,cosineannealingwarmuprestarts,None}]
                    [--scheduler_conf SCHEDULER_CONF]
                    [--token_list TOKEN_LIST]
                    [--init {chainer,xavier_uniform,xavier_normal,kaiming_uniform,kaiming_normal,None}]
                    [--input_size INPUT_SIZE] [--ctc_conf CTC_CONF]
                    [--joint_net_conf JOINT_NET_CONF]
                    [--use_preprocessor USE_PREPROCESSOR]
                    [--use_lang_prompt USE_LANG_PROMPT]
                    [--use_nlp_prompt USE_NLP_PROMPT]
                    [--token_type {bpe,char,word,phn,hugging_face,whisper_en,whisper_multilingual}]
                    [--bpemodel BPEMODEL]
                    [--non_linguistic_symbols NON_LINGUISTIC_SYMBOLS]
                    [--cleaner {None,tacotron,jaconv,vietnamese,whisper_en,whisper_basic}]
                    [--g2p {None,g2p_en,g2p_en_no_space,pyopenjtalk,pyopenjtalk_kana,pyopenjtalk_accent,pyopenjtalk_accent_with_pause,pyopenjtalk_prosody,pypinyin_g2p,pypinyin_g2p_phone,pypinyin_g2p_phone_without_prosody,espeak_ng_arabic,espeak_ng_german,espeak_ng_french,espeak_ng_spanish,espeak_ng_russian,espeak_ng_greek,espeak_ng_finnish,espeak_ng_hungarian,espeak_ng_dutch,espeak_ng_english_us_vits,espeak_ng_hindi,espeak_ng_italian,espeak_ng_ukrainian,espeak_ng_polish,g2pk,g2pk_no_space,g2pk_explicit_space,korean_jaso,korean_jaso_no_space,g2p_is}]
                    [--speech_volume_normalize SPEECH_VOLUME_NORMALIZE]
                    [--rir_scp RIR_SCP] [--rir_apply_prob RIR_APPLY_PROB]
                    [--noise_scp NOISE_SCP]
                    [--noise_apply_prob NOISE_APPLY_PROB]
                    [--noise_db_range NOISE_DB_RANGE]
                    [--short_noise_thres SHORT_NOISE_THRES]
                    [--aux_ctc_tasks AUX_CTC_TASKS [AUX_CTC_TASKS ...]]
                    [--frontend {default,sliding_window,s3prl,hf_freeze_ctc,hf_train_fsq_ctc,fused,whisper}]
                    [--frontend_conf FRONTEND_CONF] [--specaug {specaug,None}]
                    [--specaug_conf SPECAUG_CONF]
                    [--normalize {global_mvn,utterance_mvn,None}]
                    [--normalize_conf NORMALIZE_CONF]
                    [--model {espnet,maskctc,pit_espnet}]
                    [--model_conf MODEL_CONF]
                    [--preencoder {sinc,linear,None}]
                    [--preencoder_conf PREENCODER_CONF]
                    [--encoder {conformer,transformer,transformer_multispkr,contextual_block_transformer,contextual_block_conformer,vgg_rnn,rnn,wav2vec2,hubert,hubert_pretrain,torchaudiohubert,longformer,branchformer,whisper,e_branchformer,avhubert,multiconv_conformer}]
                    [--encoder_conf ENCODER_CONF]
                    [--postencoder {hugging_face_transformers,length_adaptor,None}]
                    [--postencoder_conf POSTENCODER_CONF]
                    [--decoder {transformer,lightweight_conv,lightweight_conv2d,dynamic_conv,dynamic_conv2d,rnn,transducer,mlm,whisper,hugging_face_transformers,s4,None}]
                    [--decoder_conf DECODER_CONF]
                    [--preprocessor {default,multi}]
                    [--preprocessor_conf PREPROCESSOR_CONF]
asr_train.py: error: No such file: conf/train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4.yaml
# Accounting: time=10 threads=1
# Ended (code 2) at Thu Jan  9 04:24:08 EST 2025, elapsed time 10 seconds

2025-01-09T04:25:00 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 4 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-01-09T04:25:01 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-01-09T04:25:01 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-01-09T04:25:01 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-01-09T04:25:01 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-01-09 04:25:01,266 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-01-09 04:25:01,282 (launch:237) INFO: single-node with 4gpu on distributed mode
2025-01-09 04:25:01,283 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', '--gpu', '4', 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '4', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 4 --multiprocessing_distributed True 
# Started at Thu Jan  9 04:25:01 EST 2025
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 4 --multiprocessing_distributed True
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[dl:0/4] 2025-01-09 04:25:20,753 (asr:527) INFO: Vocabulary size: 5000
[dl:0/4] 2025-01-09 04:25:23,668 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
[dl:0/4] 2025-01-09 04:25:24,758 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl:0/4] 2025-01-09 04:25:24,767 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): HfCTCFSQFrontend(
    (upstream): WavLMForCTC(
      (wavlm): WavLMModel(
        (feature_extractor): WavLMFeatureEncoder(
          (conv_layers): ModuleList(
            (0): WavLMLayerNormConvLayer(
              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (activation): GELUActivation()
            )
            (1-4): 4 x WavLMLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (activation): GELUActivation()
            )
            (5-6): 2 x WavLMLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (activation): GELUActivation()
            )
          )
        )
        (feature_projection): WavLMFeatureProjection(
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (projection): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (encoder): WavLMEncoderStableLayerNorm(
          (pos_conv_embed): WavLMPositionalConvEmbedding(
            (conv): ParametrizedConv1d(
              1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16
              (parametrizations): ModuleDict(
                (weight): ParametrizationList(
                  (0): _WeightNorm()
                )
              )
            )
            (padding): WavLMSamePadLayer()
            (activation): GELUActivation()
          )
          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (layers): ModuleList(
            (0): WavLMEncoderLayerStableLayerNorm(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
                (rel_attn_embed): Embedding(320, 16)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=4096, out_features=1024, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
            (1-23): 23 x WavLMEncoderLayerStableLayerNorm(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=4096, out_features=1024, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (lm_head): Linear(in_features=1024, out_features=42, bias=True)
    )
    (quantizer): FSQ(
      (project_in): Linear(in_features=1024, out_features=5, bias=True)
      (project_out): Linear(in_features=5, out_features=1024, bias=True)
    )
  )
  (preencoder): LinearProjection(
    (linear_out): Linear(in_features=1024, out_features=128, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=7936, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 342.86 M
    Number of trainable parameters: 338.65 M (98.8%)
    Size: 1.35 GB
    Type: torch.float32
[dl:0/4] 2025-01-09 04:25:24,767 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 6.666666666666668e-09
    maximize: False
    weight_decay: 1e-06
)
[dl:0/4] 2025-01-09 04:25:24,767 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[dl:0/4] 2025-01-09 04:25:24,767 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/config.yaml
[dl:0/4] 2025-01-09 04:25:24,926 (asr:499) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/4] 2025-01-09 04:25:25,621 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7918aa7234f0>)
[dl:0/4] 2025-01-09 04:25:25,621 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=5974, batch_bins=2000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/4] 2025-01-09 04:25:25,622 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=5974, mean=9.3, min=4, max=57
[dl:0/4] 2025-01-09 04:25:25,638 (asr:499) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/4] 2025-01-09 04:25:25,674 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7918aa3e3550>)
[dl:0/4] 2025-01-09 04:25:25,674 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=954, batch_bins=2000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/4] 2025-01-09 04:25:25,674 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=954, mean=9.5, min=4, max=55
[dl:0/4] 2025-01-09 04:25:25,686 (asr:499) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/4] 2025-01-09 04:25:25,693 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7918aa3fbd30>)
[dl:0/4] 2025-01-09 04:25:25,693 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl:0/4] 2025-01-09 04:25:25,693 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
dl:1650678:1650678 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650678:1650678 [0] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1650678:1650678 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1650678:1650678 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.20.5+cuda12.4
dl:1650681:1650681 [2] NCCL INFO cudaDriverVersion 12050
dl:1650681:1650681 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650681:1650681 [2] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1650681:1650681 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1650681:1650858 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650681:1650858 [2] NCCL INFO NET/IB : No device found.
dl:1650681:1650858 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650681:1650858 [2] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:1650681:1650858 [2] NCCL INFO Using non-device net plugin version 0
dl:1650681:1650858 [2] NCCL INFO Using network Socket
dl:1650681:1650858 [2] NCCL INFO comm 0x58699d80 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 49000 commId 0xce73e7998c3f4506 - Init START
dl:1650681:1650858 [2] NCCL INFO NVLS multicast support is not available on dev 2
dl:1650681:1650858 [2] NCCL INFO comm 0x58699d80 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
dl:1650681:1650858 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
dl:1650681:1650858 [2] NCCL INFO P2P Chunksize set to 131072
dl:1650681:1650858 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
dl:1650681:1650858 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
dl:1650681:1650858 [2] NCCL INFO Connected all rings
dl:1650681:1650858 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
dl:1650681:1650858 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
dl:1650681:1650858 [2] NCCL INFO Connected all trees
dl:1650681:1650858 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:1650681:1650858 [2] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:1650681:1650858 [2] NCCL INFO comm 0x58699d80 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 49000 commId 0xce73e7998c3f4506 - Init COMPLETE
dl:1650680:1650680 [1] NCCL INFO cudaDriverVersion 12050
dl:1650680:1650680 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650680:1650680 [1] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1650680:1650680 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1650680:1650856 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650680:1650856 [1] NCCL INFO NET/IB : No device found.
dl:1650680:1650856 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650680:1650856 [1] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:1650680:1650856 [1] NCCL INFO Using non-device net plugin version 0
dl:1650680:1650856 [1] NCCL INFO Using network Socket
dl:1650680:1650856 [1] NCCL INFO comm 0xc849470 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 21000 commId 0xce73e7998c3f4506 - Init START
dl:1650680:1650856 [1] NCCL INFO NVLS multicast support is not available on dev 1
dl:1650680:1650856 [1] NCCL INFO comm 0xc849470 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
dl:1650680:1650856 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
dl:1650680:1650856 [1] NCCL INFO P2P Chunksize set to 131072
dl:1650680:1650856 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
dl:1650680:1650856 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
dl:1650680:1650856 [1] NCCL INFO Connected all rings
dl:1650680:1650856 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
dl:1650680:1650856 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
dl:1650680:1650856 [1] NCCL INFO Connected all trees
dl:1650680:1650856 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:1650680:1650856 [1] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:1650680:1650856 [1] NCCL INFO comm 0xc849470 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 21000 commId 0xce73e7998c3f4506 - Init COMPLETE
[rank2]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
[rank1]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
dl:1650678:1650855 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650678:1650855 [0] NCCL INFO NET/IB : No device found.
dl:1650678:1650855 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650678:1650855 [0] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:1650678:1650855 [0] NCCL INFO Using non-device net plugin version 0
dl:1650678:1650855 [0] NCCL INFO Using network Socket
dl:1650678:1650855 [0] NCCL INFO comm 0x376cfbe0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 2000 commId 0xce73e7998c3f4506 - Init START
dl:1650678:1650855 [0] NCCL INFO NVLS multicast support is not available on dev 0
dl:1650678:1650855 [0] NCCL INFO comm 0x376cfbe0 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
dl:1650678:1650855 [0] NCCL INFO Channel 00/02 :    0   1   2   3
dl:1650678:1650855 [0] NCCL INFO Channel 01/02 :    0   1   2   3
dl:1650678:1650855 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
dl:1650678:1650855 [0] NCCL INFO P2P Chunksize set to 131072
dl:1650678:1650855 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
dl:1650678:1650855 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
dl:1650678:1650855 [0] NCCL INFO Connected all rings
dl:1650678:1650855 [0] NCCL INFO Connected all trees
dl:1650678:1650855 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:1650678:1650855 [0] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:1650678:1650855 [0] NCCL INFO comm 0x376cfbe0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 2000 commId 0xce73e7998c3f4506 - Init COMPLETE
[rank0]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
dl:1650683:1650683 [3] NCCL INFO cudaDriverVersion 12050
dl:1650683:1650683 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650683:1650683 [3] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1650683:1650683 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1650683:1650857 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650683:1650857 [3] NCCL INFO NET/IB : No device found.
dl:1650683:1650857 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650683:1650857 [3] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:1650683:1650857 [3] NCCL INFO Using non-device net plugin version 0
dl:1650683:1650857 [3] NCCL INFO Using network Socket
dl:1650683:1650857 [3] NCCL INFO comm 0xa9247d0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 4a000 commId 0xce73e7998c3f4506 - Init START
dl:1650683:1650857 [3] NCCL INFO NVLS multicast support is not available on dev 3
dl:1650683:1650857 [3] NCCL INFO comm 0xa9247d0 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
dl:1650683:1650857 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
dl:1650683:1650857 [3] NCCL INFO P2P Chunksize set to 131072
dl:1650683:1650857 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM
dl:1650683:1650857 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/CUMEM
dl:1650683:1650857 [3] NCCL INFO Connected all rings
dl:1650683:1650857 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
dl:1650683:1650857 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
dl:1650683:1650857 [3] NCCL INFO Connected all trees
dl:1650683:1650857 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:1650683:1650857 [3] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:1650683:1650857 [3] NCCL INFO comm 0xa9247d0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 4a000 commId 0xce73e7998c3f4506 - Init COMPLETE
[rank3]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
[dl:0/4] 2025-01-09 04:25:26,502 (trainer:311) INFO: 1/70epoch started
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
Process SpawnProcess-1:
>>>>>>> 5ebf8f1b6dc51bfa26a032f85e8e4b6ca01d6936
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
<<<<<<< HEAD
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1283, in main_worker
    distributed_option.init_options()
  File "/data/mohan/workdir/espnet/espnet2/train/distributed_utils.py", line 60, in init_options
    raise RuntimeError(
RuntimeError: LOCAL_RANK=3 is bigger than the number of visible devices: 0
Process SpawnProcess-2:
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1283, in main_worker
    distributed_option.init_options()
  File "/data/mohan/workdir/espnet/espnet2/train/distributed_utils.py", line 60, in init_options
    raise RuntimeError(
RuntimeError: LOCAL_RANK=1 is bigger than the number of visible devices: 0
Process SpawnProcess-3:
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1283, in main_worker
    distributed_option.init_options()
  File "/data/mohan/workdir/espnet/espnet2/train/distributed_utils.py", line 60, in init_options
    raise RuntimeError(
RuntimeError: LOCAL_RANK=2 is bigger than the number of visible devices: 0
W0107 23:56:33.980000 124899948611392 torch/multiprocessing/spawn.py:145] Terminating process 1585013 via signal SIGTERM
W0107 23:56:33.980000 124899948611392 torch/multiprocessing/spawn.py:145] Terminating process 1585014 via signal SIGTERM
W0107 23:56:33.980000 124899948611392 torch/multiprocessing/spawn.py:145] Terminating process 1585015 via signal SIGTERM
=======
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 614, in train_one_epoch
    retval = model(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 237, in forward
    encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 402, in encode
    encoder_out, encoder_out_lens, _ = self.encoder(feats, feats_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/encoder/e_branchformer_encoder.py", line 495, in forward
    xs_pad, masks = self.encoders(xs_pad, masks)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet/nets/pytorch_backend/transformer/repeat.py", line 30, in forward
    args = m(*args)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/encoder/e_branchformer_encoder.py", line 157, in forward
    x2 = self.cgmlp(x2, mask)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/layers/cgmlp.py", line 118, in forward
    xs_pad = self.channel_proj2(xs_pad)  # linear_units/2 -> size
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 
dl:1650678:1650859 [0] NCCL INFO [Service thread] Connection closed by localRank 0
dl:1650678:1653049 [0] NCCL INFO comm 0x376cfbe0 rank 0 nranks 4 cudaDev 0 busId 2000 - Abort COMPLETE
W0109 04:30:19.545000 126123558676288 torch/multiprocessing/spawn.py:145] Terminating process 1650680 via signal SIGTERM
W0109 04:30:19.546000 126123558676288 torch/multiprocessing/spawn.py:145] Terminating process 1650681 via signal SIGTERM
W0109 04:30:19.546000 126123558676288 torch/multiprocessing/spawn.py:145] Terminating process 1650683 via signal SIGTERM
>>>>>>> 5ebf8f1b6dc51bfa26a032f85e8e4b6ca01d6936
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1273, in main
    while not ProcessContext(processes, error_files).join():
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 177, in join
    raise ProcessExitedException(
<<<<<<< HEAD
torch.multiprocessing.spawn.ProcessExitedException: process 3 terminated with exit code 1
# Accounting: time=11 threads=1
# Ended (code 1) at Tue Jan  7 23:56:34 PST 2025, elapsed time 11 seconds

2025-01-07T23:57:40 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2025-01-07T23:57:40 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-01-07T23:57:40 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-01-07T23:57:40 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-01-07T23:57:40 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log'
2025-01-07 23:57:40,772 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-01-07 23:57:40,785 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log
2025-01-09T18:20:44 (asr.sh:1813:main) Successfully finished. [elapsed=152584s]
2025-01-09T19:12:29 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2025-01-09T19:12:29 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-01-09T19:12:29 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000
2025-01-09T19:12:29 (asr.sh:1508:main) Generate 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 12 using this script
2025-01-09T19:12:29 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/asr_inference.*.log'
2025-01-09T19:28:08 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 1 --nj 4 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2025-01-09T19:28:08 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-01-09T19:28:08 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000
2025-01-09T19:28:08 (asr.sh:1508:main) Generate 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 12 using this script
2025-01-09T19:28:08 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/asr_inference.*.log'
2025-01-09T21:17:01 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2025-01-09T21:17:02 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-01-09T21:17:02 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000
2025-01-09T21:17:02 (asr.sh:1508:main) Generate 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 12 using this script
2025-01-09T21:17:02 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/asr_inference.*.log'
2025-01-09T22:11:29 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/calculate_rtf.log'
2025-01-09T22:11:30 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/test/logdir/asr_inference.*.log'
2025-01-09T23:23:31 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/test/logdir/calculate_rtf.log'
2025-01-09T23:23:32 (asr.sh:1813:main) Successfully finished. [elapsed=7591s]
=======
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with exit code 1
# Accounting: time=320 threads=1
# Ended (code 1) at Thu Jan  9 04:30:21 EST 2025, elapsed time 320 seconds

2025-01-09T04:32:57 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 4 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-01-09T04:32:57 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-01-09T04:32:57 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-01-09T04:32:57 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-01-09T04:32:57 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-01-09 04:32:59,309 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-01-09 04:32:59,326 (launch:237) INFO: single-node with 4gpu on distributed mode
2025-01-09 04:32:59,330 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', '--gpu', '4', 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '4', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 4 --multiprocessing_distributed True 
# Started at Thu Jan  9 04:32:59 EST 2025
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 4 --multiprocessing_distributed True
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[dl:0/4] 2025-01-09 04:33:31,455 (asr:527) INFO: Vocabulary size: 5000
[dl:0/4] 2025-01-09 04:33:40,722 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
[dl:0/4] 2025-01-09 04:33:41,704 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl:0/4] 2025-01-09 04:33:41,714 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): HfCTCFSQFrontend(
    (upstream): WavLMForCTC(
      (wavlm): WavLMModel(
        (feature_extractor): WavLMFeatureEncoder(
          (conv_layers): ModuleList(
            (0): WavLMLayerNormConvLayer(
              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (activation): GELUActivation()
            )
            (1-4): 4 x WavLMLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (activation): GELUActivation()
            )
            (5-6): 2 x WavLMLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (activation): GELUActivation()
            )
          )
        )
        (feature_projection): WavLMFeatureProjection(
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (projection): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (encoder): WavLMEncoderStableLayerNorm(
          (pos_conv_embed): WavLMPositionalConvEmbedding(
            (conv): ParametrizedConv1d(
              1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16
              (parametrizations): ModuleDict(
                (weight): ParametrizationList(
                  (0): _WeightNorm()
                )
              )
            )
            (padding): WavLMSamePadLayer()
            (activation): GELUActivation()
          )
          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (layers): ModuleList(
            (0): WavLMEncoderLayerStableLayerNorm(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
                (rel_attn_embed): Embedding(320, 16)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=4096, out_features=1024, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
            (1-23): 23 x WavLMEncoderLayerStableLayerNorm(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=4096, out_features=1024, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (lm_head): Linear(in_features=1024, out_features=42, bias=True)
    )
    (quantizer): FSQ(
      (project_in): Linear(in_features=1024, out_features=5, bias=True)
      (project_out): Linear(in_features=5, out_features=1024, bias=True)
    )
  )
  (preencoder): LinearProjection(
    (linear_out): Linear(in_features=1024, out_features=128, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=7936, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 342.86 M
    Number of trainable parameters: 338.65 M (98.8%)
    Size: 1.35 GB
    Type: torch.float32
[dl:0/4] 2025-01-09 04:33:41,714 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 6.666666666666668e-09
    maximize: False
    weight_decay: 1e-06
)
[dl:0/4] 2025-01-09 04:33:41,714 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[dl:0/4] 2025-01-09 04:33:41,714 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/config.yaml
[dl:0/4] 2025-01-09 04:33:41,884 (asr:499) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/4] 2025-01-09 04:33:42,637 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7e9387011210>)
[dl:0/4] 2025-01-09 04:33:42,637 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=9198, batch_bins=1000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/4] 2025-01-09 04:33:42,639 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=9198, mean=6.1, min=4, max=32
[dl:0/4] 2025-01-09 04:33:42,660 (asr:499) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/4] 2025-01-09 04:33:42,711 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7e938701c910>)
[dl:0/4] 2025-01-09 04:33:42,712 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=1483, batch_bins=1000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/4] 2025-01-09 04:33:42,712 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=1483, mean=6.1, min=4, max=28
[dl:0/4] 2025-01-09 04:33:42,726 (asr:499) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/4] 2025-01-09 04:33:42,734 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7e9381145d20>)
[dl:0/4] 2025-01-09 04:33:42,734 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl:0/4] 2025-01-09 04:33:42,735 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
dl:1653741:1653741 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653741:1653741 [0] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1653741:1653741 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1653741:1653741 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.20.5+cuda12.4
dl:1653743:1653743 [2] NCCL INFO cudaDriverVersion 12050
dl:1653743:1653743 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653743:1653743 [2] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1653743:1653743 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1653743:1653981 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653743:1653981 [2] NCCL INFO NET/IB : No device found.
dl:1653743:1653981 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653743:1653981 [2] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:1653743:1653981 [2] NCCL INFO Using non-device net plugin version 0
dl:1653743:1653981 [2] NCCL INFO Using network Socket
dl:1653743:1653981 [2] NCCL INFO comm 0xc12daf0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 49000 commId 0x408e9fc53d949dff - Init START
dl:1653743:1653981 [2] NCCL INFO NVLS multicast support is not available on dev 2
dl:1653743:1653981 [2] NCCL INFO comm 0xc12daf0 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
dl:1653743:1653981 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
dl:1653743:1653981 [2] NCCL INFO P2P Chunksize set to 131072
dl:1653743:1653981 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
dl:1653743:1653981 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
dl:1653743:1653981 [2] NCCL INFO Connected all rings
dl:1653743:1653981 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
dl:1653743:1653981 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
dl:1653743:1653981 [2] NCCL INFO Connected all trees
dl:1653743:1653981 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:1653743:1653981 [2] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:1653743:1653981 [2] NCCL INFO comm 0xc12daf0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 49000 commId 0x408e9fc53d949dff - Init COMPLETE
[rank2]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
dl:1653744:1653744 [3] NCCL INFO cudaDriverVersion 12050
dl:1653744:1653744 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653744:1653744 [3] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1653744:1653744 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1653744:1653979 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653744:1653979 [3] NCCL INFO NET/IB : No device found.
dl:1653744:1653979 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653744:1653979 [3] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:1653744:1653979 [3] NCCL INFO Using non-device net plugin version 0
dl:1653744:1653979 [3] NCCL INFO Using network Socket
dl:1653744:1653979 [3] NCCL INFO comm 0x4e1607d0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 4a000 commId 0x408e9fc53d949dff - Init START
dl:1653744:1653979 [3] NCCL INFO NVLS multicast support is not available on dev 3
dl:1653744:1653979 [3] NCCL INFO comm 0x4e1607d0 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
dl:1653744:1653979 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
dl:1653744:1653979 [3] NCCL INFO P2P Chunksize set to 131072
dl:1653744:1653979 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM
dl:1653744:1653979 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/CUMEM
dl:1653744:1653979 [3] NCCL INFO Connected all rings
dl:1653744:1653979 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
dl:1653744:1653979 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
dl:1653744:1653979 [3] NCCL INFO Connected all trees
dl:1653744:1653979 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:1653744:1653979 [3] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:1653744:1653979 [3] NCCL INFO comm 0x4e1607d0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 4a000 commId 0x408e9fc53d949dff - Init COMPLETE
[rank3]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
dl:1653741:1653978 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653741:1653978 [0] NCCL INFO NET/IB : No device found.
dl:1653741:1653978 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653741:1653978 [0] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:1653741:1653978 [0] NCCL INFO Using non-device net plugin version 0
dl:1653741:1653978 [0] NCCL INFO Using network Socket
dl:1653741:1653978 [0] NCCL INFO comm 0x44f87ea0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 2000 commId 0x408e9fc53d949dff - Init START
dl:1653741:1653978 [0] NCCL INFO NVLS multicast support is not available on dev 0
dl:1653741:1653978 [0] NCCL INFO comm 0x44f87ea0 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
dl:1653741:1653978 [0] NCCL INFO Channel 00/02 :    0   1   2   3
dl:1653741:1653978 [0] NCCL INFO Channel 01/02 :    0   1   2   3
dl:1653741:1653978 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
dl:1653741:1653978 [0] NCCL INFO P2P Chunksize set to 131072
dl:1653741:1653978 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
dl:1653741:1653978 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
dl:1653741:1653978 [0] NCCL INFO Connected all rings
dl:1653741:1653978 [0] NCCL INFO Connected all trees
dl:1653741:1653978 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:1653741:1653978 [0] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:1653741:1653978 [0] NCCL INFO comm 0x44f87ea0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 2000 commId 0x408e9fc53d949dff - Init COMPLETE
[rank0]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
dl:1653742:1653742 [1] NCCL INFO cudaDriverVersion 12050
dl:1653742:1653742 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653742:1653742 [1] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1653742:1653742 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1653742:1653980 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653742:1653980 [1] NCCL INFO NET/IB : No device found.
dl:1653742:1653980 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653742:1653980 [1] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:1653742:1653980 [1] NCCL INFO Using non-device net plugin version 0
dl:1653742:1653980 [1] NCCL INFO Using network Socket
dl:1653742:1653980 [1] NCCL INFO comm 0x59ce0090 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 21000 commId 0x408e9fc53d949dff - Init START
dl:1653742:1653980 [1] NCCL INFO NVLS multicast support is not available on dev 1
dl:1653742:1653980 [1] NCCL INFO comm 0x59ce0090 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
dl:1653742:1653980 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
dl:1653742:1653980 [1] NCCL INFO P2P Chunksize set to 131072
dl:1653742:1653980 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
dl:1653742:1653980 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
dl:1653742:1653980 [1] NCCL INFO Connected all rings
dl:1653742:1653980 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
dl:1653742:1653980 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
dl:1653742:1653980 [1] NCCL INFO Connected all trees
dl:1653742:1653980 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:1653742:1653980 [1] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:1653742:1653980 [1] NCCL INFO comm 0x59ce0090 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 21000 commId 0x408e9fc53d949dff - Init COMPLETE
[rank1]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
[dl:0/4] 2025-01-09 04:33:43,402 (trainer:311) INFO: 1/70epoch started
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
[dl:0/4] 2025-01-09 04:39:26,872 (trainer:779) INFO: 1epoch:train:1-459batch: iter_time=4.084e-04, forward_time=0.207, loss_ctc=820.796, loss=820.796, backward_time=0.400, grad_norm=2.444e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.023, optim0_lr0=1.033e-07, train_time=11.994
[dl:0/4] 2025-01-09 04:44:24,242 (trainer:779) INFO: 1epoch:train:460-918batch: iter_time=2.724e-04, forward_time=0.202, loss_ctc=815.300, loss=815.300, backward_time=0.397, grad_norm=2.835e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.021, optim0_lr0=2.933e-07, train_time=10.376
[dl:0/4] 2025-01-09 04:49:35,130 (trainer:779) INFO: 1epoch:train:919-1377batch: iter_time=2.735e-04, forward_time=0.220, loss_ctc=792.735, loss=792.735, backward_time=0.405, grad_norm=5.410e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.021, optim0_lr0=4.867e-07, train_time=10.834
[dl:0/4] 2025-01-09 04:54:38,939 (trainer:779) INFO: 1epoch:train:1378-1836batch: iter_time=2.355e-04, forward_time=0.208, loss_ctc=651.194, loss=651.194, backward_time=0.400, grad_norm=5.732e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.021, optim0_lr0=6.767e-07, train_time=10.585
[dl:0/4] 2025-01-09 04:59:48,095 (trainer:779) INFO: 1epoch:train:1837-2295batch: iter_time=2.849e-04, forward_time=0.219, loss_ctc=560.796, loss=560.796, backward_time=0.400, grad_norm=4.010e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.022, optim0_lr0=8.667e-07, train_time=10.782
[dl:0/4] 2025-01-09 05:04:53,663 (trainer:779) INFO: 1epoch:train:2296-2754batch: iter_time=2.320e-04, forward_time=0.213, loss_ctc=480.817, loss=480.817, backward_time=0.400, grad_norm=2.325e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.020, optim0_lr0=1.060e-06, train_time=10.661
[dl:0/4] 2025-01-09 05:09:41,726 (trainer:779) INFO: 1epoch:train:2755-3213batch: iter_time=1.736e-04, forward_time=0.191, loss_ctc=419.804, loss=419.804, backward_time=0.386, grad_norm=1.545e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.016, optim0_lr0=1.250e-06, train_time=10.045
[dl:0/4] 2025-01-09 05:14:56,877 (trainer:779) INFO: 1epoch:train:3214-3672batch: iter_time=4.172e-04, forward_time=0.223, loss_ctc=383.714, loss=383.714, backward_time=0.406, grad_norm=1.326e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.022, optim0_lr0=1.440e-06, train_time=10.933
[dl:0/4] 2025-01-09 05:20:13,363 (trainer:779) INFO: 1epoch:train:3673-4131batch: iter_time=2.977e-04, forward_time=0.227, loss_ctc=335.541, loss=335.541, backward_time=0.403, grad_norm=1.116e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.024, optim0_lr0=1.633e-06, train_time=11.050
[dl:0/4] 2025-01-09 05:25:27,165 (trainer:779) INFO: 1epoch:train:4132-4590batch: iter_time=2.699e-04, forward_time=0.228, loss_ctc=349.711, loss=349.711, backward_time=0.403, grad_norm=1.068e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.025, optim0_lr0=1.823e-06, train_time=10.924
[dl:0/4] 2025-01-09 05:30:48,154 (trainer:779) INFO: 1epoch:train:4591-5049batch: iter_time=2.628e-04, forward_time=0.234, loss_ctc=346.744, loss=346.744, backward_time=0.409, grad_norm=1.034e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.025, optim0_lr0=2.013e-06, train_time=11.207
[dl:0/4] 2025-01-09 05:36:04,495 (trainer:779) INFO: 1epoch:train:5050-5508batch: iter_time=2.875e-04, forward_time=0.230, loss_ctc=339.174, loss=339.174, backward_time=0.404, grad_norm=1.001e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.024, optim0_lr0=2.207e-06, train_time=11.014
[dl:0/4] 2025-01-09 05:41:21,617 (trainer:779) INFO: 1epoch:train:5509-5967batch: iter_time=2.610e-04, forward_time=0.230, loss_ctc=320.948, loss=320.948, backward_time=0.403, grad_norm=996.920, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.038, optim0_lr0=2.397e-06, train_time=11.064
[dl:0/4] 2025-01-09 05:46:40,494 (trainer:779) INFO: 1epoch:train:5968-6426batch: iter_time=2.113e-04, forward_time=0.226, loss_ctc=237.084, loss=237.084, backward_time=0.409, grad_norm=1.664e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.022, optim0_lr0=2.587e-06, train_time=11.103
[dl:0/4] 2025-01-09 05:51:58,938 (trainer:779) INFO: 1epoch:train:6427-6885batch: iter_time=2.007e-04, forward_time=0.228, loss_ctc=178.305, loss=178.305, backward_time=0.406, grad_norm=496.213, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.021, optim0_lr0=2.780e-06, train_time=11.107
[dl:0/4] 2025-01-09 05:57:12,626 (trainer:779) INFO: 1epoch:train:6886-7344batch: iter_time=1.980e-04, forward_time=0.214, loss_ctc=172.182, loss=172.182, backward_time=0.401, grad_norm=394.727, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.020, optim0_lr0=2.973e-06, train_time=10.934
[dl:0/4] 2025-01-09 06:02:30,182 (trainer:779) INFO: 1epoch:train:7345-7803batch: iter_time=1.988e-04, forward_time=0.221, loss_ctc=171.158, loss=171.158, backward_time=0.403, grad_norm=360.756, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.020, optim0_lr0=3.163e-06, train_time=11.067
[dl:0/4] 2025-01-09 06:07:51,830 (trainer:779) INFO: 1epoch:train:7804-8262batch: iter_time=2.075e-04, forward_time=0.230, loss_ctc=168.643, loss=168.643, backward_time=0.406, grad_norm=321.227, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.021, optim0_lr0=3.353e-06, train_time=11.216
[dl:0/4] 2025-01-09 06:13:13,234 (trainer:779) INFO: 1epoch:train:8263-8721batch: iter_time=2.560e-04, forward_time=0.234, loss_ctc=164.246, loss=164.246, backward_time=0.404, grad_norm=282.617, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.023, optim0_lr0=3.547e-06, train_time=11.199
[dl:0/4] 2025-01-09 06:18:29,370 (trainer:779) INFO: 1epoch:train:8722-9180batch: iter_time=2.444e-04, forward_time=0.225, loss_ctc=155.266, loss=155.266, backward_time=0.403, grad_norm=244.800, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.024, optim0_lr0=3.737e-06, train_time=11.032
/data/mohan/workdir/espnet/espnet2/train/reporter.py:79: UserWarning: No valid stats found
  warnings.warn("No valid stats found")
/data/mohan/workdir/espnet/espnet2/train/reporter.py:79: UserWarning: No valid stats found
  warnings.warn("No valid stats found")
/data/mohan/workdir/espnet/espnet2/train/reporter.py:79: UserWarning: No valid stats found
  warnings.warn("No valid stats found")
/data/mohan/workdir/espnet/espnet2/train/reporter.py:79: UserWarning: No valid stats found
  warnings.warn("No valid stats found")
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
[dl:0/4] 2025-01-09 06:27:30,739 (trainer:365) INFO: 1epoch results: [train] iter_time=2.595e-04, forward_time=0.221, loss_ctc=392.333, loss=392.333, backward_time=0.402, grad_norm=1.727e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.023, optim0_lr0=1.923e-06, train_time=10.956, time=1 hour, 45 minutes and 8.35 seconds, total_count=9198, gpu_max_cached_mem_GB=15.240, [valid] loss_ctc=154.724, cer_ctc=1.000, loss_att=nan, acc=nan, cer=nan, wer=nan, loss=154.724, time=7 minutes and 6.43 seconds, total_count=1483, gpu_max_cached_mem_GB=15.240, [att_plot] time=1 minute and 32.49 seconds, total_count=0, gpu_max_cached_mem_GB=15.240
[dl:0/4] 2025-01-09 06:27:56,053 (trainer:433) INFO: The best model has been updated: valid.cer_ctc
[dl:0/4] 2025-01-09 06:27:56,086 (trainer:299) INFO: 2/70epoch started. Estimated time to finish: 5 days, 11 hours and 20 minutes
W0109 06:35:02.523000 136733754685248 torch/multiprocessing/spawn.py:145] Terminating process 1653742 via signal SIGTERM
W0109 06:35:02.641000 136733754685248 torch/multiprocessing/spawn.py:145] Terminating process 1653743 via signal SIGTERM
W0109 06:35:02.642000 136733754685248 torch/multiprocessing/spawn.py:145] Terminating process 1653744 via signal SIGTERM
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1273, in main
    while not ProcessContext(processes, error_files).join():
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 169, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGKILL
/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 80 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
# Accounting: time=7333 threads=1
# Ended (code 1) at Thu Jan  9 06:35:12 EST 2025, elapsed time 7333 seconds

>>>>>>> 5ebf8f1b6dc51bfa26a032f85e8e4b6ca01d6936
2025-03-20T17:28:18 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-20T17:28:18 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-20T17:28:18 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-20T17:28:18 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-20T17:28:18 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-20 17:28:18,886 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-20 17:28:18,899 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', '--gpu', '1', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Thu Mar 20 17:28:18 PDT 2025
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
usage: asr_train.py [-h] [--config CONFIG] [--print_config]
                    [--log_level {ERROR,WARNING,INFO,DEBUG,NOTSET}]
                    [--drop_last_iter DROP_LAST_ITER] [--dry_run DRY_RUN]
                    [--iterator_type {sequence,category,chunk,task,none}]
                    [--valid_iterator_type {sequence,category,chunk,task,none}]
                    [--output_dir OUTPUT_DIR] [--ngpu NGPU] [--seed SEED]
                    [--num_workers NUM_WORKERS] [--num_att_plot NUM_ATT_PLOT]
                    [--dist_backend DIST_BACKEND]
                    [--dist_init_method DIST_INIT_METHOD]
                    [--dist_world_size DIST_WORLD_SIZE]
                    [--dist_rank DIST_RANK] [--local_rank LOCAL_RANK]
                    [--dist_master_addr DIST_MASTER_ADDR]
                    [--dist_master_port DIST_MASTER_PORT]
                    [--dist_launcher {slurm,mpi,None}]
                    [--multiprocessing_distributed MULTIPROCESSING_DISTRIBUTED]
                    [--unused_parameters UNUSED_PARAMETERS]
                    [--sharded_ddp SHARDED_DDP]
                    [--use_deepspeed USE_DEEPSPEED]
                    [--deepspeed_config DEEPSPEED_CONFIG]
                    [--cudnn_enabled CUDNN_ENABLED]
                    [--cudnn_benchmark CUDNN_BENCHMARK]
                    [--cudnn_deterministic CUDNN_DETERMINISTIC]
                    [--use_tf32 USE_TF32] [--collect_stats COLLECT_STATS]
                    [--write_collected_feats WRITE_COLLECTED_FEATS]
                    [--max_epoch MAX_EPOCH] [--patience PATIENCE]
                    [--val_scheduler_criterion VAL_SCHEDULER_CRITERION VAL_SCHEDULER_CRITERION]
                    [--early_stopping_criterion EARLY_STOPPING_CRITERION EARLY_STOPPING_CRITERION EARLY_STOPPING_CRITERION]
                    [--best_model_criterion BEST_MODEL_CRITERION [BEST_MODEL_CRITERION ...]]
                    [--keep_nbest_models KEEP_NBEST_MODELS [KEEP_NBEST_MODELS ...]]
                    [--nbest_averaging_interval NBEST_AVERAGING_INTERVAL]
                    [--grad_clip GRAD_CLIP] [--grad_clip_type GRAD_CLIP_TYPE]
                    [--grad_noise GRAD_NOISE] [--accum_grad ACCUM_GRAD]
                    [--no_forward_run NO_FORWARD_RUN] [--resume RESUME]
                    [--train_dtype {float16,float32,float64}]
                    [--use_amp USE_AMP] [--log_interval LOG_INTERVAL]
                    [--use_matplotlib USE_MATPLOTLIB]
                    [--use_tensorboard USE_TENSORBOARD]
                    [--create_graph_in_tensorboard CREATE_GRAPH_IN_TENSORBOARD]
                    [--use_wandb USE_WANDB] [--wandb_project WANDB_PROJECT]
                    [--wandb_id WANDB_ID] [--wandb_entity WANDB_ENTITY]
                    [--wandb_name WANDB_NAME]
                    [--wandb_model_log_interval WANDB_MODEL_LOG_INTERVAL]
                    [--detect_anomaly DETECT_ANOMALY]
                    [--use_adapter USE_ADAPTER] [--adapter {lora,houlsby}]
                    [--save_strategy {all,adapter_only,required_grad_only}]
                    [--adapter_conf ADAPTER_CONF]
                    [--pretrain_path PRETRAIN_PATH]
                    [--init_param [INIT_PARAM ...]]
                    [--ignore_init_mismatch IGNORE_INIT_MISMATCH]
                    [--freeze_param [FREEZE_PARAM ...]]
                    [--num_iters_per_epoch NUM_ITERS_PER_EPOCH]
                    [--batch_size BATCH_SIZE]
                    [--valid_batch_size VALID_BATCH_SIZE]
                    [--batch_bins BATCH_BINS]
                    [--valid_batch_bins VALID_BATCH_BINS]
                    [--category_sample_size CATEGORY_SAMPLE_SIZE]
                    [--train_shape_file TRAIN_SHAPE_FILE]
                    [--valid_shape_file VALID_SHAPE_FILE]
                    [--batch_type {unsorted,sorted,folded,length,numel}]
                    [--valid_batch_type {unsorted,sorted,folded,length,numel,None}]
                    [--fold_length FOLD_LENGTH]
                    [--sort_in_batch {descending,ascending}]
                    [--shuffle_within_batch SHUFFLE_WITHIN_BATCH]
                    [--sort_batch {descending,ascending}]
                    [--multiple_iterator MULTIPLE_ITERATOR]
                    [--chunk_length CHUNK_LENGTH]
                    [--chunk_shift_ratio CHUNK_SHIFT_RATIO]
                    [--num_cache_chunks NUM_CACHE_CHUNKS]
                    [--chunk_excluded_key_prefixes CHUNK_EXCLUDED_KEY_PREFIXES [CHUNK_EXCLUDED_KEY_PREFIXES ...]]
                    [--chunk_default_fs CHUNK_DEFAULT_FS]
                    [--chunk_max_abs_length CHUNK_MAX_ABS_LENGTH]
                    [--chunk_discard_short_samples CHUNK_DISCARD_SHORT_SAMPLES]
                    [--train_data_path_and_name_and_type TRAIN_DATA_PATH_AND_NAME_AND_TYPE]
                    [--valid_data_path_and_name_and_type VALID_DATA_PATH_AND_NAME_AND_TYPE]
                    [--multi_task_dataset MULTI_TASK_DATASET]
                    [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS]
                    [--max_cache_size MAX_CACHE_SIZE]
                    [--max_cache_fd MAX_CACHE_FD]
                    [--allow_multi_rates ALLOW_MULTI_RATES]
                    [--valid_max_cache_size VALID_MAX_CACHE_SIZE]
                    [--exclude_weight_decay EXCLUDE_WEIGHT_DECAY]
                    [--exclude_weight_decay_conf EXCLUDE_WEIGHT_DECAY_CONF]
                    [--optim {adam,adamw,sgd,adadelta,adagrad,adamax,asgd,lbfgs,rmsprop,rprop,radam}]
                    [--optim_conf OPTIM_CONF]
                    [--scheduler {reducelronplateau,lambdalr,steplr,multisteplr,exponentiallr,cosineannealinglr,noamlr,warmuplr,piecewiselinearwarmuplr,warmupsteplr,warmupreducelronplateau,cycliclr,onecyclelr,cosineannealingwarmrestarts,cosineannealingwarmuprestarts,None}]
                    [--scheduler_conf SCHEDULER_CONF]
                    [--token_list TOKEN_LIST]
                    [--init {chainer,xavier_uniform,xavier_normal,kaiming_uniform,kaiming_normal,None}]
                    [--input_size INPUT_SIZE] [--ctc_conf CTC_CONF]
                    [--joint_net_conf JOINT_NET_CONF]
                    [--use_preprocessor USE_PREPROCESSOR]
                    [--use_lang_prompt USE_LANG_PROMPT]
                    [--use_nlp_prompt USE_NLP_PROMPT]
                    [--token_type {bpe,char,word,phn,hugging_face,whisper_en,whisper_multilingual}]
                    [--bpemodel BPEMODEL]
                    [--non_linguistic_symbols NON_LINGUISTIC_SYMBOLS]
                    [--cleaner {None,tacotron,jaconv,vietnamese,whisper_en,whisper_basic}]
                    [--g2p {None,g2p_en,g2p_en_no_space,pyopenjtalk,pyopenjtalk_kana,pyopenjtalk_accent,pyopenjtalk_accent_with_pause,pyopenjtalk_prosody,pypinyin_g2p,pypinyin_g2p_phone,pypinyin_g2p_phone_without_prosody,espeak_ng_arabic,espeak_ng_german,espeak_ng_french,espeak_ng_spanish,espeak_ng_russian,espeak_ng_greek,espeak_ng_finnish,espeak_ng_hungarian,espeak_ng_dutch,espeak_ng_english_us_vits,espeak_ng_hindi,espeak_ng_italian,espeak_ng_ukrainian,espeak_ng_polish,g2pk,g2pk_no_space,g2pk_explicit_space,korean_jaso,korean_jaso_no_space,g2p_is}]
                    [--speech_volume_normalize SPEECH_VOLUME_NORMALIZE]
                    [--rir_scp RIR_SCP] [--rir_apply_prob RIR_APPLY_PROB]
                    [--noise_scp NOISE_SCP]
                    [--noise_apply_prob NOISE_APPLY_PROB]
                    [--noise_db_range NOISE_DB_RANGE]
                    [--short_noise_thres SHORT_NOISE_THRES]
                    [--aux_ctc_tasks AUX_CTC_TASKS [AUX_CTC_TASKS ...]]
                    [--frontend {default,sliding_window,s3prl,hf_freeze_ctc,hf_train_fsq_ctc,hf_train_fsq_240_ctc,hf_train_fsq_1000_ctc,fused,whisper}]
                    [--frontend_conf FRONTEND_CONF] [--specaug {specaug,None}]
                    [--specaug_conf SPECAUG_CONF]
                    [--normalize {global_mvn,utterance_mvn,None}]
                    [--normalize_conf NORMALIZE_CONF]
                    [--model {espnet,maskctc,pit_espnet}]
                    [--model_conf MODEL_CONF]
                    [--preencoder {sinc,linear,None}]
                    [--preencoder_conf PREENCODER_CONF]
                    [--encoder {conformer,transformer,transformer_multispkr,contextual_block_transformer,contextual_block_conformer,vgg_rnn,rnn,wav2vec2,hubert,hubert_pretrain,torchaudiohubert,longformer,branchformer,whisper,e_branchformer,avhubert,multiconv_conformer}]
                    [--encoder_conf ENCODER_CONF]
                    [--postencoder {hugging_face_transformers,length_adaptor,None}]
                    [--postencoder_conf POSTENCODER_CONF]
                    [--decoder {transformer,lightweight_conv,lightweight_conv2d,dynamic_conv,dynamic_conv2d,rnn,transducer,mlm,whisper,hugging_face_transformers,s4,None}]
                    [--decoder_conf DECODER_CONF]
                    [--preprocessor {default,multi}]
                    [--preprocessor_conf PREPROCESSOR_CONF]
asr_train.py: error: No such file: conf/train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4.yaml
# Accounting: time=7 threads=1
# Ended (code 2) at Thu Mar 20 17:28:25 PDT 2025, elapsed time 7 seconds

2025-03-20T17:28:48 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-20T17:28:48 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-20T17:28:48 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-20T17:28:48 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-20T17:28:48 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-20 17:28:48,883 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-20 17:28:48,897 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', '--gpu', '1', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Thu Mar 20 17:28:48 PDT 2025
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[lambda-Lambda-Vector] 2025-03-20 17:28:54,134 (asr:531) INFO: Vocabulary size: 5000
[lambda-Lambda-Vector] 2025-03-20 17:28:55,330 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
[lambda-Lambda-Vector] 2025-03-20 17:28:55,612 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[lambda-Lambda-Vector] 2025-03-20 17:28:55,619 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): HfCTCFSQFrontend(
    (upstream): WavLMForCTC(
      (wavlm): WavLMModel(
        (feature_extractor): WavLMFeatureEncoder(
          (conv_layers): ModuleList(
            (0): WavLMLayerNormConvLayer(
              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (activation): GELUActivation()
            )
            (1-4): 4 x WavLMLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (activation): GELUActivation()
            )
            (5-6): 2 x WavLMLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (activation): GELUActivation()
            )
          )
        )
        (feature_projection): WavLMFeatureProjection(
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (projection): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (encoder): WavLMEncoderStableLayerNorm(
          (pos_conv_embed): WavLMPositionalConvEmbedding(
            (conv): ParametrizedConv1d(
              1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16
              (parametrizations): ModuleDict(
                (weight): ParametrizationList(
                  (0): _WeightNorm()
                )
              )
            )
            (padding): WavLMSamePadLayer()
            (activation): GELUActivation()
          )
          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (layers): ModuleList(
            (0): WavLMEncoderLayerStableLayerNorm(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
                (rel_attn_embed): Embedding(320, 16)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=4096, out_features=1024, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
            (1-23): 23 x WavLMEncoderLayerStableLayerNorm(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=4096, out_features=1024, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (lm_head): Linear(in_features=1024, out_features=42, bias=True)
    )
    (quantizer): FSQ(
      (project_in): Linear(in_features=1024, out_features=5, bias=True)
      (project_out): Linear(in_features=5, out_features=1024, bias=True)
    )
  )
  (preencoder): LinearProjection(
    (linear_out): Linear(in_features=768, out_features=128, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=7936, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 342.83 M
    Number of trainable parameters: 338.62 M (98.8%)
    Size: 1.35 GB
    Type: torch.float32
[lambda-Lambda-Vector] 2025-03-20 17:28:55,619 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 6.666666666666668e-09
    maximize: False
    weight_decay: 1e-06
)
[lambda-Lambda-Vector] 2025-03-20 17:28:55,619 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[lambda-Lambda-Vector] 2025-03-20 17:28:55,619 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/config.yaml
[lambda-Lambda-Vector] 2025-03-20 17:28:55,742 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-20 17:28:56,038 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x74d06c589c60>)
[lambda-Lambda-Vector] 2025-03-20 17:28:56,038 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=911, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2025-03-20 17:28:56,039 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=911, mean=61.1, min=16, max=362
[lambda-Lambda-Vector] 2025-03-20 17:28:56,048 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-20 17:28:56,182 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x74d06c132860>)
[lambda-Lambda-Vector] 2025-03-20 17:28:56,182 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=145, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2025-03-20 17:28:56,183 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=145, mean=62.3, min=1, max=309
[lambda-Lambda-Vector] 2025-03-20 17:28:56,191 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-20 17:28:56,197 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x74d06c1320e0>)
[lambda-Lambda-Vector] 2025-03-20 17:28:56,197 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[lambda-Lambda-Vector] 2025-03-20 17:28:56,197 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[lambda-Lambda-Vector] 2025-03-20 17:28:56,198 (trainer:311) INFO: 1/70epoch started
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1210, in main
    cls.main_worker(args)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 614, in train_one_epoch
    retval = model(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 237, in forward
    encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 378, in encode
    feats, feats_lengths = self._extract_feats(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 445, in _extract_feats
    feats, feats_lengths = self.frontend(speech, speech_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/frontend/hf_fsq_train.py", line 100, in forward
    outputs = self.upstream.wavlm(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 1227, in forward
    encoder_outputs = self.encoder(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 807, in forward
    layer_outputs = layer(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 650, in forward
    hidden_states, attn_weights, position_bias = self.attention(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 473, in forward
    attn_output, attn_weights = self.torch_multi_head_self_attention(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 497, in torch_multi_head_self_attention
    attn_output, attn_weights = F.multi_head_attention_forward(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py", line 5504, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.78 GiB. GPU 
# Accounting: time=11 threads=1
# Ended (code 1) at Thu Mar 20 17:28:59 PDT 2025, elapsed time 11 seconds

2025-03-20T17:30:02 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-20T17:30:02 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-20T17:30:02 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-20T17:30:02 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-20T17:30:02 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-20 17:30:02,936 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-20 17:30:02,949 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', '--gpu', '1', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Thu Mar 20 17:30:02 PDT 2025
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[lambda-Lambda-Vector] 2025-03-20 17:30:08,292 (asr:531) INFO: Vocabulary size: 5000
[lambda-Lambda-Vector] 2025-03-20 17:30:08,705 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
[lambda-Lambda-Vector] 2025-03-20 17:30:08,895 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[lambda-Lambda-Vector] 2025-03-20 17:30:08,901 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): HfCTCFSQFrontend(
    (upstream): WavLMForCTC(
      (wavlm): WavLMModel(
        (feature_extractor): WavLMFeatureEncoder(
          (conv_layers): ModuleList(
            (0): WavLMGroupNormConvLayer(
              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
              (activation): GELUActivation()
              (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)
            )
            (1-4): 4 x WavLMNoLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
              (activation): GELUActivation()
            )
            (5-6): 2 x WavLMNoLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
              (activation): GELUActivation()
            )
          )
        )
        (feature_projection): WavLMFeatureProjection(
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (projection): Linear(in_features=512, out_features=768, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (encoder): WavLMEncoder(
          (pos_conv_embed): WavLMPositionalConvEmbedding(
            (conv): ParametrizedConv1d(
              768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16
              (parametrizations): ModuleDict(
                (weight): ParametrizationList(
                  (0): _WeightNorm()
                )
              )
            )
            (padding): WavLMSamePadLayer()
            (activation): GELUActivation()
          )
          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (layers): ModuleList(
            (0): WavLMEncoderLayer(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
                (rel_attn_embed): Embedding(320, 12)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=3072, out_features=768, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1-11): 11 x WavLMEncoderLayer(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=3072, out_features=768, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (lm_head): Linear(in_features=768, out_features=42, bias=True)
    )
    (quantizer): FSQ(
      (project_in): Linear(in_features=768, out_features=5, bias=True)
      (project_out): Linear(in_features=5, out_features=768, bias=True)
    )
  )
  (preencoder): LinearProjection(
    (linear_out): Linear(in_features=768, out_features=128, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=7936, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 121.74 M
    Number of trainable parameters: 117.54 M (96.5%)
    Size: 470.16 MB
    Type: torch.float32
[lambda-Lambda-Vector] 2025-03-20 17:30:08,901 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 6.666666666666668e-09
    maximize: False
    weight_decay: 1e-06
)
[lambda-Lambda-Vector] 2025-03-20 17:30:08,901 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[lambda-Lambda-Vector] 2025-03-20 17:30:08,901 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/config.yaml
[lambda-Lambda-Vector] 2025-03-20 17:30:09,018 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-20 17:30:09,315 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x74bf0fbb4160>)
[lambda-Lambda-Vector] 2025-03-20 17:30:09,315 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=911, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2025-03-20 17:30:09,315 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=911, mean=61.1, min=16, max=362
[lambda-Lambda-Vector] 2025-03-20 17:30:09,324 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-20 17:30:09,351 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x74bf0fbb27d0>)
[lambda-Lambda-Vector] 2025-03-20 17:30:09,351 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=145, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2025-03-20 17:30:09,351 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=145, mean=62.3, min=1, max=309
[lambda-Lambda-Vector] 2025-03-20 17:30:09,359 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-20 17:30:09,472 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x74bf0fbb0bb0>)
[lambda-Lambda-Vector] 2025-03-20 17:30:09,472 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[lambda-Lambda-Vector] 2025-03-20 17:30:09,473 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[lambda-Lambda-Vector] 2025-03-20 17:30:09,475 (trainer:311) INFO: 1/70epoch started
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1210, in main
    cls.main_worker(args)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 614, in train_one_epoch
    retval = model(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 237, in forward
    encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 378, in encode
    feats, feats_lengths = self._extract_feats(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 445, in _extract_feats
    feats, feats_lengths = self.frontend(speech, speech_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/frontend/hf_fsq_train.py", line 105, in forward
    feats = outputs[-1][self.layer]
IndexError: tuple index out of range
# Accounting: time=9 threads=1
# Ended (code 1) at Thu Mar 20 17:30:11 PDT 2025, elapsed time 9 seconds

2025-03-20T17:30:35 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-20T17:30:35 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-20T17:30:35 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-20T17:30:35 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-20T17:30:35 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-20 17:30:35,911 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-20 17:30:35,924 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', '--gpu', '1', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Thu Mar 20 17:30:35 PDT 2025
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[lambda-Lambda-Vector] 2025-03-20 17:30:41,228 (asr:531) INFO: Vocabulary size: 5000
[lambda-Lambda-Vector] 2025-03-20 17:30:41,637 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
[lambda-Lambda-Vector] 2025-03-20 17:30:41,828 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[lambda-Lambda-Vector] 2025-03-20 17:30:41,833 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): HfCTCFSQFrontend(
    (upstream): WavLMForCTC(
      (wavlm): WavLMModel(
        (feature_extractor): WavLMFeatureEncoder(
          (conv_layers): ModuleList(
            (0): WavLMGroupNormConvLayer(
              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
              (activation): GELUActivation()
              (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)
            )
            (1-4): 4 x WavLMNoLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
              (activation): GELUActivation()
            )
            (5-6): 2 x WavLMNoLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
              (activation): GELUActivation()
            )
          )
        )
        (feature_projection): WavLMFeatureProjection(
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (projection): Linear(in_features=512, out_features=768, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (encoder): WavLMEncoder(
          (pos_conv_embed): WavLMPositionalConvEmbedding(
            (conv): ParametrizedConv1d(
              768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16
              (parametrizations): ModuleDict(
                (weight): ParametrizationList(
                  (0): _WeightNorm()
                )
              )
            )
            (padding): WavLMSamePadLayer()
            (activation): GELUActivation()
          )
          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (layers): ModuleList(
            (0): WavLMEncoderLayer(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
                (rel_attn_embed): Embedding(320, 12)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=3072, out_features=768, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1-11): 11 x WavLMEncoderLayer(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=3072, out_features=768, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (lm_head): Linear(in_features=768, out_features=42, bias=True)
    )
    (quantizer): FSQ(
      (project_in): Linear(in_features=768, out_features=5, bias=True)
      (project_out): Linear(in_features=5, out_features=768, bias=True)
    )
  )
  (preencoder): LinearProjection(
    (linear_out): Linear(in_features=768, out_features=128, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=7936, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 121.74 M
    Number of trainable parameters: 117.54 M (96.5%)
    Size: 470.16 MB
    Type: torch.float32
[lambda-Lambda-Vector] 2025-03-20 17:30:41,833 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 6.666666666666668e-09
    maximize: False
    weight_decay: 1e-06
)
[lambda-Lambda-Vector] 2025-03-20 17:30:41,833 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[lambda-Lambda-Vector] 2025-03-20 17:30:41,833 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/config.yaml
[lambda-Lambda-Vector] 2025-03-20 17:30:41,952 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-20 17:30:42,256 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7b8c4bbcc400>)
[lambda-Lambda-Vector] 2025-03-20 17:30:42,256 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=911, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2025-03-20 17:30:42,256 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=911, mean=61.1, min=16, max=362
[lambda-Lambda-Vector] 2025-03-20 17:30:42,264 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-20 17:30:42,292 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7b8c4bb21750>)
[lambda-Lambda-Vector] 2025-03-20 17:30:42,292 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=145, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2025-03-20 17:30:42,292 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=145, mean=62.3, min=1, max=309
[lambda-Lambda-Vector] 2025-03-20 17:30:42,300 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-20 17:30:42,415 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7b8c4bb21d50>)
[lambda-Lambda-Vector] 2025-03-20 17:30:42,415 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[lambda-Lambda-Vector] 2025-03-20 17:30:42,415 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[lambda-Lambda-Vector] 2025-03-20 17:30:42,417 (trainer:311) INFO: 1/70epoch started
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1210, in main
    cls.main_worker(args)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 614, in train_one_epoch
    retval = model(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 237, in forward
    encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 378, in encode
    feats, feats_lengths = self._extract_feats(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 445, in _extract_feats
    feats, feats_lengths = self.frontend(speech, speech_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/frontend/hf_fsq_train.py", line 100, in forward
    outputs = self.upstream.wavlm(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 1227, in forward
    encoder_outputs = self.encoder(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 722, in forward
    layer_outputs = layer(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 608, in forward
    hidden_states, attn_weights, position_bias = self.attention(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 473, in forward
    attn_output, attn_weights = self.torch_multi_head_self_attention(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 497, in torch_multi_head_self_attention
    attn_output, attn_weights = F.multi_head_attention_forward(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py", line 5450, in multi_head_attention_forward
    attn_mask = attn_mask + key_padding_mask
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 
# Accounting: time=12 threads=1
# Ended (code 1) at Thu Mar 20 17:30:47 PDT 2025, elapsed time 12 seconds

2025-03-20T17:31:15 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-20T17:31:15 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-20T17:31:15 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-20T17:31:15 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-20T17:31:15 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-20 17:31:15,749 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-20 17:31:15,762 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
2025-03-20T17:34:26 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-20T17:34:26 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-20T17:34:26 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-20T17:34:26 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-20T17:34:26 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-20 17:34:26,183 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-20 17:34:26,196 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
2025-03-20T23:43:18 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-20T23:43:18 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-20T23:43:18 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-20T23:43:18 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-20T23:43:18 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-20 23:43:18,698 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-20 23:43:18,709 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
2025-03-21T15:37:38 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-21T15:37:38 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-21T15:37:38 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-21T15:37:38 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-21T15:37:38 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-21 15:37:38,883 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-21 15:37:38,894 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
2025-03-22T05:26:42 (asr_fsq.sh:1942:main) Successfully finished. [elapsed=49744s]
2025-03-29T15:00:56 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-29T15:00:56 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-29T15:00:56 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-29T15:00:56 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-29T15:00:56 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-29 15:00:56,900 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-29 15:00:56,913 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', '--gpu', '1', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Sat Mar 29 15:00:56 PDT 2025
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[lambda-Lambda-Vector] 2025-03-29 15:01:02,831 (asr:531) INFO: Vocabulary size: 5000
[lambda-Lambda-Vector] 2025-03-29 15:01:03,204 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
[lambda-Lambda-Vector] 2025-03-29 15:01:04,169 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[lambda-Lambda-Vector] 2025-03-29 15:01:04,174 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): HfCTCFSQFrontend(
    (upstream): WavLMForCTC(
      (wavlm): WavLMModel(
        (feature_extractor): WavLMFeatureEncoder(
          (conv_layers): ModuleList(
            (0): WavLMGroupNormConvLayer(
              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
              (activation): GELUActivation()
              (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)
            )
            (1-4): 4 x WavLMNoLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
              (activation): GELUActivation()
            )
            (5-6): 2 x WavLMNoLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
              (activation): GELUActivation()
            )
          )
        )
        (feature_projection): WavLMFeatureProjection(
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (projection): Linear(in_features=512, out_features=768, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (encoder): WavLMEncoder(
          (pos_conv_embed): WavLMPositionalConvEmbedding(
            (conv): ParametrizedConv1d(
              768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16
              (parametrizations): ModuleDict(
                (weight): ParametrizationList(
                  (0): _WeightNorm()
                )
              )
            )
            (padding): WavLMSamePadLayer()
            (activation): GELUActivation()
          )
          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (layers): ModuleList(
            (0): WavLMEncoderLayer(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
                (rel_attn_embed): Embedding(320, 12)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=3072, out_features=768, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1-11): 11 x WavLMEncoderLayer(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=3072, out_features=768, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (lm_head): Linear(in_features=768, out_features=42, bias=True)
    )
    (quantizer): FSQ(
      (project_in): Linear(in_features=768, out_features=3, bias=True)
      (project_out): Linear(in_features=3, out_features=768, bias=True)
    )
  )
  (preencoder): LinearProjection(
    (linear_out): Linear(in_features=768, out_features=128, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=7936, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 121.74 M
    Number of trainable parameters: 117.54 M (96.5%)
    Size: 470.15 MB
    Type: torch.float32
[lambda-Lambda-Vector] 2025-03-29 15:01:04,174 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 6.666666666666668e-09
    maximize: False
    weight_decay: 1e-06
)
[lambda-Lambda-Vector] 2025-03-29 15:01:04,174 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[lambda-Lambda-Vector] 2025-03-29 15:01:04,175 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/config.yaml
[lambda-Lambda-Vector] 2025-03-29 15:01:04,290 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-29 15:01:04,636 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7e4d4f47b430>)
[lambda-Lambda-Vector] 2025-03-29 15:01:04,636 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=1737, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2025-03-29 15:01:04,636 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=1737, mean=32.1, min=6, max=201
[lambda-Lambda-Vector] 2025-03-29 15:01:04,645 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-29 15:01:04,672 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7e4d4f47bca0>)
[lambda-Lambda-Vector] 2025-03-29 15:01:04,672 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=275, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2025-03-29 15:01:04,672 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=275, mean=32.9, min=5, max=156
[lambda-Lambda-Vector] 2025-03-29 15:01:04,681 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-29 15:01:04,686 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7e4d4f47a860>)
[lambda-Lambda-Vector] 2025-03-29 15:01:04,686 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[lambda-Lambda-Vector] 2025-03-29 15:01:04,686 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[lambda-Lambda-Vector] 2025-03-29 15:01:04,688 (trainer:311) INFO: 1/70epoch started
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1210, in main
    cls.main_worker(args)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 677, in train_one_epoch
    scaler.scale(loss).backward()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 966.00 MiB. GPU 
# Accounting: time=39 threads=1
# Ended (code 1) at Sat Mar 29 15:01:35 PDT 2025, elapsed time 39 seconds

2025-03-29T15:02:34 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-29T15:02:34 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-29T15:02:34 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-29T15:02:34 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-29T15:02:34 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-29 15:02:35,059 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-29 15:02:35,070 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
2025-03-31T08:35:19 (asr_fsq.sh:1942:main) Successfully finished. [elapsed=149565s]
