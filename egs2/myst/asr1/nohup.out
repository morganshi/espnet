2024-10-16T02:39:47 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-16T02:39:47 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-16T02:39:47 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-16T02:39:47 (asr.sh:1407:main) Generate 'exp/asr_train_asr_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-16T02:39:47 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_raw_en_bpe5000/train.log'
2024-10-16 02:39:48,300 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_raw_en_bpe5000/train.log' --log exp/asr_train_asr_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-16 02:39:48,315 (launch:237) INFO: single-node with 2gpu on distributed mode
2024-10-16 02:39:48,316 (launch:348) INFO: log file: exp/asr_train_asr_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_raw_en_bpe5000/train.log', '--gpu', '2', 'exp/asr_train_asr_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_raw_en_bpe5000', '--config', 'conf/train_asr.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '2', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_raw_en_bpe5000/train.log ###################
# Started at Wed Oct 16 02:39:48 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed True
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[dl:0/2] 2024-10-16 02:40:16,148 (asr:523) INFO: Vocabulary size: 5000
[dl:0/2] 2024-10-16 02:40:17,873 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl:0/2] 2024-10-16 02:40:17,879 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5000, bias=True)
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 38.47 M
    Number of trainable parameters: 38.47 M (100.0%)
    Size: 153.89 MB
    Type: torch.float32
[dl:0/2] 2024-10-16 02:40:17,879 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 1.3333333333333336e-07
    maximize: False
    weight_decay: 1e-06
)
[dl:0/2] 2024-10-16 02:40:17,879 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[dl:0/2] 2024-10-16 02:40:17,879 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_raw_en_bpe5000/config.yaml
[dl:0/2] 2024-10-16 02:40:18,099 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/2] 2024-10-16 02:40:18,509 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7bb1700439a0>)
[dl:0/2] 2024-10-16 02:40:18,509 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=911, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/2] 2024-10-16 02:40:18,510 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=911, mean=61.1, min=16, max=362
[dl:0/2] 2024-10-16 02:40:18,522 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/2] 2024-10-16 02:40:18,557 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7bb15f5624d0>)
[dl:0/2] 2024-10-16 02:40:18,557 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=144, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/2] 2024-10-16 02:40:18,557 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=144, mean=62.8, min=18, max=309
[dl:0/2] 2024-10-16 02:40:18,567 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/2] 2024-10-16 02:40:18,575 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7bb197078d60>)
[dl:0/2] 2024-10-16 02:40:18,575 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl:0/2] 2024-10-16 02:40:18,575 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
dl:1672199:1672199 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1672199:1672199 [0] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1672199:1672199 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1672199:1672199 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.20.5+cuda12.4
dl:1672200:1672200 [1] NCCL INFO cudaDriverVersion 12050
dl:1672200:1672200 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1672200:1672200 [1] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1672200:1672200 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARNProcess SpawnProcess-2:
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 256, in run
    dp_model = torch.nn.parallel.DistributedDataParallel(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 798, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/utils.py", line 269, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1970, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'
 Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672200:1672270 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'
dl:1672200:1672270 [1] NCCL INFO init.cc:1475 -> 1
dl:1672200:1672270 [1] NCCL INFO group.cc:64 -> 1 [Async thread]
dl:1672200:1672200 [1] NCCL INFO group.cc:418 -> 1
dl:1672200:1672200 [1] NCCL INFO group.cc:95 -> 1
dl:1672200:1672277 [0] NCCL INFO comm 0xd913600 rank 0 nranks 0 cudaDev 0 busId 0 - Abort COMPLETE
W1016 02:40:20.461000 140242749486912 torch/multiprocessing/spawn.py:145] Terminating process 1672199 via signal SIGTERM
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1273, in main
    while not ProcessContext(processes, error_files).join():
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 177, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 1 terminated with exit code 1
# Accounting: time=33 threads=1
# Ended (code 1) at Wed Oct 16 02:40:21 EDT 2024, elapsed time 33 seconds

2024-10-16T02:41:47 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-16T02:41:47 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-16T02:41:47 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-16T02:41:47 (asr.sh:1407:main) Generate 'exp/asr_train_asr_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-16T02:41:47 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_raw_en_bpe5000/train.log'
2024-10-16 02:41:47,274 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_raw_en_bpe5000/train.log' --log exp/asr_train_asr_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-16 02:41:47,287 (launch:237) INFO: single-node with 2gpu on distributed mode
2024-10-16 02:41:47,288 (launch:348) INFO: log file: exp/asr_train_asr_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_raw_en_bpe5000/train.log', '--gpu', '2', 'exp/asr_train_asr_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_raw_en_bpe5000', '--config', 'conf/train_asr.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '2', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_raw_en_bpe5000/train.log ###################
# Started at Wed Oct 16 02:41:47 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed True
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[dl:0/2] 2024-10-16 02:42:00,295 (asr:523) INFO: Vocabulary size: 5000
[dl:0/2] 2024-10-16 02:42:01,138 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl:0/2] 2024-10-16 02:42:01,144 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5000, bias=True)
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 38.47 M
    Number of trainable parameters: 38.47 M (100.0%)
    Size: 153.89 MB
    Type: torch.float32
[dl:0/2] 2024-10-16 02:42:01,145 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 1.3333333333333336e-07
    maximize: False
    weight_decay: 1e-06
)
[dl:0/2] 2024-10-16 02:42:01,145 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[dl:0/2] 2024-10-16 02:42:01,145 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_raw_en_bpe5000/config.yaml
[dl:0/2] 2024-10-16 02:42:01,290 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/2] 2024-10-16 02:42:01,700 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x70fb9056b9a0>)
[dl:0/2] 2024-10-16 02:42:01,700 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=1737, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/2] 2024-10-16 02:42:01,701 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=1737, mean=32.1, min=6, max=201
[dl:0/2] 2024-10-16 02:42:01,713 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/2] 2024-10-16 02:42:01,746 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x70fb9038a4d0>)
[dl:0/2] 2024-10-16 02:42:01,746 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=275, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/2] 2024-10-16 02:42:01,746 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=275, mean=32.9, min=5, max=156
[dl:0/2] 2024-10-16 02:42:01,756 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/2] 2024-10-16 02:42:01,764 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x70fbb7e84d60>)
[dl:0/2] 2024-10-16 02:42:01,764 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl:0/2] 2024-10-16 02:42:01,764 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
dl:1672665:1672665 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1672665:1672665 [0] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1672665:1672665 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1672665:1672665 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.20.5+cuda12.4
dl:1672666:1672666 [1] NCCL INFO cudaDriverVersion 12050
dl:1672666:1672666 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1672666:1672666 [1] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1672666:1672666 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARNProcess SpawnProcess-2:
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 256, in run
    dp_model = torch.nn.parallel.DistributedDataParallel(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 798, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/utils.py", line 269, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1970, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'
 Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:47 NCCL WARN Cuda failure 'out of memory'

dl:1672666:1672694 [1] enqueue.cc:60 NCCL WARN Cuda failure 'out of memory'
dl:1672666:1672694 [1] NCCL INFO init.cc:1475 -> 1
dl:1672666:1672694 [1] NCCL INFO group.cc:64 -> 1 [Async thread]
dl:1672666:1672666 [1] NCCL INFO group.cc:418 -> 1
dl:1672666:1672666 [1] NCCL INFO group.cc:95 -> 1
dl:1672666:1672695 [0] NCCL INFO comm 0xcff8130 rank 0 nranks 0 cudaDev 0 busId 0 - Abort COMPLETE
W1016 02:42:03.391000 129485385557824 torch/multiprocessing/spawn.py:145] Terminating process 1672665 via signal SIGTERM
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1273, in main
    while not ProcessContext(processes, error_files).join():
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 177, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 1 terminated with exit code 1
# Accounting: time=17 threads=1
# Ended (code 1) at Wed Oct 16 02:42:04 EDT 2024, elapsed time 17 seconds

2024-10-16T07:04:33 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-16T07:04:33 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-16T07:04:33 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-16T07:04:33 (asr.sh:1407:main) Generate 'exp/asr_train_asr_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-16T07:04:33 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_raw_en_bpe5000/train.log'
2024-10-16 07:04:34,064 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_raw_en_bpe5000/train.log' --log exp/asr_train_asr_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-16 07:04:34,080 (launch:237) INFO: single-node with 2gpu on distributed mode
2024-10-16 07:04:34,081 (launch:348) INFO: log file: exp/asr_train_asr_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_raw_en_bpe5000/train.log', '--gpu', '2', 'exp/asr_train_asr_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_raw_en_bpe5000', '--config', 'conf/train_asr.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '2', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed True 
# Started at Wed Oct 16 07:04:34 UTC 2024
#
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /root/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
[nltk_data] Downloading package cmudict to /root/nltk_data...
[nltk_data]   Unzipping corpora/cmudict.zip.
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed True
Process SpawnProcess-2:
Process SpawnProcess-1:
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1317, in main_worker
    distributed_option.init_torch_distributed()
  File "/data/mohan/workdir/espnet/espnet2/train/distributed_utils.py", line 97, in init_torch_distributed
    torch.distributed.init_process_group(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
    return func(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 89, in wrapper
    func_return = func(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1312, in init_process_group
    default_pg, _ = _new_process_group_helper(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1533, in _new_process_group_helper
    backend_class = ProcessGroupNCCL(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1317, in main_worker
    distributed_option.init_torch_distributed()
ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!
  File "/data/mohan/workdir/espnet/espnet2/train/distributed_utils.py", line 97, in init_torch_distributed
    torch.distributed.init_process_group(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
    return func(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 89, in wrapper
    func_return = func(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1312, in init_process_group
    default_pg, _ = _new_process_group_helper(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1533, in _new_process_group_helper
    backend_class = ProcessGroupNCCL(
ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!
W1016 07:04:50.021000 140589378172736 torch/multiprocessing/spawn.py:145] Terminating process 13414 via signal SIGTERM
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1273, in main
    while not ProcessContext(processes, error_files).join():
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 177, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 1 terminated with exit code 1
# Accounting: time=16 threads=1
# Ended (code 1) at Wed Oct 16 07:04:50 UTC 2024, elapsed time 16 seconds

2024-10-16T03:05:54 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-16T03:05:54 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-16T03:05:54 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-16T03:05:54 (asr.sh:1407:main) Generate 'exp/asr_train_asr_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-16T03:05:54 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_raw_en_bpe5000/train.log'
2024-10-16 03:05:54,566 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_raw_en_bpe5000/train.log' --log exp/asr_train_asr_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed false -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-16 03:05:54,579 (launch:245) INFO: single-node with 2gpu using DataParallel
2024-10-16 03:05:54,580 (launch:348) INFO: log file: exp/asr_train_asr_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_raw_en_bpe5000/train.log', '--gpu', '2', 'exp/asr_train_asr_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_raw_en_bpe5000', '--config', 'conf/train_asr.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '2', '--multiprocessing_distributed', 'False']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed False 
# Started at Wed Oct 16 03:05:54 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed False
[dl] 2024-10-16 03:06:00,840 (asr:523) INFO: Vocabulary size: 5000
[dl] 2024-10-16 03:06:01,657 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl] 2024-10-16 03:06:01,663 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5000, bias=True)
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 38.47 M
    Number of trainable parameters: 38.47 M (100.0%)
    Size: 153.89 MB
    Type: torch.float32
[dl] 2024-10-16 03:06:01,663 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 1.3333333333333336e-07
    maximize: False
    weight_decay: 1e-06
)
[dl] 2024-10-16 03:06:01,663 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[dl] 2024-10-16 03:06:01,663 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_raw_en_bpe5000/config.yaml
[dl] 2024-10-16 03:06:01,812 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 03:06:02,292 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7232a7f74a00>)
[dl] 2024-10-16 03:06:02,292 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=1737, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-16 03:06:02,293 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=1737, mean=32.1, min=6, max=201
[dl] 2024-10-16 03:06:02,304 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 03:06:02,341 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7232a7d7b250>)
[dl] 2024-10-16 03:06:02,341 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=275, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-16 03:06:02,341 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=275, mean=32.9, min=5, max=156
[dl] 2024-10-16 03:06:02,352 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 03:06:02,359 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7232cefdcf10>)
[dl] 2024-10-16 03:06:02,359 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl] 2024-10-16 03:06:02,359 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[dl] 2024-10-16 03:06:02,362 (trainer:311) INFO: 1/70epoch started
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1210, in main
    cls.main_worker(args)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 614, in train_one_epoch
    retval = model(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 184, in forward
    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 189, in replicate
    return replicate(module, device_ids, not torch.is_grad_enabled())
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/replicate.py", line 110, in replicate
    param_copies = _broadcast_coalesced_reshape(params, devices, detach)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/replicate.py", line 83, in _broadcast_coalesced_reshape
    tensor_copies = Broadcast.apply(devices, *tensors)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 23, in forward
    outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 57, in broadcast_coalesced
    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU  has a total capacity of 15.63 GiB of which 2.62 MiB is free. Process 1167699 has 15.24 GiB memory in use. Including non-PyTorch memory, this process has 386.00 MiB memory in use. Of the allocated memory 104.53 MiB is allocated by PyTorch, and 13.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
# Accounting: time=13 threads=1
# Ended (code 1) at Wed Oct 16 03:06:07 EDT 2024, elapsed time 13 seconds

2024-10-16T03:07:15 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-16T03:07:16 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-16T03:07:16 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-16T03:07:16 (asr.sh:1407:main) Generate 'exp/asr_train_asr_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-16T03:07:16 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_raw_en_bpe5000/train.log'
2024-10-16 03:07:16,173 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_raw_en_bpe5000/train.log' --log exp/asr_train_asr_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed false -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-16 03:07:16,187 (launch:245) INFO: single-node with 2gpu using DataParallel
2024-10-16 03:07:16,188 (launch:348) INFO: log file: exp/asr_train_asr_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_raw_en_bpe5000/train.log', '--gpu', '2', 'exp/asr_train_asr_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_raw_en_bpe5000', '--config', 'conf/train_asr.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '2', '--multiprocessing_distributed', 'False']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed False 
# Started at Wed Oct 16 03:07:16 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed False
[dl] 2024-10-16 03:07:22,873 (asr:523) INFO: Vocabulary size: 5000
[dl] 2024-10-16 03:07:23,694 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl] 2024-10-16 03:07:23,700 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5000, bias=True)
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 38.47 M
    Number of trainable parameters: 38.47 M (100.0%)
    Size: 153.89 MB
    Type: torch.float32
[dl] 2024-10-16 03:07:23,700 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 1.3333333333333336e-07
    maximize: False
    weight_decay: 1e-06
)
[dl] 2024-10-16 03:07:23,700 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[dl] 2024-10-16 03:07:23,700 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_raw_en_bpe5000/config.yaml
[dl] 2024-10-16 03:07:23,845 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 03:07:24,234 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7375d25809d0>)
[dl] 2024-10-16 03:07:24,235 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=3286, batch_bins=4000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-16 03:07:24,235 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=3286, mean=17.0, min=5, max=111
[dl] 2024-10-16 03:07:24,246 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 03:07:24,280 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7375d2387220>)
[dl] 2024-10-16 03:07:24,281 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=514, batch_bins=4000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-16 03:07:24,281 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=514, mean=17.6, min=5, max=99
[dl] 2024-10-16 03:07:24,291 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 03:07:24,299 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7375f95dcf10>)
[dl] 2024-10-16 03:07:24,299 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl] 2024-10-16 03:07:24,299 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[dl] 2024-10-16 03:07:24,301 (trainer:311) INFO: 1/70epoch started
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1210, in main
    cls.main_worker(args)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 614, in train_one_epoch
    retval = model(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 184, in forward
    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 189, in replicate
    return replicate(module, device_ids, not torch.is_grad_enabled())
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/replicate.py", line 110, in replicate
    param_copies = _broadcast_coalesced_reshape(params, devices, detach)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/replicate.py", line 83, in _broadcast_coalesced_reshape
    tensor_copies = Broadcast.apply(devices, *tensors)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 23, in forward
    outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 57, in broadcast_coalesced
    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU  has a total capacity of 15.63 GiB of which 2.62 MiB is free. Process 1167699 has 15.24 GiB memory in use. Including non-PyTorch memory, this process has 386.00 MiB memory in use. Of the allocated memory 99.10 MiB is allocated by PyTorch, and 18.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
# Accounting: time=10 threads=1
# Ended (code 1) at Wed Oct 16 03:07:26 EDT 2024, elapsed time 10 seconds

2024-10-16T03:08:34 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-16T03:08:34 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-16T03:08:34 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-16T03:08:34 (asr.sh:1407:main) Generate 'exp/asr_train_asr_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-16T03:08:34 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_raw_en_bpe5000/train.log'
2024-10-16 03:08:34,976 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_raw_en_bpe5000/train.log' --log exp/asr_train_asr_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed false -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-16 03:08:34,991 (launch:245) INFO: single-node with 2gpu using DataParallel
2024-10-16 03:08:34,992 (launch:348) INFO: log file: exp/asr_train_asr_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_raw_en_bpe5000/train.log', '--gpu', '2', 'exp/asr_train_asr_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_raw_en_bpe5000', '--config', 'conf/train_asr.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '2', '--multiprocessing_distributed', 'False']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed False 
# Started at Wed Oct 16 03:08:35 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed False
[dl] 2024-10-16 03:08:41,180 (asr:523) INFO: Vocabulary size: 5000
[dl] 2024-10-16 03:08:41,983 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl] 2024-10-16 03:08:41,990 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5000, bias=True)
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 38.47 M
    Number of trainable parameters: 38.47 M (100.0%)
    Size: 153.89 MB
    Type: torch.float32
[dl] 2024-10-16 03:08:41,990 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 1.3333333333333336e-07
    maximize: False
    weight_decay: 1e-06
)
[dl] 2024-10-16 03:08:41,990 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[dl] 2024-10-16 03:08:41,990 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_raw_en_bpe5000/config.yaml
[dl] 2024-10-16 03:08:42,135 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 03:08:42,522 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7656958749a0>)
[dl] 2024-10-16 03:08:42,522 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=6106, batch_bins=2000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-16 03:08:42,523 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=6106, mean=9.1, min=1, max=57
[dl] 2024-10-16 03:08:42,533 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 03:08:42,568 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x76569567b190>)
[dl] 2024-10-16 03:08:42,568 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=972, batch_bins=2000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-16 03:08:42,568 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=972, mean=9.3, min=1, max=55
[dl] 2024-10-16 03:08:42,578 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 03:08:42,586 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x765695679f90>)
[dl] 2024-10-16 03:08:42,586 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl] 2024-10-16 03:08:42,586 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[dl] 2024-10-16 03:08:42,588 (trainer:311) INFO: 1/70epoch started
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1210, in main
    cls.main_worker(args)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 614, in train_one_epoch
    retval = model(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 184, in forward
    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 189, in replicate
    return replicate(module, device_ids, not torch.is_grad_enabled())
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/replicate.py", line 110, in replicate
    param_copies = _broadcast_coalesced_reshape(params, devices, detach)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/replicate.py", line 83, in _broadcast_coalesced_reshape
    tensor_copies = Broadcast.apply(devices, *tensors)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 23, in forward
    outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 57, in broadcast_coalesced
    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU  has a total capacity of 15.63 GiB of which 2.62 MiB is free. Process 1167699 has 15.24 GiB memory in use. Including non-PyTorch memory, this process has 386.00 MiB memory in use. Of the allocated memory 97.40 MiB is allocated by PyTorch, and 20.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
# Accounting: time=10 threads=1
# Ended (code 1) at Wed Oct 16 03:08:45 EDT 2024, elapsed time 10 seconds

2024-10-16T03:10:44 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-16T03:10:44 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-16T03:10:44 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-16T03:10:44 (asr.sh:1407:main) Generate 'exp/asr_train_asr_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-16T03:10:44 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_raw_en_bpe5000/train.log'
2024-10-16 03:10:44,528 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_raw_en_bpe5000/train.log' --log exp/asr_train_asr_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed false -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe5000 --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-16 03:10:44,543 (launch:348) INFO: log file: exp/asr_train_asr_raw_en_bpe5000/train.log
2024-10-16T15:54:05 (asr.sh:1808:main) Successfully finished. [elapsed=45801s]
2024-10-16T23:32:04 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_lr1e-3_warm_10000.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-16T23:32:04 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-16T23:32:04 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-16T23:32:04 (asr.sh:1407:main) Generate 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-16T23:32:04 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log'
2024-10-16 23:32:06,020 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log' --log exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-16 23:32:06,065 (launch:348) INFO: log file: exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
bash: line 1: 1875122 Killed                  ( python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True ) 2>> exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log >> exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log', '--gpu', '1', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000', '--config', 'conf/train_asr_lr1e-3_warm_10000.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Wed Oct 16 23:32:06 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[dl] 2024-10-16 23:32:24,115 (asr:523) INFO: Vocabulary size: 5000
[dl] 2024-10-16 23:32:25,797 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl] 2024-10-16 23:32:25,808 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5000, bias=True)
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 38.47 M
    Number of trainable parameters: 38.47 M (100.0%)
    Size: 153.89 MB
    Type: torch.float32
[dl] 2024-10-16 23:32:25,808 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 1e-07
    maximize: False
    weight_decay: 1e-06
)
[dl] 2024-10-16 23:32:25,808 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=10000)
[dl] 2024-10-16 23:32:25,808 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/config.yaml
[dl] 2024-10-16 23:32:26,008 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 23:32:26,445 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x79f7c9b609d0>)
[dl] 2024-10-16 23:32:26,445 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=1737, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-16 23:32:26,466 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=1737, mean=32.1, min=6, max=201
[dl] 2024-10-16 23:32:26,580 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 23:32:26,639 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x79f7c9967220>)
[dl] 2024-10-16 23:32:26,639 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=275, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-16 23:32:26,639 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=275, mean=32.9, min=5, max=156
[dl] 2024-10-16 23:32:26,650 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-16 23:32:26,657 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x79f7f0b74f40>)
[dl] 2024-10-16 23:32:26,657 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl] 2024-10-16 23:32:26,657 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[dl] 2024-10-16 23:32:26,659 (trainer:311) INFO: 1/70epoch started
[dl] 2024-10-16 23:32:56,112 (trainer:779) INFO: 1epoch:train:1-86batch: iter_time=0.003, forward_time=0.144, loss_ctc=1.647e+03, loss_att=153.798, acc=1.675e-04, loss=75.209, backward_time=0.130, grad_norm=2.396e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.031, optim0_lr0=6.500e-07, train_time=2.765
[dl] 2024-10-16 23:33:21,642 (trainer:779) INFO: 1epoch:train:87-172batch: iter_time=1.733e-04, forward_time=0.116, loss_ctc=1.394e+03, loss_att=141.067, acc=3.851e-05, loss=64.614, backward_time=0.116, grad_norm=4.016e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.700e-06, train_time=2.381
[dl] 2024-10-16 23:33:47,636 (trainer:779) INFO: 1epoch:train:173-258batch: iter_time=1.594e-04, forward_time=0.117, loss_ctc=1.184e+03, loss_att=151.364, acc=1.088e-04, loss=57.659, backward_time=0.119, grad_norm=4.166e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=2.800e-06, train_time=2.416
[dl] 2024-10-16 23:34:14,990 (trainer:779) INFO: 1epoch:train:259-344batch: iter_time=1.515e-04, forward_time=0.122, loss_ctc=906.107, loss_att=170.828, acc=1.541e-04, loss=48.926, backward_time=0.127, grad_norm=3.169e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.900e-06, train_time=2.537
[dl] 2024-10-16 23:34:42,600 (trainer:779) INFO: 1epoch:train:345-430batch: iter_time=1.545e-04, forward_time=0.124, loss_ctc=558.501, loss_att=172.980, acc=2.009e-04, loss=36.080, backward_time=0.128, grad_norm=1.800e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=4.950e-06, train_time=2.572
[dl] 2024-10-16 23:35:08,919 (trainer:779) INFO: 1epoch:train:431-516batch: iter_time=1.507e-04, forward_time=0.119, loss_ctc=265.769, loss_att=131.986, acc=1.290e-04, loss=21.515, backward_time=0.121, grad_norm=868.968, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=6.000e-06, train_time=2.472
[dl] 2024-10-16 23:35:36,284 (trainer:779) INFO: 1epoch:train:517-602batch: iter_time=1.544e-04, forward_time=0.122, loss_ctc=272.382, loss_att=165.181, acc=2.193e-04, loss=24.668, backward_time=0.128, grad_norm=610.046, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=7.100e-06, train_time=2.536
[dl] 2024-10-16 23:36:03,045 (trainer:779) INFO: 1epoch:train:603-688batch: iter_time=1.428e-04, forward_time=0.119, loss_ctc=230.793, loss_att=153.116, acc=2.565e-04, loss=22.052, backward_time=0.125, grad_norm=507.787, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=8.200e-06, train_time=2.476
[dl] 2024-10-16 23:36:28,644 (trainer:779) INFO: 1epoch:train:689-774batch: iter_time=1.600e-04, forward_time=0.116, loss_ctc=187.175, loss_att=129.722, acc=8.384e-04, loss=18.370, backward_time=0.119, grad_norm=405.132, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=9.250e-06, train_time=2.398
[dl] 2024-10-16 23:36:54,968 (trainer:779) INFO: 1epoch:train:775-860batch: iter_time=1.587e-04, forward_time=0.119, loss_ctc=199.159, loss_att=141.266, acc=0.002, loss=19.829, backward_time=0.121, grad_norm=379.917, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.030e-05, train_time=2.429
[dl] 2024-10-16 23:37:21,012 (trainer:779) INFO: 1epoch:train:861-946batch: iter_time=1.600e-04, forward_time=0.117, loss_ctc=213.723, loss_att=154.327, acc=0.005, loss=21.518, backward_time=0.120, grad_norm=358.332, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.140e-05, train_time=2.422
[dl] 2024-10-16 23:37:47,255 (trainer:779) INFO: 1epoch:train:947-1032batch: iter_time=1.606e-04, forward_time=0.118, loss_ctc=199.389, loss_att=147.073, acc=0.018, loss=20.346, backward_time=0.121, grad_norm=345.832, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.250e-05, train_time=2.442
[dl] 2024-10-16 23:38:13,086 (trainer:779) INFO: 1epoch:train:1033-1118batch: iter_time=1.525e-04, forward_time=0.117, loss_ctc=179.414, loss_att=134.468, acc=0.036, loss=18.494, backward_time=0.119, grad_norm=283.663, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.355e-05, train_time=2.396
[dl] 2024-10-16 23:38:39,674 (trainer:779) INFO: 1epoch:train:1119-1204batch: iter_time=1.510e-04, forward_time=0.120, loss_ctc=188.178, loss_att=142.456, acc=0.050, loss=19.522, backward_time=0.123, grad_norm=285.190, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.460e-05, train_time=2.473
[dl] 2024-10-16 23:39:06,006 (trainer:779) INFO: 1epoch:train:1205-1290batch: iter_time=1.703e-04, forward_time=0.119, loss_ctc=181.679, loss_att=138.242, acc=0.063, loss=18.909, backward_time=0.122, grad_norm=253.307, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.570e-05, train_time=2.463
[dl] 2024-10-16 23:39:32,079 (trainer:779) INFO: 1epoch:train:1291-1376batch: iter_time=1.536e-04, forward_time=0.117, loss_ctc=167.627, loss_att=128.496, acc=0.070, loss=17.529, backward_time=0.121, grad_norm=207.311, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.680e-05, train_time=2.414
[dl] 2024-10-16 23:39:58,214 (trainer:779) INFO: 1epoch:train:1377-1462batch: iter_time=1.460e-04, forward_time=0.118, loss_ctc=163.232, loss_att=125.359, acc=0.064, loss=17.090, backward_time=0.122, grad_norm=196.826, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.785e-05, train_time=2.442
[dl] 2024-10-16 23:40:25,272 (trainer:779) INFO: 1epoch:train:1463-1548batch: iter_time=1.499e-04, forward_time=0.122, loss_ctc=161.289, loss_att=123.045, acc=0.053, loss=16.815, backward_time=0.126, grad_norm=180.047, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.890e-05, train_time=2.497
[dl] 2024-10-16 23:40:51,528 (trainer:779) INFO: 1epoch:train:1549-1634batch: iter_time=1.500e-04, forward_time=0.119, loss_ctc=167.747, loss_att=127.206, acc=0.057, loss=17.421, backward_time=0.121, grad_norm=151.258, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=2.000e-05, train_time=2.452
[dl] 2024-10-16 23:41:17,932 (trainer:779) INFO: 1epoch:train:1635-1720batch: iter_time=1.583e-04, forward_time=0.119, loss_ctc=149.537, loss_att=113.334, acc=0.056, loss=15.524, backward_time=0.121, grad_norm=124.987, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=2.110e-05, train_time=2.453
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
[dl] 2024-10-16 23:42:56,686 (trainer:365) INFO: 1epoch results: [train] iter_time=3.188e-04, forward_time=0.120, loss_ctc=424.608, loss_att=141.492, acc=0.024, loss=28.303, backward_time=0.123, grad_norm=1.027e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.100e-05, train_time=2.472, time=8 minutes and 56.77 seconds, total_count=1737, gpu_max_cached_mem_GB=10.412, [valid] loss_ctc=147.135, cer_ctc=1.000, loss_att=111.570, acc=0.066, cer=0.694, wer=1.000, loss=122.239, time=50.89 seconds, total_count=275, gpu_max_cached_mem_GB=15.139, [att_plot] time=42.37 seconds, total_count=0, gpu_max_cached_mem_GB=15.139
[dl] 2024-10-16 23:42:58,889 (trainer:433) INFO: The best model has been updated: valid.acc
[dl] 2024-10-16 23:42:58,889 (trainer:299) INFO: 2/70epoch started. Estimated time to finish: 12 hours, 7 minutes and 3.85 seconds
[dl] 2024-10-16 23:43:24,704 (trainer:779) INFO: 2epoch:train:1-86batch: iter_time=0.003, forward_time=0.113, loss_ctc=148.422, loss_att=111.730, acc=0.061, loss=15.342, backward_time=0.115, grad_norm=102.115, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=2.235e-05, train_time=2.385
[dl] 2024-10-16 23:43:51,978 (trainer:779) INFO: 2epoch:train:87-172batch: iter_time=1.591e-04, forward_time=0.121, loss_ctc=152.119, loss_att=114.984, acc=0.058, loss=15.766, backward_time=0.124, grad_norm=96.252, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=2.340e-05, train_time=2.550
[dl] 2024-10-16 23:44:18,671 (trainer:779) INFO: 2epoch:train:173-258batch: iter_time=1.399e-04, forward_time=0.119, loss_ctc=135.612, loss_att=102.989, acc=0.056, loss=14.097, backward_time=0.120, grad_norm=74.082, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=2.450e-05, train_time=2.476
[dl] 2024-10-16 23:44:46,299 (trainer:779) INFO: 2epoch:train:259-344batch: iter_time=1.363e-04, forward_time=0.121, loss_ctc=165.808, loss_att=126.123, acc=0.064, loss=17.254, backward_time=0.125, grad_norm=70.871, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=2.560e-05, train_time=2.569
[dl] 2024-10-16 23:45:12,049 (trainer:779) INFO: 2epoch:train:345-430batch: iter_time=1.468e-04, forward_time=0.115, loss_ctc=129.215, loss_att=99.824, acc=0.058, loss=13.580, backward_time=0.117, grad_norm=50.489, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=2.665e-05, train_time=2.405
[dl] 2024-10-16 23:45:37,661 (trainer:779) INFO: 2epoch:train:431-516batch: iter_time=1.376e-04, forward_time=0.114, loss_ctc=120.046, loss_att=93.827, acc=0.061, loss=12.712, backward_time=0.115, grad_norm=40.142, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=2.770e-05, train_time=2.378
[dl] 2024-10-16 23:46:03,574 (trainer:779) INFO: 2epoch:train:517-602batch: iter_time=1.502e-04, forward_time=0.115, loss_ctc=128.196, loss_att=100.706, acc=0.076, loss=13.619, backward_time=0.118, grad_norm=37.910, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=2.880e-05, train_time=2.407
[dl] 2024-10-16 23:46:30,043 (trainer:779) INFO: 2epoch:train:603-688batch: iter_time=1.422e-04, forward_time=0.117, loss_ctc=129.789, loss_att=103.262, acc=0.080, loss=13.903, backward_time=0.120, grad_norm=34.636, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=2.990e-05, train_time=2.460
[dl] 2024-10-16 23:46:55,963 (trainer:779) INFO: 2epoch:train:689-774batch: iter_time=1.510e-04, forward_time=0.116, loss_ctc=117.389, loss_att=94.573, acc=0.082, loss=12.677, backward_time=0.117, grad_norm=26.641, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.095e-05, train_time=2.394
[dl] 2024-10-16 23:47:22,715 (trainer:779) INFO: 2epoch:train:775-860batch: iter_time=1.372e-04, forward_time=0.118, loss_ctc=140.922, loss_att=114.155, acc=0.088, loss=15.273, backward_time=0.122, grad_norm=29.012, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.200e-05, train_time=2.493
[dl] 2024-10-16 23:47:49,439 (trainer:779) INFO: 2epoch:train:861-946batch: iter_time=1.431e-04, forward_time=0.118, loss_ctc=123.298, loss_att=100.792, acc=0.087, loss=13.443, backward_time=0.121, grad_norm=22.678, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.310e-05, train_time=2.502
[dl] 2024-10-16 23:48:15,637 (trainer:779) INFO: 2epoch:train:947-1032batch: iter_time=1.439e-04, forward_time=0.116, loss_ctc=112.332, loss_att=92.548, acc=0.086, loss=12.310, backward_time=0.118, grad_norm=18.389, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.420e-05, train_time=2.427
[dl] 2024-10-16 23:48:41,651 (trainer:779) INFO: 2epoch:train:1033-1118batch: iter_time=1.546e-04, forward_time=0.116, loss_ctc=116.905, loss_att=96.625, acc=0.093, loss=12.839, backward_time=0.118, grad_norm=15.270, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.525e-05, train_time=2.404
[dl] 2024-10-16 23:49:07,542 (trainer:779) INFO: 2epoch:train:1119-1204batch: iter_time=1.589e-04, forward_time=0.115, loss_ctc=127.707, loss_att=105.397, acc=0.099, loss=14.011, backward_time=0.116, grad_norm=16.685, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.630e-05, train_time=2.434
[dl] 2024-10-16 23:49:33,924 (trainer:779) INFO: 2epoch:train:1205-1290batch: iter_time=1.535e-04, forward_time=0.117, loss_ctc=116.839, loss_att=96.298, acc=0.115, loss=12.808, backward_time=0.118, grad_norm=13.628, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.740e-05, train_time=2.442
[dl] 2024-10-16 23:49:59,187 (trainer:779) INFO: 2epoch:train:1291-1376batch: iter_time=1.382e-04, forward_time=0.113, loss_ctc=94.407, loss_att=78.110, acc=0.113, loss=10.375, backward_time=0.113, grad_norm=10.510, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.850e-05, train_time=2.352
[dl] 2024-10-16 23:50:24,923 (trainer:779) INFO: 2epoch:train:1377-1462batch: iter_time=1.544e-04, forward_time=0.115, loss_ctc=112.842, loss_att=92.647, acc=0.121, loss=12.338, backward_time=0.116, grad_norm=10.497, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.955e-05, train_time=2.392
[dl] 2024-10-16 23:50:51,134 (trainer:779) INFO: 2epoch:train:1463-1548batch: iter_time=1.623e-04, forward_time=0.117, loss_ctc=110.090, loss_att=90.014, acc=0.122, loss=12.005, backward_time=0.119, grad_norm=9.539, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=4.060e-05, train_time=2.451
[dl] 2024-10-16 23:51:17,396 (trainer:779) INFO: 2epoch:train:1549-1634batch: iter_time=1.429e-04, forward_time=0.117, loss_ctc=122.327, loss_att=99.629, acc=0.119, loss=13.305, backward_time=0.119, grad_norm=16.621, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=4.170e-05, train_time=2.423
[dl] 2024-10-16 23:51:43,685 (trainer:779) INFO: 2epoch:train:1635-1720batch: iter_time=1.418e-04, forward_time=0.116, loss_ctc=116.994, loss_att=94.753, acc=0.124, loss=12.678, backward_time=0.119, grad_norm=12.002, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=4.280e-05, train_time=2.451
[dl] 2024-10-16 23:53:15,438 (trainer:365) INFO: 2epoch results: [train] iter_time=2.672e-04, forward_time=0.116, loss_ctc=125.054, loss_att=99.683, acc=0.089, loss=13.412, backward_time=0.119, grad_norm=35.056, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.270e-05, train_time=2.441, time=8 minutes and 50.01 seconds, total_count=3474, gpu_max_cached_mem_GB=15.139, [valid] loss_ctc=109.617, cer_ctc=1.000, loss_att=88.279, acc=0.130, cer=0.713, wer=1.000, loss=94.680, time=50.05 seconds, total_count=550, gpu_max_cached_mem_GB=15.139, [att_plot] time=36.48 seconds, total_count=0, gpu_max_cached_mem_GB=15.139
[dl] 2024-10-16 23:53:19,893 (trainer:433) INFO: The best model has been updated: valid.acc
[dl] 2024-10-16 23:53:19,893 (trainer:299) INFO: 3/70epoch started. Estimated time to finish: 11 hours, 50 minutes and 9.94 seconds
[dl] 2024-10-16 23:53:46,947 (trainer:779) INFO: 3epoch:train:1-86batch: iter_time=0.003, forward_time=0.119, loss_ctc=117.684, loss_att=94.982, acc=0.129, loss=12.724, backward_time=0.121, grad_norm=11.917, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.012, optim0_lr0=4.405e-05, train_time=2.528
[dl] 2024-10-16 23:54:13,168 (trainer:779) INFO: 3epoch:train:87-172batch: iter_time=1.528e-04, forward_time=0.117, loss_ctc=104.824, loss_att=84.494, acc=0.129, loss=11.324, backward_time=0.118, grad_norm=12.918, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=4.510e-05, train_time=2.444
[dl] 2024-10-16 23:54:39,812 (trainer:779) INFO: 3epoch:train:173-258batch: iter_time=1.363e-04, forward_time=0.118, loss_ctc=109.418, loss_att=87.847, acc=0.132, loss=11.790, backward_time=0.120, grad_norm=10.162, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=4.620e-05, train_time=2.465
[dl] 2024-10-16 23:55:05,703 (trainer:779) INFO: 3epoch:train:259-344batch: iter_time=1.526e-04, forward_time=0.115, loss_ctc=110.929, loss_att=88.591, acc=0.134, loss=11.912, backward_time=0.117, grad_norm=13.313, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=4.730e-05, train_time=2.411
[dl] 2024-10-16 23:55:31,999 (trainer:779) INFO: 3epoch:train:345-430batch: iter_time=1.456e-04, forward_time=0.116, loss_ctc=127.658, loss_att=101.478, acc=0.134, loss=13.666, backward_time=0.120, grad_norm=9.051, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=4.835e-05, train_time=2.459
[dl] 2024-10-16 23:55:58,957 (trainer:779) INFO: 3epoch:train:431-516batch: iter_time=1.275e-04, forward_time=0.119, loss_ctc=129.494, loss_att=102.825, acc=0.131, loss=13.853, backward_time=0.122, grad_norm=11.013, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=4.940e-05, train_time=2.490
[dl] 2024-10-16 23:56:25,088 (trainer:779) INFO: 3epoch:train:517-602batch: iter_time=1.364e-04, forward_time=0.116, loss_ctc=121.496, loss_att=96.078, acc=0.135, loss=12.963, backward_time=0.118, grad_norm=12.070, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.050e-05, train_time=2.442
[dl] 2024-10-16 23:56:51,224 (trainer:779) INFO: 3epoch:train:603-688batch: iter_time=1.526e-04, forward_time=0.116, loss_ctc=111.712, loss_att=88.327, acc=0.139, loss=11.918, backward_time=0.119, grad_norm=12.215, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.160e-05, train_time=2.425
[dl] 2024-10-16 23:57:17,922 (trainer:779) INFO: 3epoch:train:689-774batch: iter_time=1.472e-04, forward_time=0.119, loss_ctc=110.216, loss_att=86.619, acc=0.141, loss=11.712, backward_time=0.121, grad_norm=11.528, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.265e-05, train_time=2.497
[dl] 2024-10-16 23:57:44,943 (trainer:779) INFO: 3epoch:train:775-860batch: iter_time=1.535e-04, forward_time=0.119, loss_ctc=115.510, loss_att=91.125, acc=0.142, loss=12.305, backward_time=0.123, grad_norm=13.604, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.370e-05, train_time=2.496
[dl] 2024-10-16 23:58:10,348 (trainer:779) INFO: 3epoch:train:861-946batch: iter_time=1.520e-04, forward_time=0.113, loss_ctc=105.998, loss_att=83.261, acc=0.140, loss=11.260, backward_time=0.115, grad_norm=10.350, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.480e-05, train_time=2.387
[dl] 2024-10-16 23:58:36,626 (trainer:779) INFO: 3epoch:train:947-1032batch: iter_time=1.570e-04, forward_time=0.116, loss_ctc=121.068, loss_att=94.337, acc=0.151, loss=12.795, backward_time=0.119, grad_norm=14.415, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.590e-05, train_time=2.428
[dl] 2024-10-16 23:59:03,121 (trainer:779) INFO: 3epoch:train:1033-1118batch: iter_time=1.407e-04, forward_time=0.117, loss_ctc=104.291, loss_att=81.287, acc=0.146, loss=11.024, backward_time=0.119, grad_norm=15.973, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=5.695e-05, train_time=2.480
[dl] 2024-10-16 23:59:28,976 (trainer:779) INFO: 3epoch:train:1119-1204batch: iter_time=1.455e-04, forward_time=0.115, loss_ctc=111.426, loss_att=86.489, acc=0.151, loss=11.746, backward_time=0.116, grad_norm=13.922, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.800e-05, train_time=2.407
[dl] 2024-10-16 23:59:54,681 (trainer:779) INFO: 3epoch:train:1205-1290batch: iter_time=1.364e-04, forward_time=0.115, loss_ctc=120.496, loss_att=93.361, acc=0.151, loss=12.688, backward_time=0.115, grad_norm=15.299, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.910e-05, train_time=2.379
[dl] 2024-10-17 00:00:20,554 (trainer:779) INFO: 3epoch:train:1291-1376batch: iter_time=1.464e-04, forward_time=0.116, loss_ctc=96.907, loss_att=75.204, acc=0.152, loss=10.214, backward_time=0.115, grad_norm=11.440, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=6.020e-05, train_time=2.406
[dl] 2024-10-17 00:00:46,299 (trainer:779) INFO: 3epoch:train:1377-1462batch: iter_time=1.420e-04, forward_time=0.116, loss_ctc=109.348, loss_att=84.579, acc=0.154, loss=11.501, backward_time=0.115, grad_norm=8.704, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=6.125e-05, train_time=2.418
[dl] 2024-10-17 00:01:12,313 (trainer:779) INFO: 3epoch:train:1463-1548batch: iter_time=1.464e-04, forward_time=0.117, loss_ctc=106.748, loss_att=82.511, acc=0.158, loss=11.223, backward_time=0.116, grad_norm=9.988, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=6.230e-05, train_time=2.410
[dl] 2024-10-17 00:01:37,853 (trainer:779) INFO: 3epoch:train:1549-1634batch: iter_time=1.591e-04, forward_time=0.115, loss_ctc=100.552, loss_att=77.415, acc=0.159, loss=10.544, backward_time=0.114, grad_norm=12.439, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=6.340e-05, train_time=2.375
[dl] 2024-10-17 00:02:04,868 (trainer:779) INFO: 3epoch:train:1635-1720batch: iter_time=1.487e-04, forward_time=0.120, loss_ctc=121.131, loss_att=92.747, acc=0.166, loss=12.658, backward_time=0.121, grad_norm=16.010, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=6.450e-05, train_time=2.500
[dl] 2024-10-17 00:03:32,172 (trainer:365) INFO: 3epoch results: [train] iter_time=2.805e-04, forward_time=0.117, loss_ctc=111.858, loss_att=87.874, acc=0.144, loss=11.884, backward_time=0.118, grad_norm=12.324, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.440e-05, train_time=2.440, time=8 minutes and 49.89 seconds, total_count=5211, gpu_max_cached_mem_GB=15.139, [valid] loss_ctc=107.303, cer_ctc=0.996, loss_att=81.672, acc=0.169, cer=0.705, wer=1.000, loss=89.361, time=50.12 seconds, total_count=825, gpu_max_cached_mem_GB=15.139, [att_plot] time=32.27 seconds, total_count=0, gpu_max_cached_mem_GB=15.139
[dl] 2024-10-17 00:03:36,435 (trainer:433) INFO: The best model has been updated: valid.acc
[dl] 2024-10-17 00:03:36,435 (trainer:299) INFO: 4/70epoch started. Estimated time to finish: 11 hours, 35 minutes and 58.32 seconds
[dl] 2024-10-17 00:04:02,302 (trainer:779) INFO: 4epoch:train:1-86batch: iter_time=0.003, forward_time=0.113, loss_ctc=94.738, loss_att=72.695, acc=0.169, loss=9.913, backward_time=0.112, grad_norm=12.533, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.011, optim0_lr0=6.575e-05, train_time=2.393
[dl] 2024-10-17 00:04:28,964 (trainer:779) INFO: 4epoch:train:87-172batch: iter_time=1.499e-04, forward_time=0.119, loss_ctc=112.012, loss_att=85.561, acc=0.172, loss=11.687, backward_time=0.119, grad_norm=13.796, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=6.680e-05, train_time=2.472
[dl] 2024-10-17 00:04:55,332 (trainer:779) INFO: 4epoch:train:173-258batch: iter_time=1.557e-04, forward_time=0.118, loss_ctc=94.212, loss_att=72.073, acc=0.173, loss=9.839, backward_time=0.117, grad_norm=12.756, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=6.790e-05, train_time=2.475
[dl] 2024-10-17 00:05:22,587 (trainer:779) INFO: 4epoch:train:259-344batch: iter_time=1.424e-04, forward_time=0.121, loss_ctc=120.605, loss_att=91.828, acc=0.165, loss=12.558, backward_time=0.122, grad_norm=20.593, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=6.900e-05, train_time=2.526
[dl] 2024-10-17 00:05:48,299 (trainer:779) INFO: 4epoch:train:345-430batch: iter_time=1.567e-04, forward_time=0.115, loss_ctc=107.675, loss_att=82.009, acc=0.171, loss=11.214, backward_time=0.115, grad_norm=11.903, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=7.005e-05, train_time=2.420
[dl] 2024-10-17 00:06:14,460 (trainer:779) INFO: 4epoch:train:431-516batch: iter_time=1.501e-04, forward_time=0.116, loss_ctc=111.586, loss_att=84.844, acc=0.174, loss=11.608, backward_time=0.117, grad_norm=23.402, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=7.110e-05, train_time=2.410
[dl] 2024-10-17 00:06:40,278 (trainer:779) INFO: 4epoch:train:517-602batch: iter_time=1.476e-04, forward_time=0.115, loss_ctc=111.535, loss_att=84.549, acc=0.179, loss=11.581, backward_time=0.116, grad_norm=19.013, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=7.220e-05, train_time=2.393
[dl] 2024-10-17 00:07:06,587 (trainer:779) INFO: 4epoch:train:603-688batch: iter_time=1.699e-04, forward_time=0.118, loss_ctc=106.794, loss_att=80.808, acc=0.173, loss=11.075, backward_time=0.117, grad_norm=12.782, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=7.330e-05, train_time=2.451
[dl] 2024-10-17 00:07:35,005 (trainer:779) INFO: 4epoch:train:689-774batch: iter_time=0.018, forward_time=0.121, loss_ctc=120.427, loss_att=91.364, acc=0.177, loss=12.510, backward_time=0.123, grad_norm=11.969, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=7.435e-05, train_time=2.603
[dl] 2024-10-17 00:08:58,221 (trainer:779) INFO: 4epoch:train:775-860batch: iter_time=0.049, forward_time=0.463, loss_ctc=114.179, loss_att=86.336, acc=0.175, loss=11.836, backward_time=0.278, grad_norm=15.649, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.011, optim0_lr0=7.540e-05, train_time=7.511
[dl] 2024-10-17 00:10:04,570 (trainer:779) INFO: 4epoch:train:861-946batch: iter_time=1.412e-04, forward_time=0.378, loss_ctc=103.366, loss_att=78.256, acc=0.184, loss=10.724, backward_time=0.242, grad_norm=12.605, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=7.650e-05, train_time=6.214
[dl] 2024-10-17 00:10:32,146 (trainer:779) INFO: 4epoch:train:947-1032batch: iter_time=0.014, forward_time=0.118, loss_ctc=112.772, loss_att=85.004, acc=0.186, loss=11.667, backward_time=0.120, grad_norm=20.018, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=7.760e-05, train_time=2.595
[dl] 2024-10-17 00:11:02,408 (trainer:779) INFO: 4epoch:train:1033-1118batch: iter_time=0.063, forward_time=0.114, loss_ctc=104.685, loss_att=79.027, acc=0.184, loss=10.841, backward_time=0.114, grad_norm=16.177, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=7.865e-05, train_time=2.812
[dl] 2024-10-17 00:11:34,349 (trainer:779) INFO: 4epoch:train:1119-1204batch: iter_time=0.071, forward_time=0.118, loss_ctc=113.798, loss_att=85.639, acc=0.196, loss=11.761, backward_time=0.119, grad_norm=20.388, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=7.970e-05, train_time=2.926
[dl] 2024-10-17 00:12:04,475 (trainer:779) INFO: 4epoch:train:1205-1290batch: iter_time=0.047, forward_time=0.119, loss_ctc=120.350, loss_att=90.596, acc=0.188, loss=12.440, backward_time=0.120, grad_norm=19.813, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=8.080e-05, train_time=2.849
[dl] 2024-10-17 00:12:42,463 (trainer:779) INFO: 4epoch:train:1291-1376batch: iter_time=0.148, forward_time=0.117, loss_ctc=105.144, loss_att=78.928, acc=0.212, loss=10.849, backward_time=0.117, grad_norm=15.644, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=8.190e-05, train_time=3.515
[dl] 2024-10-17 00:13:21,709 (trainer:779) INFO: 4epoch:train:1377-1462batch: iter_time=0.162, forward_time=0.118, loss_ctc=103.314, loss_att=77.373, acc=0.207, loss=10.644, backward_time=0.118, grad_norm=24.726, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=8.295e-05, train_time=3.645
[dl] 2024-10-17 00:13:59,988 (trainer:779) INFO: 4epoch:train:1463-1548batch: iter_time=0.144, forward_time=0.128, loss_ctc=97.613, loss_att=73.482, acc=0.202, loss=10.090, backward_time=0.120, grad_norm=14.835, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.011, optim0_lr0=8.400e-05, train_time=3.471
[dl] 2024-10-17 00:14:41,067 (trainer:779) INFO: 4epoch:train:1549-1634batch: iter_time=0.125, forward_time=0.148, loss_ctc=129.779, loss_att=97.291, acc=0.193, loss=13.380, backward_time=0.135, grad_norm=21.331, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=8.510e-05, train_time=3.940
[dl] 2024-10-17 00:15:17,088 (trainer:779) INFO: 4epoch:train:1635-1720batch: iter_time=0.128, forward_time=0.116, loss_ctc=121.269, loss_att=90.738, acc=0.194, loss=12.487, backward_time=0.117, grad_norm=14.429, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=8.620e-05, train_time=3.332
[dl] 2024-10-17 00:17:29,498 (trainer:365) INFO: 4epoch results: [train] iter_time=0.049, forward_time=0.149, loss_ctc=109.676, loss_att=82.966, acc=0.184, loss=11.372, backward_time=0.133, grad_norm=16.725, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=7.610e-05, train_time=3.258, time=11 minutes and 47.51 seconds, total_count=6948, gpu_max_cached_mem_GB=15.139, [valid] loss_ctc=105.084, cer_ctc=0.973, loss_att=77.956, acc=0.218, cer=0.759, wer=1.000, loss=86.094, time=1 minute and 34.09 seconds, total_count=1100, gpu_max_cached_mem_GB=15.139, [att_plot] time=31.46 seconds, total_count=0, gpu_max_cached_mem_GB=15.139
[dl] 2024-10-17 00:17:35,535 (trainer:433) INFO: The best model has been updated: valid.acc
[dl] 2024-10-17 00:17:35,536 (trainer:299) INFO: 5/70epoch started. Estimated time to finish: 12 hours, 24 minutes and 56.46 seconds
[dl] 2024-10-17 00:18:07,595 (trainer:779) INFO: 5epoch:train:1-86batch: iter_time=0.068, forward_time=0.118, loss_ctc=106.458, loss_att=79.399, acc=0.217, loss=10.940, backward_time=0.118, grad_norm=20.454, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.011, optim0_lr0=8.745e-05, train_time=3.059
[dl] 2024-10-17 00:18:34,770 (trainer:779) INFO: 5epoch:train:87-172batch: iter_time=0.005, forward_time=0.120, loss_ctc=108.772, loss_att=80.976, acc=0.211, loss=11.164, backward_time=0.121, grad_norm=23.758, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=8.850e-05, train_time=2.492
[dl] 2024-10-17 00:19:02,082 (trainer:779) INFO: 5epoch:train:173-258batch: iter_time=1.470e-04, forward_time=0.121, loss_ctc=109.576, loss_att=81.466, acc=0.213, loss=11.237, backward_time=0.122, grad_norm=21.802, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=8.960e-05, train_time=2.539
[dl] 2024-10-17 00:19:34,322 (trainer:779) INFO: 5epoch:train:259-344batch: iter_time=0.075, forward_time=0.117, loss_ctc=107.317, loss_att=80.130, acc=0.209, loss=11.036, backward_time=0.117, grad_norm=15.614, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=9.070e-05, train_time=2.986
[dl] 2024-10-17 00:20:08,735 (trainer:779) INFO: 5epoch:train:345-430batch: iter_time=0.100, forward_time=0.119, loss_ctc=114.469, loss_att=84.945, acc=0.219, loss=11.725, backward_time=0.119, grad_norm=23.305, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.011, optim0_lr0=9.175e-05, train_time=3.274
[dl] 2024-10-17 00:20:34,533 (trainer:779) INFO: 5epoch:train:431-516batch: iter_time=1.619e-04, forward_time=0.114, loss_ctc=100.913, loss_att=75.347, acc=0.222, loss=10.377, backward_time=0.115, grad_norm=22.099, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=9.280e-05, train_time=2.396
[dl] 2024-10-17 00:21:00,128 (trainer:779) INFO: 5epoch:train:517-602batch: iter_time=1.422e-04, forward_time=0.114, loss_ctc=105.680, loss_att=78.385, acc=0.219, loss=10.822, backward_time=0.115, grad_norm=11.645, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=9.390e-05, train_time=2.368
[dl] 2024-10-17 00:21:26,630 (trainer:779) INFO: 5epoch:train:603-688batch: iter_time=1.385e-04, forward_time=0.117, loss_ctc=113.287, loss_att=83.896, acc=0.220, loss=11.589, backward_time=0.119, grad_norm=17.318, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=9.500e-05, train_time=2.470
[dl] 2024-10-17 00:21:52,365 (trainer:779) INFO: 5epoch:train:689-774batch: iter_time=1.341e-04, forward_time=0.114, loss_ctc=108.758, loss_att=80.871, acc=0.209, loss=11.155, backward_time=0.116, grad_norm=15.900, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=9.605e-05, train_time=2.384
[dl] 2024-10-17 00:22:17,377 (trainer:779) INFO: 5epoch:train:775-860batch: iter_time=1.516e-04, forward_time=0.112, loss_ctc=93.349, loss_att=69.214, acc=0.226, loss=9.557, backward_time=0.111, grad_norm=11.252, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=9.710e-05, train_time=2.347
[dl] 2024-10-17 00:22:43,872 (trainer:779) INFO: 5epoch:train:861-946batch: iter_time=1.403e-04, forward_time=0.117, loss_ctc=118.864, loss_att=88.113, acc=0.215, loss=12.167, backward_time=0.119, grad_norm=17.787, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=9.820e-05, train_time=2.463
[dl] 2024-10-17 00:23:11,614 (trainer:779) INFO: 5epoch:train:947-1032batch: iter_time=0.028, forward_time=0.114, loss_ctc=105.161, loss_att=77.836, acc=0.214, loss=10.754, backward_time=0.114, grad_norm=25.324, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=9.930e-05, train_time=2.567
[dl] 2024-10-17 00:23:40,044 (trainer:779) INFO: 5epoch:train:1033-1118batch: iter_time=0.031, forward_time=0.116, loss_ctc=111.585, loss_att=82.293, acc=0.228, loss=11.385, backward_time=0.118, grad_norm=17.215, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.003e-04, train_time=2.680
[dl] 2024-10-17 00:24:07,147 (trainer:779) INFO: 5epoch:train:1119-1204batch: iter_time=0.026, forward_time=0.113, loss_ctc=99.548, loss_att=73.467, acc=0.229, loss=10.161, backward_time=0.114, grad_norm=14.441, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.014e-04, train_time=2.515
[dl] 2024-10-17 00:24:33,363 (trainer:779) INFO: 5epoch:train:1205-1290batch: iter_time=0.005, forward_time=0.116, loss_ctc=105.189, loss_att=77.663, acc=0.216, loss=10.740, backward_time=0.116, grad_norm=17.618, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.025e-04, train_time=2.427
[dl] 2024-10-17 00:25:00,270 (trainer:779) INFO: 5epoch:train:1291-1376batch: iter_time=1.448e-04, forward_time=0.120, loss_ctc=114.009, loss_att=84.305, acc=0.217, loss=11.652, backward_time=0.121, grad_norm=12.481, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.036e-04, train_time=2.499
[dl] 2024-10-17 00:25:28,306 (trainer:779) INFO: 5epoch:train:1377-1462batch: iter_time=0.025, forward_time=0.116, loss_ctc=98.766, loss_att=72.880, acc=0.232, loss=10.081, backward_time=0.116, grad_norm=14.430, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.046e-04, train_time=2.636
[dl] 2024-10-17 00:25:55,094 (trainer:779) INFO: 5epoch:train:1463-1548batch: iter_time=0.003, forward_time=0.118, loss_ctc=113.841, loss_att=83.650, acc=0.219, loss=11.588, backward_time=0.119, grad_norm=27.992, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.057e-04, train_time=2.484
[dl] 2024-10-17 00:26:21,502 (trainer:779) INFO: 5epoch:train:1549-1634batch: iter_time=1.407e-04, forward_time=0.117, loss_ctc=112.370, loss_att=82.356, acc=0.215, loss=11.420, backward_time=0.119, grad_norm=24.890, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.068e-04, train_time=2.457
[dl] 2024-10-17 00:26:50,443 (trainer:779) INFO: 5epoch:train:1635-1720batch: iter_time=0.020, forward_time=0.123, loss_ctc=112.132, loss_att=82.229, acc=0.232, loss=11.400, backward_time=0.122, grad_norm=17.182, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.079e-04, train_time=2.678
[dl] 2024-10-17 00:28:16,417 (trainer:365) INFO: 5epoch results: [train] iter_time=0.019, forward_time=0.117, loss_ctc=107.466, loss_att=79.579, acc=0.219, loss=10.993, backward_time=0.117, grad_norm=18.555, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=9.780e-05, train_time=2.578, time=9 minutes and 19.99 seconds, total_count=8685, gpu_max_cached_mem_GB=15.139, [valid] loss_ctc=103.334, cer_ctc=0.963, loss_att=75.181, acc=0.243, cer=0.750, wer=1.000, loss=83.626, time=50.04 seconds, total_count=1375, gpu_max_cached_mem_GB=15.139, [att_plot] time=30.85 seconds, total_count=0, gpu_max_cached_mem_GB=15.139
[dl] 2024-10-17 00:28:20,927 (trainer:433) INFO: The best model has been updated: valid.acc
[dl] 2024-10-17 00:28:20,928 (trainer:299) INFO: 6/70epoch started. Estimated time to finish: 12 hours, 6 minutes and 45.49 seconds
[dl] 2024-10-17 00:28:48,229 (trainer:779) INFO: 6epoch:train:1-86batch: iter_time=0.003, forward_time=0.119, loss_ctc=118.075, loss_att=86.642, acc=0.224, loss=12.009, backward_time=0.121, grad_norm=13.589, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.091e-04, train_time=2.534
[dl] 2024-10-17 00:29:14,412 (trainer:779) INFO: 6epoch:train:87-172batch: iter_time=1.511e-04, forward_time=0.116, loss_ctc=108.383, loss_att=79.117, acc=0.231, loss=10.987, backward_time=0.119, grad_norm=15.329, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.102e-04, train_time=2.451
[dl] 2024-10-17 00:29:41,182 (trainer:779) INFO: 6epoch:train:173-258batch: iter_time=1.543e-04, forward_time=0.118, loss_ctc=108.007, loss_att=79.003, acc=0.228, loss=10.963, backward_time=0.120, grad_norm=18.264, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.113e-04, train_time=2.497
[dl] 2024-10-17 00:30:08,016 (trainer:779) INFO: 6epoch:train:259-344batch: iter_time=1.483e-04, forward_time=0.119, loss_ctc=107.421, loss_att=78.366, acc=0.252, loss=10.885, backward_time=0.120, grad_norm=14.476, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.124e-04, train_time=2.483
[dl] 2024-10-17 00:30:34,424 (trainer:779) INFO: 6epoch:train:345-430batch: iter_time=1.477e-04, forward_time=0.118, loss_ctc=107.993, loss_att=78.763, acc=0.248, loss=10.942, backward_time=0.119, grad_norm=12.667, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.135e-04, train_time=2.477
[dl] 2024-10-17 00:31:00,563 (trainer:779) INFO: 6epoch:train:431-516batch: iter_time=1.502e-04, forward_time=0.116, loss_ctc=98.345, loss_att=71.727, acc=0.237, loss=9.964, backward_time=0.117, grad_norm=18.593, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.145e-04, train_time=2.414
[dl] 2024-10-17 00:31:26,734 (trainer:779) INFO: 6epoch:train:517-602batch: iter_time=1.494e-04, forward_time=0.116, loss_ctc=103.854, loss_att=75.594, acc=0.243, loss=10.509, backward_time=0.117, grad_norm=19.787, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.156e-04, train_time=2.437
[dl] 2024-10-17 00:31:53,447 (trainer:779) INFO: 6epoch:train:603-688batch: iter_time=1.542e-04, forward_time=0.119, loss_ctc=108.268, loss_att=78.823, acc=0.230, loss=10.957, backward_time=0.120, grad_norm=21.815, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.167e-04, train_time=2.482
[dl] 2024-10-17 00:32:19,288 (trainer:779) INFO: 6epoch:train:689-774batch: iter_time=1.471e-04, forward_time=0.115, loss_ctc=103.887, loss_att=75.502, acc=0.236, loss=10.502, backward_time=0.117, grad_norm=13.514, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.177e-04, train_time=2.402
[dl] 2024-10-17 00:32:45,805 (trainer:779) INFO: 6epoch:train:775-860batch: iter_time=1.801e-04, forward_time=0.118, loss_ctc=112.737, loss_att=81.981, acc=0.230, loss=11.401, backward_time=0.119, grad_norm=13.778, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.188e-04, train_time=2.463
[dl] 2024-10-17 00:33:24,287 (trainer:779) INFO: 6epoch:train:861-946batch: iter_time=0.151, forward_time=0.121, loss_ctc=107.668, loss_att=78.378, acc=0.242, loss=10.896, backward_time=0.120, grad_norm=22.586, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.011, optim0_lr0=1.199e-04, train_time=3.551
[dl] 2024-10-17 00:34:08,417 (trainer:779) INFO: 6epoch:train:947-1032batch: iter_time=0.221, forward_time=0.120, loss_ctc=103.496, loss_att=75.244, acc=0.236, loss=10.465, backward_time=0.119, grad_norm=15.212, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.210e-04, train_time=4.072
[dl] 2024-10-17 00:34:54,850 (trainer:779) INFO: 6epoch:train:1033-1118batch: iter_time=0.249, forward_time=0.119, loss_ctc=100.413, loss_att=72.910, acc=0.250, loss=10.145, backward_time=0.117, grad_norm=14.192, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.220e-04, train_time=4.292
[dl] 2024-10-17 00:35:45,825 (trainer:779) INFO: 6epoch:train:1119-1204batch: iter_time=0.300, forward_time=0.119, loss_ctc=101.458, loss_att=73.424, acc=0.254, loss=10.229, backward_time=0.119, grad_norm=13.640, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.231e-04, train_time=4.717
[dl] 2024-10-17 00:36:37,721 (trainer:779) INFO: 6epoch:train:1205-1290batch: iter_time=0.321, forward_time=0.116, loss_ctc=98.403, loss_att=71.162, acc=0.251, loss=9.917, backward_time=0.116, grad_norm=16.225, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.242e-04, train_time=4.910
[dl] 2024-10-17 00:37:23,419 (trainer:779) INFO: 6epoch:train:1291-1376batch: iter_time=0.242, forward_time=0.118, loss_ctc=109.378, loss_att=79.350, acc=0.232, loss=11.045, backward_time=0.119, grad_norm=13.865, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.253e-04, train_time=4.199
[dl] 2024-10-17 00:38:21,341 (trainer:779) INFO: 6epoch:train:1377-1462batch: iter_time=0.399, forward_time=0.115, loss_ctc=90.153, loss_att=65.015, acc=0.253, loss=9.070, backward_time=0.112, grad_norm=18.113, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.264e-04, train_time=5.585
[dl] 2024-10-17 00:39:11,104 (trainer:779) INFO: 6epoch:train:1463-1548batch: iter_time=0.282, forward_time=0.122, loss_ctc=121.683, loss_att=87.936, acc=0.231, loss=12.258, backward_time=0.122, grad_norm=19.512, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.011, optim0_lr0=1.274e-04, train_time=4.469
[dl] 2024-10-17 00:40:00,954 (trainer:779) INFO: 6epoch:train:1549-1634batch: iter_time=0.285, forward_time=0.121, loss_ctc=108.160, loss_att=78.379, acc=0.235, loss=10.914, backward_time=0.120, grad_norm=20.356, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.011, optim0_lr0=1.285e-04, train_time=4.599
[dl] 2024-10-17 00:40:48,562 (trainer:779) INFO: 6epoch:train:1635-1720batch: iter_time=0.262, forward_time=0.120, loss_ctc=104.457, loss_att=75.320, acc=0.246, loss=10.508, backward_time=0.119, grad_norm=22.035, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=1.296e-04, train_time=4.504
# Accounting: time=5207 threads=1
# Ended (code 137) at Thu Oct 17 00:58:54 EDT 2024, elapsed time 5207 seconds

2024-10-17T01:20:31 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_lr1e-3_warm_10000.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-17T01:20:31 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-17T01:20:31 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-17T01:20:31 (asr.sh:1407:main) Generate 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-17T01:20:32 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log'
2024-10-17 01:20:32,193 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log' --log exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-17 01:20:32,209 (launch:348) INFO: log file: exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
bash: line 1: 1911186 Killed                  ( python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True ) 2>> exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log >> exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log', '--gpu', '1', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000', '--config', 'conf/train_asr_lr1e-3_warm_10000.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Thu Oct 17 01:20:32 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[dl] 2024-10-17 01:20:39,864 (asr:523) INFO: Vocabulary size: 5000
[dl] 2024-10-17 01:20:45,024 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl] 2024-10-17 01:20:45,031 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5000, bias=True)
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 38.47 M
    Number of trainable parameters: 38.47 M (100.0%)
    Size: 153.89 MB
    Type: torch.float32
[dl] 2024-10-17 01:20:45,031 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 1e-07
    maximize: False
    weight_decay: 1e-06
)
[dl] 2024-10-17 01:20:45,031 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=10000)
[dl] 2024-10-17 01:20:45,031 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/config.yaml
[dl] 2024-10-17 01:20:45,233 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-17 01:20:46,779 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7df63ed55840>)
[dl] 2024-10-17 01:20:46,779 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=1737, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-17 01:20:46,780 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=1737, mean=32.1, min=6, max=201
[dl] 2024-10-17 01:20:47,100 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-17 01:20:47,252 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7df63eb62fb0>)
[dl] 2024-10-17 01:20:47,252 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=275, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-17 01:20:47,252 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=275, mean=32.9, min=5, max=156
[dl] 2024-10-17 01:20:47,263 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-17 01:20:47,270 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7df63eb62dd0>)
[dl] 2024-10-17 01:20:47,270 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl] 2024-10-17 01:20:47,270 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
# Accounting: time=55 threads=1
# Ended (code 137) at Thu Oct 17 01:21:27 EDT 2024, elapsed time 55 seconds

2024-10-17T03:50:11 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_lr1e-3_warm_10000.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-17T03:50:11 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-17T03:50:11 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-17T03:50:11 (asr.sh:1407:main) Generate 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-17T03:50:11 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log'
2024-10-17 03:50:11,309 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log' --log exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-17 03:50:11,323 (launch:348) INFO: log file: exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
bash: line 1: 1939269 Killed                  ( python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True ) 2>> exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log >> exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log', '--gpu', '1', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000', '--config', 'conf/train_asr_lr1e-3_warm_10000.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Thu Oct 17 03:50:11 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[dl] 2024-10-17 03:50:18,858 (asr:523) INFO: Vocabulary size: 5000
[dl] 2024-10-17 03:50:19,675 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl] 2024-10-17 03:50:19,681 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5000, bias=True)
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 38.47 M
    Number of trainable parameters: 38.47 M (100.0%)
    Size: 153.89 MB
    Type: torch.float32
[dl] 2024-10-17 03:50:19,681 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 1e-07
    maximize: False
    weight_decay: 1e-06
)
[dl] 2024-10-17 03:50:19,681 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=10000)
[dl] 2024-10-17 03:50:19,681 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/config.yaml
[dl] 2024-10-17 03:50:19,824 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-17 03:50:20,326 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7bd29b26d8a0>)
[dl] 2024-10-17 03:50:20,326 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=1737, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-17 03:50:20,327 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=1737, mean=32.1, min=6, max=201
[dl] 2024-10-17 03:50:20,338 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-17 03:50:20,387 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7bd29b07b010>)
[dl] 2024-10-17 03:50:20,387 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=275, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-17 03:50:20,387 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=275, mean=32.9, min=5, max=156
[dl] 2024-10-17 03:50:20,397 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-17 03:50:20,405 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7bd29b0b2ef0>)
[dl] 2024-10-17 03:50:20,405 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl] 2024-10-17 03:50:20,405 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[dl] 2024-10-17 03:50:24,771 (trainer:174) INFO: The training was resumed using exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/checkpoint.pth
[dl] 2024-10-17 03:50:24,774 (trainer:311) INFO: 6/70epoch started
[dl] 2024-10-17 03:50:56,047 (trainer:779) INFO: 6epoch:train:1-86batch: iter_time=0.004, forward_time=0.151, loss_ctc=118.148, loss_att=86.612, acc=0.226, loss=12.009, backward_time=0.138, grad_norm=14.542, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.045, optim0_lr0=1.091e-04, train_time=2.935
# Accounting: time=63 threads=1
# Ended (code 137) at Thu Oct 17 03:51:14 EDT 2024, elapsed time 63 seconds

2024-10-17T03:52:52 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_lr1e-3_warm_10000.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-17T03:52:52 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-17T03:52:52 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-17T03:52:52 (asr.sh:1407:main) Generate 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-17T03:52:52 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log'
2024-10-17 03:52:53,104 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log' --log exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-17 03:52:53,118 (launch:348) INFO: log file: exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
bash: line 1: 1940291 Killed                  ( python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True ) 2>> exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log >> exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log', '--gpu', '1', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000', '--config', 'conf/train_asr_lr1e-3_warm_10000.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Thu Oct 17 03:52:53 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[dl] 2024-10-17 03:52:59,232 (asr:523) INFO: Vocabulary size: 5000
[dl] 2024-10-17 03:53:00,031 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl] 2024-10-17 03:53:00,037 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(5000, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=5000, bias=True)
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Identity()
          (q_norm): Identity()
          (k_norm): Identity()
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 38.47 M
    Number of trainable parameters: 38.47 M (100.0%)
    Size: 153.89 MB
    Type: torch.float32
[dl] 2024-10-17 03:53:00,037 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 1e-07
    maximize: False
    weight_decay: 1e-06
)
[dl] 2024-10-17 03:53:00,037 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=10000)
[dl] 2024-10-17 03:53:00,037 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/config.yaml
[dl] 2024-10-17 03:53:00,183 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-17 03:53:00,563 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x707a00b79870>)
[dl] 2024-10-17 03:53:00,563 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=1737, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-17 03:53:00,564 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=1737, mean=32.1, min=6, max=201
[dl] 2024-10-17 03:53:00,574 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-17 03:53:00,608 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x707a00986fe0>)
[dl] 2024-10-17 03:53:00,608 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=275, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[dl] 2024-10-17 03:53:00,609 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=275, mean=32.9, min=5, max=156
[dl] 2024-10-17 03:53:00,619 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl] 2024-10-17 03:53:00,626 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x707a009baec0>)
[dl] 2024-10-17 03:53:00,626 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl] 2024-10-17 03:53:00,626 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[dl] 2024-10-17 03:53:01,083 (trainer:174) INFO: The training was resumed using exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/checkpoint.pth
[dl] 2024-10-17 03:53:01,085 (trainer:311) INFO: 6/70epoch started
[dl] 2024-10-17 03:53:29,158 (trainer:779) INFO: 6epoch:train:1-86batch: iter_time=0.003, forward_time=0.134, loss_ctc=118.149, loss_att=86.612, acc=0.226, loss=12.009, backward_time=0.122, grad_norm=14.566, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.026, optim0_lr0=1.091e-04, train_time=2.618
# Accounting: time=61 threads=1
# Ended (code 137) at Thu Oct 17 03:53:54 EDT 2024, elapsed time 61 seconds

2024-10-17T03:54:29 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_lr1e-3_warm_10000.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-17T03:54:29 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-17T03:54:29 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-17T03:54:29 (asr.sh:1407:main) Generate 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-17T03:54:29 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log'
2024-10-17 03:54:29,277 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log' --log exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000 --config conf/train_asr_lr1e-3_warm_10000.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-17 03:54:29,290 (launch:348) INFO: log file: exp/asr_train_asr_lr1e-3_warm_10000_raw_en_bpe5000/train.log
2024-10-18T14:36:39 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer.yaml --inference_config conf/decode_asr.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-18T14:36:39 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-18T14:36:39 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-18T14:36:39 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-18T14:36:39 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_ebranchformer_raw_en_bpe5000/train.log'
2024-10-18 14:36:39,884 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_ebranchformer_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_ebranchformer_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_ebranchformer_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ebranchformer_raw_en_bpe5000 --config conf/train_asr_wavlm_ebranchformer.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-18 14:36:39,902 (launch:237) INFO: single-node with 2gpu on distributed mode
2024-10-18 14:36:39,903 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_ebranchformer_raw_en_bpe5000/train.log
2024-10-25T23:34:37 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference false --inference_nj 8 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-25T23:34:37 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-25T23:34:37 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-25T23:34:37 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-25T23:34:37 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log'
2024-10-25 23:34:37,674 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-25 23:34:37,689 (launch:237) INFO: single-node with 2gpu on distributed mode
2024-10-25 23:34:37,690 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log', '--gpu', '2', 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_ebranchformer_onlyctc.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '2', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log ###################
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,030 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,030 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,030 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,030 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,030 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,030 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,030 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,030 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.grep_a.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.grep_a.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.grep_a.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,031 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.grep_a.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.grep_a.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,032 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.grep_a.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.grep_a.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.grep_a.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,033 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.grep_a.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.k_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.k_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.v_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.v_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.q_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.q_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.out_proj.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.out_proj.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.grep_linear.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.grep_linear.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.fc1.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.fc1.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.fc2.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.fc2.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.final_layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.final_layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.layer_norm.weight.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,034 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.layer_norm.bias.requires_grad = False
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,404 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,413 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): S3prlFrontend(
    (upstream): S3PRLUpstream(
      (upstream): UpstreamExpert(
        (model): WavLM(
          (feature_extractor): ConvFeatureExtractionModel(
            (conv_layers): ModuleList(
              (0): Sequential(
                (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
                (1): Dropout(p=0.0, inplace=False)
                (2): Sequential(
                  (0): TransposeLast()
                  (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (2): TransposeLast()
                )
                (3): GELU(approximate='none')
              )
              (1-4): 4 x Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
                (1): Dropout(p=0.0, inplace=False)
                (2): Sequential(
                  (0): TransposeLast()
                  (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (2): TransposeLast()
                )
                (3): GELU(approximate='none')
              )
              (5-6): 2 x Sequential(
                (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
                (1): Dropout(p=0.0, inplace=False)
                (2): Sequential(
                  (0): TransposeLast()
                  (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (2): TransposeLast()
                )
                (3): GELU(approximate='none')
              )
            )
          )
          (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)
          (dropout_input): Dropout(p=0.0, inplace=False)
          (dropout_features): Dropout(p=0.0, inplace=False)
          (encoder): TransformerEncoder(
            (pos_conv): Sequential(
              (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
              (1): SamePad()
              (2): GELU(approximate='none')
            )
            (layers): ModuleList(
              (0): TransformerSentenceEncoderLayer(
                (self_attn): MultiheadAttention(
                  (dropout_module): Dropout(p=0.0, inplace=False)
                  (relative_attention_bias): Embedding(320, 16)
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (grep_linear): Linear(in_features=64, out_features=8, bias=True)
                )
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
                (dropout3): Dropout(p=0.0, inplace=False)
                (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (1-23): 23 x TransformerSentenceEncoderLayer(
                (self_attn): MultiheadAttention(
                  (dropout_module): Dropout(p=0.0, inplace=False)
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (grep_linear): Linear(in_features=64, out_features=8, bias=True)
                )
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
                (dropout3): Dropout(p=0.0, inplace=False)
                (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (featurizer): Featurizer()
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (preencoder): LinearProjection(
    (linear_out): Linear(in_features=1024, out_features=128, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=7936, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 342.80 M
    Number of trainable parameters: 27.35 M (8.0%)
    Size: 109.41 MB
    Type: torch.float32
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,413 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 1.3333333333333336e-07
    maximize: False
    weight_decay: 1e-06
)
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,413 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,413 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,579 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,982 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7853b7a3d450>)
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,982 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=3286, batch_bins=4000000, sort_in_batch=descending, sort_batch=descending)
[SPAPL-Psychic:0/2] 2024-10-25 23:35:06,983 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=3286, mean=17.0, min=5, max=111
[SPAPL-Psychic:0/2] 2024-10-25 23:35:07,003 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[SPAPL-Psychic:0/2] 2024-10-25 23:35:07,186 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7853b784aad0>)
[SPAPL-Psychic:0/2] 2024-10-25 23:35:07,186 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=514, batch_bins=4000000, sort_in_batch=descending, sort_batch=descending)
[SPAPL-Psychic:0/2] 2024-10-25 23:35:07,186 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=514, mean=17.6, min=5, max=99
[SPAPL-Psychic:0/2] 2024-10-25 23:35:07,198 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[SPAPL-Psychic:0/2] 2024-10-25 23:35:07,207 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7853be7a84c0>)
[SPAPL-Psychic:0/2] 2024-10-25 23:35:07,207 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[SPAPL-Psychic:0/2] 2024-10-25 23:35:07,207 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
SPAPL-Psychic:2552223:2552223 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
SPAPL-Psychic:2552223:2552223 [0] NCCL INFO Bootstrap : Using enp179s0f0:164.67.196.69<0>
SPAPL-Psychic:2552223:2552223 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
SPAPL-Psychic:2552223:2552223 [0] NCCL INFO cudaDriverVersion 12060
NCCL version 2.20.5+cuda12.4
Process SpawnProcess-2:
Process SpawnProcess-1:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 256, in run
    dp_model = torch.nn.parallel.DistributedDataParallel(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 798, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/utils.py", line 269, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 256, in run
    dp_model = torch.nn.parallel.DistributedDataParallel(
torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1970, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
nvmlDeviceGetHandleByIndex(0) failed: Unknown Error
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 798, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/distributed/utils.py", line 269, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1970, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
nvmlDeviceGetHandleByIndex(0) failed: Unknown Error
SPAPL-Psychic:2552224:2552224 [1] NCCL INFO cudaDriverVersion 12060
SPAPL-Psychic:2552224:2552224 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
SPAPL-Psychic:2552224:2552224 [1] NCCL INFO Bootstrap : Using enp179s0f0:164.67.196.69<0>
SPAPL-Psychic:2552224:2552224 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO NET/IB : No device found.
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO NET/Socket : Using [0]enp179s0f0:164.67.196.69<0>
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO Using non-device net plugin version 0
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO Using network Socket

SPAPL-Psychic:2552224:2552406 [1] misc/nvmlwrap.cc:127 NCCL WARN nvmlDeviceGetHandleByIndex(0) failed: Unknown Error
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO misc/nvmlwrap.cc:185 -> 2
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO init.cc:338 -> 2
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO init.cc:1492 -> 2
SPAPL-Psychic:2552224:2552406 [1] NCCL INFO group.cc:64 -> 2 [Async thread]
SPAPL-Psychic:2552224:2552224 [1] NCCL INFO group.cc:418 -> 2
SPAPL-Psychic:2552224:2552224 [1] NCCL INFO group.cc:95 -> 2
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO NET/IB : No device found.
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO NET/Socket : Using [0]enp179s0f0:164.67.196.69<0>
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO Using non-device net plugin version 0
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO Using network Socket

SPAPL-Psychic:2552223:2552405 [0] misc/nvmlwrap.cc:127 NCCL WARN nvmlDeviceGetHandleByIndex(0) failed: Unknown Error
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO misc/nvmlwrap.cc:185 -> 2
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO init.cc:338 -> 2
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO init.cc:1492 -> 2
SPAPL-Psychic:2552223:2552405 [0] NCCL INFO group.cc:64 -> 2 [Async thread]
SPAPL-Psychic:2552223:2552223 [0] NCCL INFO group.cc:418 -> 2
SPAPL-Psychic:2552223:2552223 [0] NCCL INFO group.cc:95 -> 2
SPAPL-Psychic:2552223:2552408 [0] NCCL INFO comm 0xc6b3a70 rank 0 nranks 2 cudaDev 0 busId 1a000 - Abort COMPLETE
SPAPL-Psychic:2552224:2552409 [0] NCCL INFO comm 0x597e67d0 rank 1 nranks 2 cudaDev 1 busId 67000 - Abort COMPLETE
W1025 23:35:08.599000 130328679135040 torch/multiprocessing/spawn.py:145] Terminating process 2552224 via signal SIGTERM
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1273, in main
    while not ProcessContext(processes, error_files).join():
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 177, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with exit code 1
# Accounting: time=32 threads=1
# Ended (code 1) at Fri Oct 25 23:35:09 PDT 2024, elapsed time 32 seconds

2024-10-26T03:25:50 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 8 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-26T03:25:50 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-26T03:25:50 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-26T03:25:50 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-26T03:25:50 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log'
2024-10-26 03:25:51,956 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-26 03:25:51,972 (launch:237) INFO: single-node with 4gpu on distributed mode
2024-10-26 03:25:51,992 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log', '--gpu', '4', 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_ebranchformer_onlyctc.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '4', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log ###################
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.grep_linear.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn.grep_linear.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn_layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.self_attn_layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.fc1.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.fc1.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.fc2.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.fc2.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.final_layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.final_layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.grep_a.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.k_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.k_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.v_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,962 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.v_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.q_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.q_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.out_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.out_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.grep_linear.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn.grep_linear.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn_layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.self_attn_layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.fc1.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.fc1.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.fc2.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.fc2.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.final_layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.final_layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.grep_a.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.k_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.k_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.v_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.v_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.q_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.q_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.out_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.out_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.grep_linear.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn.grep_linear.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn_layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.self_attn_layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.fc1.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.fc1.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.fc2.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.fc2.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.final_layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.final_layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.grep_a.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.k_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.k_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.v_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.v_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.q_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.q_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.out_proj.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.out_proj.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.grep_linear.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn.grep_linear.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn_layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.self_attn_layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.fc1.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.fc1.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.fc2.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.fc2.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.final_layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,963 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.final_layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,964 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,964 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.encoder.layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,964 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.layer_norm.weight.requires_grad = False
[dl:0/4] 2024-10-26 03:26:36,964 (abs_task:1359) INFO: Setting frontend.upstream.upstream.model.layer_norm.bias.requires_grad = False
[dl:0/4] 2024-10-26 03:26:37,871 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl:0/4] 2024-10-26 03:26:37,886 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): S3prlFrontend(
    (upstream): S3PRLUpstream(
      (upstream): UpstreamExpert(
        (model): WavLM(
          (feature_extractor): ConvFeatureExtractionModel(
            (conv_layers): ModuleList(
              (0): Sequential(
                (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
                (1): Dropout(p=0.0, inplace=False)
                (2): Sequential(
                  (0): TransposeLast()
                  (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (2): TransposeLast()
                )
                (3): GELU(approximate='none')
              )
              (1-4): 4 x Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
                (1): Dropout(p=0.0, inplace=False)
                (2): Sequential(
                  (0): TransposeLast()
                  (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (2): TransposeLast()
                )
                (3): GELU(approximate='none')
              )
              (5-6): 2 x Sequential(
                (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
                (1): Dropout(p=0.0, inplace=False)
                (2): Sequential(
                  (0): TransposeLast()
                  (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (2): TransposeLast()
                )
                (3): GELU(approximate='none')
              )
            )
          )
          (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)
          (dropout_input): Dropout(p=0.0, inplace=False)
          (dropout_features): Dropout(p=0.0, inplace=False)
          (encoder): TransformerEncoder(
            (pos_conv): Sequential(
              (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
              (1): SamePad()
              (2): GELU(approximate='none')
            )
            (layers): ModuleList(
              (0): TransformerSentenceEncoderLayer(
                (self_attn): MultiheadAttention(
                  (dropout_module): Dropout(p=0.0, inplace=False)
                  (relative_attention_bias): Embedding(320, 16)
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (grep_linear): Linear(in_features=64, out_features=8, bias=True)
                )
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
                (dropout3): Dropout(p=0.0, inplace=False)
                (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (1-23): 23 x TransformerSentenceEncoderLayer(
                (self_attn): MultiheadAttention(
                  (dropout_module): Dropout(p=0.0, inplace=False)
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (grep_linear): Linear(in_features=64, out_features=8, bias=True)
                )
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
                (dropout3): Dropout(p=0.0, inplace=False)
                (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (featurizer): Featurizer()
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (preencoder): LinearProjection(
    (linear_out): Linear(in_features=1024, out_features=128, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=7936, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 342.80 M
    Number of trainable parameters: 27.35 M (8.0%)
    Size: 109.41 MB
    Type: torch.float32
[dl:0/4] 2024-10-26 03:26:37,886 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 1.3333333333333336e-07
    maximize: False
    weight_decay: 1e-06
)
[dl:0/4] 2024-10-26 03:26:37,886 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[dl:0/4] 2024-10-26 03:26:37,926 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
[dl:0/4] 2024-10-26 03:26:38,104 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/4] 2024-10-26 03:26:38,629 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x747130a7e710>)
[dl:0/4] 2024-10-26 03:26:38,629 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=3286, batch_bins=4000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/4] 2024-10-26 03:26:38,629 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=3286, mean=17.0, min=5, max=111
[dl:0/4] 2024-10-26 03:26:38,685 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/4] 2024-10-26 03:26:38,891 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7471308723e0>)
[dl:0/4] 2024-10-26 03:26:38,891 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=514, batch_bins=4000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/4] 2024-10-26 03:26:38,891 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=514, mean=17.6, min=5, max=99
[dl:0/4] 2024-10-26 03:26:38,902 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/4] 2024-10-26 03:26:38,908 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7471308a3d00>)
[dl:0/4] 2024-10-26 03:26:38,908 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl:0/4] 2024-10-26 03:26:38,908 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
dl:979817:979817 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979817:979817 [0] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:979817:979817 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:979817:979817 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.20.5+cuda12.4
dl:979817:980016 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979817:980016 [0] NCCL INFO NET/IB : No device found.
dl:979817:980016 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979817:980016 [0] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:979817:980016 [0] NCCL INFO Using non-device net plugin version 0
dl:979817:980016 [0] NCCL INFO Using network Socket
dl:979817:980016 [0] NCCL INFO comm 0xa768d40 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 2000 commId 0x47133566ce6f830e - Init START
dl:979817:980016 [0] NCCL INFO NVLS multicast support is not available on dev 0
dl:979817:980016 [0] NCCL INFO comm 0xa768d40 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
dl:979817:980016 [0] NCCL INFO Channel 00/02 :    0   1   2   3
dl:979817:980016 [0] NCCL INFO Channel 01/02 :    0   1   2   3
dl:979817:980016 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
dl:979817:980016 [0] NCCL INFO P2P Chunksize set to 131072
dl:979817:980016 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
dl:979817:980016 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
dl:979817:980016 [0] NCCL INFO Connected all rings
dl:979817:980016 [0] NCCL INFO Connected all trees
dl:979817:980016 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:979817:980016 [0] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:979817:980016 [0] NCCL INFO comm 0xa768d40 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 2000 commId 0x47133566ce6f830e - Init COMPLETE
dl:979818:979818 [1] NCCL INFO cudaDriverVersion 12050
dl:979818:979818 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979818:979818 [1] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:979818:979818 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:979818:980019 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979818:980019 [1] NCCL INFO NET/IB : No device found.
dl:979818:980019 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979818:980019 [1] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:979818:980019 [1] NCCL INFO Using non-device net plugin version 0
dl:979818:980019 [1] NCCL INFO Using network Socket
dl:979818:980019 [1] NCCL INFO comm 0x5a602d70 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 21000 commId 0x47133566ce6f830e - Init START
dl:979818:980019 [1] NCCL INFO NVLS multicast support is not available on dev 1
dl:979818:980019 [1] NCCL INFO comm 0x5a602d70 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
dl:979818:980019 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
dl:979818:980019 [1] NCCL INFO P2P Chunksize set to 131072
dl:979818:980019 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
dl:979818:980019 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
dl:979818:980019 [1] NCCL INFO Connected all rings
dl:979818:980019 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
dl:979818:980019 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
dl:979818:980019 [1] NCCL INFO Connected all trees
dl:979818:980019 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:979818:980019 [1] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:979818:980019 [1] NCCL INFO comm 0x5a602d70 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 21000 commId 0x47133566ce6f830e - Init COMPLETE
[rank0]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
dl:979820:979820 [3] NCCL INFO cudaDriverVersion 12050
dl:979820:979820 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979820:979820 [3] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:979820:979820 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:979820:980017 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979820:980017 [3] NCCL INFO NET/IB : No device found.
dl:979820:980017 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979820:980017 [3] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:979820:980017 [3] NCCL INFO Using non-device net plugin version 0
dl:979820:980017 [3] NCCL INFO Using network Socket
dl:979820:980017 [3] NCCL INFO comm 0x5ae256f0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 4a000 commId 0x47133566ce6f830e - Init START
dl:979820:980017 [3] NCCL INFO NVLS multicast support is not available on dev 3
dl:979820:980017 [3] NCCL INFO comm 0x5ae256f0 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
dl:979820:980017 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
dl:979820:980017 [3] NCCL INFO P2P Chunksize set to 131072
dl:979820:980017 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM
dl:979820:980017 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/CUMEM
dl:979820:980017 [3] NCCL INFO Connected all rings
dl:979820:980017 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
dl:979820:980017 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
dl:979820:980017 [3] NCCL INFO Connected all trees
dl:979820:980017 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:979820:980017 [3] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:979820:980017 [3] NCCL INFO comm 0x5ae256f0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 4a000 commId 0x47133566ce6f830e - Init COMPLETE
[rank1]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
[rank3]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
dl:979819:979819 [2] NCCL INFO cudaDriverVersion 12050
dl:979819:979819 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979819:979819 [2] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:979819:979819 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:979819:980018 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979819:980018 [2] NCCL INFO NET/IB : No device found.
dl:979819:980018 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:979819:980018 [2] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:979819:980018 [2] NCCL INFO Using non-device net plugin version 0
dl:979819:980018 [2] NCCL INFO Using network Socket
dl:979819:980018 [2] NCCL INFO comm 0x5af571c0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 49000 commId 0x47133566ce6f830e - Init START
dl:979819:980018 [2] NCCL INFO NVLS multicast support is not available on dev 2
dl:979819:980018 [2] NCCL INFO comm 0x5af571c0 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
dl:979819:980018 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
dl:979819:980018 [2] NCCL INFO P2P Chunksize set to 131072
dl:979819:980018 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
dl:979819:980018 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
dl:979819:980018 [2] NCCL INFO Connected all rings
dl:979819:980018 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
dl:979819:980018 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
dl:979819:980018 [2] NCCL INFO Connected all trees
dl:979819:980018 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:979819:980018 [2] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:979819:980018 [2] NCCL INFO comm 0x5af571c0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 49000 commId 0x47133566ce6f830e - Init COMPLETE
[rank2]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
[dl:0/4] 2024-10-26 03:26:40,360 (trainer:311) INFO: 1/70epoch started
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Process SpawnProcess-1:
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 614, in train_one_epoch
    retval = model(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 237, in forward
    encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 378, in encode
    feats, feats_lengths = self._extract_feats(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 445, in _extract_feats
    feats, feats_lengths = self.frontend(speech, speech_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/frontend/s3prl.py", line 99, in forward
    feats, feats_lens = self.upstream(input, input_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/nn/upstream.py", line 209, in forward
    hidden_states = self.upstream(wavs_list)["hidden_states"]
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/interfaces.py", line 103, in __call__
    result = super().__call__(wavs, *args, **kwargs) or {}
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/expert.py", line 83, in forward
    features, feat_padding_mask = self.model.extract_features(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 389, in extract_features
    x, layer_results = self.encoder(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 592, in forward
    x, layer_results = self.extract_features(x, padding_mask, streaming_mask, layer)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 626, in extract_features
    x, z, pos_bias = layer(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 725, in forward
    x, attn, pos_bias = self.self_attn(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/modules.py", line 556, in forward
    x, attn = F.multi_head_attention_forward(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py", line 5504, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 252.00 MiB. GPU 
dl:979817:980021 [0] NCCL INFO [Service thread] Connection closed by localRank 0
dl:979817:981444 [0] NCCL INFO comm 0xa768d40 rank 0 nranks 4 cudaDev 0 busId 2000 - Abort COMPLETE
W1026 03:27:34.863000 123852957493056 torch/multiprocessing/spawn.py:145] Terminating process 979818 via signal SIGTERM
W1026 03:27:34.864000 123852957493056 torch/multiprocessing/spawn.py:145] Terminating process 979819 via signal SIGTERM
W1026 03:27:34.864000 123852957493056 torch/multiprocessing/spawn.py:145] Terminating process 979820 via signal SIGTERM
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1273, in main
    while not ProcessContext(processes, error_files).join():
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 177, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with exit code 1
# Accounting: time=105 threads=1
# Ended (code 1) at Sat Oct 26 03:27:37 EDT 2024, elapsed time 105 seconds

2024-10-26T03:28:21 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 8 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-26T03:28:22 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-26T03:28:22 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-26T03:28:22 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-26T03:28:22 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log'
2024-10-26 03:28:22,972 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-26 03:28:22,988 (launch:237) INFO: single-node with 4gpu on distributed mode
2024-10-26 03:28:23,004 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log
2024-10-26T04:11:34 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-26T04:11:34 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-26T04:11:34 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-26T04:11:34 (asr.sh:1407:main) Generate 'exp/asr_train_asr_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-26T04:11:34 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_onlyctc_raw_en_bpe5000/train.log'
2024-10-26 04:11:36,224 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_onlyctc_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_onlyctc_raw_en_bpe5000 --config conf/train_asr_onlyctc.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-26 04:11:36,239 (launch:237) INFO: single-node with 4gpu on distributed mode
2024-10-26 04:11:36,257 (launch:348) INFO: log file: exp/asr_train_asr_onlyctc_raw_en_bpe5000/train.log
2024-10-26T15:08:42 (asr.sh:1813:main) Successfully finished. [elapsed=39428s]
2024-10-26T15:47:30 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 8 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-26T15:47:30 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-26T15:47:30 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-26T15:47:30 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-26T15:47:30 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log'
2024-10-26 15:47:30,661 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-26 15:47:30,675 (launch:237) INFO: single-node with 4gpu on distributed mode
2024-10-26 15:47:30,678 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log
2024-10-26T16:59:00 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 8 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-26T16:59:00 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-26T16:59:00 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-26T16:59:00 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-26T16:59:00 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log'
2024-10-26 16:59:00,503 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-26 16:59:00,519 (launch:237) INFO: single-node with 4gpu on distributed mode
2024-10-26 16:59:00,519 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log
2024-10-26T16:59:33 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 8 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-26T16:59:33 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-26T16:59:33 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-26T16:59:33 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-26T16:59:33 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log'
2024-10-26 16:59:33,280 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-26 16:59:33,293 (launch:237) INFO: single-node with 4gpu on distributed mode
2024-10-26 16:59:33,294 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log
2024-10-26T16:59:47 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 8 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-26T16:59:47 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-26T16:59:47 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-26T16:59:47 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-26T16:59:47 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log'
2024-10-26 16:59:47,846 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-26 16:59:47,861 (launch:237) INFO: single-node with 4gpu on distributed mode
2024-10-26 16:59:47,862 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/train.log
<<<<<<< HEAD
2024-10-27T17:15:02 (asr.sh:1813:main) Successfully finished. [elapsed=87315s]
2024-10-27T17:29:48 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 8 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-27T17:29:48 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-27T17:29:48 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000
2024-10-27T17:29:48 (asr.sh:1508:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/run.sh'. You can resume the process from stage 12 using this script
2024-10-27T17:29:48 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/asr_inference.*.log'
run.pl: 8 / 8 failed, log is in exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/asr_inference.*.log
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.1.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.1 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:29:48 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.1.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.1 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:29:56,423 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:29:56,578 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:29:56,808 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:29:56,808 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:30:02,643 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:30:13,129 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
=======
2024-10-30T23:11:56 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_onlyctc_finetune.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-30T23:11:57 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-30T23:11:57 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-30T23:11:57 (asr.sh:1407:main) Generate 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-30T23:11:57 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log'
2024-10-30 23:11:59,536 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log' --log exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000 --config conf/train_asr_onlyctc_finetune.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-30 23:11:59,571 (launch:237) INFO: single-node with 2gpu on distributed mode
2024-10-30 23:11:59,572 (launch:348) INFO: log file: exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log
2024-10-31T20:45:13 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_onlyctc_finetune.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-31T20:45:13 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-31T20:45:13 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-31T20:45:13 (asr.sh:1407:main) Generate 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-31T20:45:13 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log'
2024-10-31 20:45:13,632 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log' --log exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000 --config conf/train_asr_onlyctc_finetune.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-31 20:45:13,645 (launch:237) INFO: single-node with 2gpu on distributed mode
2024-10-31 20:45:13,646 (launch:348) INFO: log file: exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log', '--gpu', '2', 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000', '--config', 'conf/train_asr_onlyctc_finetune.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '2', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
>>>>>>> cea3af74175b4f1b718b2c4afeb0a5f028e5698c
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
<<<<<<< HEAD
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 813, in inference
    speech2text = Speech2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 698, in from_pretrained
    return Speech2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 140, in __init__
    asr_model, asr_train_args = task.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
=======
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000 --config conf/train_asr_onlyctc_finetune.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed True 
# Started at Thu Oct 31 20:45:13 PDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000 --config conf/train_asr_onlyctc_finetune.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 2 --multiprocessing_distributed True
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[lambda-Lambda-Vector:0/2] 2024-10-31 20:45:25,055 (asr:523) INFO: Vocabulary size: 5000
[lambda-Lambda-Vector:0/2] 2024-10-31 20:45:25,224 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
Process SpawnProcess-2:
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1466, in main_worker
    load_pretrained_model(
  File "/data/mohan/workdir/espnet/espnet2/torch_utils/load_pretrained_model.py", line 99, in load_pretrained_model
    src_state = torch.load(path, map_location=map_location)
>>>>>>> cea3af74175b4f1b718b2c4afeb0a5f028e5698c
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
<<<<<<< HEAD
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth'
# Accounting: time=26 threads=1
# Ended (code 1) at Sun Oct 27 17:30:14 EDT 2024, elapsed time 26 seconds
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.2.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.2 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:29:48 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.2.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.2 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:29:56,354 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:29:56,510 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:29:56,814 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:29:56,815 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:30:02,643 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:30:13,316 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 813, in inference
    speech2text = Speech2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 698, in from_pretrained
    return Speech2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 140, in __init__
    asr_model, asr_train_args = task.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth'
# Accounting: time=26 threads=1
# Ended (code 1) at Sun Oct 27 17:30:14 EDT 2024, elapsed time 26 seconds
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.3.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.3 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:29:48 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.3.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.3 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:29:56,232 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:29:56,398 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:29:56,813 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:29:56,864 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:30:02,643 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:30:13,168 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 813, in inference
    speech2text = Speech2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 698, in from_pretrained
    return Speech2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 140, in __init__
    asr_model, asr_train_args = task.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth'
# Accounting: time=26 threads=1
# Ended (code 1) at Sun Oct 27 17:30:14 EDT 2024, elapsed time 26 seconds
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.4.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.4 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:29:48 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.4.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.4 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:29:56,363 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:29:56,522 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:29:56,814 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:29:56,814 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:30:02,643 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:30:13,167 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 813, in inference
    speech2text = Speech2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 698, in from_pretrained
    return Speech2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 140, in __init__
    asr_model, asr_train_args = task.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth'
# Accounting: time=26 threads=1
# Ended (code 1) at Sun Oct 27 17:30:14 EDT 2024, elapsed time 26 seconds
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.5.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.5 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:29:48 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.5.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.5 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:29:56,820 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:29:56,974 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:29:57,044 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:29:57,044 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:30:02,643 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:30:13,027 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 813, in inference
    speech2text = Speech2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 698, in from_pretrained
    return Speech2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 140, in __init__
    asr_model, asr_train_args = task.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth'
# Accounting: time=26 threads=1
# Ended (code 1) at Sun Oct 27 17:30:14 EDT 2024, elapsed time 26 seconds
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.6.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.6 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:29:48 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.6.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.6 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:29:56,232 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:29:56,401 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:29:56,813 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:29:56,814 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:30:02,643 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:30:13,123 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 813, in inference
    speech2text = Speech2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 698, in from_pretrained
    return Speech2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 140, in __init__
    asr_model, asr_train_args = task.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth'
# Accounting: time=26 threads=1
# Ended (code 1) at Sun Oct 27 17:30:14 EDT 2024, elapsed time 26 seconds
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.7.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.7 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:29:48 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.7.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.7 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:29:56,611 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:29:56,767 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:29:56,835 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:29:56,836 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:30:02,643 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:30:13,029 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 813, in inference
    speech2text = Speech2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 698, in from_pretrained
    return Speech2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 140, in __init__
    asr_model, asr_train_args = task.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth'
# Accounting: time=26 threads=1
# Ended (code 1) at Sun Oct 27 17:30:14 EDT 2024, elapsed time 26 seconds
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.8.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.8 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:29:48 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/keys.8.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.acc.ave/dev/logdir/output.8 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:29:56,811 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:29:56,964 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:29:57,033 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:29:57,034 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:30:02,643 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:30:13,119 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 813, in inference
    speech2text = Speech2Text.from_pretrained(
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 698, in from_pretrained
    return Speech2Text(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 140, in __init__
    asr_model, asr_train_args = task.build_model_from_file(
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 2323, in build_model_from_file
    torch.load(model_file, map_location=device),
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.acc.ave.pth'
# Accounting: time=26 threads=1
# Ended (code 1) at Sun Oct 27 17:30:14 EDT 2024, elapsed time 26 seconds
2024-10-27T17:30:57 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 8 --inference_asr_model valid.cer_ctc.ave.pth --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-27T17:30:57 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-27T17:30:57 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000
2024-10-27T17:30:57 (asr.sh:1508:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/run.sh'. You can resume the process from stage 12 using this script
2024-10-27T17:30:57 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/asr_inference.*.log'
run.pl: 2 / 8 failed, log is in exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/asr_inference.*.log
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/keys.1.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.cer_ctc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/output.1 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:30:57 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/keys.1.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.cer_ctc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/output.1 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:31:06,011 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:31:06,167 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:31:06,237 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:31:06,238 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:31:07,210 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:31:16,230 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
2024-10-27 17:31:18,748 (asr_inference:372) INFO: BatchBeamSearch implementation is selected.
2024-10-27 17:31:18,748 (asr_inference:383) INFO: Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict()
)
2024-10-27 17:31:18,748 (asr_inference:384) INFO: Decoding device=cpu, dtype=float32
2024-10-27 17:31:18,750 (asr_inference:462) INFO: Text tokenizer: SentencepiecesTokenizer(model="data/en_token_list/bpe_unigram5000/bpe.model")
2024-10-27 17:31:18,752 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
2024-10-27 17:31:20,138 (asr_inference:509) INFO: speech length: 51000
2024-10-27 17:31:22,331 (beam_search:428) INFO: decoder input length: 39
2024-10-27 17:31:22,331 (beam_search:429) INFO: max output length: 39
2024-10-27 17:31:22,331 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:22,381 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:22,381 (beam_search:476) INFO:  -1.07 * 1.0 =  -1.07 for ctc
2024-10-27 17:31:22,381 (beam_search:479) INFO: total log probability: -1.07
2024-10-27 17:31:22,381 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:31:22,381 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:22,381 (beam_search:483) INFO: best hypo: ▁I▁AM▁REALLY▁GOOD▁HOW▁ABOUT▁YOU

2024-10-27 17:31:22,389 (asr_inference:509) INFO: speech length: 131718
2024-10-27 17:31:27,516 (beam_search:428) INFO: decoder input length: 102
2024-10-27 17:31:27,517 (beam_search:429) INFO: max output length: 102
2024-10-27 17:31:27,517 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:27,595 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:27,596 (beam_search:476) INFO:  -2.51 * 1.0 =  -2.51 for ctc
2024-10-27 17:31:27,596 (beam_search:479) INFO: total log probability: -2.51
2024-10-27 17:31:27,596 (beam_search:480) INFO: normalized log probability: -0.42
2024-10-27 17:31:27,596 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:27,596 (beam_search:483) INFO: best hypo: ▁BEEN▁ABOUT▁AND▁ENERGY

2024-10-27 17:31:27,598 (asr_inference:509) INFO: speech length: 103798
2024-10-27 17:31:31,593 (beam_search:428) INFO: decoder input length: 80
2024-10-27 17:31:31,594 (beam_search:429) INFO: max output length: 80
2024-10-27 17:31:31,594 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:31,738 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:31,739 (beam_search:476) INFO:  -2.80 * 1.0 =  -2.80 for ctc
2024-10-27 17:31:31,739 (beam_search:479) INFO: total log probability: -2.80
2024-10-27 17:31:31,739 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:31:31,739 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:31,739 (beam_search:483) INFO: best hypo: ▁IS▁A▁D▁CELL▁BATTERY▁WIRES▁AND▁A▁LIGHT▁BULB

2024-10-27 17:31:31,741 (asr_inference:509) INFO: speech length: 141660
2024-10-27 17:31:37,561 (beam_search:428) INFO: decoder input length: 110
2024-10-27 17:31:37,561 (beam_search:429) INFO: max output length: 110
2024-10-27 17:31:37,561 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:37,916 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:37,916 (beam_search:476) INFO:  -1.78 * 1.0 =  -1.78 for ctc
2024-10-27 17:31:37,916 (beam_search:479) INFO: total log probability: -1.78
2024-10-27 17:31:37,916 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:31:37,916 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:37,916 (beam_search:483) INFO: best hypo: ▁SO▁YOU▁CAN▁POWER▁YOUR▁OR▁LIGHT▁BULB▁SO▁IT▁WILL▁LIGHT▁UP▁OR▁SHOW▁YOU▁THAT▁THE▁ENERGY

2024-10-27 17:31:37,919 (asr_inference:509) INFO: speech length: 136837
2024-10-27 17:31:42,855 (beam_search:428) INFO: decoder input length: 106
2024-10-27 17:31:42,855 (beam_search:429) INFO: max output length: 106
2024-10-27 17:31:42,855 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:43,142 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:43,142 (beam_search:476) INFO:  -1.43 * 1.0 =  -1.43 for ctc
2024-10-27 17:31:43,142 (beam_search:479) INFO: total log probability: -1.43
2024-10-27 17:31:43,142 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:31:43,142 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:43,142 (beam_search:483) INFO: best hypo: ▁THEY▁HELP▁THE▁ENERGY▁GO▁THROUGH▁THE▁D▁SO▁THE▁BULB▁CAN▁ACTUALLY▁LIGHT▁UP

2024-10-27 17:31:43,144 (asr_inference:509) INFO: speech length: 41378
2024-10-27 17:31:44,777 (beam_search:428) INFO: decoder input length: 31
2024-10-27 17:31:44,778 (beam_search:429) INFO: max output length: 31
2024-10-27 17:31:44,778 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:44,793 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:44,793 (beam_search:476) INFO:  -0.23 * 1.0 =  -0.23 for ctc
2024-10-27 17:31:44,794 (beam_search:479) INFO: total log probability: -0.23
2024-10-27 17:31:44,794 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:31:44,794 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:44,794 (beam_search:483) INFO: best hypo: ▁I▁THINK

2024-10-27 17:31:44,796 (asr_inference:509) INFO: speech length: 115167
2024-10-27 17:31:49,021 (beam_search:428) INFO: decoder input length: 89
2024-10-27 17:31:49,021 (beam_search:429) INFO: max output length: 89
2024-10-27 17:31:49,021 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:49,196 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:49,196 (beam_search:476) INFO:  -2.80 * 1.0 =  -2.80 for ctc
2024-10-27 17:31:49,196 (beam_search:479) INFO: total log probability: -2.80
2024-10-27 17:31:49,196 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:31:49,196 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:49,196 (beam_search:483) INFO: best hypo: ▁A▁PATHWAY▁FOR▁THE▁ENERGY▁TO▁GO▁SO▁SO▁YOU▁CAN▁THE

2024-10-27 17:31:49,198 (asr_inference:509) INFO: speech length: 77898
2024-10-27 17:31:52,037 (beam_search:428) INFO: decoder input length: 60
2024-10-27 17:31:52,037 (beam_search:429) INFO: max output length: 60
2024-10-27 17:31:52,037 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:52,066 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:52,066 (beam_search:476) INFO:  -0.82 * 1.0 =  -0.82 for ctc
2024-10-27 17:31:52,066 (beam_search:479) INFO: total log probability: -0.82
2024-10-27 17:31:52,066 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:31:52,066 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:52,066 (beam_search:483) INFO: best hypo: ▁ENERGY▁AND

2024-10-27 17:31:52,068 (asr_inference:509) INFO: speech length: 139616
2024-10-27 17:31:57,089 (beam_search:428) INFO: decoder input length: 108
2024-10-27 17:31:57,090 (beam_search:429) INFO: max output length: 108
2024-10-27 17:31:57,090 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:57,326 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:57,326 (beam_search:476) INFO:  -2.60 * 1.0 =  -2.60 for ctc
2024-10-27 17:31:57,326 (beam_search:479) INFO: total log probability: -2.60
2024-10-27 17:31:57,326 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:31:57,326 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:57,326 (beam_search:483) INFO: best hypo: ▁LIGHT▁BULBS▁ARE▁SO▁YOU▁CAN▁SEE▁THAT▁THE▁ENERGY▁AND▁YOU▁CAN

2024-10-27 17:31:57,328 (asr_inference:509) INFO: speech length: 25707
2024-10-27 17:31:58,463 (beam_search:428) INFO: decoder input length: 19
2024-10-27 17:31:58,463 (beam_search:429) INFO: max output length: 19
2024-10-27 17:31:58,463 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:58,478 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:58,479 (beam_search:476) INFO:  -0.58 * 1.0 =  -0.58 for ctc
2024-10-27 17:31:58,479 (beam_search:479) INFO: total log probability: -0.58
2024-10-27 17:31:58,479 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:31:58,479 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:58,479 (beam_search:483) INFO: best hypo: ▁YOU▁MORE▁ABOUT▁WHAT

2024-10-27 17:31:58,481 (asr_inference:509) INFO: speech length: 32263
2024-10-27 17:31:59,809 (beam_search:428) INFO: decoder input length: 24
2024-10-27 17:31:59,809 (beam_search:429) INFO: max output length: 24
2024-10-27 17:31:59,809 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:59,833 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:59,833 (beam_search:476) INFO:  -0.94 * 1.0 =  -0.94 for ctc
2024-10-27 17:31:59,833 (beam_search:479) INFO: total log probability: -0.94
2024-10-27 17:31:59,833 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:31:59,833 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:59,833 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁TO▁LIGHT▁UP

2024-10-27 17:31:59,835 (asr_inference:509) INFO: speech length: 30317
2024-10-27 17:32:01,123 (beam_search:428) INFO: decoder input length: 23
2024-10-27 17:32:01,123 (beam_search:429) INFO: max output length: 23
2024-10-27 17:32:01,123 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:01,142 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:01,142 (beam_search:476) INFO:  -0.44 * 1.0 =  -0.44 for ctc
2024-10-27 17:32:01,142 (beam_search:479) INFO: total log probability: -0.44
2024-10-27 17:32:01,142 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:32:01,142 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:01,142 (beam_search:483) INFO: best hypo: ▁I▁SEE▁A▁CIRCUIT

2024-10-27 17:32:01,144 (asr_inference:509) INFO: speech length: 245972
2024-10-27 17:32:11,417 (beam_search:428) INFO: decoder input length: 191
2024-10-27 17:32:11,417 (beam_search:429) INFO: max output length: 191
2024-10-27 17:32:11,417 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:12,199 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:12,199 (beam_search:476) INFO:  -5.38 * 1.0 =  -5.38 for ctc
2024-10-27 17:32:12,199 (beam_search:479) INFO: total log probability: -5.38
2024-10-27 17:32:12,199 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:32:12,199 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:12,199 (beam_search:483) INFO: best hypo: ▁IS▁THROUGH▁THE▁POSITIVE▁AND▁GOING▁OUT▁THE▁SIDE▁INTO▁THE▁BULB▁BUT▁IS▁OUT▁THE▁BULB▁TO▁THE▁POSITIVE▁IT▁A▁CIRCUIT

2024-10-27 17:32:12,202 (asr_inference:509) INFO: speech length: 244624
2024-10-27 17:32:22,481 (beam_search:428) INFO: decoder input length: 190
2024-10-27 17:32:22,481 (beam_search:429) INFO: max output length: 190
2024-10-27 17:32:22,481 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:23,570 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:23,570 (beam_search:476) INFO:  -6.65 * 1.0 =  -6.65 for ctc
2024-10-27 17:32:23,570 (beam_search:479) INFO: total log probability: -6.65
2024-10-27 17:32:23,570 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:32:23,570 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:23,570 (beam_search:483) INFO: best hypo: ▁IT▁BY▁THROUGH▁THE▁WIRES▁GOING▁TO▁THE▁THE▁BULB▁AND▁THERE▁IS▁ANOTHER▁ONE▁CONNECTED▁TO▁THE▁BOTTOM▁OF▁THE▁BULB▁OUT▁TO▁THE▁POSITIVE▁THE▁BULB▁HAVE▁A▁CIRCUIT▁IT▁UP

2024-10-27 17:32:23,573 (asr_inference:509) INFO: speech length: 321835
2024-10-27 17:32:37,175 (beam_search:428) INFO: decoder input length: 250
2024-10-27 17:32:37,175 (beam_search:429) INFO: max output length: 250
2024-10-27 17:32:37,175 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:37,510 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:37,510 (beam_search:476) INFO:  -1.58 * 1.0 =  -1.58 for ctc
2024-10-27 17:32:37,510 (beam_search:479) INFO: total log probability: -1.58
2024-10-27 17:32:37,510 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:32:37,510 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:37,510 (beam_search:483) INFO: best hypo: ▁I'M▁TALKING▁ON▁ME▁SOME

2024-10-27 17:32:37,513 (asr_inference:509) INFO: speech length: 50375
2024-10-27 17:32:39,364 (beam_search:428) INFO: decoder input length: 38
2024-10-27 17:32:39,364 (beam_search:429) INFO: max output length: 38
2024-10-27 17:32:39,364 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:39,392 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:39,392 (beam_search:476) INFO:  -1.00 * 1.0 =  -1.00 for ctc
2024-10-27 17:32:39,392 (beam_search:479) INFO: total log probability: -1.00
2024-10-27 17:32:39,392 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:32:39,392 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:39,392 (beam_search:483) INFO: best hypo: ▁I▁THAT▁IT▁LIKE

2024-10-27 17:32:39,394 (asr_inference:509) INFO: speech length: 61737
2024-10-27 17:32:41,671 (beam_search:428) INFO: decoder input length: 47
2024-10-27 17:32:41,671 (beam_search:429) INFO: max output length: 47
2024-10-27 17:32:41,671 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:41,688 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:41,688 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 17:32:41,688 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 17:32:41,688 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:32:41,688 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:41,688 (beam_search:483) INFO: best hypo: ▁THE

2024-10-27 17:32:41,691 (asr_inference:509) INFO: speech length: 65092
2024-10-27 17:32:44,136 (beam_search:428) INFO: decoder input length: 50
2024-10-27 17:32:44,136 (beam_search:429) INFO: max output length: 50
2024-10-27 17:32:44,136 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:44,219 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:44,219 (beam_search:476) INFO:  -1.39 * 1.0 =  -1.39 for ctc
2024-10-27 17:32:44,219 (beam_search:479) INFO: total log probability: -1.39
2024-10-27 17:32:44,219 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:32:44,219 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:44,219 (beam_search:483) INFO: best hypo: ▁I▁THAT▁IT'S▁THE▁LIGHT▁BULB▁UP

2024-10-27 17:32:44,221 (asr_inference:509) INFO: speech length: 125325
2024-10-27 17:32:49,066 (beam_search:428) INFO: decoder input length: 97
2024-10-27 17:32:49,066 (beam_search:429) INFO: max output length: 97
2024-10-27 17:32:49,066 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:49,303 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:49,303 (beam_search:476) INFO:  -3.76 * 1.0 =  -3.76 for ctc
2024-10-27 17:32:49,303 (beam_search:479) INFO: total log probability: -3.76
2024-10-27 17:32:49,303 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:32:49,303 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:49,304 (beam_search:483) INFO: best hypo: ▁BE▁CONNECTED▁TO▁MAKE▁THE▁ENERGY▁THROUGH▁THE▁WIRES▁SO▁THE▁SO▁THE▁LIGHT▁CAN

2024-10-27 17:32:49,305 (asr_inference:509) INFO: speech length: 230094
2024-10-27 17:32:58,367 (beam_search:428) INFO: decoder input length: 179
2024-10-27 17:32:58,367 (beam_search:429) INFO: max output length: 179
2024-10-27 17:32:58,367 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:59,603 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:59,603 (beam_search:476) INFO:  -5.67 * 1.0 =  -5.67 for ctc
2024-10-27 17:32:59,603 (beam_search:479) INFO: total log probability: -5.67
2024-10-27 17:32:59,603 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:32:59,603 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:59,603 (beam_search:483) INFO: best hypo: ▁THAT▁IT▁WILL▁NOT▁LIGHT▁UP▁BECAUSE▁THERE▁NEEDS▁ONE▁TO▁BE▁AT▁THE▁OF▁THE▁BULB▁FOR▁THE▁ENERGY▁TO▁OUT▁IT'S▁AT▁THE▁BOTTOM▁THEN▁THERE▁IS▁ONLY▁JUST▁ONE▁BULB▁TO▁IT▁OR

2024-10-27 17:32:59,606 (asr_inference:509) INFO: speech length: 35675
2024-10-27 17:33:00,994 (beam_search:428) INFO: decoder input length: 27
2024-10-27 17:33:00,994 (beam_search:429) INFO: max output length: 27
2024-10-27 17:33:00,994 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:01,032 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:01,032 (beam_search:476) INFO:  -1.38 * 1.0 =  -1.38 for ctc
2024-10-27 17:33:01,032 (beam_search:479) INFO: total log probability: -1.38
2024-10-27 17:33:01,032 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:33:01,033 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:01,033 (beam_search:483) INFO: best hypo: ▁I▁NOTICED▁THAT▁IT'S▁LIGHTING▁UP▁OF

2024-10-27 17:33:01,035 (asr_inference:509) INFO: speech length: 124270
2024-10-27 17:33:05,522 (beam_search:428) INFO: decoder input length: 96
2024-10-27 17:33:05,522 (beam_search:429) INFO: max output length: 96
2024-10-27 17:33:05,522 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:05,751 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:05,751 (beam_search:476) INFO:  -2.05 * 1.0 =  -2.05 for ctc
2024-10-27 17:33:05,751 (beam_search:479) INFO: total log probability: -2.05
2024-10-27 17:33:05,751 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:33:05,751 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:05,751 (beam_search:483) INFO: best hypo: ▁WE'VE▁BEEN▁AND▁HOW▁THEY▁AND▁TO▁MAKE▁HOW▁TO▁MAKE▁A

2024-10-27 17:33:05,753 (asr_inference:509) INFO: speech length: 174326
2024-10-27 17:33:12,459 (beam_search:428) INFO: decoder input length: 135
2024-10-27 17:33:12,459 (beam_search:429) INFO: max output length: 135
2024-10-27 17:33:12,460 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:12,905 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:12,905 (beam_search:476) INFO:  -3.92 * 1.0 =  -3.92 for ctc
2024-10-27 17:33:12,905 (beam_search:479) INFO: total log probability: -3.92
2024-10-27 17:33:12,905 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:33:12,905 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:12,906 (beam_search:483) INFO: best hypo: ▁WIRES▁CONNECTING▁TO▁THE▁D▁GOING▁TO▁THE▁MOTOR▁THE▁MOTOR▁RUN▁AND▁IT▁IS▁GOING▁TO▁THROUGH▁A

2024-10-27 17:33:12,908 (asr_inference:509) INFO: speech length: 182507
2024-10-27 17:33:19,808 (beam_search:428) INFO: decoder input length: 142
2024-10-27 17:33:19,808 (beam_search:429) INFO: max output length: 142
2024-10-27 17:33:19,808 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:20,318 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:20,318 (beam_search:476) INFO:  -3.20 * 1.0 =  -3.20 for ctc
2024-10-27 17:33:20,318 (beam_search:479) INFO: total log probability: -3.20
2024-10-27 17:33:20,318 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:33:20,318 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:20,318 (beam_search:483) INFO: best hypo: ▁THE▁IS▁AND▁IT▁IS▁IMPORTANT▁AND▁IT▁IS▁IMPORTANT▁BECAUSE▁THEN▁YOU▁KNOW▁THE▁ENERGY▁IS▁ACTUALLY▁GOING▁THROUGH▁OR

2024-10-27 17:33:20,321 (asr_inference:509) INFO: speech length: 87561
2024-10-27 17:33:23,439 (beam_search:428) INFO: decoder input length: 67
2024-10-27 17:33:23,439 (beam_search:429) INFO: max output length: 67
2024-10-27 17:33:23,439 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:23,551 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:23,551 (beam_search:476) INFO:  -2.86 * 1.0 =  -2.86 for ctc
2024-10-27 17:33:23,551 (beam_search:479) INFO: total log probability: -2.86
2024-10-27 17:33:23,551 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:33:23,551 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:23,551 (beam_search:483) INFO: best hypo: ▁THE▁IS▁GOING▁THROUGH▁A▁CIRCUIT▁GOING▁TO▁THE▁IT

2024-10-27 17:33:23,553 (asr_inference:509) INFO: speech length: 248676
2024-10-27 17:33:34,047 (beam_search:428) INFO: decoder input length: 193
2024-10-27 17:33:34,047 (beam_search:429) INFO: max output length: 193
2024-10-27 17:33:34,047 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:34,687 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:34,688 (beam_search:476) INFO:  -4.84 * 1.0 =  -4.84 for ctc
2024-10-27 17:33:34,688 (beam_search:479) INFO: total log probability: -4.84
2024-10-27 17:33:34,688 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:33:34,688 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:34,688 (beam_search:483) INFO: best hypo: ▁WHAT▁IS▁ENERGY▁GO▁THROUGH▁THE▁WIRES▁GOING▁THROUGH▁A▁CIRCUIT▁GOING▁TO▁THE▁MOTOR▁IT▁THE▁GO▁TO

2024-10-27 17:33:34,690 (asr_inference:509) INFO: speech length: 163727
2024-10-27 17:33:40,809 (beam_search:428) INFO: decoder input length: 127
2024-10-27 17:33:40,810 (beam_search:429) INFO: max output length: 127
2024-10-27 17:33:40,810 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:41,150 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:41,150 (beam_search:476) INFO:  -3.57 * 1.0 =  -3.57 for ctc
2024-10-27 17:33:41,151 (beam_search:479) INFO: total log probability: -3.57
2024-10-27 17:33:41,151 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:33:41,151 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:41,151 (beam_search:483) INFO: best hypo: ▁THIS▁IS▁ABOUT▁THE▁SO▁IT▁YOU▁CAN▁TURN▁IT▁OFF▁AND▁ON▁MOVING▁THE

2024-10-27 17:33:41,153 (asr_inference:509) INFO: speech length: 16962
2024-10-27 17:33:41,919 (beam_search:428) INFO: decoder input length: 12
2024-10-27 17:33:41,919 (beam_search:429) INFO: max output length: 12
2024-10-27 17:33:41,919 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:41,928 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:41,928 (beam_search:476) INFO:  -0.91 * 1.0 =  -0.91 for ctc
2024-10-27 17:33:41,928 (beam_search:479) INFO: total log probability: -0.91
2024-10-27 17:33:41,928 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:33:41,928 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:41,928 (beam_search:483) INFO: best hypo: ▁THE▁IS

2024-10-27 17:33:41,931 (asr_inference:509) INFO: speech length: 333295
2024-10-27 17:33:57,134 (beam_search:428) INFO: decoder input length: 259
2024-10-27 17:33:57,134 (beam_search:429) INFO: max output length: 259
2024-10-27 17:33:57,134 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:59,292 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:59,292 (beam_search:476) INFO:  -7.45 * 1.0 =  -7.45 for ctc
2024-10-27 17:33:59,292 (beam_search:479) INFO: total log probability: -7.45
2024-10-27 17:33:59,292 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:33:59,292 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:59,292 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁WASN'T▁TO▁THE▁OTHER▁WIRE▁IT▁GOING▁IT▁NOT▁GOING▁TO▁A▁IT▁NOT▁MOVE▁OR▁DO▁ANYTHING▁LIKE▁THAT▁BUT▁NOW▁THAT▁THE▁WIRE▁THAT▁THE▁SWITCH▁HAS▁BEEN▁CONNECTED▁IT▁IS▁GOING▁AGAIN▁BECAUSE▁IT▁IS▁A▁CIRCUIT▁NOW

2024-10-27 17:33:59,295 (asr_inference:509) INFO: speech length: 69826
2024-10-27 17:34:01,766 (beam_search:428) INFO: decoder input length: 54
2024-10-27 17:34:01,766 (beam_search:429) INFO: max output length: 54
2024-10-27 17:34:01,766 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:01,819 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:01,819 (beam_search:476) INFO:  -0.66 * 1.0 =  -0.66 for ctc
2024-10-27 17:34:01,819 (beam_search:479) INFO: total log probability: -0.66
2024-10-27 17:34:01,819 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:34:01,819 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:01,819 (beam_search:483) INFO: best hypo: ▁CAUSE▁IT▁IS▁NOT▁A▁CIRCUIT

2024-10-27 17:34:01,822 (asr_inference:509) INFO: speech length: 420308
2024-10-27 17:34:21,780 (beam_search:428) INFO: decoder input length: 327
2024-10-27 17:34:21,780 (beam_search:429) INFO: max output length: 327
2024-10-27 17:34:21,780 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:25,475 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:25,475 (beam_search:476) INFO: -11.51 * 1.0 = -11.51 for ctc
2024-10-27 17:34:25,475 (beam_search:479) INFO: total log probability: -11.51
2024-10-27 17:34:25,475 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:34:25,475 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:25,475 (beam_search:483) INFO: best hypo: ▁A▁CIRCUIT▁SO▁YOU▁HAVE▁TO▁THE▁SWITCH▁BACK▁BECAUSE▁THE▁ENERGY▁HAS▁CONNECTED▁TO▁THAT▁THING▁THAT▁THE▁WIRE▁CONNECTED▁TO▁WITH▁THE▁AND▁THE▁SWITCH▁HAS▁TO▁CONNECT▁TO▁THE▁OTHER▁OR▁METAL▁SO▁THE▁ENERGY▁IS▁CONNECTED▁TO▁THE▁METAL▁SO▁IT▁WILL▁GO▁THROUGH▁TO▁MAKE▁A▁CIRCUIT▁GOING▁UP▁THE▁WHAT▁YOU▁OR

2024-10-27 17:34:25,478 (asr_inference:509) INFO: speech length: 233892
2024-10-27 17:34:35,068 (beam_search:428) INFO: decoder input length: 182
2024-10-27 17:34:35,068 (beam_search:429) INFO: max output length: 182
2024-10-27 17:34:35,068 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:35,756 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:35,756 (beam_search:476) INFO:  -4.91 * 1.0 =  -4.91 for ctc
2024-10-27 17:34:35,756 (beam_search:479) INFO: total log probability: -4.91
2024-10-27 17:34:35,756 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:34:35,756 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:35,756 (beam_search:483) INFO: best hypo: ▁I▁THAT▁WHEN▁THE▁TO▁THAT▁OTHER▁METAL▁CONNECTED▁TO▁THE▁WIRE▁IT▁BUT▁IS▁NOT▁AND▁IT▁IS▁IT▁NOT▁GO

2024-10-27 17:34:35,759 (asr_inference:509) INFO: speech length: 210514
2024-10-27 17:34:44,361 (beam_search:428) INFO: decoder input length: 163
2024-10-27 17:34:44,361 (beam_search:429) INFO: max output length: 163
2024-10-27 17:34:44,361 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:45,079 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:45,080 (beam_search:476) INFO:  -3.92 * 1.0 =  -3.92 for ctc
2024-10-27 17:34:45,080 (beam_search:479) INFO: total log probability: -3.92
2024-10-27 17:34:45,080 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:34:45,080 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:45,080 (beam_search:483) INFO: best hypo: ▁WHEN▁THE▁SWITCH▁IS▁OPEN▁IT▁NOT▁IT▁NOT▁THE▁ENERGY▁DOES▁NOT▁FLOW▁THROUGH▁TO▁THE▁MOTOR▁IT▁NOT▁CONNECT▁AND▁NOT▁MAKING▁IT▁GO

2024-10-27 17:34:45,082 (asr_inference:509) INFO: speech length: 268425
2024-10-27 17:34:56,176 (beam_search:428) INFO: decoder input length: 209
2024-10-27 17:34:56,176 (beam_search:429) INFO: max output length: 209
2024-10-27 17:34:56,176 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:57,714 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:57,714 (beam_search:476) INFO:  -9.49 * 1.0 =  -9.49 for ctc
2024-10-27 17:34:57,714 (beam_search:479) INFO: total log probability: -9.49
2024-10-27 17:34:57,714 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:34:57,714 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:57,714 (beam_search:483) INFO: best hypo: ▁ABOUT▁IF▁YOU▁DON'T▁A▁YOU▁CAN▁GO▁OUT▁IN▁THE▁AND▁THEN▁IT▁WILL▁AND▁THEN▁IT▁WILL▁GET▁ENERGY▁FROM▁THE▁SUN▁IF▁IT▁IS▁CONNECTED▁TO▁THE▁THE▁ENERGY▁WILL▁THROUGH▁THE▁WIRE▁TO▁YOU▁TO▁IT

2024-10-27 17:34:57,717 (asr_inference:509) INFO: speech length: 238996
2024-10-27 17:35:07,391 (beam_search:428) INFO: decoder input length: 186
2024-10-27 17:35:07,391 (beam_search:429) INFO: max output length: 186
2024-10-27 17:35:07,391 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:08,493 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:08,493 (beam_search:476) INFO:  -5.87 * 1.0 =  -5.87 for ctc
2024-10-27 17:35:08,493 (beam_search:479) INFO: total log probability: -5.87
2024-10-27 17:35:08,493 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:35:08,493 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:08,493 (beam_search:483) INFO: best hypo: ▁IS▁IS▁THE▁SUN▁MAKING▁THES▁ENERGY▁CONNECT▁THROUGH▁CONNECT▁THROUGH▁TO▁THE▁TO▁THELAR▁TO▁THE▁SOLAR▁CELL▁THE▁ENERGY▁GO▁THROUGH▁THE▁WIRES▁TO▁THE▁IT▁GO

2024-10-27 17:35:08,496 (asr_inference:509) INFO: speech length: 64137
2024-10-27 17:35:10,813 (beam_search:428) INFO: decoder input length: 49
2024-10-27 17:35:10,813 (beam_search:429) INFO: max output length: 49
2024-10-27 17:35:10,813 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:10,900 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:10,900 (beam_search:476) INFO:  -1.06 * 1.0 =  -1.06 for ctc
2024-10-27 17:35:10,900 (beam_search:479) INFO: total log probability: -1.06
2024-10-27 17:35:10,900 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:35:10,900 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:10,900 (beam_search:483) INFO: best hypo: ▁THE▁SOLAR▁CELL▁HAS▁TO▁BE▁CONNECTED▁TO▁WIRES▁THE

2024-10-27 17:35:10,902 (asr_inference:509) INFO: speech length: 130569
2024-10-27 17:35:15,656 (beam_search:428) INFO: decoder input length: 101
2024-10-27 17:35:15,656 (beam_search:429) INFO: max output length: 101
2024-10-27 17:35:15,656 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:15,988 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:15,988 (beam_search:476) INFO:  -2.38 * 1.0 =  -2.38 for ctc
2024-10-27 17:35:15,988 (beam_search:479) INFO: total log probability: -2.38
2024-10-27 17:35:15,988 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:35:15,988 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:15,988 (beam_search:483) INFO: best hypo: ▁THE▁SOLAR▁CELL▁IS▁WITH▁THE▁SUNS▁ENERGYING▁IT▁GO▁THROUGH▁THE▁WIRES▁TO▁THE▁MOTOR

2024-10-27 17:35:15,990 (asr_inference:509) INFO: speech length: 74839
2024-10-27 17:35:18,697 (beam_search:428) INFO: decoder input length: 57
2024-10-27 17:35:18,698 (beam_search:429) INFO: max output length: 57
2024-10-27 17:35:18,698 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:18,840 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:18,841 (beam_search:476) INFO:  -1.97 * 1.0 =  -1.97 for ctc
2024-10-27 17:35:18,841 (beam_search:479) INFO: total log probability: -1.97
2024-10-27 17:35:18,841 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:35:18,841 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:18,841 (beam_search:483) INFO: best hypo: ▁THE▁IMPORTANT▁ABOUT▁THAT▁IS▁THAT▁THE▁ENERGY▁IS▁GOING▁THROUGH▁THE▁WIRES▁TO▁THE

2024-10-27 17:35:18,843 (asr_inference:509) INFO: speech length: 74632
2024-10-27 17:35:21,507 (beam_search:428) INFO: decoder input length: 57
2024-10-27 17:35:21,507 (beam_search:429) INFO: max output length: 57
2024-10-27 17:35:21,507 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:21,619 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:21,619 (beam_search:476) INFO:  -2.42 * 1.0 =  -2.42 for ctc
2024-10-27 17:35:21,619 (beam_search:479) INFO: total log probability: -2.42
2024-10-27 17:35:21,619 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:35:21,619 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:21,619 (beam_search:483) INFO: best hypo: ▁THE▁SOLAR▁CELL▁WOULD▁NOT▁ENERGY▁FROM▁THE▁AND▁IT▁WOULD

2024-10-27 17:35:21,622 (asr_inference:509) INFO: speech length: 141077
2024-10-27 17:35:27,215 (beam_search:428) INFO: decoder input length: 109
2024-10-27 17:35:27,215 (beam_search:429) INFO: max output length: 109
2024-10-27 17:35:27,215 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:27,574 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:27,574 (beam_search:476) INFO:  -2.27 * 1.0 =  -2.27 for ctc
2024-10-27 17:35:27,574 (beam_search:479) INFO: total log probability: -2.27
2024-10-27 17:35:27,574 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:35:27,574 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:27,574 (beam_search:483) INFO: best hypo: ▁IF▁THE▁SUN▁IS▁OUT▁THEN▁THERE▁IS▁ENERGY▁FLOWING▁FROM▁THE▁TO▁THE▁SOLAR▁CELL▁IT▁LIGHT▁UP

2024-10-27 17:35:27,576 (asr_inference:509) INFO: speech length: 20742
2024-10-27 17:35:28,482 (beam_search:428) INFO: decoder input length: 15
2024-10-27 17:35:28,482 (beam_search:429) INFO: max output length: 15
2024-10-27 17:35:28,482 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:28,491 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:28,491 (beam_search:476) INFO:  -0.50 * 1.0 =  -0.50 for ctc
2024-10-27 17:35:28,491 (beam_search:479) INFO: total log probability: -0.50
2024-10-27 17:35:28,491 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:35:28,491 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:28,491 (beam_search:483) INFO: best hypo: ▁IT▁IS

2024-10-27 17:35:28,493 (asr_inference:509) INFO: speech length: 191161
2024-10-27 17:35:35,746 (beam_search:428) INFO: decoder input length: 148
2024-10-27 17:35:35,746 (beam_search:429) INFO: max output length: 148
2024-10-27 17:35:35,746 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:36,177 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:36,177 (beam_search:476) INFO:  -5.22 * 1.0 =  -5.22 for ctc
2024-10-27 17:35:36,177 (beam_search:479) INFO: total log probability: -5.22
2024-10-27 17:35:36,177 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:35:36,177 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:36,177 (beam_search:483) INFO: best hypo: ▁THAT▁THE▁ENERGY▁GO▁THROUGH▁THE▁WIRES▁MAKING▁IT▁GO▁TO▁THE▁MOTOR▁IT▁HAVE▁ENERGY▁IT▁GO

2024-10-27 17:35:36,179 (asr_inference:509) INFO: speech length: 40751
2024-10-27 17:35:37,758 (beam_search:428) INFO: decoder input length: 31
2024-10-27 17:35:37,759 (beam_search:429) INFO: max output length: 31
2024-10-27 17:35:37,759 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:37,792 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:37,793 (beam_search:476) INFO:  -2.19 * 1.0 =  -2.19 for ctc
2024-10-27 17:35:37,793 (beam_search:479) INFO: total log probability: -2.19
2024-10-27 17:35:37,793 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 17:35:37,793 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:37,793 (beam_search:483) INFO: best hypo: ▁I▁THAT▁IT▁IS▁NOT▁GOING

2024-10-27 17:35:37,795 (asr_inference:509) INFO: speech length: 58003
2024-10-27 17:35:40,139 (beam_search:428) INFO: decoder input length: 44
2024-10-27 17:35:40,139 (beam_search:429) INFO: max output length: 44
2024-10-27 17:35:40,139 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:40,198 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:40,198 (beam_search:476) INFO:  -1.31 * 1.0 =  -1.31 for ctc
2024-10-27 17:35:40,198 (beam_search:479) INFO: total log probability: -1.31
2024-10-27 17:35:40,198 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:35:40,198 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:40,198 (beam_search:483) INFO: best hypo: ▁WHAT▁IS'T▁IN▁THE▁IS▁ENERGY

2024-10-27 17:35:40,201 (asr_inference:509) INFO: speech length: 258863
2024-10-27 17:35:50,812 (beam_search:428) INFO: decoder input length: 201
2024-10-27 17:35:50,812 (beam_search:429) INFO: max output length: 201
2024-10-27 17:35:50,812 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:51,694 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:51,694 (beam_search:476) INFO:  -5.94 * 1.0 =  -5.94 for ctc
2024-10-27 17:35:51,694 (beam_search:479) INFO: total log probability: -5.94
2024-10-27 17:35:51,694 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:35:51,694 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:51,694 (beam_search:483) INFO: best hypo: ▁THAT▁THAT▁THE▁ENERGY▁IS▁GOING▁THROUGH▁THE▁WIRES▁A▁CIRCUIT▁THE▁THE▁ENERGY▁TO▁THE▁WIRE▁IT▁GOING▁ANDS▁YOU▁A▁WIRE

2024-10-27 17:35:51,697 (asr_inference:509) INFO: speech length: 29000
2024-10-27 17:35:53,012 (beam_search:428) INFO: decoder input length: 22
2024-10-27 17:35:53,012 (beam_search:429) INFO: max output length: 22
2024-10-27 17:35:53,012 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:53,027 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:53,027 (beam_search:476) INFO:  -0.61 * 1.0 =  -0.61 for ctc
2024-10-27 17:35:53,027 (beam_search:479) INFO: total log probability: -0.61
2024-10-27 17:35:53,027 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:35:53,027 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:53,027 (beam_search:483) INFO: best hypo: ▁HOW▁ARE▁YOU

2024-10-27 17:35:53,030 (asr_inference:509) INFO: speech length: 209556
2024-10-27 17:36:01,242 (beam_search:428) INFO: decoder input length: 163
2024-10-27 17:36:01,242 (beam_search:429) INFO: max output length: 163
2024-10-27 17:36:01,242 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:01,575 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:01,575 (beam_search:476) INFO:  -2.98 * 1.0 =  -2.98 for ctc
2024-10-27 17:36:01,575 (beam_search:479) INFO: total log probability: -2.98
2024-10-27 17:36:01,575 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:36:01,575 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:01,575 (beam_search:483) INFO: best hypo: ▁BEEN▁ABOUT▁OTHER▁METAL▁THAT▁HAVE▁THAT▁CAN▁ENERGY▁TO▁ENERGY

2024-10-27 17:36:01,578 (asr_inference:509) INFO: speech length: 460409
2024-10-27 17:36:24,067 (beam_search:428) INFO: decoder input length: 359
2024-10-27 17:36:24,068 (beam_search:429) INFO: max output length: 359
2024-10-27 17:36:24,068 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:28,088 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:28,088 (beam_search:476) INFO:  -6.18 * 1.0 =  -6.18 for ctc
2024-10-27 17:36:28,088 (beam_search:479) INFO: total log probability: -6.18
2024-10-27 17:36:28,088 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:36:28,088 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:28,088 (beam_search:483) INFO: best hypo: ▁THE▁D▁CELL▁BATTERY▁HAS▁A▁WIRE▁THROUGH▁THE▁SIDE▁GOING▁TO▁THE▁THE▁IS▁OPEN▁BUT▁THE▁NAIL▁IS▁AND▁ITS▁METAL▁SO▁THE▁ENERGY▁CAN▁FLOW▁THROUGH▁GOING▁TO▁THE▁OTHER▁GOING▁TO▁THE▁OTHER▁WIRE▁TO▁THE▁MOTOR▁IT▁GO▁AND▁THEN▁THE▁SIDE▁IS▁JUST▁THE▁WIRE▁GOING▁TO▁THE▁MOTOR

2024-10-27 17:36:28,091 (asr_inference:509) INFO: speech length: 44009
2024-10-27 17:36:29,708 (beam_search:428) INFO: decoder input length: 33
2024-10-27 17:36:29,708 (beam_search:429) INFO: max output length: 33
2024-10-27 17:36:29,708 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:29,721 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:29,721 (beam_search:476) INFO:  -1.59 * 1.0 =  -1.59 for ctc
2024-10-27 17:36:29,721 (beam_search:479) INFO: total log probability: -1.59
2024-10-27 17:36:29,721 (beam_search:480) INFO: normalized log probability: -0.53
2024-10-27 17:36:29,721 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:29,721 (beam_search:483) INFO: best hypo: ▁OR

2024-10-27 17:36:29,724 (asr_inference:509) INFO: speech length: 241360
2024-10-27 17:36:39,273 (beam_search:428) INFO: decoder input length: 188
2024-10-27 17:36:39,273 (beam_search:429) INFO: max output length: 188
2024-10-27 17:36:39,273 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:40,375 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:40,375 (beam_search:476) INFO:  -5.38 * 1.0 =  -5.38 for ctc
2024-10-27 17:36:40,375 (beam_search:479) INFO: total log probability: -5.38
2024-10-27 17:36:40,375 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:36:40,375 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:40,376 (beam_search:483) INFO: best hypo: ▁THAT▁THE▁PARTS▁OF▁THE▁CIRCUIT▁ARE▁THE▁THAT▁IS▁THAT▁IS▁ENERGY▁IS▁FLOWING▁BACK▁THROUGH▁THE▁D▁LIKE▁IT▁IS▁UM▁THE▁ENERGY▁MAKING▁IT▁THROUGH▁THE▁D▁CELL

2024-10-27 17:36:40,378 (asr_inference:509) INFO: speech length: 55213
2024-10-27 17:36:42,402 (beam_search:428) INFO: decoder input length: 42
2024-10-27 17:36:42,402 (beam_search:429) INFO: max output length: 42
2024-10-27 17:36:42,402 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:42,452 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:42,452 (beam_search:476) INFO:  -1.30 * 1.0 =  -1.30 for ctc
2024-10-27 17:36:42,452 (beam_search:479) INFO: total log probability: -1.30
2024-10-27 17:36:42,452 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:36:42,452 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:42,452 (beam_search:483) INFO: best hypo: ▁THE▁AND▁THE▁AND▁ALL▁THAT▁STUFF

2024-10-27 17:36:42,454 (asr_inference:509) INFO: speech length: 186686
2024-10-27 17:36:49,540 (beam_search:428) INFO: decoder input length: 145
2024-10-27 17:36:49,540 (beam_search:429) INFO: max output length: 145
2024-10-27 17:36:49,540 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:49,959 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:49,959 (beam_search:476) INFO:  -2.90 * 1.0 =  -2.90 for ctc
2024-10-27 17:36:49,959 (beam_search:479) INFO: total log probability: -2.90
2024-10-27 17:36:49,959 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:36:49,959 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:49,959 (beam_search:483) INFO: best hypo: ▁I▁THAT▁THE▁IS▁AND▁THAT▁METAL▁THE▁ENERGY▁IS▁FLOWING▁THROUGH▁AND▁GOING▁TO▁THE

2024-10-27 17:36:49,961 (asr_inference:509) INFO: speech length: 69977
2024-10-27 17:36:52,387 (beam_search:428) INFO: decoder input length: 54
2024-10-27 17:36:52,387 (beam_search:429) INFO: max output length: 54
2024-10-27 17:36:52,387 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:52,470 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:52,471 (beam_search:476) INFO:  -0.98 * 1.0 =  -0.98 for ctc
2024-10-27 17:36:52,471 (beam_search:479) INFO: total log probability: -0.98
2024-10-27 17:36:52,471 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:36:52,471 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:52,471 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁IS▁GOING▁THROUGH▁THE▁BECAUSE▁THE▁IS

2024-10-27 17:36:52,473 (asr_inference:509) INFO: speech length: 188346
2024-10-27 17:36:59,763 (beam_search:428) INFO: decoder input length: 146
2024-10-27 17:36:59,763 (beam_search:429) INFO: max output length: 146
2024-10-27 17:36:59,763 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:00,299 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:00,300 (beam_search:476) INFO:  -3.71 * 1.0 =  -3.71 for ctc
2024-10-27 17:37:00,300 (beam_search:479) INFO: total log probability: -3.71
2024-10-27 17:37:00,300 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:37:00,300 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:00,300 (beam_search:483) INFO: best hypo: ▁THAT▁THE▁STICK▁IS▁NOT▁METAL▁AND▁IT▁SO▁THE▁ENERGY▁CANT▁THROUGH▁THAT▁SO▁THE▁IS▁NOT▁RUNNING

2024-10-27 17:37:00,303 (asr_inference:509) INFO: speech length: 141012
2024-10-27 17:37:05,353 (beam_search:428) INFO: decoder input length: 109
2024-10-27 17:37:05,354 (beam_search:429) INFO: max output length: 109
2024-10-27 17:37:05,354 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:05,535 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:05,536 (beam_search:476) INFO:  -2.50 * 1.0 =  -2.50 for ctc
2024-10-27 17:37:05,536 (beam_search:479) INFO: total log probability: -2.50
2024-10-27 17:37:05,536 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:37:05,536 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:05,536 (beam_search:483) INFO: best hypo: ▁IS▁NOT▁BECAUSE▁THE▁THES▁THE▁IS▁NOT

2024-10-27 17:37:05,538 (asr_inference:509) INFO: speech length: 113339
2024-10-27 17:37:09,630 (beam_search:428) INFO: decoder input length: 88
2024-10-27 17:37:09,630 (beam_search:429) INFO: max output length: 88
2024-10-27 17:37:09,630 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:09,843 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:09,844 (beam_search:476) INFO:  -4.37 * 1.0 =  -4.37 for ctc
2024-10-27 17:37:09,844 (beam_search:479) INFO: total log probability: -4.37
2024-10-27 17:37:09,844 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:37:09,844 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:09,844 (beam_search:483) INFO: best hypo: ▁IT▁HAS▁TO▁DO▁WITH▁BECAUSE▁IT▁IS▁NOT▁METAL▁NOT▁THROUGH▁IT▁WE▁JUST

2024-10-27 17:37:09,846 (asr_inference:509) INFO: speech length: 84810
2024-10-27 17:37:12,908 (beam_search:428) INFO: decoder input length: 65
2024-10-27 17:37:12,908 (beam_search:429) INFO: max output length: 65
2024-10-27 17:37:12,908 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:13,010 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:13,010 (beam_search:476) INFO:  -2.07 * 1.0 =  -2.07 for ctc
2024-10-27 17:37:13,010 (beam_search:479) INFO: total log probability: -2.07
2024-10-27 17:37:13,010 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:37:13,010 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:13,010 (beam_search:483) INFO: best hypo: ▁BECAUSE▁IF▁IT▁IS▁METAL▁THEN▁IT▁NEEDS▁TO

2024-10-27 17:37:13,012 (asr_inference:509) INFO: speech length: 67458
2024-10-27 17:37:15,392 (beam_search:428) INFO: decoder input length: 52
2024-10-27 17:37:15,392 (beam_search:429) INFO: max output length: 52
2024-10-27 17:37:15,392 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:15,457 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:15,458 (beam_search:476) INFO:  -1.39 * 1.0 =  -1.39 for ctc
2024-10-27 17:37:15,458 (beam_search:479) INFO: total log probability: -1.39
2024-10-27 17:37:15,458 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:37:15,458 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:15,458 (beam_search:483) INFO: best hypo: ▁DIFFERENT▁BECAUSE▁THEY▁ARET▁MADE▁OF

2024-10-27 17:37:15,460 (asr_inference:509) INFO: speech length: 76733
2024-10-27 17:37:18,336 (beam_search:428) INFO: decoder input length: 59
2024-10-27 17:37:18,336 (beam_search:429) INFO: max output length: 59
2024-10-27 17:37:18,336 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:18,483 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:18,484 (beam_search:476) INFO:  -1.99 * 1.0 =  -1.99 for ctc
2024-10-27 17:37:18,484 (beam_search:479) INFO: total log probability: -1.99
2024-10-27 17:37:18,484 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:37:18,484 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:18,484 (beam_search:483) INFO: best hypo: ▁THEY▁ARE▁BECAUSE▁THEY'RE▁NOT▁MADE▁OUT▁OF▁METAL▁AND▁ENERGY▁CAN▁NOT▁THROUGH

2024-10-27 17:37:18,486 (asr_inference:509) INFO: speech length: 72739
2024-10-27 17:37:21,079 (beam_search:428) INFO: decoder input length: 56
2024-10-27 17:37:21,079 (beam_search:429) INFO: max output length: 56
2024-10-27 17:37:21,079 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:21,154 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:21,154 (beam_search:476) INFO:  -2.48 * 1.0 =  -2.48 for ctc
2024-10-27 17:37:21,154 (beam_search:479) INFO: total log probability: -2.48
2024-10-27 17:37:21,154 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:37:21,154 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:21,155 (beam_search:483) INFO: best hypo: ▁BECAUSE▁THEY'RE▁NOT▁OUT▁OF▁METAL

2024-10-27 17:37:21,157 (asr_inference:509) INFO: speech length: 99645
2024-10-27 17:37:24,785 (beam_search:428) INFO: decoder input length: 77
2024-10-27 17:37:24,785 (beam_search:429) INFO: max output length: 77
2024-10-27 17:37:24,785 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:24,973 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:24,973 (beam_search:476) INFO:  -2.17 * 1.0 =  -2.17 for ctc
2024-10-27 17:37:24,973 (beam_search:479) INFO: total log probability: -2.17
2024-10-27 17:37:24,973 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:37:24,973 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:24,973 (beam_search:483) INFO: best hypo: ▁THEY▁ARE▁BECAUSE▁THEY'RE▁MADE▁OUT▁OF▁METAL▁AND▁ENERGY▁CAN▁METAL

2024-10-27 17:37:24,976 (asr_inference:509) INFO: speech length: 30907
2024-10-27 17:37:26,279 (beam_search:428) INFO: decoder input length: 23
2024-10-27 17:37:26,279 (beam_search:429) INFO: max output length: 23
2024-10-27 17:37:26,279 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:26,295 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:26,295 (beam_search:476) INFO:  -0.87 * 1.0 =  -0.87 for ctc
2024-10-27 17:37:26,295 (beam_search:479) INFO: total log probability: -0.87
2024-10-27 17:37:26,295 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:37:26,296 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:26,296 (beam_search:483) INFO: best hypo: ▁THEY▁ARE▁ALL

2024-10-27 17:37:26,298 (asr_inference:509) INFO: speech length: 124698
2024-10-27 17:37:30,850 (beam_search:428) INFO: decoder input length: 96
2024-10-27 17:37:30,850 (beam_search:429) INFO: max output length: 96
2024-10-27 17:37:30,850 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:30,986 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:30,986 (beam_search:476) INFO:  -0.49 * 1.0 =  -0.49 for ctc
2024-10-27 17:37:30,986 (beam_search:479) INFO: total log probability: -0.49
2024-10-27 17:37:30,986 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:37:30,986 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:30,986 (beam_search:483) INFO: best hypo: ▁WE▁HAVE▁BEEN▁LEARNING▁ABOUTS▁AND▁HOW

2024-10-27 17:37:30,989 (asr_inference:509) INFO: speech length: 185267
2024-10-27 17:37:37,780 (beam_search:428) INFO: decoder input length: 144
2024-10-27 17:37:37,780 (beam_search:429) INFO: max output length: 144
2024-10-27 17:37:37,780 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:38,089 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:38,089 (beam_search:476) INFO:  -3.06 * 1.0 =  -3.06 for ctc
2024-10-27 17:37:38,089 (beam_search:479) INFO: total log probability: -3.06
2024-10-27 17:37:38,089 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:37:38,089 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:38,089 (beam_search:483) INFO: best hypo: ▁I▁A▁A▁AND▁IS▁A▁ON▁THE▁AND▁THEN▁THE▁IS

2024-10-27 17:37:38,093 (asr_inference:509) INFO: speech length: 149559
2024-10-27 17:37:43,702 (beam_search:428) INFO: decoder input length: 116
2024-10-27 17:37:43,702 (beam_search:429) INFO: max output length: 116
2024-10-27 17:37:43,702 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:43,999 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:43,999 (beam_search:476) INFO:  -3.43 * 1.0 =  -3.43 for ctc
2024-10-27 17:37:43,999 (beam_search:479) INFO: total log probability: -3.43
2024-10-27 17:37:43,999 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:37:43,999 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:43,999 (beam_search:483) INFO: best hypo: ▁BECAUSE▁IT▁IS▁WHICH▁IS▁THE▁ENERGY▁IT▁SO▁KNOW▁IT▁IS▁ENERGY▁NOT▁OTHER

2024-10-27 17:37:44,002 (asr_inference:509) INFO: speech length: 182284
2024-10-27 17:37:50,795 (beam_search:428) INFO: decoder input length: 141
2024-10-27 17:37:50,795 (beam_search:429) INFO: max output length: 141
2024-10-27 17:37:50,795 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:51,440 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:51,440 (beam_search:476) INFO:  -7.71 * 1.0 =  -7.71 for ctc
2024-10-27 17:37:51,440 (beam_search:479) INFO: total log probability: -7.71
2024-10-27 17:37:51,440 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 17:37:51,440 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:51,441 (beam_search:483) INFO: best hypo: ▁BECAUSE▁WE▁DIDN'T▁HAVE▁ENERGY▁WE'T▁HAVE▁LIGHT▁AND▁LIKE▁THAT▁AND▁WE▁WOULDN▁HAVE▁THE▁BECAUSE▁THE▁IS▁MADE▁OF▁ENERGY

2024-10-27 17:37:51,443 (asr_inference:509) INFO: speech length: 76976
2024-10-27 17:37:54,198 (beam_search:428) INFO: decoder input length: 59
2024-10-27 17:37:54,198 (beam_search:429) INFO: max output length: 59
2024-10-27 17:37:54,198 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:54,269 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:54,270 (beam_search:476) INFO:  -0.76 * 1.0 =  -0.76 for ctc
2024-10-27 17:37:54,270 (beam_search:479) INFO: total log probability: -0.76
2024-10-27 17:37:54,270 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:37:54,270 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:54,270 (beam_search:483) INFO: best hypo: ▁THAT▁IT▁IS▁BECAUSE▁OF▁THE▁AND

2024-10-27 17:37:54,272 (asr_inference:509) INFO: speech length: 60480
2024-10-27 17:37:56,573 (beam_search:428) INFO: decoder input length: 46
2024-10-27 17:37:56,573 (beam_search:429) INFO: max output length: 46
2024-10-27 17:37:56,573 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:56,615 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:56,615 (beam_search:476) INFO:  -2.41 * 1.0 =  -2.41 for ctc
2024-10-27 17:37:56,615 (beam_search:479) INFO: total log probability: -2.41
2024-10-27 17:37:56,615 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 17:37:56,615 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:56,615 (beam_search:483) INFO: best hypo: ▁IS▁THE▁THING▁MAKES▁IT

2024-10-27 17:37:56,618 (asr_inference:509) INFO: speech length: 36913
2024-10-27 17:37:58,153 (beam_search:428) INFO: decoder input length: 28
2024-10-27 17:37:58,153 (beam_search:429) INFO: max output length: 28
2024-10-27 17:37:58,153 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:58,176 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:58,176 (beam_search:476) INFO:  -0.72 * 1.0 =  -0.72 for ctc
2024-10-27 17:37:58,176 (beam_search:479) INFO: total log probability: -0.72
2024-10-27 17:37:58,176 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:37:58,176 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:58,176 (beam_search:483) INFO: best hypo: ▁IT'T▁LIKE

2024-10-27 17:37:58,178 (asr_inference:509) INFO: speech length: 261916
2024-10-27 17:38:08,885 (beam_search:428) INFO: decoder input length: 204
2024-10-27 17:38:08,886 (beam_search:429) INFO: max output length: 204
2024-10-27 17:38:08,886 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:09,990 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:09,990 (beam_search:476) INFO:  -3.73 * 1.0 =  -3.73 for ctc
2024-10-27 17:38:09,990 (beam_search:479) INFO: total log probability: -3.73
2024-10-27 17:38:09,990 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:38:09,991 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:09,991 (beam_search:483) INFO: best hypo: ▁I▁SEE▁THAT▁THE▁IS▁AND▁THED▁ENERGY▁FROM▁THE▁IS▁GOING▁TO▁THE▁GOING▁TO▁THE▁IT▁LIGHT▁BUT▁IT▁ISING▁THE▁WAX▁ISING▁AWAY▁ENERGY

2024-10-27 17:38:09,993 (asr_inference:509) INFO: speech length: 40464
2024-10-27 17:38:11,558 (beam_search:428) INFO: decoder input length: 31
2024-10-27 17:38:11,558 (beam_search:429) INFO: max output length: 31
2024-10-27 17:38:11,558 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:11,579 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:11,579 (beam_search:476) INFO:  -1.10 * 1.0 =  -1.10 for ctc
2024-10-27 17:38:11,579 (beam_search:479) INFO: total log probability: -1.10
2024-10-27 17:38:11,579 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:38:11,579 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:11,579 (beam_search:483) INFO: best hypo: ▁IS▁AND▁AND

2024-10-27 17:38:11,581 (asr_inference:509) INFO: speech length: 168041
2024-10-27 17:38:17,850 (beam_search:428) INFO: decoder input length: 130
2024-10-27 17:38:17,850 (beam_search:429) INFO: max output length: 130
2024-10-27 17:38:17,850 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:18,142 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:18,142 (beam_search:476) INFO:  -1.68 * 1.0 =  -1.68 for ctc
2024-10-27 17:38:18,142 (beam_search:479) INFO: total log probability: -1.68
2024-10-27 17:38:18,142 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:38:18,142 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:18,142 (beam_search:483) INFO: best hypo: ▁ENERGY▁IS▁OUT▁THE▁BECAUSE▁IT▁IS▁BEING▁AND▁ENERGY▁IS▁THE

2024-10-27 17:38:18,145 (asr_inference:509) INFO: speech length: 149225
2024-10-27 17:38:23,665 (beam_search:428) INFO: decoder input length: 116
2024-10-27 17:38:23,665 (beam_search:429) INFO: max output length: 116
2024-10-27 17:38:23,665 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:24,034 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:24,035 (beam_search:476) INFO:  -5.52 * 1.0 =  -5.52 for ctc
2024-10-27 17:38:24,035 (beam_search:479) INFO: total log probability: -5.52
2024-10-27 17:38:24,035 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 17:38:24,035 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:24,035 (beam_search:483) INFO: best hypo: ▁USING▁ENERGY▁BECAUSE▁SHE'S▁GOING▁A▁BUT▁SHE'S▁ALSO▁ENERGY▁BECAUSE▁THE▁HAS▁IT

2024-10-27 17:38:24,037 (asr_inference:509) INFO: speech length: 79123
2024-10-27 17:38:26,965 (beam_search:428) INFO: decoder input length: 61
2024-10-27 17:38:26,965 (beam_search:429) INFO: max output length: 61
2024-10-27 17:38:26,965 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:27,037 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:27,037 (beam_search:476) INFO:  -1.22 * 1.0 =  -1.22 for ctc
2024-10-27 17:38:27,037 (beam_search:479) INFO: total log probability: -1.22
2024-10-27 17:38:27,037 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:38:27,037 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:27,037 (beam_search:483) INFO: best hypo: ▁ISING▁IT▁ENERGY▁AND▁THE▁IS

2024-10-27 17:38:27,040 (asr_inference:509) INFO: speech length: 62715
2024-10-27 17:38:29,357 (beam_search:428) INFO: decoder input length: 48
2024-10-27 17:38:29,358 (beam_search:429) INFO: max output length: 48
2024-10-27 17:38:29,358 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:29,395 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:29,395 (beam_search:476) INFO:  -0.58 * 1.0 =  -0.58 for ctc
2024-10-27 17:38:29,395 (beam_search:479) INFO: total log probability: -0.58
2024-10-27 17:38:29,395 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:38:29,395 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:29,395 (beam_search:483) INFO: best hypo: ▁CAN▁BECAUSE▁ALL▁THAT

2024-10-27 17:38:29,397 (asr_inference:509) INFO: speech length: 80935
2024-10-27 17:38:32,362 (beam_search:428) INFO: decoder input length: 62
2024-10-27 17:38:32,363 (beam_search:429) INFO: max output length: 62
2024-10-27 17:38:32,363 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:32,432 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:32,432 (beam_search:476) INFO:  -1.83 * 1.0 =  -1.83 for ctc
2024-10-27 17:38:32,432 (beam_search:479) INFO: total log probability: -1.83
2024-10-27 17:38:32,432 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:38:32,432 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:32,432 (beam_search:483) INFO: best hypo: ▁AND▁ENERGY▁WHICH▁IS▁OF▁BUT

2024-10-27 17:38:32,435 (asr_inference:509) INFO: speech length: 278942
2024-10-27 17:38:44,071 (beam_search:428) INFO: decoder input length: 217
2024-10-27 17:38:44,072 (beam_search:429) INFO: max output length: 217
2024-10-27 17:38:44,072 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:44,721 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:44,721 (beam_search:476) INFO:  -5.15 * 1.0 =  -5.15 for ctc
2024-10-27 17:38:44,721 (beam_search:479) INFO: total log probability: -5.15
2024-10-27 17:38:44,721 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 17:38:44,721 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:44,722 (beam_search:483) INFO: best hypo: ▁WHICH▁SO▁THE▁THE▁THE▁ISD▁ENERGY▁THE▁HEAT▁SO▁THE▁AND▁THE▁MORE▁MUCH

2024-10-27 17:38:44,724 (asr_inference:509) INFO: speech length: 36911
2024-10-27 17:38:46,233 (beam_search:428) INFO: decoder input length: 28
2024-10-27 17:38:46,233 (beam_search:429) INFO: max output length: 28
2024-10-27 17:38:46,233 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:46,253 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:46,253 (beam_search:476) INFO:  -1.09 * 1.0 =  -1.09 for ctc
2024-10-27 17:38:46,253 (beam_search:479) INFO: total log probability: -1.09
2024-10-27 17:38:46,253 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:38:46,253 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:46,253 (beam_search:483) INFO: best hypo: ▁WILL▁BURN▁TO

2024-10-27 17:38:46,256 (asr_inference:509) INFO: speech length: 19601
2024-10-27 17:38:47,165 (beam_search:428) INFO: decoder input length: 14
2024-10-27 17:38:47,166 (beam_search:429) INFO: max output length: 14
2024-10-27 17:38:47,166 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:47,174 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:47,175 (beam_search:476) INFO:  -1.28 * 1.0 =  -1.28 for ctc
2024-10-27 17:38:47,175 (beam_search:479) INFO: total log probability: -1.28
2024-10-27 17:38:47,175 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 17:38:47,175 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:47,175 (beam_search:483) INFO: best hypo: S▁THAT

2024-10-27 17:38:47,177 (asr_inference:509) INFO: speech length: 118320
2024-10-27 17:38:51,412 (beam_search:428) INFO: decoder input length: 91
2024-10-27 17:38:51,412 (beam_search:429) INFO: max output length: 91
2024-10-27 17:38:51,412 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:51,532 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:51,532 (beam_search:476) INFO:  -1.51 * 1.0 =  -1.51 for ctc
2024-10-27 17:38:51,532 (beam_search:479) INFO: total log probability: -1.51
2024-10-27 17:38:51,532 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:38:51,532 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:51,532 (beam_search:483) INFO: best hypo: ▁I▁THINKS▁IS▁A▁SOLAR

2024-10-27 17:38:51,535 (asr_inference:509) INFO: speech length: 140065
2024-10-27 17:38:56,705 (beam_search:428) INFO: decoder input length: 108
2024-10-27 17:38:56,706 (beam_search:429) INFO: max output length: 108
2024-10-27 17:38:56,706 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:57,108 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:57,108 (beam_search:476) INFO:  -5.38 * 1.0 =  -5.38 for ctc
2024-10-27 17:38:57,108 (beam_search:479) INFO: total log probability: -5.38
2024-10-27 17:38:57,108 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:38:57,108 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:57,108 (beam_search:483) INFO: best hypo: ▁THE▁TO▁THELAR▁TO▁SOLAR▁ENERGY▁AND▁IT▁WILL▁GO▁THROUGH▁THATING▁TO▁THE▁WIRES▁GOING▁TO▁THE▁IT

2024-10-27 17:38:57,112 (asr_inference:509) INFO: speech length: 51200
2024-10-27 17:38:59,194 (beam_search:428) INFO: decoder input length: 39
2024-10-27 17:38:59,194 (beam_search:429) INFO: max output length: 39
2024-10-27 17:38:59,194 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:59,224 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:59,224 (beam_search:476) INFO:  -1.42 * 1.0 =  -1.42 for ctc
2024-10-27 17:38:59,224 (beam_search:479) INFO: total log probability: -1.42
2024-10-27 17:38:59,224 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 17:38:59,224 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:59,224 (beam_search:483) INFO: best hypo: ▁OKAY▁WE▁THIS

2024-10-27 17:38:59,226 (asr_inference:509) INFO: speech length: 147498
2024-10-27 17:39:04,599 (beam_search:428) INFO: decoder input length: 114
2024-10-27 17:39:04,600 (beam_search:429) INFO: max output length: 114
2024-10-27 17:39:04,600 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:04,889 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:04,889 (beam_search:476) INFO:  -3.08 * 1.0 =  -3.08 for ctc
2024-10-27 17:39:04,889 (beam_search:479) INFO: total log probability: -3.08
2024-10-27 17:39:04,890 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:39:04,890 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:04,890 (beam_search:483) INFO: best hypo: ▁IT▁IS▁ONLY▁A▁ONE▁TIME▁AND▁COAL▁IS▁JUST▁AND▁IT▁CAN▁FOR▁A

2024-10-27 17:39:04,893 (asr_inference:509) INFO: speech length: 22500
2024-10-27 17:39:05,912 (beam_search:428) INFO: decoder input length: 17
2024-10-27 17:39:05,912 (beam_search:429) INFO: max output length: 17
2024-10-27 17:39:05,912 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:05,927 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:05,927 (beam_search:476) INFO:  -0.83 * 1.0 =  -0.83 for ctc
2024-10-27 17:39:05,927 (beam_search:479) INFO: total log probability: -0.83
2024-10-27 17:39:05,927 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:39:05,927 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:05,927 (beam_search:483) INFO: best hypo: ▁IT▁WILL▁THE▁AIR

2024-10-27 17:39:05,929 (asr_inference:509) INFO: speech length: 18500
2024-10-27 17:39:06,829 (beam_search:428) INFO: decoder input length: 13
2024-10-27 17:39:06,829 (beam_search:429) INFO: max output length: 13
2024-10-27 17:39:06,829 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:06,844 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:06,844 (beam_search:476) INFO:  -0.01 * 1.0 =  -0.01 for ctc
2024-10-27 17:39:06,844 (beam_search:479) INFO: total log probability: -0.01
2024-10-27 17:39:06,844 (beam_search:480) INFO: normalized log probability: -0.00
2024-10-27 17:39:06,844 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:06,844 (beam_search:483) INFO: best hypo: ▁I▁DON'T▁KNOW

2024-10-27 17:39:06,847 (asr_inference:509) INFO: speech length: 23918
2024-10-27 17:39:07,924 (beam_search:428) INFO: decoder input length: 18
2024-10-27 17:39:07,925 (beam_search:429) INFO: max output length: 18
2024-10-27 17:39:07,925 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:07,930 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:07,930 (beam_search:476) INFO:  -0.42 * 1.0 =  -0.42 for ctc
2024-10-27 17:39:07,930 (beam_search:479) INFO: total log probability: -0.42
2024-10-27 17:39:07,930 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:39:07,930 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:07,930 (beam_search:483) INFO: best hypo: 

2024-10-27 17:39:07,932 (asr_inference:509) INFO: speech length: 87500
2024-10-27 17:39:11,199 (beam_search:428) INFO: decoder input length: 67
2024-10-27 17:39:11,199 (beam_search:429) INFO: max output length: 67
2024-10-27 17:39:11,199 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:11,250 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:11,250 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 17:39:11,250 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 17:39:11,250 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:39:11,250 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:11,250 (beam_search:483) INFO: best hypo: ▁WELL▁NOT▁I▁NOT

2024-10-27 17:39:11,252 (asr_inference:509) INFO: speech length: 27984
2024-10-27 17:39:12,412 (beam_search:428) INFO: decoder input length: 21
2024-10-27 17:39:12,412 (beam_search:429) INFO: max output length: 21
2024-10-27 17:39:12,412 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:12,433 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:12,433 (beam_search:476) INFO:  -0.34 * 1.0 =  -0.34 for ctc
2024-10-27 17:39:12,433 (beam_search:479) INFO: total log probability: -0.34
2024-10-27 17:39:12,433 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:39:12,433 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:12,433 (beam_search:483) INFO: best hypo: ▁COULD▁YOU▁SAY▁THAT▁AGAIN

2024-10-27 17:39:12,435 (asr_inference:509) INFO: speech length: 42914
2024-10-27 17:39:14,104 (beam_search:428) INFO: decoder input length: 33
2024-10-27 17:39:14,104 (beam_search:429) INFO: max output length: 33
2024-10-27 17:39:14,104 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:14,134 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:14,134 (beam_search:476) INFO:  -1.24 * 1.0 =  -1.24 for ctc
2024-10-27 17:39:14,134 (beam_search:479) INFO: total log probability: -1.24
2024-10-27 17:39:14,134 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:39:14,134 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:14,134 (beam_search:483) INFO: best hypo: ▁THIS▁IS▁A▁PARALLEL▁CIRCUIT

2024-10-27 17:39:14,136 (asr_inference:509) INFO: speech length: 270455
2024-10-27 17:39:25,315 (beam_search:428) INFO: decoder input length: 210
2024-10-27 17:39:25,315 (beam_search:429) INFO: max output length: 210
2024-10-27 17:39:25,315 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:26,313 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:26,313 (beam_search:476) INFO:  -5.37 * 1.0 =  -5.37 for ctc
2024-10-27 17:39:26,313 (beam_search:479) INFO: total log probability: -5.37
2024-10-27 17:39:26,313 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:39:26,313 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:26,313 (beam_search:483) INFO: best hypo: ▁IS▁GOING▁AROUND▁AND▁IT▁IS▁A▁AND▁IT▁IS▁A▁CIRCUIT▁AND▁IT▁IS▁LIGHTING▁UP▁THE▁BULBS▁WITH▁ONE▁D▁AND▁WE▁DID▁THIS▁IN▁AND

2024-10-27 17:39:26,316 (asr_inference:509) INFO: speech length: 73187
2024-10-27 17:39:28,965 (beam_search:428) INFO: decoder input length: 56
2024-10-27 17:39:28,965 (beam_search:429) INFO: max output length: 56
2024-10-27 17:39:28,965 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:28,989 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:28,989 (beam_search:476) INFO:  -0.29 * 1.0 =  -0.29 for ctc
2024-10-27 17:39:28,989 (beam_search:479) INFO: total log probability: -0.29
2024-10-27 17:39:28,989 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:39:28,989 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:28,989 (beam_search:483) INFO: best hypo: ▁AND

2024-10-27 17:39:28,992 (asr_inference:509) INFO: speech length: 110606
2024-10-27 17:39:33,328 (beam_search:428) INFO: decoder input length: 85
2024-10-27 17:39:33,328 (beam_search:429) INFO: max output length: 85
2024-10-27 17:39:33,328 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:33,420 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:33,420 (beam_search:476) INFO:  -0.53 * 1.0 =  -0.53 for ctc
2024-10-27 17:39:33,420 (beam_search:479) INFO: total log probability: -0.53
2024-10-27 17:39:33,420 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:39:33,420 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:33,420 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁BECAUSE▁IT▁IS

2024-10-27 17:39:33,422 (asr_inference:509) INFO: speech length: 152653
2024-10-27 17:39:39,023 (beam_search:428) INFO: decoder input length: 118
2024-10-27 17:39:39,023 (beam_search:429) INFO: max output length: 118
2024-10-27 17:39:39,023 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:39,184 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:39,185 (beam_search:476) INFO:  -1.66 * 1.0 =  -1.66 for ctc
2024-10-27 17:39:39,185 (beam_search:479) INFO: total log probability: -1.66
2024-10-27 17:39:39,185 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:39:39,185 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:39,185 (beam_search:483) INFO: best hypo: ▁I▁THAT▁THAT▁IT▁IS▁A▁A▁CIRCUIT

2024-10-27 17:39:39,187 (asr_inference:509) INFO: speech length: 133741
2024-10-27 17:39:43,985 (beam_search:428) INFO: decoder input length: 103
2024-10-27 17:39:43,986 (beam_search:429) INFO: max output length: 103
2024-10-27 17:39:43,986 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:44,114 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:44,114 (beam_search:476) INFO:  -1.72 * 1.0 =  -1.72 for ctc
2024-10-27 17:39:44,114 (beam_search:479) INFO: total log probability: -1.72
2024-10-27 17:39:44,114 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:39:44,114 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:44,114 (beam_search:483) INFO: best hypo: ▁THAT▁IS▁GOING▁THROUGH▁A▁THAT▁IS

2024-10-27 17:39:44,116 (asr_inference:509) INFO: speech length: 88621
2024-10-27 17:39:47,283 (beam_search:428) INFO: decoder input length: 68
2024-10-27 17:39:47,283 (beam_search:429) INFO: max output length: 68
2024-10-27 17:39:47,283 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:47,383 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:47,383 (beam_search:476) INFO:  -3.44 * 1.0 =  -3.44 for ctc
2024-10-27 17:39:47,383 (beam_search:479) INFO: total log probability: -3.44
2024-10-27 17:39:47,383 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 17:39:47,383 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:47,383 (beam_search:483) INFO: best hypo: ▁THEY▁ARE▁WITH▁ONLY▁AND▁THAT▁VERY▁TO

2024-10-27 17:39:47,385 (asr_inference:509) INFO: speech length: 181949
2024-10-27 17:39:54,296 (beam_search:428) INFO: decoder input length: 141
2024-10-27 17:39:54,296 (beam_search:429) INFO: max output length: 141
2024-10-27 17:39:54,296 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:54,425 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:54,425 (beam_search:476) INFO:  -4.30 * 1.0 =  -4.30 for ctc
2024-10-27 17:39:54,425 (beam_search:479) INFO: total log probability: -4.30
2024-10-27 17:39:54,425 (beam_search:480) INFO: normalized log probability: -0.61
2024-10-27 17:39:54,425 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:54,425 (beam_search:483) INFO: best hypo: ▁THE▁AND▁STUFF▁IT▁A

2024-10-27 17:39:54,427 (asr_inference:509) INFO: speech length: 82654
2024-10-27 17:39:57,438 (beam_search:428) INFO: decoder input length: 64
2024-10-27 17:39:57,438 (beam_search:429) INFO: max output length: 64
2024-10-27 17:39:57,438 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:57,529 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:57,529 (beam_search:476) INFO:  -1.61 * 1.0 =  -1.61 for ctc
2024-10-27 17:39:57,530 (beam_search:479) INFO: total log probability: -1.61
2024-10-27 17:39:57,530 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:39:57,530 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:57,530 (beam_search:483) INFO: best hypo: ▁I▁THAT▁THE▁BULBS▁ARE▁LIGHTING▁IN▁A

2024-10-27 17:39:57,533 (asr_inference:509) INFO: speech length: 91132
2024-10-27 17:40:00,795 (beam_search:428) INFO: decoder input length: 70
2024-10-27 17:40:00,795 (beam_search:429) INFO: max output length: 70
2024-10-27 17:40:00,795 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:00,861 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:00,861 (beam_search:476) INFO:  -0.64 * 1.0 =  -0.64 for ctc
2024-10-27 17:40:00,862 (beam_search:479) INFO: total log probability: -0.64
2024-10-27 17:40:00,862 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:40:00,862 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:00,862 (beam_search:483) INFO: best hypo: ▁THATS▁I▁DO▁NOT

2024-10-27 17:40:00,864 (asr_inference:509) INFO: speech length: 29672
2024-10-27 17:40:02,056 (beam_search:428) INFO: decoder input length: 22
2024-10-27 17:40:02,056 (beam_search:429) INFO: max output length: 22
2024-10-27 17:40:02,056 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:02,074 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:02,074 (beam_search:476) INFO:  -1.13 * 1.0 =  -1.13 for ctc
2024-10-27 17:40:02,074 (beam_search:479) INFO: total log probability: -1.13
2024-10-27 17:40:02,074 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:40:02,074 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:02,074 (beam_search:483) INFO: best hypo: ▁COULD▁YOU▁THAT▁AGAIN

2024-10-27 17:40:02,077 (asr_inference:509) INFO: speech length: 59977
2024-10-27 17:40:04,279 (beam_search:428) INFO: decoder input length: 46
2024-10-27 17:40:04,279 (beam_search:429) INFO: max output length: 46
2024-10-27 17:40:04,279 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:04,346 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:04,346 (beam_search:476) INFO:  -0.92 * 1.0 =  -0.92 for ctc
2024-10-27 17:40:04,346 (beam_search:479) INFO: total log probability: -0.92
2024-10-27 17:40:04,346 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:40:04,346 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:04,346 (beam_search:483) INFO: best hypo: ▁THEY▁ARE▁IN▁A▁CIRCUIT▁AND▁THEY▁ARE

2024-10-27 17:40:04,348 (asr_inference:509) INFO: speech length: 22500
2024-10-27 17:40:05,398 (beam_search:428) INFO: decoder input length: 17
2024-10-27 17:40:05,398 (beam_search:429) INFO: max output length: 17
2024-10-27 17:40:05,398 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:05,416 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:05,416 (beam_search:476) INFO:  -0.04 * 1.0 =  -0.04 for ctc
2024-10-27 17:40:05,416 (beam_search:479) INFO: total log probability: -0.04
2024-10-27 17:40:05,416 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 17:40:05,416 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:05,416 (beam_search:483) INFO: best hypo: ▁I▁DON'T▁KNOW

2024-10-27 17:40:05,418 (asr_inference:509) INFO: speech length: 78188
2024-10-27 17:40:08,191 (beam_search:428) INFO: decoder input length: 60
2024-10-27 17:40:08,191 (beam_search:429) INFO: max output length: 60
2024-10-27 17:40:08,191 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:08,305 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:08,305 (beam_search:476) INFO:  -1.12 * 1.0 =  -1.12 for ctc
2024-10-27 17:40:08,305 (beam_search:479) INFO: total log probability: -1.12
2024-10-27 17:40:08,306 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:40:08,306 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:08,306 (beam_search:483) INFO: best hypo: ▁IT▁GOES▁BACK▁INTO▁THE▁POSITIVE▁SIDE▁AND▁THE▁NEGATIVE▁SIDE

2024-10-27 17:40:08,308 (asr_inference:509) INFO: speech length: 55918
2024-10-27 17:40:10,513 (beam_search:428) INFO: decoder input length: 43
2024-10-27 17:40:10,513 (beam_search:429) INFO: max output length: 43
2024-10-27 17:40:10,513 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:10,571 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:10,571 (beam_search:476) INFO:  -1.99 * 1.0 =  -1.99 for ctc
2024-10-27 17:40:10,571 (beam_search:479) INFO: total log probability: -1.99
2024-10-27 17:40:10,571 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:40:10,571 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:10,571 (beam_search:483) INFO: best hypo: ▁THAT▁ENERGY▁IS▁AND▁IT▁IS▁A▁CIRCUIT

2024-10-27 17:40:10,574 (asr_inference:509) INFO: speech length: 85554
2024-10-27 17:40:13,720 (beam_search:428) INFO: decoder input length: 66
2024-10-27 17:40:13,720 (beam_search:429) INFO: max output length: 66
2024-10-27 17:40:13,720 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:13,827 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:13,827 (beam_search:476) INFO:  -1.19 * 1.0 =  -1.19 for ctc
2024-10-27 17:40:13,827 (beam_search:479) INFO: total log probability: -1.19
2024-10-27 17:40:13,827 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:40:13,827 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:13,828 (beam_search:483) INFO: best hypo: ▁I▁DON'T▁KNOW▁THE▁ARE▁GOING▁IN

2024-10-27 17:40:13,830 (asr_inference:509) INFO: speech length: 305190
2024-10-27 17:40:26,932 (beam_search:428) INFO: decoder input length: 237
2024-10-27 17:40:26,932 (beam_search:429) INFO: max output length: 237
2024-10-27 17:40:26,932 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:28,275 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:28,276 (beam_search:476) INFO:  -3.72 * 1.0 =  -3.72 for ctc
2024-10-27 17:40:28,276 (beam_search:479) INFO: total log probability: -3.72
2024-10-27 17:40:28,276 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:40:28,276 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:28,276 (beam_search:483) INFO: best hypo: ▁I▁THAT▁WHEN▁IT▁IS▁IT▁IS▁GOING▁BUT▁WHEN▁IT▁IS'T▁CONNECTED▁IT▁IS▁GOING▁FOR▁SOME▁BUT▁I▁YOUR▁SO▁I▁THINK▁I'M▁RIGHT

2024-10-27 17:40:28,278 (asr_inference:509) INFO: speech length: 49331
2024-10-27 17:40:30,112 (beam_search:428) INFO: decoder input length: 38
2024-10-27 17:40:30,112 (beam_search:429) INFO: max output length: 38
2024-10-27 17:40:30,112 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:30,138 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:30,138 (beam_search:476) INFO:  -0.03 * 1.0 =  -0.03 for ctc
2024-10-27 17:40:30,138 (beam_search:479) INFO: total log probability: -0.03
2024-10-27 17:40:30,138 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 17:40:30,138 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:30,138 (beam_search:483) INFO: best hypo: ▁IT▁IS▁THE

2024-10-27 17:40:30,141 (asr_inference:509) INFO: speech length: 189576
2024-10-27 17:40:37,376 (beam_search:428) INFO: decoder input length: 147
2024-10-27 17:40:37,376 (beam_search:429) INFO: max output length: 147
2024-10-27 17:40:37,376 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:37,650 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:37,650 (beam_search:476) INFO:  -2.37 * 1.0 =  -2.37 for ctc
2024-10-27 17:40:37,650 (beam_search:479) INFO: total log probability: -2.37
2024-10-27 17:40:37,650 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:40:37,650 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:37,650 (beam_search:483) INFO: best hypo: ▁IT'S▁BECAUSE▁IT▁IS▁AND▁AND▁LIKE▁YEAH▁YEAH

2024-10-27 17:40:37,653 (asr_inference:509) INFO: speech length: 36479
2024-10-27 17:40:39,031 (beam_search:428) INFO: decoder input length: 27
2024-10-27 17:40:39,032 (beam_search:429) INFO: max output length: 27
2024-10-27 17:40:39,032 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:39,046 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:39,046 (beam_search:476) INFO:  -1.43 * 1.0 =  -1.43 for ctc
2024-10-27 17:40:39,046 (beam_search:479) INFO: total log probability: -1.43
2024-10-27 17:40:39,046 (beam_search:480) INFO: normalized log probability: -0.36
2024-10-27 17:40:39,046 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:39,046 (beam_search:483) INFO: best hypo: ▁DONT

2024-10-27 17:40:39,048 (asr_inference:509) INFO: speech length: 58548
2024-10-27 17:40:41,225 (beam_search:428) INFO: decoder input length: 45
2024-10-27 17:40:41,225 (beam_search:429) INFO: max output length: 45
2024-10-27 17:40:41,225 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:41,255 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:41,255 (beam_search:476) INFO:  -0.58 * 1.0 =  -0.58 for ctc
2024-10-27 17:40:41,255 (beam_search:479) INFO: total log probability: -0.58
2024-10-27 17:40:41,255 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:40:41,255 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:41,255 (beam_search:483) INFO: best hypo: ▁BECAUSE▁THEY▁DO

2024-10-27 17:40:41,258 (asr_inference:509) INFO: speech length: 50179
2024-10-27 17:40:43,253 (beam_search:428) INFO: decoder input length: 38
2024-10-27 17:40:43,254 (beam_search:429) INFO: max output length: 38
2024-10-27 17:40:43,254 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:43,305 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:43,305 (beam_search:476) INFO:  -2.15 * 1.0 =  -2.15 for ctc
2024-10-27 17:40:43,305 (beam_search:479) INFO: total log probability: -2.15
2024-10-27 17:40:43,305 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:40:43,305 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:43,305 (beam_search:483) INFO: best hypo: ▁I▁THE▁CIRCUIT▁WOULD▁BE▁FOR▁A▁PARALLEL

2024-10-27 17:40:43,308 (asr_inference:509) INFO: speech length: 327275
2024-10-27 17:40:57,567 (beam_search:428) INFO: decoder input length: 255
2024-10-27 17:40:57,567 (beam_search:429) INFO: max output length: 255
2024-10-27 17:40:57,567 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:58,558 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:58,558 (beam_search:476) INFO:  -5.51 * 1.0 =  -5.51 for ctc
2024-10-27 17:40:58,558 (beam_search:479) INFO: total log probability: -5.51
2024-10-27 17:40:58,558 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:40:58,558 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:58,558 (beam_search:483) INFO: best hypo: ▁I▁THAT▁THE▁THAT▁TWO▁MAGNETS▁HAVE▁AND▁WHICH▁IS▁AND▁AND▁THAT▁THEY▁ARE▁TO▁THE▁WHICH▁IS▁OF▁AND

2024-10-27 17:40:58,562 (asr_inference:509) INFO: speech length: 255482
2024-10-27 17:41:09,212 (beam_search:428) INFO: decoder input length: 199
2024-10-27 17:41:09,212 (beam_search:429) INFO: max output length: 199
2024-10-27 17:41:09,212 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:09,700 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:09,700 (beam_search:476) INFO:  -4.76 * 1.0 =  -4.76 for ctc
2024-10-27 17:41:09,700 (beam_search:479) INFO: total log probability: -4.76
2024-10-27 17:41:09,700 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 17:41:09,700 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:09,700 (beam_search:483) INFO: best hypo: ▁THEY▁STICK▁TO▁AS▁STEEL▁AND▁BUT▁NOT▁OTHER▁STUFF▁STICK▁TO

2024-10-27 17:41:09,703 (asr_inference:509) INFO: speech length: 206532
2024-10-27 17:41:18,362 (beam_search:428) INFO: decoder input length: 160
2024-10-27 17:41:18,362 (beam_search:429) INFO: max output length: 160
2024-10-27 17:41:18,362 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:18,684 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:18,684 (beam_search:476) INFO:  -3.16 * 1.0 =  -3.16 for ctc
2024-10-27 17:41:18,684 (beam_search:479) INFO: total log probability: -3.16
2024-10-27 17:41:18,684 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:41:18,684 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:18,684 (beam_search:483) INFO: best hypo: ▁BECAUSE▁THE▁IS▁AND▁THEY▁LIKE▁TO▁STICK▁TO▁METAL▁AND

2024-10-27 17:41:18,687 (asr_inference:509) INFO: speech length: 42885
2024-10-27 17:41:20,376 (beam_search:428) INFO: decoder input length: 33
2024-10-27 17:41:20,376 (beam_search:429) INFO: max output length: 33
2024-10-27 17:41:20,376 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:20,417 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:20,418 (beam_search:476) INFO:  -2.17 * 1.0 =  -2.17 for ctc
2024-10-27 17:41:20,418 (beam_search:479) INFO: total log probability: -2.17
2024-10-27 17:41:20,418 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:41:20,418 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:20,418 (beam_search:483) INFO: best hypo: ▁THE▁NAIL▁OUT▁OF▁STEEL▁AND▁IRON

2024-10-27 17:41:20,420 (asr_inference:509) INFO: speech length: 378809
2024-10-27 17:41:37,641 (beam_search:428) INFO: decoder input length: 295
2024-10-27 17:41:37,641 (beam_search:429) INFO: max output length: 295
2024-10-27 17:41:37,641 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:37,836 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:37,836 (beam_search:476) INFO:  -1.05 * 1.0 =  -1.05 for ctc
2024-10-27 17:41:37,836 (beam_search:479) INFO: total log probability: -1.05
2024-10-27 17:41:37,836 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:41:37,836 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:37,836 (beam_search:483) INFO: best hypo: ▁PAPER▁AND▁PLASTIC

2024-10-27 17:41:37,838 (asr_inference:509) INFO: speech length: 212833
2024-10-27 17:41:46,246 (beam_search:428) INFO: decoder input length: 165
2024-10-27 17:41:46,246 (beam_search:429) INFO: max output length: 165
2024-10-27 17:41:46,246 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:46,744 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:46,744 (beam_search:476) INFO:  -4.69 * 1.0 =  -4.69 for ctc
2024-10-27 17:41:46,744 (beam_search:479) INFO: total log probability: -4.69
2024-10-27 17:41:46,744 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:41:46,744 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:46,744 (beam_search:483) INFO: best hypo: ▁THE▁NAIL▁IS▁OUT▁OF▁STEEL▁AND▁IRON▁IT▁STICK▁TO▁THE▁MAGNET▁BUT▁THE▁IS▁NOT

2024-10-27 17:41:46,746 (asr_inference:509) INFO: speech length: 128130
2024-10-27 17:41:51,446 (beam_search:428) INFO: decoder input length: 99
2024-10-27 17:41:51,446 (beam_search:429) INFO: max output length: 99
2024-10-27 17:41:51,446 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:51,499 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:51,499 (beam_search:476) INFO:  -1.20 * 1.0 =  -1.20 for ctc
2024-10-27 17:41:51,499 (beam_search:479) INFO: total log probability: -1.20
2024-10-27 17:41:51,499 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 17:41:51,499 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:51,499 (beam_search:483) INFO: best hypo: ▁AND▁OTHER

2024-10-27 17:41:51,501 (asr_inference:509) INFO: speech length: 236101
2024-10-27 17:42:00,961 (beam_search:428) INFO: decoder input length: 183
2024-10-27 17:42:00,961 (beam_search:429) INFO: max output length: 183
2024-10-27 17:42:00,961 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:01,259 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:01,259 (beam_search:476) INFO:  -5.63 * 1.0 =  -5.63 for ctc
2024-10-27 17:42:01,259 (beam_search:479) INFO: total log probability: -5.63
2024-10-27 17:42:01,259 (beam_search:480) INFO: normalized log probability: -0.51
2024-10-27 17:42:01,259 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:01,259 (beam_search:483) INFO: best hypo: ▁HAVE▁OR▁IN▁IT▁OR▁OTHER▁SO▁NOT▁STICK

2024-10-27 17:42:01,262 (asr_inference:509) INFO: speech length: 409252
2024-10-27 17:42:21,043 (beam_search:428) INFO: decoder input length: 319
2024-10-27 17:42:21,043 (beam_search:429) INFO: max output length: 319
2024-10-27 17:42:21,043 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:23,223 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:23,224 (beam_search:476) INFO: -11.33 * 1.0 = -11.33 for ctc
2024-10-27 17:42:23,224 (beam_search:479) INFO: total log probability: -11.33
2024-10-27 17:42:23,224 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 17:42:23,224 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:23,224 (beam_search:483) INFO: best hypo: N'T▁BECAUSE▁IT▁DOESN'T▁HAVE▁ANY▁BECAUSE▁IT▁DOESN'T▁HAVE▁ANY▁OTHER▁OR▁AND▁ITN'T▁HAVE▁A▁BUT▁THE▁NAIL▁DOES▁HAVE▁IRON▁STEEL▁I▁THAT▁A

2024-10-27 17:42:23,226 (asr_inference:509) INFO: speech length: 23864
2024-10-27 17:42:24,287 (beam_search:428) INFO: decoder input length: 18
2024-10-27 17:42:24,288 (beam_search:429) INFO: max output length: 18
2024-10-27 17:42:24,288 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:24,299 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:24,299 (beam_search:476) INFO:  -0.34 * 1.0 =  -0.34 for ctc
2024-10-27 17:42:24,299 (beam_search:479) INFO: total log probability: -0.34
2024-10-27 17:42:24,299 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:42:24,299 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:24,299 (beam_search:483) INFO: best hypo: ▁THEY▁HAVE

2024-10-27 17:42:24,301 (asr_inference:509) INFO: speech length: 89521
2024-10-27 17:42:27,562 (beam_search:428) INFO: decoder input length: 69
2024-10-27 17:42:27,562 (beam_search:429) INFO: max output length: 69
2024-10-27 17:42:27,562 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:27,630 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:27,630 (beam_search:476) INFO:  -0.19 * 1.0 =  -0.19 for ctc
2024-10-27 17:42:27,630 (beam_search:479) INFO: total log probability: -0.19
2024-10-27 17:42:27,630 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 17:42:27,630 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:27,630 (beam_search:483) INFO: best hypo: ▁I▁DON'T▁KNOW

2024-10-27 17:42:27,633 (asr_inference:509) INFO: speech length: 76698
2024-10-27 17:42:30,502 (beam_search:428) INFO: decoder input length: 59
2024-10-27 17:42:30,502 (beam_search:429) INFO: max output length: 59
2024-10-27 17:42:30,502 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:30,612 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:30,612 (beam_search:476) INFO:  -2.64 * 1.0 =  -2.64 for ctc
2024-10-27 17:42:30,612 (beam_search:479) INFO: total log probability: -2.64
2024-10-27 17:42:30,612 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:42:30,612 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:30,612 (beam_search:483) INFO: best hypo: ▁I'M▁AND▁I▁DON'T▁REALLY▁HAVE▁FOR

2024-10-27 17:42:30,614 (asr_inference:509) INFO: speech length: 96391
2024-10-27 17:42:34,095 (beam_search:428) INFO: decoder input length: 74
2024-10-27 17:42:34,095 (beam_search:429) INFO: max output length: 74
2024-10-27 17:42:34,095 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:34,179 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:34,179 (beam_search:476) INFO:  -0.49 * 1.0 =  -0.49 for ctc
2024-10-27 17:42:34,179 (beam_search:479) INFO: total log probability: -0.49
2024-10-27 17:42:34,179 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:42:34,179 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:34,179 (beam_search:483) INFO: best hypo: ▁I'M▁SORT▁OF▁REALLY

2024-10-27 17:42:34,181 (asr_inference:509) INFO: speech length: 80405
2024-10-27 17:42:37,258 (beam_search:428) INFO: decoder input length: 62
2024-10-27 17:42:37,258 (beam_search:429) INFO: max output length: 62
2024-10-27 17:42:37,258 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:37,368 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:37,369 (beam_search:476) INFO:  -0.80 * 1.0 =  -0.80 for ctc
2024-10-27 17:42:37,369 (beam_search:479) INFO: total log probability: -0.80
2024-10-27 17:42:37,369 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:42:37,369 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:37,369 (beam_search:483) INFO: best hypo: ▁THE▁IS▁NOT▁BECAUSE▁IT'S▁NOT▁OF▁THE

2024-10-27 17:42:37,371 (asr_inference:509) INFO: speech length: 73535
2024-10-27 17:42:40,067 (beam_search:428) INFO: decoder input length: 56
2024-10-27 17:42:40,067 (beam_search:429) INFO: max output length: 56
2024-10-27 17:42:40,067 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:40,192 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:40,192 (beam_search:476) INFO:  -3.38 * 1.0 =  -3.38 for ctc
2024-10-27 17:42:40,192 (beam_search:479) INFO: total log probability: -3.38
2024-10-27 17:42:40,192 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:42:40,192 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:40,192 (beam_search:483) INFO: best hypo: ▁IS▁MADE▁OUT▁OF▁AND▁STEEL▁BUT▁THE▁OTHER▁ONE▁IS▁MADE▁OUT▁OF

2024-10-27 17:42:40,194 (asr_inference:509) INFO: speech length: 66629
2024-10-27 17:42:42,619 (beam_search:428) INFO: decoder input length: 51
2024-10-27 17:42:42,620 (beam_search:429) INFO: max output length: 51
2024-10-27 17:42:42,620 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:42,708 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:42,709 (beam_search:476) INFO:  -2.13 * 1.0 =  -2.13 for ctc
2024-10-27 17:42:42,709 (beam_search:479) INFO: total log probability: -2.13
2024-10-27 17:42:42,709 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:42:42,709 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:42,709 (beam_search:483) INFO: best hypo: ▁WELL▁ONE▁IS▁AND▁ONE▁IS▁OUT▁OF▁STEEL▁AND▁THE

2024-10-27 17:42:42,711 (asr_inference:509) INFO: speech length: 73237
2024-10-27 17:42:45,348 (beam_search:428) INFO: decoder input length: 56
2024-10-27 17:42:45,349 (beam_search:429) INFO: max output length: 56
2024-10-27 17:42:45,349 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:45,480 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:45,480 (beam_search:476) INFO:  -2.64 * 1.0 =  -2.64 for ctc
2024-10-27 17:42:45,480 (beam_search:479) INFO: total log probability: -2.64
2024-10-27 17:42:45,480 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:42:45,480 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:45,480 (beam_search:483) INFO: best hypo: ▁I▁DON'T▁KNOW▁BUT▁I▁DON'T▁REALLY▁HAVE▁FOR▁PLEASE▁UP

2024-10-27 17:42:45,482 (asr_inference:509) INFO: speech length: 78525
2024-10-27 17:42:48,362 (beam_search:428) INFO: decoder input length: 60
2024-10-27 17:42:48,362 (beam_search:429) INFO: max output length: 60
2024-10-27 17:42:48,362 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:48,411 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:48,411 (beam_search:476) INFO:  -0.75 * 1.0 =  -0.75 for ctc
2024-10-27 17:42:48,411 (beam_search:479) INFO: total log probability: -0.75
2024-10-27 17:42:48,412 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:42:48,412 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:48,412 (beam_search:483) INFO: best hypo: ▁AND▁AND▁OTHER▁AND

2024-10-27 17:42:48,415 (asr_inference:509) INFO: speech length: 188327
2024-10-27 17:42:55,872 (beam_search:428) INFO: decoder input length: 146
2024-10-27 17:42:55,872 (beam_search:429) INFO: max output length: 146
2024-10-27 17:42:55,872 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:56,195 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:56,195 (beam_search:476) INFO:  -2.93 * 1.0 =  -2.93 for ctc
2024-10-27 17:42:56,195 (beam_search:479) INFO: total log probability: -2.93
2024-10-27 17:42:56,195 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:42:56,195 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:56,195 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THE▁THAT▁THAT▁HAVE▁HAVE▁THE▁THAT▁IT▁STICK▁TO▁THE

2024-10-27 17:42:56,197 (asr_inference:509) INFO: speech length: 105843
2024-10-27 17:42:59,899 (beam_search:428) INFO: decoder input length: 82
2024-10-27 17:42:59,899 (beam_search:429) INFO: max output length: 82
2024-10-27 17:42:59,899 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:00,096 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:00,097 (beam_search:476) INFO:  -1.14 * 1.0 =  -1.14 for ctc
2024-10-27 17:43:00,097 (beam_search:479) INFO: total log probability: -1.14
2024-10-27 17:43:00,097 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:43:00,097 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:00,097 (beam_search:483) INFO: best hypo: ▁I'M▁I'M▁SO▁I▁DON'T▁REALLY▁KNOW▁IS

2024-10-27 17:43:00,099 (asr_inference:509) INFO: speech length: 18913
2024-10-27 17:43:01,044 (beam_search:428) INFO: decoder input length: 14
2024-10-27 17:43:01,044 (beam_search:429) INFO: max output length: 14
2024-10-27 17:43:01,044 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:01,053 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:01,054 (beam_search:476) INFO:  -0.84 * 1.0 =  -0.84 for ctc
2024-10-27 17:43:01,054 (beam_search:479) INFO: total log probability: -0.84
2024-10-27 17:43:01,054 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:43:01,054 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:01,054 (beam_search:483) INFO: best hypo: ▁WE▁BEEN

2024-10-27 17:43:01,057 (asr_inference:509) INFO: speech length: 84384
2024-10-27 17:43:04,178 (beam_search:428) INFO: decoder input length: 65
2024-10-27 17:43:04,178 (beam_search:429) INFO: max output length: 65
2024-10-27 17:43:04,178 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:04,231 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:04,231 (beam_search:476) INFO:  -0.90 * 1.0 =  -0.90 for ctc
2024-10-27 17:43:04,231 (beam_search:479) INFO: total log probability: -0.90
2024-10-27 17:43:04,231 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:43:04,231 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:04,231 (beam_search:483) INFO: best hypo: ▁THEY▁ARE▁AND▁VERY

2024-10-27 17:43:04,233 (asr_inference:509) INFO: speech length: 63720
2024-10-27 17:43:06,530 (beam_search:428) INFO: decoder input length: 49
2024-10-27 17:43:06,530 (beam_search:429) INFO: max output length: 49
2024-10-27 17:43:06,530 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:06,557 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:06,557 (beam_search:476) INFO:  -0.29 * 1.0 =  -0.29 for ctc
2024-10-27 17:43:06,557 (beam_search:479) INFO: total log probability: -0.29
2024-10-27 17:43:06,557 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:43:06,557 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:06,557 (beam_search:483) INFO: best hypo: ▁THEY▁WOULD

2024-10-27 17:43:06,560 (asr_inference:509) INFO: speech length: 192753
2024-10-27 17:43:14,532 (beam_search:428) INFO: decoder input length: 150
2024-10-27 17:43:14,532 (beam_search:429) INFO: max output length: 150
2024-10-27 17:43:14,532 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:15,010 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:15,010 (beam_search:476) INFO:  -3.59 * 1.0 =  -3.59 for ctc
2024-10-27 17:43:15,010 (beam_search:479) INFO: total log probability: -3.59
2024-10-27 17:43:15,010 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:43:15,010 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:15,010 (beam_search:483) INFO: best hypo: ▁THE▁EACH▁OTHER▁BECAUSE▁YOU▁THE▁AND▁OR▁AND▁CANT▁GO▁TOGETHER▁I▁YOU▁WERE▁THAN▁THIS

2024-10-27 17:43:15,013 (asr_inference:509) INFO: speech length: 170536
2024-10-27 17:43:21,613 (beam_search:428) INFO: decoder input length: 132
2024-10-27 17:43:21,613 (beam_search:429) INFO: max output length: 132
2024-10-27 17:43:21,613 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:21,817 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:21,817 (beam_search:476) INFO:  -3.95 * 1.0 =  -3.95 for ctc
2024-10-27 17:43:21,817 (beam_search:479) INFO: total log probability: -3.95
2024-10-27 17:43:21,817 (beam_search:480) INFO: normalized log probability: -0.36
2024-10-27 17:43:21,817 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:21,817 (beam_search:483) INFO: best hypo: ▁AND▁NORTH▁AND▁AND▁THE▁OF▁I▁A▁YOU

2024-10-27 17:43:21,819 (asr_inference:509) INFO: speech length: 69713
2024-10-27 17:43:24,376 (beam_search:428) INFO: decoder input length: 53
2024-10-27 17:43:24,376 (beam_search:429) INFO: max output length: 53
2024-10-27 17:43:24,376 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:24,420 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:24,421 (beam_search:476) INFO:  -1.33 * 1.0 =  -1.33 for ctc
2024-10-27 17:43:24,421 (beam_search:479) INFO: total log probability: -1.33
2024-10-27 17:43:24,421 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:43:24,421 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:24,421 (beam_search:483) INFO: best hypo: ▁ITS▁AND▁AND

2024-10-27 17:43:24,424 (asr_inference:509) INFO: speech length: 121814
2024-10-27 17:43:28,825 (beam_search:428) INFO: decoder input length: 94
2024-10-27 17:43:28,825 (beam_search:429) INFO: max output length: 94
2024-10-27 17:43:28,825 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:28,966 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:28,966 (beam_search:476) INFO:  -3.59 * 1.0 =  -3.59 for ctc
2024-10-27 17:43:28,966 (beam_search:479) INFO: total log probability: -3.59
2024-10-27 17:43:28,966 (beam_search:480) INFO: normalized log probability: -0.36
2024-10-27 17:43:28,966 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:28,966 (beam_search:483) INFO: best hypo: ▁SOME▁ARE▁THE▁OF▁I▁A▁YOU▁MUCH

2024-10-27 17:43:28,968 (asr_inference:509) INFO: speech length: 315781
2024-10-27 17:43:42,548 (beam_search:428) INFO: decoder input length: 246
2024-10-27 17:43:42,548 (beam_search:429) INFO: max output length: 246
2024-10-27 17:43:42,548 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:43,874 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:43,874 (beam_search:476) INFO:  -8.39 * 1.0 =  -8.39 for ctc
2024-10-27 17:43:43,874 (beam_search:479) INFO: total log probability: -8.39
2024-10-27 17:43:43,874 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:43:43,874 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:43,874 (beam_search:483) INFO: best hypo: ▁THE▁NORTH▁SIDE▁OF▁THE▁AND▁THE▁SOUTH▁THE▁OF▁THE▁BECAUSE▁WE▁HAVE▁BECAUSE▁THE▁NORTH▁THE▁NORTH▁AND▁THE▁ARE▁MAGNETIC▁TO▁US▁FROM▁THES▁SOLAR

2024-10-27 17:43:43,878 (asr_inference:509) INFO: speech length: 75041
2024-10-27 17:43:46,637 (beam_search:428) INFO: decoder input length: 58
2024-10-27 17:43:46,637 (beam_search:429) INFO: max output length: 58
2024-10-27 17:43:46,638 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:46,674 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:46,674 (beam_search:476) INFO:  -1.31 * 1.0 =  -1.31 for ctc
2024-10-27 17:43:46,674 (beam_search:479) INFO: total log probability: -1.31
2024-10-27 17:43:46,674 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:43:46,674 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:46,674 (beam_search:483) INFO: best hypo: ▁A▁WITH▁A

2024-10-27 17:43:46,676 (asr_inference:509) INFO: speech length: 166169
2024-10-27 17:43:53,160 (beam_search:428) INFO: decoder input length: 129
2024-10-27 17:43:53,160 (beam_search:429) INFO: max output length: 129
2024-10-27 17:43:53,160 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:53,331 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:53,331 (beam_search:476) INFO:  -2.55 * 1.0 =  -2.55 for ctc
2024-10-27 17:43:53,331 (beam_search:479) INFO: total log probability: -2.55
2024-10-27 17:43:53,331 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 17:43:53,331 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:53,331 (beam_search:483) INFO: best hypo: ▁IT▁IS▁A▁MAGNETIC▁AND▁IT▁TO

2024-10-27 17:43:53,334 (asr_inference:509) INFO: speech length: 35952
2024-10-27 17:43:54,761 (beam_search:428) INFO: decoder input length: 27
2024-10-27 17:43:54,761 (beam_search:429) INFO: max output length: 27
2024-10-27 17:43:54,761 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:54,780 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:54,780 (beam_search:476) INFO:  -0.38 * 1.0 =  -0.38 for ctc
2024-10-27 17:43:54,780 (beam_search:479) INFO: total log probability: -0.38
2024-10-27 17:43:54,781 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:43:54,781 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:54,781 (beam_search:483) INFO: best hypo: ▁BECAUSE▁OF▁THE

2024-10-27 17:43:54,782 (asr_inference:509) INFO: speech length: 59510
2024-10-27 17:43:56,975 (beam_search:428) INFO: decoder input length: 45
2024-10-27 17:43:56,975 (beam_search:429) INFO: max output length: 45
2024-10-27 17:43:56,975 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:57,016 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:57,016 (beam_search:476) INFO:  -0.52 * 1.0 =  -0.52 for ctc
2024-10-27 17:43:57,016 (beam_search:479) INFO: total log probability: -0.52
2024-10-27 17:43:57,016 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:43:57,016 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:57,016 (beam_search:483) INFO: best hypo: ▁IT▁IS▁GOING▁THROUGH▁THE

2024-10-27 17:43:57,018 (asr_inference:509) INFO: speech length: 45095
2024-10-27 17:43:58,723 (beam_search:428) INFO: decoder input length: 34
2024-10-27 17:43:58,723 (beam_search:429) INFO: max output length: 34
2024-10-27 17:43:58,723 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:58,751 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:58,751 (beam_search:476) INFO:  -0.13 * 1.0 =  -0.13 for ctc
2024-10-27 17:43:58,751 (beam_search:479) INFO: total log probability: -0.13
2024-10-27 17:43:58,751 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 17:43:58,751 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:58,751 (beam_search:483) INFO: best hypo: ▁BECAUSE▁THERE▁IS▁LIKE

2024-10-27 17:43:58,753 (asr_inference:509) INFO: speech length: 57820
2024-10-27 17:44:00,802 (beam_search:428) INFO: decoder input length: 44
2024-10-27 17:44:00,802 (beam_search:429) INFO: max output length: 44
2024-10-27 17:44:00,802 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:00,835 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:00,836 (beam_search:476) INFO:  -0.29 * 1.0 =  -0.29 for ctc
2024-10-27 17:44:00,836 (beam_search:479) INFO: total log probability: -0.29
2024-10-27 17:44:00,836 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:44:00,836 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:00,836 (beam_search:483) INFO: best hypo: ▁BECAUSE▁THEY▁HAVE▁AND

2024-10-27 17:44:00,838 (asr_inference:509) INFO: speech length: 179151
2024-10-27 17:44:07,809 (beam_search:428) INFO: decoder input length: 139
2024-10-27 17:44:07,809 (beam_search:429) INFO: max output length: 139
2024-10-27 17:44:07,809 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:08,146 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:08,146 (beam_search:476) INFO:  -2.26 * 1.0 =  -2.26 for ctc
2024-10-27 17:44:08,146 (beam_search:479) INFO: total log probability: -2.26
2024-10-27 17:44:08,146 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:44:08,146 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:08,146 (beam_search:483) INFO: best hypo: ▁THE▁WOULDN'T▁BUT▁THE▁WOULD▁BECAUSE▁IT'S▁FOR▁MAGNETISM▁TO

2024-10-27 17:44:08,149 (asr_inference:509) INFO: speech length: 182457
2024-10-27 17:44:15,296 (beam_search:428) INFO: decoder input length: 142
2024-10-27 17:44:15,297 (beam_search:429) INFO: max output length: 142
2024-10-27 17:44:15,297 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:15,760 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:15,760 (beam_search:476) INFO:  -2.56 * 1.0 =  -2.56 for ctc
2024-10-27 17:44:15,760 (beam_search:479) INFO: total log probability: -2.56
2024-10-27 17:44:15,761 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:44:15,761 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:15,761 (beam_search:483) INFO: best hypo: ▁IT▁WORKED▁WELL▁THE▁ONE▁DIDN'T▁WORK▁AND▁THE▁I▁ONE▁YEAHT▁I▁MEAN▁DID

2024-10-27 17:44:15,763 (asr_inference:509) INFO: speech length: 81499
2024-10-27 17:44:18,664 (beam_search:428) INFO: decoder input length: 63
2024-10-27 17:44:18,664 (beam_search:429) INFO: max output length: 63
2024-10-27 17:44:18,664 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:18,741 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:18,741 (beam_search:476) INFO:  -1.91 * 1.0 =  -1.91 for ctc
2024-10-27 17:44:18,741 (beam_search:479) INFO: total log probability: -1.91
2024-10-27 17:44:18,741 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:44:18,741 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:18,741 (beam_search:483) INFO: best hypo: ▁ITS▁TO▁GO▁THROUGH▁BECAUSE▁THE

2024-10-27 17:44:18,743 (asr_inference:509) INFO: speech length: 35953
2024-10-27 17:44:20,173 (beam_search:428) INFO: decoder input length: 27
2024-10-27 17:44:20,173 (beam_search:429) INFO: max output length: 27
2024-10-27 17:44:20,173 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:20,191 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:20,191 (beam_search:476) INFO:  -1.07 * 1.0 =  -1.07 for ctc
2024-10-27 17:44:20,191 (beam_search:479) INFO: total log probability: -1.07
2024-10-27 17:44:20,191 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:44:20,191 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:20,191 (beam_search:483) INFO: best hypo: ▁IT▁IS▁NOT

2024-10-27 17:44:20,194 (asr_inference:509) INFO: speech length: 224293
2024-10-27 17:44:29,316 (beam_search:428) INFO: decoder input length: 174
2024-10-27 17:44:29,316 (beam_search:429) INFO: max output length: 174
2024-10-27 17:44:29,316 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:29,791 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:29,791 (beam_search:476) INFO:  -6.75 * 1.0 =  -6.75 for ctc
2024-10-27 17:44:29,791 (beam_search:479) INFO: total log probability: -6.75
2024-10-27 17:44:29,791 (beam_search:480) INFO: normalized log probability: -0.40
2024-10-27 17:44:29,791 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:29,791 (beam_search:483) INFO: best hypo: ▁THE▁ISING▁THROUGH▁THE▁NAIL▁WHEN▁YOU▁THE▁SOME▁IN▁THE▁NAIL▁THE▁STICK

2024-10-27 17:44:29,793 (asr_inference:509) INFO: speech length: 41375
2024-10-27 17:44:31,345 (beam_search:428) INFO: decoder input length: 31
2024-10-27 17:44:31,345 (beam_search:429) INFO: max output length: 31
2024-10-27 17:44:31,345 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:31,369 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:31,369 (beam_search:476) INFO:  -1.35 * 1.0 =  -1.35 for ctc
2024-10-27 17:44:31,369 (beam_search:479) INFO: total log probability: -1.35
2024-10-27 17:44:31,369 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:44:31,369 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:31,369 (beam_search:483) INFO: best hypo: ▁WE▁THAT▁IN▁SCIENCE

2024-10-27 17:44:31,371 (asr_inference:509) INFO: speech length: 44802
2024-10-27 17:44:33,069 (beam_search:428) INFO: decoder input length: 34
2024-10-27 17:44:33,069 (beam_search:429) INFO: max output length: 34
2024-10-27 17:44:33,069 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:33,086 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:33,087 (beam_search:476) INFO:  -0.56 * 1.0 =  -0.56 for ctc
2024-10-27 17:44:33,087 (beam_search:479) INFO: total log probability: -0.56
2024-10-27 17:44:33,087 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:44:33,087 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:33,087 (beam_search:483) INFO: best hypo: ▁THEY▁ARE

2024-10-27 17:44:33,089 (asr_inference:509) INFO: speech length: 54165
2024-10-27 17:44:35,173 (beam_search:428) INFO: decoder input length: 41
2024-10-27 17:44:35,173 (beam_search:429) INFO: max output length: 41
2024-10-27 17:44:35,173 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:35,189 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:35,189 (beam_search:476) INFO:  -0.31 * 1.0 =  -0.31 for ctc
2024-10-27 17:44:35,189 (beam_search:479) INFO: total log probability: -0.31
2024-10-27 17:44:35,189 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:44:35,189 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:35,189 (beam_search:483) INFO: best hypo: ▁AND

2024-10-27 17:44:35,191 (asr_inference:509) INFO: speech length: 50938
2024-10-27 17:44:37,217 (beam_search:428) INFO: decoder input length: 39
2024-10-27 17:44:37,217 (beam_search:429) INFO: max output length: 39
2024-10-27 17:44:37,217 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:37,228 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:37,229 (beam_search:476) INFO:  -0.07 * 1.0 =  -0.07 for ctc
2024-10-27 17:44:37,229 (beam_search:479) INFO: total log probability: -0.07
2024-10-27 17:44:37,229 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:44:37,229 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:37,229 (beam_search:483) INFO: best hypo: 

2024-10-27 17:44:37,231 (asr_inference:509) INFO: speech length: 115447
2024-10-27 17:44:41,525 (beam_search:428) INFO: decoder input length: 89
2024-10-27 17:44:41,525 (beam_search:429) INFO: max output length: 89
2024-10-27 17:44:41,525 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:41,607 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:41,607 (beam_search:476) INFO:  -1.39 * 1.0 =  -1.39 for ctc
2024-10-27 17:44:41,607 (beam_search:479) INFO: total log probability: -1.39
2024-10-27 17:44:41,607 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:44:41,607 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:41,607 (beam_search:483) INFO: best hypo: ▁IT▁WOULD▁NOT▁NO▁MORE

2024-10-27 17:44:41,609 (asr_inference:509) INFO: speech length: 36431
2024-10-27 17:44:42,991 (beam_search:428) INFO: decoder input length: 27
2024-10-27 17:44:42,991 (beam_search:429) INFO: max output length: 27
2024-10-27 17:44:42,991 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:43,005 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:43,005 (beam_search:476) INFO:  -0.44 * 1.0 =  -0.44 for ctc
2024-10-27 17:44:43,005 (beam_search:479) INFO: total log probability: -0.44
2024-10-27 17:44:43,005 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:44:43,005 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:43,005 (beam_search:483) INFO: best hypo: ▁NOT▁MUCH

2024-10-27 17:44:43,007 (asr_inference:509) INFO: speech length: 20897
2024-10-27 17:44:43,998 (beam_search:428) INFO: decoder input length: 15
2024-10-27 17:44:43,998 (beam_search:429) INFO: max output length: 15
2024-10-27 17:44:43,998 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:44,015 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:44,015 (beam_search:476) INFO:  -0.02 * 1.0 =  -0.02 for ctc
2024-10-27 17:44:44,015 (beam_search:479) INFO: total log probability: -0.02
2024-10-27 17:44:44,015 (beam_search:480) INFO: normalized log probability: -0.00
2024-10-27 17:44:44,015 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:44,015 (beam_search:483) INFO: best hypo: ▁I▁DON'T▁KNOW

2024-10-27 17:44:44,017 (asr_inference:509) INFO: speech length: 55027
2024-10-27 17:44:46,210 (beam_search:428) INFO: decoder input length: 42
2024-10-27 17:44:46,210 (beam_search:429) INFO: max output length: 42
2024-10-27 17:44:46,210 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:46,241 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:46,242 (beam_search:476) INFO:  -2.04 * 1.0 =  -2.04 for ctc
2024-10-27 17:44:46,242 (beam_search:479) INFO: total log probability: -2.04
2024-10-27 17:44:46,242 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 17:44:46,242 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:46,242 (beam_search:483) INFO: best hypo: ▁THE▁OF▁IS▁GOING

2024-10-27 17:44:46,244 (asr_inference:509) INFO: speech length: 123157
2024-10-27 17:44:50,728 (beam_search:428) INFO: decoder input length: 95
2024-10-27 17:44:50,728 (beam_search:429) INFO: max output length: 95
2024-10-27 17:44:50,728 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:50,857 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:50,857 (beam_search:476) INFO:  -2.11 * 1.0 =  -2.11 for ctc
2024-10-27 17:44:50,857 (beam_search:479) INFO: total log probability: -2.11
2024-10-27 17:44:50,857 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:44:50,857 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:50,857 (beam_search:483) INFO: best hypo: ▁THE▁MAGNET▁TURNS▁OFF▁ITS▁OFF

2024-10-27 17:44:50,859 (asr_inference:509) INFO: speech length: 44508
2024-10-27 17:44:52,666 (beam_search:428) INFO: decoder input length: 34
2024-10-27 17:44:52,667 (beam_search:429) INFO: max output length: 34
2024-10-27 17:44:52,667 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:52,705 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:52,705 (beam_search:476) INFO:  -0.39 * 1.0 =  -0.39 for ctc
2024-10-27 17:44:52,705 (beam_search:479) INFO: total log probability: -0.39
2024-10-27 17:44:52,705 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:44:52,705 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:52,705 (beam_search:483) INFO: best hypo: ▁DON'T▁DON'T

2024-10-27 17:44:52,707 (asr_inference:509) INFO: speech length: 44498
2024-10-27 17:44:54,411 (beam_search:428) INFO: decoder input length: 34
2024-10-27 17:44:54,411 (beam_search:429) INFO: max output length: 34
2024-10-27 17:44:54,411 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:54,429 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:54,429 (beam_search:476) INFO:  -0.59 * 1.0 =  -0.59 for ctc
2024-10-27 17:44:54,429 (beam_search:479) INFO: total log probability: -0.59
2024-10-27 17:44:54,429 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:44:54,429 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:54,429 (beam_search:483) INFO: best hypo: ▁IT▁IS

2024-10-27 17:44:54,432 (asr_inference:509) INFO: speech length: 194705
2024-10-27 17:45:02,030 (beam_search:428) INFO: decoder input length: 151
2024-10-27 17:45:02,030 (beam_search:429) INFO: max output length: 151
2024-10-27 17:45:02,030 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:02,395 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:02,395 (beam_search:476) INFO:  -3.11 * 1.0 =  -3.11 for ctc
2024-10-27 17:45:02,395 (beam_search:479) INFO: total log probability: -3.11
2024-10-27 17:45:02,395 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:45:02,395 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:02,395 (beam_search:483) INFO: best hypo: ▁THERE▁ARE▁WASHERS▁IN▁THE▁OTHER▁THAT▁ARE▁TO▁THE▁MAGNETIC▁OF▁THE▁TWO

2024-10-27 17:45:02,398 (asr_inference:509) INFO: speech length: 26869
2024-10-27 17:45:03,564 (beam_search:428) INFO: decoder input length: 20
2024-10-27 17:45:03,564 (beam_search:429) INFO: max output length: 20
2024-10-27 17:45:03,564 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:03,572 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:03,573 (beam_search:476) INFO:  -0.64 * 1.0 =  -0.64 for ctc
2024-10-27 17:45:03,573 (beam_search:479) INFO: total log probability: -0.64
2024-10-27 17:45:03,573 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:45:03,573 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:03,573 (beam_search:483) INFO: best hypo: ▁I

2024-10-27 17:45:03,575 (asr_inference:509) INFO: speech length: 156229
2024-10-27 17:45:09,383 (beam_search:428) INFO: decoder input length: 121
2024-10-27 17:45:09,384 (beam_search:429) INFO: max output length: 121
2024-10-27 17:45:09,384 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:09,519 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:09,519 (beam_search:476) INFO:  -2.39 * 1.0 =  -2.39 for ctc
2024-10-27 17:45:09,519 (beam_search:479) INFO: total log probability: -2.39
2024-10-27 17:45:09,519 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 17:45:09,519 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:09,519 (beam_search:483) INFO: best hypo: ▁I▁THAT▁THE▁ARE▁LIKE▁THE

2024-10-27 17:45:09,522 (asr_inference:509) INFO: speech length: 47746
2024-10-27 17:45:11,353 (beam_search:428) INFO: decoder input length: 36
2024-10-27 17:45:11,353 (beam_search:429) INFO: max output length: 36
2024-10-27 17:45:11,353 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:11,367 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:11,367 (beam_search:476) INFO:  -0.68 * 1.0 =  -0.68 for ctc
2024-10-27 17:45:11,367 (beam_search:479) INFO: total log probability: -0.68
2024-10-27 17:45:11,367 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:45:11,367 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:11,367 (beam_search:483) INFO: best hypo: ▁IS

2024-10-27 17:45:11,369 (asr_inference:509) INFO: speech length: 62159
2024-10-27 17:45:13,765 (beam_search:428) INFO: decoder input length: 48
2024-10-27 17:45:13,765 (beam_search:429) INFO: max output length: 48
2024-10-27 17:45:13,765 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:13,776 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:13,776 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 17:45:13,776 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 17:45:13,776 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 17:45:13,776 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:13,776 (beam_search:483) INFO: best hypo: 

2024-10-27 17:45:13,778 (asr_inference:509) INFO: speech length: 27865
2024-10-27 17:45:14,945 (beam_search:428) INFO: decoder input length: 21
2024-10-27 17:45:14,946 (beam_search:429) INFO: max output length: 21
2024-10-27 17:45:14,946 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:14,964 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:14,964 (beam_search:476) INFO:  -0.94 * 1.0 =  -0.94 for ctc
2024-10-27 17:45:14,964 (beam_search:479) INFO: total log probability: -0.94
2024-10-27 17:45:14,964 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:45:14,964 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:14,964 (beam_search:483) INFO: best hypo: ▁WHAT▁IS▁ALL▁ABOUT

2024-10-27 17:45:14,966 (asr_inference:509) INFO: speech length: 72495
2024-10-27 17:45:17,663 (beam_search:428) INFO: decoder input length: 56
2024-10-27 17:45:17,664 (beam_search:429) INFO: max output length: 56
2024-10-27 17:45:17,664 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:17,732 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:17,733 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 17:45:17,733 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 17:45:17,733 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:45:17,733 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:17,733 (beam_search:483) INFO: best hypo: ▁HOW▁CAN▁I▁PUT▁THEM▁TOGETHER▁PLEASE

2024-10-27 17:45:17,735 (asr_inference:509) INFO: speech length: 69064
2024-10-27 17:45:20,298 (beam_search:428) INFO: decoder input length: 53
2024-10-27 17:45:20,298 (beam_search:429) INFO: max output length: 53
2024-10-27 17:45:20,298 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:20,340 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:20,340 (beam_search:476) INFO:  -0.76 * 1.0 =  -0.76 for ctc
2024-10-27 17:45:20,340 (beam_search:479) INFO: total log probability: -0.76
2024-10-27 17:45:20,340 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:45:20,340 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:20,340 (beam_search:483) INFO: best hypo: ▁THEN▁SOMETHING▁WILL▁I

2024-10-27 17:45:20,342 (asr_inference:509) INFO: speech length: 92547
2024-10-27 17:45:23,587 (beam_search:428) INFO: decoder input length: 71
2024-10-27 17:45:23,587 (beam_search:429) INFO: max output length: 71
2024-10-27 17:45:23,587 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:23,704 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:23,704 (beam_search:476) INFO:  -2.20 * 1.0 =  -2.20 for ctc
2024-10-27 17:45:23,704 (beam_search:479) INFO: total log probability: -2.20
2024-10-27 17:45:23,704 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:45:23,704 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:23,704 (beam_search:483) INFO: best hypo: ▁I▁WHAT▁YOU'S▁TALKING▁ABOUT▁YOU▁OF▁ME

2024-10-27 17:45:23,706 (asr_inference:509) INFO: speech length: 124649
2024-10-27 17:45:28,233 (beam_search:428) INFO: decoder input length: 96
2024-10-27 17:45:28,233 (beam_search:429) INFO: max output length: 96
2024-10-27 17:45:28,233 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:28,435 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:28,435 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 17:45:28,435 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 17:45:28,435 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:45:28,435 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:28,435 (beam_search:483) INFO: best hypo: ▁I▁DIDN'T▁SAY▁THAT▁AT▁ALL▁I▁DID▁SAY▁THAT

2024-10-27 17:45:28,437 (asr_inference:509) INFO: speech length: 187820
2024-10-27 17:45:35,816 (beam_search:428) INFO: decoder input length: 146
2024-10-27 17:45:35,816 (beam_search:429) INFO: max output length: 146
2024-10-27 17:45:35,816 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:36,003 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:36,003 (beam_search:476) INFO:  -2.82 * 1.0 =  -2.82 for ctc
2024-10-27 17:45:36,003 (beam_search:479) INFO: total log probability: -2.82
2024-10-27 17:45:36,003 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 17:45:36,003 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:36,003 (beam_search:483) INFO: best hypo: ▁SHOW▁I▁TO▁THE▁THATS▁IT

2024-10-27 17:45:36,006 (asr_inference:509) INFO: speech length: 144332
2024-10-27 17:45:41,639 (beam_search:428) INFO: decoder input length: 112
2024-10-27 17:45:41,639 (beam_search:429) INFO: max output length: 112
2024-10-27 17:45:41,639 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:41,923 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:41,924 (beam_search:476) INFO:  -1.87 * 1.0 =  -1.87 for ctc
2024-10-27 17:45:41,924 (beam_search:479) INFO: total log probability: -1.87
2024-10-27 17:45:41,924 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:45:41,924 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:41,924 (beam_search:483) INFO: best hypo: ▁THAT'S▁WHY▁IT▁WAS'T▁BECAUSE▁IT▁THE▁GET▁IT

2024-10-27 17:45:41,926 (asr_inference:509) INFO: speech length: 198047
2024-10-27 17:45:49,649 (beam_search:428) INFO: decoder input length: 154
2024-10-27 17:45:49,649 (beam_search:429) INFO: max output length: 154
2024-10-27 17:45:49,649 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:50,231 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:50,232 (beam_search:476) INFO:  -3.23 * 1.0 =  -3.23 for ctc
2024-10-27 17:45:50,232 (beam_search:479) INFO: total log probability: -3.23
2024-10-27 17:45:50,232 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:45:50,232 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:50,232 (beam_search:483) INFO: best hypo: S▁NO▁I▁JUST▁DO▁THIS▁AND▁THEN▁YOU▁AND▁YOU▁MY▁HAND▁YOU▁DON'T▁TO▁DO▁IT

2024-10-27 17:45:50,234 (asr_inference:509) INFO: speech length: 59461
2024-10-27 17:45:52,483 (beam_search:428) INFO: decoder input length: 45
2024-10-27 17:45:52,483 (beam_search:429) INFO: max output length: 45
2024-10-27 17:45:52,483 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:52,529 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:52,529 (beam_search:476) INFO:  -0.78 * 1.0 =  -0.78 for ctc
2024-10-27 17:45:52,529 (beam_search:479) INFO: total log probability: -0.78
2024-10-27 17:45:52,529 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:45:52,529 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:52,529 (beam_search:483) INFO: best hypo: ▁IT'S▁NOT▁A▁MY

2024-10-27 17:45:52,532 (asr_inference:509) INFO: speech length: 151906
2024-10-27 17:45:57,954 (beam_search:428) INFO: decoder input length: 118
2024-10-27 17:45:57,955 (beam_search:429) INFO: max output length: 118
2024-10-27 17:45:57,955 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:58,144 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:58,145 (beam_search:476) INFO:  -2.86 * 1.0 =  -2.86 for ctc
2024-10-27 17:45:58,145 (beam_search:479) INFO: total log probability: -2.86
2024-10-27 17:45:58,145 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:45:58,145 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:58,145 (beam_search:483) INFO: best hypo: ▁SORRY▁NOT▁HOW▁IT▁GOES▁IT▁YOU'RE

2024-10-27 17:45:58,147 (asr_inference:509) INFO: speech length: 216874
2024-10-27 17:46:07,129 (beam_search:428) INFO: decoder input length: 168
2024-10-27 17:46:07,129 (beam_search:429) INFO: max output length: 168
2024-10-27 17:46:07,129 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:07,331 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:07,331 (beam_search:476) INFO:  -1.41 * 1.0 =  -1.41 for ctc
2024-10-27 17:46:07,331 (beam_search:479) INFO: total log probability: -1.41
2024-10-27 17:46:07,332 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:46:07,332 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:07,332 (beam_search:483) INFO: best hypo: ▁IT▁HAS▁A▁LIKE▁AND▁TOGETHER

2024-10-27 17:46:07,335 (asr_inference:509) INFO: speech length: 93108
2024-10-27 17:46:10,951 (beam_search:428) INFO: decoder input length: 72
2024-10-27 17:46:10,951 (beam_search:429) INFO: max output length: 72
2024-10-27 17:46:10,951 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:10,986 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:10,986 (beam_search:476) INFO:  -1.16 * 1.0 =  -1.16 for ctc
2024-10-27 17:46:10,986 (beam_search:479) INFO: total log probability: -1.16
2024-10-27 17:46:10,986 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 17:46:10,986 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:10,986 (beam_search:483) INFO: best hypo: ▁WITH▁A

2024-10-27 17:46:10,989 (asr_inference:509) INFO: speech length: 157709
2024-10-27 17:46:16,695 (beam_search:428) INFO: decoder input length: 122
2024-10-27 17:46:16,695 (beam_search:429) INFO: max output length: 122
2024-10-27 17:46:16,695 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:16,886 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:16,887 (beam_search:476) INFO:  -1.84 * 1.0 =  -1.84 for ctc
2024-10-27 17:46:16,887 (beam_search:479) INFO: total log probability: -1.84
2024-10-27 17:46:16,887 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:46:16,887 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:16,887 (beam_search:483) INFO: best hypo: ▁WAS▁WITH▁A▁WAS▁WITHS▁WITH▁A

2024-10-27 17:46:16,890 (asr_inference:509) INFO: speech length: 30944
2024-10-27 17:46:18,192 (beam_search:428) INFO: decoder input length: 23
2024-10-27 17:46:18,192 (beam_search:429) INFO: max output length: 23
2024-10-27 17:46:18,192 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:18,212 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:18,212 (beam_search:476) INFO:  -0.39 * 1.0 =  -0.39 for ctc
2024-10-27 17:46:18,212 (beam_search:479) INFO: total log probability: -0.39
2024-10-27 17:46:18,212 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:46:18,212 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:18,212 (beam_search:483) INFO: best hypo: ▁IT▁GOES▁TO▁THE

2024-10-27 17:46:18,215 (asr_inference:509) INFO: speech length: 66586
2024-10-27 17:46:20,642 (beam_search:428) INFO: decoder input length: 51
2024-10-27 17:46:20,642 (beam_search:429) INFO: max output length: 51
2024-10-27 17:46:20,642 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:20,706 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:20,707 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 17:46:20,707 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 17:46:20,707 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:46:20,707 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:20,707 (beam_search:483) INFO: best hypo: ▁EVERYTHING▁THERE'S▁A▁IN▁YOU

2024-10-27 17:46:20,709 (asr_inference:509) INFO: speech length: 130418
2024-10-27 17:46:25,698 (beam_search:428) INFO: decoder input length: 101
2024-10-27 17:46:25,698 (beam_search:429) INFO: max output length: 101
2024-10-27 17:46:25,698 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:25,910 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:25,910 (beam_search:476) INFO:  -1.78 * 1.0 =  -1.78 for ctc
2024-10-27 17:46:25,910 (beam_search:479) INFO: total log probability: -1.78
2024-10-27 17:46:25,910 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:46:25,910 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:25,910 (beam_search:483) INFO: best hypo: ▁IM▁GOOD▁AND▁I▁HAVE▁BEEN▁ABOUT▁AND▁HOW▁THEY

2024-10-27 17:46:25,913 (asr_inference:509) INFO: speech length: 265623
2024-10-27 17:46:37,330 (beam_search:428) INFO: decoder input length: 207
2024-10-27 17:46:37,330 (beam_search:429) INFO: max output length: 207
2024-10-27 17:46:37,330 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:37,971 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:37,971 (beam_search:476) INFO:  -5.47 * 1.0 =  -5.47 for ctc
2024-10-27 17:46:37,971 (beam_search:479) INFO: total log probability: -5.47
2024-10-27 17:46:37,971 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 17:46:37,971 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:37,971 (beam_search:483) INFO: best hypo: ▁THEY▁ARE▁A▁IS▁IS▁THE▁THE▁WIRES▁THE▁ENERGY▁AND▁A▁LIGHT▁BULB▁THE▁OF▁ENERGY

2024-10-27 17:46:37,973 (asr_inference:509) INFO: speech length: 28270
2024-10-27 17:46:39,117 (beam_search:428) INFO: decoder input length: 21
2024-10-27 17:46:39,117 (beam_search:429) INFO: max output length: 21
2024-10-27 17:46:39,117 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:39,138 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:39,138 (beam_search:476) INFO:  -0.37 * 1.0 =  -0.37 for ctc
2024-10-27 17:46:39,138 (beam_search:479) INFO: total log probability: -0.37
2024-10-27 17:46:39,139 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:46:39,139 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:39,139 (beam_search:483) INFO: best hypo: ▁I'M▁NOT▁SURE

2024-10-27 17:46:39,141 (asr_inference:509) INFO: speech length: 19387
2024-10-27 17:46:39,993 (beam_search:428) INFO: decoder input length: 14
2024-10-27 17:46:39,993 (beam_search:429) INFO: max output length: 14
2024-10-27 17:46:39,993 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:40,004 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:40,004 (beam_search:476) INFO:  -1.21 * 1.0 =  -1.21 for ctc
2024-10-27 17:46:40,004 (beam_search:479) INFO: total log probability: -1.21
2024-10-27 17:46:40,004 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:46:40,004 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:40,004 (beam_search:483) INFO: best hypo: ▁A▁D▁BATTERY

2024-10-27 17:46:40,007 (asr_inference:509) INFO: speech length: 85956
2024-10-27 17:46:43,159 (beam_search:428) INFO: decoder input length: 66
2024-10-27 17:46:43,159 (beam_search:429) INFO: max output length: 66
2024-10-27 17:46:43,159 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:43,276 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:43,276 (beam_search:476) INFO:  -3.31 * 1.0 =  -3.31 for ctc
2024-10-27 17:46:43,276 (beam_search:479) INFO: total log probability: -3.31
2024-10-27 17:46:43,276 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 17:46:43,276 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:43,276 (beam_search:483) INFO: best hypo: ▁WIRES▁AND▁THEY▁THEY▁ENERGY▁FROM▁THE▁TO▁THE▁BULB

2024-10-27 17:46:43,278 (asr_inference:509) INFO: speech length: 159209
2024-10-27 17:46:50,006 (beam_search:428) INFO: decoder input length: 123
2024-10-27 17:46:50,006 (beam_search:429) INFO: max output length: 123
2024-10-27 17:46:50,006 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:50,277 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:50,277 (beam_search:476) INFO:  -2.09 * 1.0 =  -2.09 for ctc
2024-10-27 17:46:50,277 (beam_search:479) INFO: total log probability: -2.09
2024-10-27 17:46:50,277 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:46:50,277 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:50,277 (beam_search:483) INFO: best hypo: ▁THEY▁ARE▁THE▁SOURCE▁TO▁THE▁BATTERY▁TO▁LIGHT▁THE▁LIGHT▁BULB

2024-10-27 17:46:50,280 (asr_inference:509) INFO: speech length: 141831
2024-10-27 17:46:55,435 (beam_search:428) INFO: decoder input length: 110
2024-10-27 17:46:55,435 (beam_search:429) INFO: max output length: 110
2024-10-27 17:46:55,435 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:55,833 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:55,833 (beam_search:476) INFO:  -3.33 * 1.0 =  -3.33 for ctc
2024-10-27 17:46:55,833 (beam_search:479) INFO: total log probability: -3.33
2024-10-27 17:46:55,833 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:46:55,833 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:55,833 (beam_search:483) INFO: best hypo: ▁THAT▁THE▁D▁CELL▁IS▁A▁SOURCE▁OF▁ENERGY▁BECAUSE▁YOU▁CANT▁LIGHT▁A▁LIGHT▁BULB▁WITHOUT▁A▁OF▁ENERGY

2024-10-27 17:46:55,836 (asr_inference:509) INFO: speech length: 76508
2024-10-27 17:46:58,613 (beam_search:428) INFO: decoder input length: 59
2024-10-27 17:46:58,613 (beam_search:429) INFO: max output length: 59
2024-10-27 17:46:58,613 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:58,667 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:58,667 (beam_search:476) INFO:  -1.64 * 1.0 =  -1.64 for ctc
2024-10-27 17:46:58,668 (beam_search:479) INFO: total log probability: -1.64
2024-10-27 17:46:58,668 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:46:58,668 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:58,668 (beam_search:483) INFO: best hypo: ▁A▁D▁ENERGY▁IN▁A

2024-10-27 17:46:58,670 (asr_inference:509) INFO: speech length: 84514
2024-10-27 17:47:01,756 (beam_search:428) INFO: decoder input length: 65
2024-10-27 17:47:01,756 (beam_search:429) INFO: max output length: 65
2024-10-27 17:47:01,756 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:01,829 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:01,829 (beam_search:476) INFO:  -4.00 * 1.0 =  -4.00 for ctc
2024-10-27 17:47:01,829 (beam_search:479) INFO: total log probability: -4.00
2024-10-27 17:47:01,829 (beam_search:480) INFO: normalized log probability: -0.50
2024-10-27 17:47:01,829 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:01,829 (beam_search:483) INFO: best hypo: ▁THEY▁ENERGY▁FROM▁THE▁TO▁THE

2024-10-27 17:47:01,832 (asr_inference:509) INFO: speech length: 192302
2024-10-27 17:47:09,600 (beam_search:428) INFO: decoder input length: 149
2024-10-27 17:47:09,600 (beam_search:429) INFO: max output length: 149
2024-10-27 17:47:09,600 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:10,153 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:10,153 (beam_search:476) INFO:  -3.56 * 1.0 =  -3.56 for ctc
2024-10-27 17:47:10,153 (beam_search:479) INFO: total log probability: -3.56
2024-10-27 17:47:10,153 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:47:10,153 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:10,153 (beam_search:483) INFO: best hypo: ▁IT▁THAT▁THE▁WIRES▁ARE▁A▁PATHWAY▁SO▁THAT▁THE▁ENERGY▁FROM▁THE▁D▁CAN▁MOVE▁TO▁THE▁LIGHT▁BULB

2024-10-27 17:47:10,156 (asr_inference:509) INFO: speech length: 64758
2024-10-27 17:47:12,494 (beam_search:428) INFO: decoder input length: 50
2024-10-27 17:47:12,494 (beam_search:429) INFO: max output length: 50
2024-10-27 17:47:12,494 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:12,539 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:12,540 (beam_search:476) INFO:  -0.62 * 1.0 =  -0.62 for ctc
2024-10-27 17:47:12,540 (beam_search:479) INFO: total log probability: -0.62
2024-10-27 17:47:12,540 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:47:12,540 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:12,540 (beam_search:483) INFO: best hypo: ▁WIRES▁ELECTRICITY▁IN▁A▁CIRCUIT

2024-10-27 17:47:12,542 (asr_inference:509) INFO: speech length: 192292
2024-10-27 17:47:20,275 (beam_search:428) INFO: decoder input length: 149
2024-10-27 17:47:20,275 (beam_search:429) INFO: max output length: 149
2024-10-27 17:47:20,275 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:20,764 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:20,765 (beam_search:476) INFO:  -2.82 * 1.0 =  -2.82 for ctc
2024-10-27 17:47:20,765 (beam_search:479) INFO: total log probability: -2.82
2024-10-27 17:47:20,765 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:47:20,765 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:20,765 (beam_search:483) INFO: best hypo: ▁MAKE▁MAKE▁LIGHT▁BY▁USING▁A▁THES▁UP▁UNTIL▁IT'S▁ENOUGH▁THAT▁IT

2024-10-27 17:47:20,768 (asr_inference:509) INFO: speech length: 351257
2024-10-27 17:47:36,791 (beam_search:428) INFO: decoder input length: 273
2024-10-27 17:47:36,792 (beam_search:429) INFO: max output length: 273
2024-10-27 17:47:36,792 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:38,227 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:38,228 (beam_search:476) INFO:  -7.78 * 1.0 =  -7.78 for ctc
2024-10-27 17:47:38,228 (beam_search:479) INFO: total log probability: -7.78
2024-10-27 17:47:38,228 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:47:38,228 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:38,228 (beam_search:483) INFO: best hypo: ▁THEY▁USE▁THE▁D▁TO▁TO▁TO▁ENERGY▁THROUGH▁THE▁AND▁THEN▁THE▁ENERGY▁THE▁ENERGY▁IS▁GOING▁THROUGH▁IT▁MAKES▁WHEN▁IT'S▁THE▁IS▁IT

2024-10-27 17:47:38,231 (asr_inference:509) INFO: speech length: 106944
2024-10-27 17:47:42,046 (beam_search:428) INFO: decoder input length: 83
2024-10-27 17:47:42,046 (beam_search:429) INFO: max output length: 83
2024-10-27 17:47:42,047 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:42,183 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:42,183 (beam_search:476) INFO:  -1.57 * 1.0 =  -1.57 for ctc
2024-10-27 17:47:42,183 (beam_search:479) INFO: total log probability: -1.57
2024-10-27 17:47:42,183 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:47:42,183 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:42,183 (beam_search:483) INFO: best hypo: ▁IT▁THAT▁ITS▁THE▁ENERGY▁ANDS▁INTO

2024-10-27 17:47:42,185 (asr_inference:509) INFO: speech length: 259507
2024-10-27 17:47:52,769 (beam_search:428) INFO: decoder input length: 202
2024-10-27 17:47:52,769 (beam_search:429) INFO: max output length: 202
2024-10-27 17:47:52,769 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:53,290 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:53,290 (beam_search:476) INFO:  -2.17 * 1.0 =  -2.17 for ctc
2024-10-27 17:47:53,290 (beam_search:479) INFO: total log probability: -2.17
2024-10-27 17:47:53,291 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:47:53,291 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:53,291 (beam_search:483) INFO: best hypo: ▁IT▁THAT▁THE▁ENERGY▁IS▁THROUGH▁AND▁THAT▁THE▁ENERGY▁IS▁INTO▁ENERGY▁ENERGY

2024-10-27 17:47:53,294 (asr_inference:509) INFO: speech length: 186079
2024-10-27 17:48:00,692 (beam_search:428) INFO: decoder input length: 144
2024-10-27 17:48:00,692 (beam_search:429) INFO: max output length: 144
2024-10-27 17:48:00,692 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:01,418 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:01,418 (beam_search:476) INFO:  -1.96 * 1.0 =  -1.96 for ctc
2024-10-27 17:48:01,418 (beam_search:479) INFO: total log probability: -1.96
2024-10-27 17:48:01,418 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:48:01,418 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:01,418 (beam_search:483) INFO: best hypo: ▁I▁SEE▁THAT▁THE▁I▁SEE▁THAT▁THE▁ENERGY▁IS▁THROUGH▁THE▁WIRES▁TO▁THE▁LIGHT▁BULB▁AND▁THAT▁IT'S▁AND▁THE▁LIGHT▁BULB▁IS▁LIGHTING▁UP

2024-10-27 17:48:01,422 (asr_inference:509) INFO: speech length: 198459
2024-10-27 17:48:09,116 (beam_search:428) INFO: decoder input length: 154
2024-10-27 17:48:09,116 (beam_search:429) INFO: max output length: 154
2024-10-27 17:48:09,116 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:09,762 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:09,762 (beam_search:476) INFO:  -2.52 * 1.0 =  -2.52 for ctc
2024-10-27 17:48:09,762 (beam_search:479) INFO: total log probability: -2.52
2024-10-27 17:48:09,762 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:48:09,762 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:09,762 (beam_search:483) INFO: best hypo: ▁THE▁THAT▁THE▁ENERGY▁IS▁IS▁IN▁IS▁FROM▁THE▁POSITIVE▁AND▁THE▁THE▁THAT▁THE▁ENERGY▁IS▁OUT▁OF▁IS▁THE▁NEGATIVE

2024-10-27 17:48:09,765 (asr_inference:509) INFO: speech length: 149060
2024-10-27 17:48:15,345 (beam_search:428) INFO: decoder input length: 115
2024-10-27 17:48:15,345 (beam_search:429) INFO: max output length: 115
2024-10-27 17:48:15,345 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:15,590 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:15,590 (beam_search:476) INFO:  -0.81 * 1.0 =  -0.81 for ctc
2024-10-27 17:48:15,590 (beam_search:479) INFO: total log probability: -0.81
2024-10-27 17:48:15,590 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:48:15,590 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:15,590 (beam_search:483) INFO: best hypo: ▁ELECTRICITY▁IN▁FROM▁THE▁POSITIVE▁AND▁ELECTRICITY▁OUT▁FROM▁THE▁NEGATIVE

2024-10-27 17:48:15,593 (asr_inference:509) INFO: speech length: 180060
2024-10-27 17:48:22,823 (beam_search:428) INFO: decoder input length: 140
2024-10-27 17:48:22,823 (beam_search:429) INFO: max output length: 140
2024-10-27 17:48:22,823 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:23,274 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:23,274 (beam_search:476) INFO:  -1.82 * 1.0 =  -1.82 for ctc
2024-10-27 17:48:23,274 (beam_search:479) INFO: total log probability: -1.82
2024-10-27 17:48:23,274 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:48:23,274 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:23,274 (beam_search:483) INFO: best hypo: ▁THE▁NEGATIVE▁SIDE▁OF▁THE▁OF▁THE▁THE▁D▁CELL▁BATTERY▁IS▁IS▁WHERE▁THE▁ENERGY▁IS▁FLOWING▁OUT

2024-10-27 17:48:23,277 (asr_inference:509) INFO: speech length: 51406
2024-10-27 17:48:25,124 (beam_search:428) INFO: decoder input length: 39
2024-10-27 17:48:25,124 (beam_search:429) INFO: max output length: 39
2024-10-27 17:48:25,124 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:25,159 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:25,160 (beam_search:476) INFO:  -0.94 * 1.0 =  -0.94 for ctc
2024-10-27 17:48:25,160 (beam_search:479) INFO: total log probability: -0.94
2024-10-27 17:48:25,160 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:48:25,160 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:25,160 (beam_search:483) INFO: best hypo: ▁ELECTRICITY▁IS▁GETTING▁BY▁THE

2024-10-27 17:48:25,162 (asr_inference:509) INFO: speech length: 91296
2024-10-27 17:48:28,467 (beam_search:428) INFO: decoder input length: 70
2024-10-27 17:48:28,467 (beam_search:429) INFO: max output length: 70
2024-10-27 17:48:28,467 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:28,555 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:28,555 (beam_search:476) INFO:  -1.15 * 1.0 =  -1.15 for ctc
2024-10-27 17:48:28,556 (beam_search:479) INFO: total log probability: -1.15
2024-10-27 17:48:28,556 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:48:28,556 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:28,556 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁THE▁POSITIVE▁SIDE▁OF▁THE

2024-10-27 17:48:28,559 (asr_inference:509) INFO: speech length: 71341
2024-10-27 17:48:31,193 (beam_search:428) INFO: decoder input length: 55
2024-10-27 17:48:31,193 (beam_search:429) INFO: max output length: 55
2024-10-27 17:48:31,193 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:31,234 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:31,235 (beam_search:476) INFO:  -1.31 * 1.0 =  -1.31 for ctc
2024-10-27 17:48:31,235 (beam_search:479) INFO: total log probability: -1.31
2024-10-27 17:48:31,235 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:48:31,235 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:31,235 (beam_search:483) INFO: best hypo: ▁THE▁BLUE▁ELECTRICITY▁FLOWING

2024-10-27 17:48:31,237 (asr_inference:509) INFO: speech length: 225061
2024-10-27 17:48:40,364 (beam_search:428) INFO: decoder input length: 175
2024-10-27 17:48:40,364 (beam_search:429) INFO: max output length: 175
2024-10-27 17:48:40,364 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:40,904 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:40,904 (beam_search:476) INFO:  -2.10 * 1.0 =  -2.10 for ctc
2024-10-27 17:48:40,904 (beam_search:479) INFO: total log probability: -2.10
2024-10-27 17:48:40,904 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:48:40,904 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:40,905 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁INTO▁THE▁POSITIVE▁SIDE▁AND▁THE▁ENERGY▁OUT▁FROM▁THE▁POSITIVE▁FROM▁THE▁SIDE

2024-10-27 17:48:40,907 (asr_inference:509) INFO: speech length: 93843
2024-10-27 17:48:44,377 (beam_search:428) INFO: decoder input length: 72
2024-10-27 17:48:44,377 (beam_search:429) INFO: max output length: 72
2024-10-27 17:48:44,377 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:44,502 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:44,503 (beam_search:476) INFO:  -2.52 * 1.0 =  -2.52 for ctc
2024-10-27 17:48:44,503 (beam_search:479) INFO: total log probability: -2.52
2024-10-27 17:48:44,503 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:48:44,503 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:44,503 (beam_search:483) INFO: best hypo: ▁THAT▁THE▁LIGHT▁BULB▁WORKS▁IN▁BOTH▁THE▁THE▁ARE

2024-10-27 17:48:44,505 (asr_inference:509) INFO: speech length: 391132
2024-10-27 17:49:02,650 (beam_search:428) INFO: decoder input length: 305
2024-10-27 17:49:02,650 (beam_search:429) INFO: max output length: 305
2024-10-27 17:49:02,650 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:03,096 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:03,097 (beam_search:476) INFO:  -2.06 * 1.0 =  -2.06 for ctc
2024-10-27 17:49:03,097 (beam_search:479) INFO: total log probability: -2.06
2024-10-27 17:49:03,097 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:49:03,097 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:03,097 (beam_search:483) INFO: best hypo: ▁THE▁ELECTRICITY▁IS▁FLOWING▁AWAY▁FROM▁THE▁SIDE

2024-10-27 17:49:03,099 (asr_inference:509) INFO: speech length: 143961
2024-10-27 17:49:08,373 (beam_search:428) INFO: decoder input length: 111
2024-10-27 17:49:08,373 (beam_search:429) INFO: max output length: 111
2024-10-27 17:49:08,373 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:08,684 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:08,685 (beam_search:476) INFO:  -3.98 * 1.0 =  -3.98 for ctc
2024-10-27 17:49:08,685 (beam_search:479) INFO: total log probability: -3.98
2024-10-27 17:49:08,685 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:49:08,685 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:08,685 (beam_search:483) INFO: best hypo: ▁THAT▁ONE▁OF▁THE▁IS▁THE▁BULB▁AND▁ONE▁OF▁THE▁WIRES▁IS▁TOUCHING▁THE▁BULB

2024-10-27 17:49:08,687 (asr_inference:509) INFO: speech length: 168688
2024-10-27 17:49:15,070 (beam_search:428) INFO: decoder input length: 131
2024-10-27 17:49:15,070 (beam_search:429) INFO: max output length: 131
2024-10-27 17:49:15,070 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:15,452 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:15,452 (beam_search:476) INFO:  -4.07 * 1.0 =  -4.07 for ctc
2024-10-27 17:49:15,452 (beam_search:479) INFO: total log probability: -4.07
2024-10-27 17:49:15,452 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:49:15,452 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:15,452 (beam_search:483) INFO: best hypo: ▁THAT▁ONE▁OF▁THE▁WIRES▁IS▁TOUCHING▁THE▁AND▁ONE▁OF▁THE▁WIRES▁IS▁THE▁BULB

2024-10-27 17:49:15,455 (asr_inference:509) INFO: speech length: 257346
2024-10-27 17:49:26,050 (beam_search:428) INFO: decoder input length: 200
2024-10-27 17:49:26,050 (beam_search:429) INFO: max output length: 200
2024-10-27 17:49:26,050 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:27,021 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:27,021 (beam_search:476) INFO:  -6.11 * 1.0 =  -6.11 for ctc
2024-10-27 17:49:27,021 (beam_search:479) INFO: total log probability: -6.11
2024-10-27 17:49:27,021 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:49:27,021 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:27,021 (beam_search:483) INFO: best hypo: ▁THE▁WIRES▁HAVE▁TO▁BE▁THE▁NEGATIVE▁AND▁POSITIVE▁OF▁THE▁TO▁ELECTRICITY▁FLOW▁THROUGH▁AND▁THE▁OTHER▁OF▁THE▁WIRE▁HAS▁TO▁TO▁TOUCH▁THE▁AND▁THE

2024-10-27 17:49:27,024 (asr_inference:509) INFO: speech length: 24433
2024-10-27 17:49:28,043 (beam_search:428) INFO: decoder input length: 18
2024-10-27 17:49:28,043 (beam_search:429) INFO: max output length: 18
2024-10-27 17:49:28,043 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:28,059 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:28,059 (beam_search:476) INFO:  -1.39 * 1.0 =  -1.39 for ctc
2024-10-27 17:49:28,059 (beam_search:479) INFO: total log probability: -1.39
2024-10-27 17:49:28,059 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:49:28,059 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:28,059 (beam_search:483) INFO: best hypo: ▁IT▁WOULD▁NOT▁WORK

2024-10-27 17:49:28,061 (asr_inference:509) INFO: speech length: 107814
2024-10-27 17:49:31,919 (beam_search:428) INFO: decoder input length: 83
2024-10-27 17:49:31,919 (beam_search:429) INFO: max output length: 83
2024-10-27 17:49:31,919 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:32,081 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:32,081 (beam_search:476) INFO:  -3.42 * 1.0 =  -3.42 for ctc
2024-10-27 17:49:32,081 (beam_search:479) INFO: total log probability: -3.42
2024-10-27 17:49:32,081 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:49:32,081 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:32,081 (beam_search:483) INFO: best hypo: ▁THAT▁THE▁TWO▁WIRES▁HAVE▁TO▁BE▁THE▁BULB▁AND▁THE

2024-10-27 17:49:32,083 (asr_inference:509) INFO: speech length: 402462
2024-10-27 17:49:50,743 (beam_search:428) INFO: decoder input length: 313
2024-10-27 17:49:50,743 (beam_search:429) INFO: max output length: 313
2024-10-27 17:49:50,743 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:51,988 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:51,988 (beam_search:476) INFO:  -7.05 * 1.0 =  -7.05 for ctc
2024-10-27 17:49:51,990 (beam_search:479) INFO: total log probability: -7.05
2024-10-27 17:49:51,990 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 17:49:51,990 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:51,990 (beam_search:483) INFO: best hypo: ▁THE▁THE▁THE▁ARE▁TOUCHING▁THE▁ON▁THE▁IS▁IS▁NOT▁BECAUSE▁BECAUSE▁THEY▁DON'T▁LET▁THE▁THROUGH

2024-10-27 17:49:51,993 (asr_inference:509) INFO: speech length: 108925
2024-10-27 17:49:55,862 (beam_search:428) INFO: decoder input length: 84
2024-10-27 17:49:55,862 (beam_search:429) INFO: max output length: 84
2024-10-27 17:49:55,862 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:55,969 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:55,969 (beam_search:476) INFO:  -1.73 * 1.0 =  -1.73 for ctc
2024-10-27 17:49:55,969 (beam_search:479) INFO: total log probability: -1.73
2024-10-27 17:49:55,969 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:49:55,969 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:55,969 (beam_search:483) INFO: best hypo: ▁WE▁HAVE▁BEEN▁LEARNING▁ABOUT▁AND▁OTHER

2024-10-27 17:49:55,971 (asr_inference:509) INFO: speech length: 225573
2024-10-27 17:50:05,020 (beam_search:428) INFO: decoder input length: 175
2024-10-27 17:50:05,020 (beam_search:429) INFO: max output length: 175
2024-10-27 17:50:05,020 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:05,437 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:05,437 (beam_search:476) INFO:  -4.30 * 1.0 =  -4.30 for ctc
2024-10-27 17:50:05,437 (beam_search:479) INFO: total log probability: -4.30
2024-10-27 17:50:05,437 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 17:50:05,437 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:05,437 (beam_search:483) INFO: best hypo: ▁THE▁IS▁ISING▁THE▁ENERGY▁TO▁THE▁THE▁WIRES▁AND▁THEN▁THE

2024-10-27 17:50:05,440 (asr_inference:509) INFO: speech length: 155180
2024-10-27 17:50:11,161 (beam_search:428) INFO: decoder input length: 120
2024-10-27 17:50:11,161 (beam_search:429) INFO: max output length: 120
2024-10-27 17:50:11,161 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:11,462 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:11,462 (beam_search:476) INFO:  -1.62 * 1.0 =  -1.62 for ctc
2024-10-27 17:50:11,462 (beam_search:479) INFO: total log probability: -1.62
2024-10-27 17:50:11,462 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:50:11,462 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:11,462 (beam_search:483) INFO: best hypo: ▁THE▁IS▁AND▁IT▁IS▁IMPORTANT▁BECAUSE▁IT▁THAT▁ENERGY▁IS▁THROUGH▁THE▁WIRES

2024-10-27 17:50:11,466 (asr_inference:509) INFO: speech length: 133339
2024-10-27 17:50:16,286 (beam_search:428) INFO: decoder input length: 103
2024-10-27 17:50:16,286 (beam_search:429) INFO: max output length: 103
2024-10-27 17:50:16,286 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:16,419 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:16,419 (beam_search:476) INFO:  -2.08 * 1.0 =  -2.08 for ctc
2024-10-27 17:50:16,419 (beam_search:479) INFO: total log probability: -2.08
2024-10-27 17:50:16,419 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:50:16,419 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:16,419 (beam_search:483) INFO: best hypo: ▁THE▁I▁THAT▁THE▁ENERGY▁IS▁TO

2024-10-27 17:50:16,421 (asr_inference:509) INFO: speech length: 101956
2024-10-27 17:50:20,106 (beam_search:428) INFO: decoder input length: 79
2024-10-27 17:50:20,106 (beam_search:429) INFO: max output length: 79
2024-10-27 17:50:20,106 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:20,273 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:20,273 (beam_search:476) INFO:  -0.91 * 1.0 =  -0.91 for ctc
2024-10-27 17:50:20,273 (beam_search:479) INFO: total log probability: -0.91
2024-10-27 17:50:20,273 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:50:20,273 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:20,273 (beam_search:483) INFO: best hypo: ▁IT▁COULD▁BE▁THE▁WIRE▁SO▁THAT▁THE▁ENERGY▁CAN▁GO▁THROUGH

2024-10-27 17:50:20,276 (asr_inference:509) INFO: speech length: 44118
2024-10-27 17:50:21,982 (beam_search:428) INFO: decoder input length: 33
2024-10-27 17:50:21,983 (beam_search:429) INFO: max output length: 33
2024-10-27 17:50:21,983 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:22,005 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:22,005 (beam_search:476) INFO:  -0.30 * 1.0 =  -0.30 for ctc
2024-10-27 17:50:22,005 (beam_search:479) INFO: total log probability: -0.30
2024-10-27 17:50:22,005 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:50:22,005 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:22,005 (beam_search:483) INFO: best hypo: ▁THE▁THE▁ENERGY

2024-10-27 17:50:22,007 (asr_inference:509) INFO: speech length: 41119
2024-10-27 17:50:23,549 (beam_search:428) INFO: decoder input length: 31
2024-10-27 17:50:23,549 (beam_search:429) INFO: max output length: 31
2024-10-27 17:50:23,549 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:23,569 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:23,570 (beam_search:476) INFO:  -1.31 * 1.0 =  -1.31 for ctc
2024-10-27 17:50:23,570 (beam_search:479) INFO: total log probability: -1.31
2024-10-27 17:50:23,570 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:50:23,570 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:23,570 (beam_search:483) INFO: best hypo: ▁THIS▁IS▁A

2024-10-27 17:50:23,572 (asr_inference:509) INFO: speech length: 27884
2024-10-27 17:50:24,713 (beam_search:428) INFO: decoder input length: 21
2024-10-27 17:50:24,713 (beam_search:429) INFO: max output length: 21
2024-10-27 17:50:24,713 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:24,731 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:24,731 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 17:50:24,731 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 17:50:24,731 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:50:24,731 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:24,731 (beam_search:483) INFO: best hypo: ▁THE▁SWITCH▁IS▁OFF

2024-10-27 17:50:24,734 (asr_inference:509) INFO: speech length: 57002
2024-10-27 17:50:26,979 (beam_search:428) INFO: decoder input length: 44
2024-10-27 17:50:26,979 (beam_search:429) INFO: max output length: 44
2024-10-27 17:50:26,979 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:27,019 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:27,019 (beam_search:476) INFO:  -0.99 * 1.0 =  -0.99 for ctc
2024-10-27 17:50:27,019 (beam_search:479) INFO: total log probability: -0.99
2024-10-27 17:50:27,019 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:50:27,019 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:27,019 (beam_search:483) INFO: best hypo: ▁THE▁IS▁THE▁IS▁MOVING

2024-10-27 17:50:27,021 (asr_inference:509) INFO: speech length: 116200
2024-10-27 17:50:31,332 (beam_search:428) INFO: decoder input length: 90
2024-10-27 17:50:31,332 (beam_search:429) INFO: max output length: 90
2024-10-27 17:50:31,332 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:31,468 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:31,468 (beam_search:476) INFO:  -1.61 * 1.0 =  -1.61 for ctc
2024-10-27 17:50:31,468 (beam_search:479) INFO: total log probability: -1.61
2024-10-27 17:50:31,468 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:50:31,468 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:31,469 (beam_search:483) INFO: best hypo: ▁BECAUSE▁THE▁SWITCH▁LETS▁THE▁ENERGY▁THROUGH

2024-10-27 17:50:31,472 (asr_inference:509) INFO: speech length: 107371
2024-10-27 17:50:35,336 (beam_search:428) INFO: decoder input length: 83
2024-10-27 17:50:35,336 (beam_search:429) INFO: max output length: 83
2024-10-27 17:50:35,336 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:35,520 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:35,520 (beam_search:476) INFO:  -0.61 * 1.0 =  -0.61 for ctc
2024-10-27 17:50:35,520 (beam_search:479) INFO: total log probability: -0.61
2024-10-27 17:50:35,520 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:50:35,520 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:35,520 (beam_search:483) INFO: best hypo: ▁WHEN▁THE▁SWITCH▁IS▁OPEN▁THE▁THE▁ELECTRICITY▁IS▁NOT▁FLOWING▁THROUGH

2024-10-27 17:50:35,522 (asr_inference:509) INFO: speech length: 116319
2024-10-27 17:50:39,761 (beam_search:428) INFO: decoder input length: 90
2024-10-27 17:50:39,761 (beam_search:429) INFO: max output length: 90
2024-10-27 17:50:39,761 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:39,871 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:39,872 (beam_search:476) INFO:  -0.41 * 1.0 =  -0.41 for ctc
2024-10-27 17:50:39,872 (beam_search:479) INFO: total log probability: -0.41
2024-10-27 17:50:39,872 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:50:39,872 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:39,872 (beam_search:483) INFO: best hypo: ▁THE▁THE▁SWITCH▁HAS▁TO▁BE

2024-10-27 17:50:39,874 (asr_inference:509) INFO: speech length: 192340
2024-10-27 17:50:47,340 (beam_search:428) INFO: decoder input length: 149
2024-10-27 17:50:47,340 (beam_search:429) INFO: max output length: 149
2024-10-27 17:50:47,340 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:47,783 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:47,783 (beam_search:476) INFO:  -4.34 * 1.0 =  -4.34 for ctc
2024-10-27 17:50:47,783 (beam_search:479) INFO: total log probability: -4.34
2024-10-27 17:50:47,783 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:50:47,783 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:47,784 (beam_search:483) INFO: best hypo: ▁I▁THAT▁EVERY▁TIME▁THE▁SWITCH▁IS▁OPEN▁THE▁THE▁AND▁THE▁SWITCH▁IS▁THE▁MOTOR

2024-10-27 17:50:47,786 (asr_inference:509) INFO: speech length: 79280
2024-10-27 17:50:50,667 (beam_search:428) INFO: decoder input length: 61
2024-10-27 17:50:50,667 (beam_search:429) INFO: max output length: 61
2024-10-27 17:50:50,667 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:50,752 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:50,753 (beam_search:476) INFO:  -0.95 * 1.0 =  -0.95 for ctc
2024-10-27 17:50:50,753 (beam_search:479) INFO: total log probability: -0.95
2024-10-27 17:50:50,753 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:50:50,753 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:50,753 (beam_search:483) INFO: best hypo: ▁THEY▁ENERGY▁SO▁AND▁OTHER▁THINGS▁CAN▁RUN

2024-10-27 17:50:50,755 (asr_inference:509) INFO: speech length: 164385
2024-10-27 17:50:56,940 (beam_search:428) INFO: decoder input length: 127
2024-10-27 17:50:56,941 (beam_search:429) INFO: max output length: 127
2024-10-27 17:50:56,941 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:57,274 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:57,274 (beam_search:476) INFO:  -3.26 * 1.0 =  -3.26 for ctc
2024-10-27 17:50:57,274 (beam_search:479) INFO: total log probability: -3.26
2024-10-27 17:50:57,274 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:50:57,274 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:57,274 (beam_search:483) INFO: best hypo: ▁THE▁SOLAR▁ISING▁FROM▁THE▁SUN▁AND▁THAT▁TO▁ENERGY▁TO▁POWER▁THE

2024-10-27 17:50:57,276 (asr_inference:509) INFO: speech length: 146457
2024-10-27 17:51:02,712 (beam_search:428) INFO: decoder input length: 113
2024-10-27 17:51:02,712 (beam_search:429) INFO: max output length: 113
2024-10-27 17:51:02,712 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:02,990 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:02,990 (beam_search:476) INFO:  -3.88 * 1.0 =  -3.88 for ctc
2024-10-27 17:51:02,990 (beam_search:479) INFO: total log probability: -3.88
2024-10-27 17:51:02,990 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:51:02,991 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:02,991 (beam_search:483) INFO: best hypo: ▁THE▁MOTOR▁THE▁SOLAR▁CELL▁HAS▁TO▁THE▁IN▁FOR▁THE▁MOTOR▁TO

2024-10-27 17:51:02,993 (asr_inference:509) INFO: speech length: 74245
2024-10-27 17:51:05,733 (beam_search:428) INFO: decoder input length: 57
2024-10-27 17:51:05,733 (beam_search:429) INFO: max output length: 57
2024-10-27 17:51:05,733 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:05,791 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:05,791 (beam_search:476) INFO:  -1.82 * 1.0 =  -1.82 for ctc
2024-10-27 17:51:05,791 (beam_search:479) INFO: total log probability: -1.82
2024-10-27 17:51:05,791 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:51:05,792 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:05,792 (beam_search:483) INFO: best hypo: ▁THE▁IS▁THEN▁THE▁SOLAR

2024-10-27 17:51:05,794 (asr_inference:509) INFO: speech length: 129851
2024-10-27 17:51:10,447 (beam_search:428) INFO: decoder input length: 100
2024-10-27 17:51:10,448 (beam_search:429) INFO: max output length: 100
2024-10-27 17:51:10,448 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:10,526 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:10,526 (beam_search:476) INFO:  -0.44 * 1.0 =  -0.44 for ctc
2024-10-27 17:51:10,526 (beam_search:479) INFO: total log probability: -0.44
2024-10-27 17:51:10,526 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:51:10,526 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:10,527 (beam_search:483) INFO: best hypo: ▁IS▁IMPORTANT▁IS▁THAT

2024-10-27 17:51:10,529 (asr_inference:509) INFO: speech length: 18056
2024-10-27 17:51:11,350 (beam_search:428) INFO: decoder input length: 13
2024-10-27 17:51:11,351 (beam_search:429) INFO: max output length: 13
2024-10-27 17:51:11,351 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:11,363 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:11,363 (beam_search:476) INFO:  -1.20 * 1.0 =  -1.20 for ctc
2024-10-27 17:51:11,363 (beam_search:479) INFO: total log probability: -1.20
2024-10-27 17:51:11,363 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:51:11,363 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:11,363 (beam_search:483) INFO: best hypo: ▁THE▁MOTOR'T

2024-10-27 17:51:11,365 (asr_inference:509) INFO: speech length: 163607
2024-10-27 17:51:17,538 (beam_search:428) INFO: decoder input length: 127
2024-10-27 17:51:17,538 (beam_search:429) INFO: max output length: 127
2024-10-27 17:51:17,538 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:17,825 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:17,825 (beam_search:476) INFO:  -0.46 * 1.0 =  -0.46 for ctc
2024-10-27 17:51:17,825 (beam_search:479) INFO: total log probability: -0.46
2024-10-27 17:51:17,825 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 17:51:17,825 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:17,825 (beam_search:483) INFO: best hypo: ▁THE▁MOTOR▁DOESN'T▁RUN▁WHEN▁IT▁IS▁IT▁IS

2024-10-27 17:51:17,827 (asr_inference:509) INFO: speech length: 116416
2024-10-27 17:51:22,147 (beam_search:428) INFO: decoder input length: 90
2024-10-27 17:51:22,147 (beam_search:429) INFO: max output length: 90
2024-10-27 17:51:22,147 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:22,471 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:22,472 (beam_search:476) INFO:  -1.81 * 1.0 =  -1.81 for ctc
2024-10-27 17:51:22,472 (beam_search:479) INFO: total log probability: -1.81
2024-10-27 17:51:22,472 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:51:22,472 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:22,472 (beam_search:483) INFO: best hypo: ▁WHEN▁THE▁IS▁OUT▁THE▁MOTOR▁BUT▁IF▁IT'S▁OR▁THERE'S▁NO▁THE▁MOTOR▁WON'T

2024-10-27 17:51:22,474 (asr_inference:509) INFO: speech length: 140310
2024-10-27 17:51:27,477 (beam_search:428) INFO: decoder input length: 109
2024-10-27 17:51:27,477 (beam_search:429) INFO: max output length: 109
2024-10-27 17:51:27,477 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:27,793 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:27,793 (beam_search:476) INFO:  -2.99 * 1.0 =  -2.99 for ctc
2024-10-27 17:51:27,793 (beam_search:479) INFO: total log probability: -2.99
2024-10-27 17:51:27,793 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:51:27,793 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:27,793 (beam_search:483) INFO: best hypo: ▁WHEN▁THERE'S▁NO▁SUN▁THERE▁IS▁WHEN▁THERE▁IS▁NO▁SUN▁THE▁MOTOR▁NOT

2024-10-27 17:51:27,795 (asr_inference:509) INFO: speech length: 64101
2024-10-27 17:51:30,284 (beam_search:428) INFO: decoder input length: 49
2024-10-27 17:51:30,284 (beam_search:429) INFO: max output length: 49
2024-10-27 17:51:30,284 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:30,361 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:30,361 (beam_search:476) INFO:  -1.02 * 1.0 =  -1.02 for ctc
2024-10-27 17:51:30,361 (beam_search:479) INFO: total log probability: -1.02
2024-10-27 17:51:30,361 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:51:30,361 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:30,362 (beam_search:483) INFO: best hypo: ▁THE▁SOLAR▁CELL▁IS▁NOT▁GETTING▁ENOUGH▁ENERGY

2024-10-27 17:51:30,364 (asr_inference:509) INFO: speech length: 100460
2024-10-27 17:51:34,043 (beam_search:428) INFO: decoder input length: 77
2024-10-27 17:51:34,043 (beam_search:429) INFO: max output length: 77
2024-10-27 17:51:34,043 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:34,121 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:34,121 (beam_search:476) INFO:  -1.04 * 1.0 =  -1.04 for ctc
2024-10-27 17:51:34,121 (beam_search:479) INFO: total log probability: -1.04
2024-10-27 17:51:34,121 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:51:34,121 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:34,121 (beam_search:483) INFO: best hypo: ▁THIS▁IS▁THE▁CIRCUIT▁THE

2024-10-27 17:51:34,124 (asr_inference:509) INFO: speech length: 54551
2024-10-27 17:51:36,098 (beam_search:428) INFO: decoder input length: 42
2024-10-27 17:51:36,098 (beam_search:429) INFO: max output length: 42
2024-10-27 17:51:36,098 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:36,163 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:36,163 (beam_search:476) INFO:  -1.65 * 1.0 =  -1.65 for ctc
2024-10-27 17:51:36,163 (beam_search:479) INFO: total log probability: -1.65
2024-10-27 17:51:36,163 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:51:36,163 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:36,163 (beam_search:483) INFO: best hypo: ▁WE'VE▁BEEN▁LEARNING▁ABOUT▁CIRCUITS▁AND▁OTHER▁THINGS

2024-10-27 17:51:36,166 (asr_inference:509) INFO: speech length: 56710
2024-10-27 17:51:38,307 (beam_search:428) INFO: decoder input length: 43
2024-10-27 17:51:38,307 (beam_search:429) INFO: max output length: 43
2024-10-27 17:51:38,307 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:38,334 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:38,334 (beam_search:476) INFO:  -0.14 * 1.0 =  -0.14 for ctc
2024-10-27 17:51:38,334 (beam_search:479) INFO: total log probability: -0.14
2024-10-27 17:51:38,334 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 17:51:38,334 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:38,335 (beam_search:483) INFO: best hypo: ▁A▁CIRCUIT▁IS

2024-10-27 17:51:38,337 (asr_inference:509) INFO: speech length: 94510
2024-10-27 17:51:41,797 (beam_search:428) INFO: decoder input length: 73
2024-10-27 17:51:41,797 (beam_search:429) INFO: max output length: 73
2024-10-27 17:51:41,797 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:41,858 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:41,859 (beam_search:476) INFO:  -1.31 * 1.0 =  -1.31 for ctc
2024-10-27 17:51:41,859 (beam_search:479) INFO: total log probability: -1.31
2024-10-27 17:51:41,859 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:51:41,859 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:41,859 (beam_search:483) INFO: best hypo: ▁IS▁A▁NAIL▁THE

2024-10-27 17:51:41,861 (asr_inference:509) INFO: speech length: 57636
2024-10-27 17:51:44,008 (beam_search:428) INFO: decoder input length: 44
2024-10-27 17:51:44,008 (beam_search:429) INFO: max output length: 44
2024-10-27 17:51:44,008 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:44,043 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:44,043 (beam_search:476) INFO:  -1.92 * 1.0 =  -1.92 for ctc
2024-10-27 17:51:44,043 (beam_search:479) INFO: total log probability: -1.92
2024-10-27 17:51:44,043 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 17:51:44,043 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:44,043 (beam_search:483) INFO: best hypo: ▁ALL▁METAL▁CONDUCT▁ELECTRICITY

2024-10-27 17:51:44,045 (asr_inference:509) INFO: speech length: 129184
2024-10-27 17:51:48,695 (beam_search:428) INFO: decoder input length: 100
2024-10-27 17:51:48,695 (beam_search:429) INFO: max output length: 100
2024-10-27 17:51:48,695 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:48,821 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:48,821 (beam_search:476) INFO:  -0.75 * 1.0 =  -0.75 for ctc
2024-10-27 17:51:48,821 (beam_search:479) INFO: total log probability: -0.75
2024-10-27 17:51:48,821 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:51:48,821 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:48,821 (beam_search:483) INFO: best hypo: ▁ALL▁OF▁THE▁IN▁THE▁ARE▁ARE

2024-10-27 17:51:48,824 (asr_inference:509) INFO: speech length: 52227
2024-10-27 17:51:50,732 (beam_search:428) INFO: decoder input length: 40
2024-10-27 17:51:50,732 (beam_search:429) INFO: max output length: 40
2024-10-27 17:51:50,732 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:50,752 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:50,752 (beam_search:476) INFO:  -0.41 * 1.0 =  -0.41 for ctc
2024-10-27 17:51:50,752 (beam_search:479) INFO: total log probability: -0.41
2024-10-27 17:51:50,752 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:51:50,752 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:50,752 (beam_search:483) INFO: best hypo: ▁ALL▁ELECTRICITY

2024-10-27 17:51:50,754 (asr_inference:509) INFO: speech length: 32801
2024-10-27 17:51:52,037 (beam_search:428) INFO: decoder input length: 25
2024-10-27 17:51:52,037 (beam_search:429) INFO: max output length: 25
2024-10-27 17:51:52,037 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:52,048 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:52,048 (beam_search:476) INFO:  -0.50 * 1.0 =  -0.50 for ctc
2024-10-27 17:51:52,048 (beam_search:479) INFO: total log probability: -0.50
2024-10-27 17:51:52,048 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:51:52,048 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:52,048 (beam_search:483) INFO: best hypo: ▁THE

2024-10-27 17:51:52,051 (asr_inference:509) INFO: speech length: 64380
2024-10-27 17:51:54,533 (beam_search:428) INFO: decoder input length: 49
2024-10-27 17:51:54,534 (beam_search:429) INFO: max output length: 49
2024-10-27 17:51:54,534 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:54,561 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:54,561 (beam_search:476) INFO:  -1.11 * 1.0 =  -1.11 for ctc
2024-10-27 17:51:54,561 (beam_search:479) INFO: total log probability: -1.11
2024-10-27 17:51:54,561 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 17:51:54,561 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:54,561 (beam_search:483) INFO: best hypo: ▁I▁A

2024-10-27 17:51:54,564 (asr_inference:509) INFO: speech length: 131550
2024-10-27 17:51:59,507 (beam_search:428) INFO: decoder input length: 102
2024-10-27 17:51:59,508 (beam_search:429) INFO: max output length: 102
2024-10-27 17:51:59,508 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:59,571 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:59,572 (beam_search:476) INFO:  -1.26 * 1.0 =  -1.26 for ctc
2024-10-27 17:51:59,572 (beam_search:479) INFO: total log probability: -1.26
2024-10-27 17:51:59,572 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:51:59,572 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:59,572 (beam_search:483) INFO: best hypo: ▁ALL▁ALL▁ELECTRICITY

2024-10-27 17:51:59,574 (asr_inference:509) INFO: speech length: 31749
2024-10-27 17:52:00,822 (beam_search:428) INFO: decoder input length: 24
2024-10-27 17:52:00,822 (beam_search:429) INFO: max output length: 24
2024-10-27 17:52:00,822 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:00,838 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:00,838 (beam_search:476) INFO:  -0.15 * 1.0 =  -0.15 for ctc
2024-10-27 17:52:00,838 (beam_search:479) INFO: total log probability: -0.15
2024-10-27 17:52:00,838 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 17:52:00,838 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:00,838 (beam_search:483) INFO: best hypo: ▁IT▁IS▁NOT

2024-10-27 17:52:00,840 (asr_inference:509) INFO: speech length: 151965
2024-10-27 17:52:06,383 (beam_search:428) INFO: decoder input length: 118
2024-10-27 17:52:06,383 (beam_search:429) INFO: max output length: 118
2024-10-27 17:52:06,383 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:06,493 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:06,493 (beam_search:476) INFO:  -2.00 * 1.0 =  -2.00 for ctc
2024-10-27 17:52:06,493 (beam_search:479) INFO: total log probability: -2.00
2024-10-27 17:52:06,493 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 17:52:06,494 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:06,494 (beam_search:483) INFO: best hypo: ▁A▁WITH▁THE▁WILL▁NOT

2024-10-27 17:52:06,496 (asr_inference:509) INFO: speech length: 258082
2024-10-27 17:52:17,623 (beam_search:428) INFO: decoder input length: 201
2024-10-27 17:52:17,623 (beam_search:429) INFO: max output length: 201
2024-10-27 17:52:17,623 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:18,312 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:18,312 (beam_search:476) INFO:  -2.33 * 1.0 =  -2.33 for ctc
2024-10-27 17:52:18,312 (beam_search:479) INFO: total log probability: -2.33
2024-10-27 17:52:18,313 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:52:18,313 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:18,313 (beam_search:483) INFO: best hypo: ▁THE▁THAT▁ARE▁DON'T▁WORK▁WITH▁THE▁CIRCUIT▁AND▁AND▁THE▁ARE▁AND▁THE▁INSULATORS▁ARE▁NOT

2024-10-27 17:52:18,315 (asr_inference:509) INFO: speech length: 113471
2024-10-27 17:52:22,503 (beam_search:428) INFO: decoder input length: 88
2024-10-27 17:52:22,504 (beam_search:429) INFO: max output length: 88
2024-10-27 17:52:22,504 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:22,635 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:22,635 (beam_search:476) INFO:  -1.59 * 1.0 =  -1.59 for ctc
2024-10-27 17:52:22,635 (beam_search:479) INFO: total log probability: -1.59
2024-10-27 17:52:22,635 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:52:22,635 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:22,635 (beam_search:483) INFO: best hypo: ▁ARE▁NOT▁MADE▁OF▁THE▁CIRCUIT▁WILL▁NOT

2024-10-27 17:52:22,637 (asr_inference:509) INFO: speech length: 33068
2024-10-27 17:52:24,015 (beam_search:428) INFO: decoder input length: 25
2024-10-27 17:52:24,015 (beam_search:429) INFO: max output length: 25
2024-10-27 17:52:24,015 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:24,035 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:24,035 (beam_search:476) INFO:  -0.29 * 1.0 =  -0.29 for ctc
2024-10-27 17:52:24,035 (beam_search:479) INFO: total log probability: -0.29
2024-10-27 17:52:24,036 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:52:24,036 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:24,036 (beam_search:483) INFO: best hypo: ▁THE▁MOTOR▁NOT▁RUN

2024-10-27 17:52:24,037 (asr_inference:509) INFO: speech length: 124422
2024-10-27 17:52:28,477 (beam_search:428) INFO: decoder input length: 96
2024-10-27 17:52:28,477 (beam_search:429) INFO: max output length: 96
2024-10-27 17:52:28,477 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:28,630 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:28,630 (beam_search:476) INFO:  -2.72 * 1.0 =  -2.72 for ctc
2024-10-27 17:52:28,630 (beam_search:479) INFO: total log probability: -2.72
2024-10-27 17:52:28,630 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:52:28,630 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:28,630 (beam_search:483) INFO: best hypo: ▁WHEN▁I▁TO▁A▁CIRCUIT▁THE▁CIRCUIT▁IS▁NOT

2024-10-27 17:52:28,633 (asr_inference:509) INFO: speech length: 39443
2024-10-27 17:52:30,222 (beam_search:428) INFO: decoder input length: 30
2024-10-27 17:52:30,222 (beam_search:429) INFO: max output length: 30
2024-10-27 17:52:30,222 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:30,241 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:30,241 (beam_search:476) INFO:  -0.26 * 1.0 =  -0.26 for ctc
2024-10-27 17:52:30,241 (beam_search:479) INFO: total log probability: -0.26
2024-10-27 17:52:30,241 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:52:30,241 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:30,241 (beam_search:483) INFO: best hypo: ▁THE▁IS▁OPEN

2024-10-27 17:52:30,243 (asr_inference:509) INFO: speech length: 41953
2024-10-27 17:52:31,879 (beam_search:428) INFO: decoder input length: 32
2024-10-27 17:52:31,880 (beam_search:429) INFO: max output length: 32
2024-10-27 17:52:31,880 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:31,893 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:31,893 (beam_search:476) INFO:  -0.26 * 1.0 =  -0.26 for ctc
2024-10-27 17:52:31,893 (beam_search:479) INFO: total log probability: -0.26
2024-10-27 17:52:31,893 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:52:31,893 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:31,893 (beam_search:483) INFO: best hypo: ▁ARE

2024-10-27 17:52:31,896 (asr_inference:509) INFO: speech length: 78258
2024-10-27 17:52:34,889 (beam_search:428) INFO: decoder input length: 60
2024-10-27 17:52:34,889 (beam_search:429) INFO: max output length: 60
2024-10-27 17:52:34,889 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:34,963 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:34,963 (beam_search:476) INFO:  -1.96 * 1.0 =  -1.96 for ctc
2024-10-27 17:52:34,963 (beam_search:479) INFO: total log probability: -1.96
2024-10-27 17:52:34,963 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:52:34,963 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:34,963 (beam_search:483) INFO: best hypo: ▁WE▁HAVE▁BEEN▁ABOUT▁HOW▁ENERGY▁IS

2024-10-27 17:52:34,965 (asr_inference:509) INFO: speech length: 118301
2024-10-27 17:52:39,254 (beam_search:428) INFO: decoder input length: 91
2024-10-27 17:52:39,254 (beam_search:429) INFO: max output length: 91
2024-10-27 17:52:39,254 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:39,371 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:39,371 (beam_search:476) INFO:  -1.19 * 1.0 =  -1.19 for ctc
2024-10-27 17:52:39,371 (beam_search:479) INFO: total log probability: -1.19
2024-10-27 17:52:39,371 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:52:39,371 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:39,371 (beam_search:483) INFO: best hypo: ▁IT▁ITS▁THINGS▁LIKES▁AND

2024-10-27 17:52:39,373 (asr_inference:509) INFO: speech length: 59919
2024-10-27 17:52:41,527 (beam_search:428) INFO: decoder input length: 46
2024-10-27 17:52:41,527 (beam_search:429) INFO: max output length: 46
2024-10-27 17:52:41,527 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:41,568 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:41,569 (beam_search:476) INFO:  -0.88 * 1.0 =  -0.88 for ctc
2024-10-27 17:52:41,569 (beam_search:479) INFO: total log probability: -0.88
2024-10-27 17:52:41,569 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:52:41,569 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:41,569 (beam_search:483) INFO: best hypo: ▁I▁SEE▁THE▁AND▁THE

2024-10-27 17:52:41,572 (asr_inference:509) INFO: speech length: 47231
2024-10-27 17:52:43,389 (beam_search:428) INFO: decoder input length: 36
2024-10-27 17:52:43,389 (beam_search:429) INFO: max output length: 36
2024-10-27 17:52:43,389 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:43,403 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:43,403 (beam_search:476) INFO:  -0.07 * 1.0 =  -0.07 for ctc
2024-10-27 17:52:43,403 (beam_search:479) INFO: total log probability: -0.07
2024-10-27 17:52:43,403 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 17:52:43,403 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:43,403 (beam_search:483) INFO: best hypo: ▁THE

2024-10-27 17:52:43,405 (asr_inference:509) INFO: speech length: 42919
2024-10-27 17:52:45,168 (beam_search:428) INFO: decoder input length: 33
2024-10-27 17:52:45,168 (beam_search:429) INFO: max output length: 33
2024-10-27 17:52:45,168 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:45,195 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:45,195 (beam_search:476) INFO:  -0.49 * 1.0 =  -0.49 for ctc
2024-10-27 17:52:45,195 (beam_search:479) INFO: total log probability: -0.49
2024-10-27 17:52:45,195 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:52:45,195 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:45,195 (beam_search:483) INFO: best hypo: ▁ITT▁VERY▁HOT

2024-10-27 17:52:45,198 (asr_inference:509) INFO: speech length: 43172
2024-10-27 17:52:46,948 (beam_search:428) INFO: decoder input length: 33
2024-10-27 17:52:46,948 (beam_search:429) INFO: max output length: 33
2024-10-27 17:52:46,948 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:46,970 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:46,970 (beam_search:476) INFO:  -1.66 * 1.0 =  -1.66 for ctc
2024-10-27 17:52:46,970 (beam_search:479) INFO: total log probability: -1.66
2024-10-27 17:52:46,970 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 17:52:46,970 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:46,970 (beam_search:483) INFO: best hypo: ▁THEY▁HAD▁NO

2024-10-27 17:52:46,972 (asr_inference:509) INFO: speech length: 50952
2024-10-27 17:52:48,850 (beam_search:428) INFO: decoder input length: 39
2024-10-27 17:52:48,850 (beam_search:429) INFO: max output length: 39
2024-10-27 17:52:48,850 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:48,876 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:48,876 (beam_search:476) INFO:  -2.03 * 1.0 =  -2.03 for ctc
2024-10-27 17:52:48,876 (beam_search:479) INFO: total log probability: -2.03
2024-10-27 17:52:48,876 (beam_search:480) INFO: normalized log probability: -0.41
2024-10-27 17:52:48,876 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:48,876 (beam_search:483) INFO: best hypo: ▁HAVE▁ABOUT▁ENERGY

2024-10-27 17:52:48,878 (asr_inference:509) INFO: speech length: 70638
2024-10-27 17:52:51,406 (beam_search:428) INFO: decoder input length: 54
2024-10-27 17:52:51,406 (beam_search:429) INFO: max output length: 54
2024-10-27 17:52:51,407 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:51,440 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:51,440 (beam_search:476) INFO:  -0.80 * 1.0 =  -0.80 for ctc
2024-10-27 17:52:51,440 (beam_search:479) INFO: total log probability: -0.80
2024-10-27 17:52:51,440 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:52:51,440 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:51,440 (beam_search:483) INFO: best hypo: ▁FROM▁TOS

2024-10-27 17:52:51,442 (asr_inference:509) INFO: speech length: 83546
2024-10-27 17:52:54,492 (beam_search:428) INFO: decoder input length: 64
2024-10-27 17:52:54,492 (beam_search:429) INFO: max output length: 64
2024-10-27 17:52:54,492 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:54,532 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:54,532 (beam_search:476) INFO:  -1.14 * 1.0 =  -1.14 for ctc
2024-10-27 17:52:54,532 (beam_search:479) INFO: total log probability: -1.14
2024-10-27 17:52:54,532 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:52:54,532 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:54,532 (beam_search:483) INFO: best hypo: ▁I▁THE▁AND

2024-10-27 17:52:54,535 (asr_inference:509) INFO: speech length: 90250
2024-10-27 17:52:57,837 (beam_search:428) INFO: decoder input length: 70
2024-10-27 17:52:57,837 (beam_search:429) INFO: max output length: 70
2024-10-27 17:52:57,837 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:57,901 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:57,901 (beam_search:476) INFO:  -2.24 * 1.0 =  -2.24 for ctc
2024-10-27 17:52:57,901 (beam_search:479) INFO: total log probability: -2.24
2024-10-27 17:52:57,901 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 17:52:57,901 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:57,901 (beam_search:483) INFO: best hypo: ▁THE▁AND▁SO▁THE▁IS

2024-10-27 17:52:57,903 (asr_inference:509) INFO: speech length: 50954
2024-10-27 17:52:59,818 (beam_search:428) INFO: decoder input length: 39
2024-10-27 17:52:59,818 (beam_search:429) INFO: max output length: 39
2024-10-27 17:52:59,818 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:59,838 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:59,838 (beam_search:476) INFO:  -1.07 * 1.0 =  -1.07 for ctc
2024-10-27 17:52:59,838 (beam_search:479) INFO: total log probability: -1.07
2024-10-27 17:52:59,838 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 17:52:59,838 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:59,838 (beam_search:483) INFO: best hypo: ▁THERE▁NO

2024-10-27 17:52:59,841 (asr_inference:509) INFO: speech length: 436641
2024-10-27 17:53:20,967 (beam_search:428) INFO: decoder input length: 340
2024-10-27 17:53:20,967 (beam_search:429) INFO: max output length: 340
2024-10-27 17:53:20,967 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:21,608 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:21,608 (beam_search:476) INFO:  -2.32 * 1.0 =  -2.32 for ctc
2024-10-27 17:53:21,608 (beam_search:479) INFO: total log probability: -2.32
2024-10-27 17:53:21,608 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:53:21,608 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:21,608 (beam_search:483) INFO: best hypo: ▁I▁SEE▁IT▁AND▁AND▁IT▁AND▁IT▁MAKES▁THE

2024-10-27 17:53:21,611 (asr_inference:509) INFO: speech length: 172658
2024-10-27 17:53:28,241 (beam_search:428) INFO: decoder input length: 134
2024-10-27 17:53:28,241 (beam_search:429) INFO: max output length: 134
2024-10-27 17:53:28,241 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:28,451 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:28,451 (beam_search:476) INFO:  -1.76 * 1.0 =  -1.76 for ctc
2024-10-27 17:53:28,451 (beam_search:479) INFO: total log probability: -1.76
2024-10-27 17:53:28,451 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:53:28,452 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:28,452 (beam_search:483) INFO: best hypo: ▁THE▁IS▁IT▁MAKE▁THE▁AND▁THE▁IS

2024-10-27 17:53:28,454 (asr_inference:509) INFO: speech length: 122940
2024-10-27 17:53:32,989 (beam_search:428) INFO: decoder input length: 95
2024-10-27 17:53:32,989 (beam_search:429) INFO: max output length: 95
2024-10-27 17:53:32,989 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:33,119 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:33,119 (beam_search:476) INFO:  -2.22 * 1.0 =  -2.22 for ctc
2024-10-27 17:53:33,120 (beam_search:479) INFO: total log probability: -2.22
2024-10-27 17:53:33,120 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:53:33,120 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:33,120 (beam_search:483) INFO: best hypo: ▁THE▁IS▁THE▁HANDS▁ON▁THE▁IS

2024-10-27 17:53:33,122 (asr_inference:509) INFO: speech length: 128146
2024-10-27 17:53:37,887 (beam_search:428) INFO: decoder input length: 99
2024-10-27 17:53:37,887 (beam_search:429) INFO: max output length: 99
2024-10-27 17:53:37,887 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:38,070 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:38,071 (beam_search:476) INFO:  -3.94 * 1.0 =  -3.94 for ctc
2024-10-27 17:53:38,071 (beam_search:479) INFO: total log probability: -3.94
2024-10-27 17:53:38,071 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 17:53:38,071 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:38,071 (beam_search:483) INFO: best hypo: ▁THE▁USING▁ENERGY▁BECAUSE▁SHE▁SHE▁NEED▁FROM▁THE▁TO

2024-10-27 17:53:38,073 (asr_inference:509) INFO: speech length: 207211
2024-10-27 17:53:46,344 (beam_search:428) INFO: decoder input length: 161
2024-10-27 17:53:46,344 (beam_search:429) INFO: max output length: 161
2024-10-27 17:53:46,344 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:46,762 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:46,762 (beam_search:476) INFO:  -3.91 * 1.0 =  -3.91 for ctc
2024-10-27 17:53:46,762 (beam_search:479) INFO: total log probability: -3.91
2024-10-27 17:53:46,762 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:53:46,762 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:46,762 (beam_search:483) INFO: best hypo: ▁THE▁OF▁ENERGY▁IS▁IS▁SHE▁IS▁USING▁FROM▁THE▁TO▁TO▁UP▁THE

2024-10-27 17:53:46,765 (asr_inference:509) INFO: speech length: 89872
2024-10-27 17:53:50,115 (beam_search:428) INFO: decoder input length: 69
2024-10-27 17:53:50,116 (beam_search:429) INFO: max output length: 69
2024-10-27 17:53:50,116 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:50,150 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:50,151 (beam_search:476) INFO:  -1.64 * 1.0 =  -1.64 for ctc
2024-10-27 17:53:50,151 (beam_search:479) INFO: total log probability: -1.64
2024-10-27 17:53:50,151 (beam_search:480) INFO: normalized log probability: -0.41
2024-10-27 17:53:50,151 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:50,151 (beam_search:483) INFO: best hypo: ▁AND▁OF

2024-10-27 17:53:50,153 (asr_inference:509) INFO: speech length: 192199
2024-10-27 17:53:57,447 (beam_search:428) INFO: decoder input length: 149
2024-10-27 17:53:57,448 (beam_search:429) INFO: max output length: 149
2024-10-27 17:53:57,448 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:57,884 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:57,884 (beam_search:476) INFO:  -5.96 * 1.0 =  -5.96 for ctc
2024-10-27 17:53:57,884 (beam_search:479) INFO: total log probability: -5.96
2024-10-27 17:53:57,884 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 17:53:57,884 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:57,884 (beam_search:483) INFO: best hypo: ▁WHEN▁WE▁IT▁WILL▁DIOXIDE▁IS▁FOR▁THE▁BECAUSE▁IT▁AND▁THAT▁THE▁AIR▁WILL▁BE

2024-10-27 17:53:57,887 (asr_inference:509) INFO: speech length: 41301
2024-10-27 17:53:59,412 (beam_search:428) INFO: decoder input length: 31
2024-10-27 17:53:59,413 (beam_search:429) INFO: max output length: 31
2024-10-27 17:53:59,413 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:59,433 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:59,434 (beam_search:476) INFO:  -0.36 * 1.0 =  -0.36 for ctc
2024-10-27 17:53:59,434 (beam_search:479) INFO: total log probability: -0.36
2024-10-27 17:53:59,434 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:53:59,434 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:59,434 (beam_search:483) INFO: best hypo: ▁IT▁GETS▁AND

2024-10-27 17:53:59,436 (asr_inference:509) INFO: speech length: 132445
2024-10-27 17:54:04,446 (beam_search:428) INFO: decoder input length: 102
2024-10-27 17:54:04,446 (beam_search:429) INFO: max output length: 102
2024-10-27 17:54:04,446 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:04,584 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:04,585 (beam_search:476) INFO:  -1.52 * 1.0 =  -1.52 for ctc
2024-10-27 17:54:04,585 (beam_search:479) INFO: total log probability: -1.52
2024-10-27 17:54:04,585 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:54:04,585 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:04,585 (beam_search:483) INFO: best hypo: ▁THE▁SOLAR▁DOES▁NOT▁BUT▁THE

2024-10-27 17:54:04,588 (asr_inference:509) INFO: speech length: 105874
2024-10-27 17:54:08,377 (beam_search:428) INFO: decoder input length: 82
2024-10-27 17:54:08,377 (beam_search:429) INFO: max output length: 82
2024-10-27 17:54:08,377 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:08,430 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:08,430 (beam_search:476) INFO:  -0.62 * 1.0 =  -0.62 for ctc
2024-10-27 17:54:08,430 (beam_search:479) INFO: total log probability: -0.62
2024-10-27 17:54:08,430 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:54:08,430 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:08,430 (beam_search:483) INFO: best hypo: ▁THE▁FOR▁IS

2024-10-27 17:54:08,433 (asr_inference:509) INFO: speech length: 90635
2024-10-27 17:54:11,628 (beam_search:428) INFO: decoder input length: 70
2024-10-27 17:54:11,628 (beam_search:429) INFO: max output length: 70
2024-10-27 17:54:11,628 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:11,675 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:11,676 (beam_search:476) INFO:  -0.55 * 1.0 =  -0.55 for ctc
2024-10-27 17:54:11,676 (beam_search:479) INFO: total log probability: -0.55
2024-10-27 17:54:11,676 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:54:11,676 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:11,676 (beam_search:483) INFO: best hypo: ▁THEY▁CAN▁BE

2024-10-27 17:54:11,678 (asr_inference:509) INFO: speech length: 114000
2024-10-27 17:54:16,042 (beam_search:428) INFO: decoder input length: 88
2024-10-27 17:54:16,042 (beam_search:429) INFO: max output length: 88
2024-10-27 17:54:16,042 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:16,112 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:16,112 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 17:54:16,112 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 17:54:16,112 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:54:16,112 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:16,112 (beam_search:483) INFO: best hypo: ▁THEY▁THEY▁WORK▁ON

2024-10-27 17:54:16,114 (asr_inference:509) INFO: speech length: 68385
2024-10-27 17:54:18,623 (beam_search:428) INFO: decoder input length: 52
2024-10-27 17:54:18,624 (beam_search:429) INFO: max output length: 52
2024-10-27 17:54:18,624 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:18,663 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:18,663 (beam_search:476) INFO:  -0.78 * 1.0 =  -0.78 for ctc
2024-10-27 17:54:18,663 (beam_search:479) INFO: total log probability: -0.78
2024-10-27 17:54:18,663 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:54:18,663 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:18,663 (beam_search:483) INFO: best hypo: ▁THEY▁BURN▁AND▁THEY

2024-10-27 17:54:18,666 (asr_inference:509) INFO: speech length: 70006
2024-10-27 17:54:21,132 (beam_search:428) INFO: decoder input length: 54
2024-10-27 17:54:21,132 (beam_search:429) INFO: max output length: 54
2024-10-27 17:54:21,132 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:21,200 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:21,201 (beam_search:476) INFO:  -0.82 * 1.0 =  -0.82 for ctc
2024-10-27 17:54:21,201 (beam_search:479) INFO: total log probability: -0.82
2024-10-27 17:54:21,201 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:54:21,201 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:21,201 (beam_search:483) INFO: best hypo: ▁ONE▁IS▁A▁AND▁IS▁A▁SOLID

2024-10-27 17:54:21,203 (asr_inference:509) INFO: speech length: 124234
2024-10-27 17:54:25,880 (beam_search:428) INFO: decoder input length: 96
2024-10-27 17:54:25,881 (beam_search:429) INFO: max output length: 96
2024-10-27 17:54:25,881 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:26,091 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:26,092 (beam_search:476) INFO:  -3.00 * 1.0 =  -3.00 for ctc
2024-10-27 17:54:26,092 (beam_search:479) INFO: total log probability: -3.00
2024-10-27 17:54:26,092 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:54:26,092 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:26,092 (beam_search:483) INFO: best hypo: ▁THE▁SOLARN'T▁GIVE▁BUT▁THE▁AND▁THE▁GAS

2024-10-27 17:54:26,095 (asr_inference:509) INFO: speech length: 64641
2024-10-27 17:54:28,697 (beam_search:428) INFO: decoder input length: 50
2024-10-27 17:54:28,697 (beam_search:429) INFO: max output length: 50
2024-10-27 17:54:28,697 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:28,735 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:28,735 (beam_search:476) INFO:  -1.66 * 1.0 =  -1.66 for ctc
2024-10-27 17:54:28,735 (beam_search:479) INFO: total log probability: -1.66
2024-10-27 17:54:28,735 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 17:54:28,735 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:28,735 (beam_search:483) INFO: best hypo: ▁THAT▁IS▁FOR▁THE

2024-10-27 17:54:28,737 (asr_inference:509) INFO: speech length: 301687
2024-10-27 17:54:41,271 (beam_search:428) INFO: decoder input length: 235
2024-10-27 17:54:41,271 (beam_search:429) INFO: max output length: 235
2024-10-27 17:54:41,271 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:41,938 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:41,938 (beam_search:476) INFO:  -1.60 * 1.0 =  -1.60 for ctc
2024-10-27 17:54:41,938 (beam_search:479) INFO: total log probability: -1.60
2024-10-27 17:54:41,938 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:54:41,938 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:41,938 (beam_search:483) INFO: best hypo: ▁THE▁COAL▁AND▁THE▁GASOLINE▁IS▁FOR▁BUT▁THE▁SOLAR▁AND▁THE▁WILL▁NOT

2024-10-27 17:54:41,940 (asr_inference:509) INFO: speech length: 151088
2024-10-27 17:54:47,705 (beam_search:428) INFO: decoder input length: 117
2024-10-27 17:54:47,705 (beam_search:429) INFO: max output length: 117
2024-10-27 17:54:47,705 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:48,085 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:48,085 (beam_search:476) INFO:  -1.27 * 1.0 =  -1.27 for ctc
2024-10-27 17:54:48,085 (beam_search:479) INFO: total log probability: -1.27
2024-10-27 17:54:48,085 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:54:48,085 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:48,085 (beam_search:483) INFO: best hypo: ▁IT▁WILL▁THE▁AIR▁AND▁IT▁AND▁IT▁WILL▁RUN▁OUT▁IF▁YOU▁USE▁IT▁TOO▁MUCH

2024-10-27 17:54:48,088 (asr_inference:509) INFO: speech length: 64676
2024-10-27 17:54:50,546 (beam_search:428) INFO: decoder input length: 50
2024-10-27 17:54:50,546 (beam_search:429) INFO: max output length: 50
2024-10-27 17:54:50,546 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:50,571 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:50,572 (beam_search:476) INFO:  -0.91 * 1.0 =  -0.91 for ctc
2024-10-27 17:54:50,572 (beam_search:479) INFO: total log probability: -0.91
2024-10-27 17:54:50,572 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:54:50,572 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:50,572 (beam_search:483) INFO: best hypo: ▁GAS▁AND

2024-10-27 17:54:50,574 (asr_inference:509) INFO: speech length: 27165
2024-10-27 17:54:51,754 (beam_search:428) INFO: decoder input length: 20
2024-10-27 17:54:51,754 (beam_search:429) INFO: max output length: 20
2024-10-27 17:54:51,754 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:51,770 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:51,770 (beam_search:476) INFO:  -1.48 * 1.0 =  -1.48 for ctc
2024-10-27 17:54:51,770 (beam_search:479) INFO: total log probability: -1.48
2024-10-27 17:54:51,770 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 17:54:51,770 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:51,770 (beam_search:483) INFO: best hypo: ▁WE▁HAVE▁BEEN

2024-10-27 17:54:51,772 (asr_inference:509) INFO: speech length: 50857
2024-10-27 17:54:53,661 (beam_search:428) INFO: decoder input length: 39
2024-10-27 17:54:53,661 (beam_search:429) INFO: max output length: 39
2024-10-27 17:54:53,661 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:53,692 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:53,692 (beam_search:476) INFO:  -1.61 * 1.0 =  -1.61 for ctc
2024-10-27 17:54:53,692 (beam_search:479) INFO: total log probability: -1.61
2024-10-27 17:54:53,692 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 17:54:53,692 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:53,692 (beam_search:483) INFO: best hypo: ▁A▁IS▁A▁PARALLEL

2024-10-27 17:54:53,695 (asr_inference:509) INFO: speech length: 195092
2024-10-27 17:55:01,326 (beam_search:428) INFO: decoder input length: 151
2024-10-27 17:55:01,326 (beam_search:429) INFO: max output length: 151
2024-10-27 17:55:01,326 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:01,697 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:01,697 (beam_search:476) INFO:  -3.78 * 1.0 =  -3.78 for ctc
2024-10-27 17:55:01,697 (beam_search:479) INFO: total log probability: -3.78
2024-10-27 17:55:01,697 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:55:01,697 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:01,697 (beam_search:483) INFO: best hypo: ▁THE▁D▁IS▁THE▁THE▁WIRES▁ARE▁THE▁ENERGY▁AND▁THE▁ARE▁THE▁ENERGY

2024-10-27 17:55:01,700 (asr_inference:509) INFO: speech length: 68981
2024-10-27 17:55:04,266 (beam_search:428) INFO: decoder input length: 53
2024-10-27 17:55:04,266 (beam_search:429) INFO: max output length: 53
2024-10-27 17:55:04,266 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:04,299 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:04,300 (beam_search:476) INFO:  -1.25 * 1.0 =  -1.25 for ctc
2024-10-27 17:55:04,300 (beam_search:479) INFO: total log probability: -1.25
2024-10-27 17:55:04,300 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:55:04,300 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:04,300 (beam_search:483) INFO: best hypo: ▁AND▁PARALLEL▁CIRCUITS

2024-10-27 17:55:04,302 (asr_inference:509) INFO: speech length: 32288
2024-10-27 17:55:05,668 (beam_search:428) INFO: decoder input length: 24
2024-10-27 17:55:05,668 (beam_search:429) INFO: max output length: 24
2024-10-27 17:55:05,668 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:05,678 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:05,678 (beam_search:476) INFO:  -0.34 * 1.0 =  -0.34 for ctc
2024-10-27 17:55:05,678 (beam_search:479) INFO: total log probability: -0.34
2024-10-27 17:55:05,678 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:55:05,678 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:05,678 (beam_search:483) INFO: best hypo: ▁IT

2024-10-27 17:55:05,681 (asr_inference:509) INFO: speech length: 140192
2024-10-27 17:55:10,776 (beam_search:428) INFO: decoder input length: 109
2024-10-27 17:55:10,776 (beam_search:429) INFO: max output length: 109
2024-10-27 17:55:10,776 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:10,963 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:10,963 (beam_search:476) INFO:  -0.50 * 1.0 =  -0.50 for ctc
2024-10-27 17:55:10,963 (beam_search:479) INFO: total log probability: -0.50
2024-10-27 17:55:10,963 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:55:10,963 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:10,963 (beam_search:483) INFO: best hypo: ▁THE▁THE▁LIGHT▁BULB▁IN▁ENERGY▁AND▁OUT▁ENERGY

2024-10-27 17:55:10,965 (asr_inference:509) INFO: speech length: 20418
2024-10-27 17:55:11,948 (beam_search:428) INFO: decoder input length: 15
2024-10-27 17:55:11,949 (beam_search:429) INFO: max output length: 15
2024-10-27 17:55:11,949 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:11,959 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:11,959 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 17:55:11,959 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 17:55:11,959 (beam_search:480) INFO: normalized log probability: -0.38
2024-10-27 17:55:11,959 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:11,959 (beam_search:483) INFO: best hypo: ▁IT▁MORE

2024-10-27 17:55:11,962 (asr_inference:509) INFO: speech length: 211825
2024-10-27 17:55:20,448 (beam_search:428) INFO: decoder input length: 164
2024-10-27 17:55:20,448 (beam_search:429) INFO: max output length: 164
2024-10-27 17:55:20,448 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:20,872 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:20,872 (beam_search:476) INFO:  -3.09 * 1.0 =  -3.09 for ctc
2024-10-27 17:55:20,872 (beam_search:479) INFO: total log probability: -3.09
2024-10-27 17:55:20,872 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:55:20,872 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:20,872 (beam_search:483) INFO: best hypo: ▁SO▁THE▁CIRCUIT▁IT▁DOESN'T▁AS▁AS▁A▁PARALLEL▁CIRCUIT

2024-10-27 17:55:20,874 (asr_inference:509) INFO: speech length: 167579
2024-10-27 17:55:27,192 (beam_search:428) INFO: decoder input length: 130
2024-10-27 17:55:27,193 (beam_search:429) INFO: max output length: 130
2024-10-27 17:55:27,193 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:27,326 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:27,326 (beam_search:476) INFO:  -1.75 * 1.0 =  -1.75 for ctc
2024-10-27 17:55:27,326 (beam_search:479) INFO: total log probability: -1.75
2024-10-27 17:55:27,326 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:55:27,326 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:27,326 (beam_search:483) INFO: best hypo: ▁FROM▁ONE▁END▁AND▁ANOTHER

2024-10-27 17:55:27,330 (asr_inference:509) INFO: speech length: 199851
2024-10-27 17:55:35,203 (beam_search:428) INFO: decoder input length: 155
2024-10-27 17:55:35,203 (beam_search:429) INFO: max output length: 155
2024-10-27 17:55:35,203 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:35,838 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:35,838 (beam_search:476) INFO:  -5.03 * 1.0 =  -5.03 for ctc
2024-10-27 17:55:35,838 (beam_search:479) INFO: total log probability: -5.03
2024-10-27 17:55:35,838 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:55:35,838 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:35,838 (beam_search:483) INFO: best hypo: ▁I▁ENERGY▁GOES▁INTO▁ONE▁LIGHT▁BULB▁COMES▁OUT▁AND▁THEN▁LIGHT▁BULB▁AND▁THEN▁TO▁THE▁LIGHT▁BULB▁LIGHT▁AND▁BLAH

2024-10-27 17:55:35,841 (asr_inference:509) INFO: speech length: 21732
2024-10-27 17:55:36,844 (beam_search:428) INFO: decoder input length: 16
2024-10-27 17:55:36,844 (beam_search:429) INFO: max output length: 16
2024-10-27 17:55:36,844 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:36,852 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:36,853 (beam_search:476) INFO:  -0.19 * 1.0 =  -0.19 for ctc
2024-10-27 17:55:36,853 (beam_search:479) INFO: total log probability: -0.19
2024-10-27 17:55:36,853 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:55:36,853 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:36,853 (beam_search:483) INFO: best hypo: ▁YOU

2024-10-27 17:55:36,856 (asr_inference:509) INFO: speech length: 34557
2024-10-27 17:55:38,326 (beam_search:428) INFO: decoder input length: 26
2024-10-27 17:55:38,326 (beam_search:429) INFO: max output length: 26
2024-10-27 17:55:38,326 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:38,340 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:38,340 (beam_search:476) INFO:  -0.73 * 1.0 =  -0.73 for ctc
2024-10-27 17:55:38,340 (beam_search:479) INFO: total log probability: -0.73
2024-10-27 17:55:38,340 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:55:38,340 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:38,340 (beam_search:483) INFO: best hypo: ▁I▁NEED

2024-10-27 17:55:38,343 (asr_inference:509) INFO: speech length: 251672
2024-10-27 17:55:48,621 (beam_search:428) INFO: decoder input length: 196
2024-10-27 17:55:48,621 (beam_search:429) INFO: max output length: 196
2024-10-27 17:55:48,621 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:49,266 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:49,266 (beam_search:476) INFO:  -3.96 * 1.0 =  -3.96 for ctc
2024-10-27 17:55:49,266 (beam_search:479) INFO: total log probability: -3.96
2024-10-27 17:55:49,266 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:55:49,266 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:49,266 (beam_search:483) INFO: best hypo: ▁THE▁TWO▁DON'T▁MAKE▁JUST▁MAKE▁THEM▁MAKE▁MAKE▁THEM▁NOT▁NOT▁NOT▁MORE▁POWERFUL

2024-10-27 17:55:49,269 (asr_inference:509) INFO: speech length: 58147
2024-10-27 17:55:51,562 (beam_search:428) INFO: decoder input length: 44
2024-10-27 17:55:51,562 (beam_search:429) INFO: max output length: 44
2024-10-27 17:55:51,562 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:51,594 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:51,595 (beam_search:476) INFO:  -0.54 * 1.0 =  -0.54 for ctc
2024-10-27 17:55:51,595 (beam_search:479) INFO: total log probability: -0.54
2024-10-27 17:55:51,595 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:55:51,595 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:51,595 (beam_search:483) INFO: best hypo: ▁THEY▁ALL▁HAVE▁TWO

2024-10-27 17:55:51,597 (asr_inference:509) INFO: speech length: 101022
2024-10-27 17:55:55,185 (beam_search:428) INFO: decoder input length: 78
2024-10-27 17:55:55,185 (beam_search:429) INFO: max output length: 78
2024-10-27 17:55:55,185 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:55,340 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:55,340 (beam_search:476) INFO:  -0.97 * 1.0 =  -0.97 for ctc
2024-10-27 17:55:55,340 (beam_search:479) INFO: total log probability: -0.97
2024-10-27 17:55:55,340 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:55:55,340 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:55,340 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁HAS▁TO▁GO▁THROUGH▁MANY▁MORE▁WIRES▁THE▁ENERGY▁WILL

2024-10-27 17:55:55,342 (asr_inference:509) INFO: speech length: 102972
2024-10-27 17:55:59,020 (beam_search:428) INFO: decoder input length: 79
2024-10-27 17:55:59,020 (beam_search:429) INFO: max output length: 79
2024-10-27 17:55:59,020 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:59,134 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:59,135 (beam_search:476) INFO:  -1.02 * 1.0 =  -1.02 for ctc
2024-10-27 17:55:59,135 (beam_search:479) INFO: total log probability: -1.02
2024-10-27 17:55:59,135 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:55:59,135 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:59,135 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁HAS▁TO▁GO▁THROUGH▁MORE▁WIRES

2024-10-27 17:55:59,137 (asr_inference:509) INFO: speech length: 38008
2024-10-27 17:56:00,674 (beam_search:428) INFO: decoder input length: 29
2024-10-27 17:56:00,674 (beam_search:429) INFO: max output length: 29
2024-10-27 17:56:00,674 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:00,689 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:00,690 (beam_search:476) INFO:  -0.80 * 1.0 =  -0.80 for ctc
2024-10-27 17:56:00,690 (beam_search:479) INFO: total log probability: -0.80
2024-10-27 17:56:00,690 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:56:00,690 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:00,690 (beam_search:483) INFO: best hypo: ▁THERE▁IS

2024-10-27 17:56:00,692 (asr_inference:509) INFO: speech length: 18000
2024-10-27 17:56:01,636 (beam_search:428) INFO: decoder input length: 13
2024-10-27 17:56:01,636 (beam_search:429) INFO: max output length: 13
2024-10-27 17:56:01,636 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:01,649 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:01,649 (beam_search:476) INFO:  -0.23 * 1.0 =  -0.23 for ctc
2024-10-27 17:56:01,649 (beam_search:479) INFO: total log probability: -0.23
2024-10-27 17:56:01,649 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:56:01,649 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:01,649 (beam_search:483) INFO: best hypo: ▁I'M▁GOOD

2024-10-27 17:56:01,652 (asr_inference:509) INFO: speech length: 78551
2024-10-27 17:56:04,626 (beam_search:428) INFO: decoder input length: 60
2024-10-27 17:56:04,626 (beam_search:429) INFO: max output length: 60
2024-10-27 17:56:04,626 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:04,681 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:04,681 (beam_search:476) INFO:  -2.18 * 1.0 =  -2.18 for ctc
2024-10-27 17:56:04,681 (beam_search:479) INFO: total log probability: -2.18
2024-10-27 17:56:04,681 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 17:56:04,681 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:04,681 (beam_search:483) INFO: best hypo: ▁I▁LEARNING▁ABOUT▁HOW▁MAGNETS

2024-10-27 17:56:04,683 (asr_inference:509) INFO: speech length: 65619
2024-10-27 17:56:06,979 (beam_search:428) INFO: decoder input length: 50
2024-10-27 17:56:06,979 (beam_search:429) INFO: max output length: 50
2024-10-27 17:56:06,979 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:07,019 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:07,020 (beam_search:476) INFO:  -0.59 * 1.0 =  -0.59 for ctc
2024-10-27 17:56:07,020 (beam_search:479) INFO: total log probability: -0.59
2024-10-27 17:56:07,020 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:56:07,020 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:07,020 (beam_search:483) INFO: best hypo: ▁THE▁ARE▁STICKING▁TOGETHER

2024-10-27 17:56:07,022 (asr_inference:509) INFO: speech length: 33540
2024-10-27 17:56:08,331 (beam_search:428) INFO: decoder input length: 25
2024-10-27 17:56:08,332 (beam_search:429) INFO: max output length: 25
2024-10-27 17:56:08,332 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:08,345 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:08,345 (beam_search:476) INFO:  -0.99 * 1.0 =  -0.99 for ctc
2024-10-27 17:56:08,345 (beam_search:479) INFO: total log probability: -0.99
2024-10-27 17:56:08,345 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:56:08,345 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:08,345 (beam_search:483) INFO: best hypo: ▁THEY▁OTHER

2024-10-27 17:56:08,347 (asr_inference:509) INFO: speech length: 47107
2024-10-27 17:56:10,113 (beam_search:428) INFO: decoder input length: 36
2024-10-27 17:56:10,113 (beam_search:429) INFO: max output length: 36
2024-10-27 17:56:10,113 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:10,142 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:10,142 (beam_search:476) INFO:  -0.29 * 1.0 =  -0.29 for ctc
2024-10-27 17:56:10,142 (beam_search:479) INFO: total log probability: -0.29
2024-10-27 17:56:10,142 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:56:10,142 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:10,142 (beam_search:483) INFO: best hypo: ▁THE▁ARE▁EACH▁OTHER

2024-10-27 17:56:10,144 (asr_inference:509) INFO: speech length: 29195
2024-10-27 17:56:11,428 (beam_search:428) INFO: decoder input length: 22
2024-10-27 17:56:11,428 (beam_search:429) INFO: max output length: 22
2024-10-27 17:56:11,428 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:11,444 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:11,444 (beam_search:476) INFO:  -2.51 * 1.0 =  -2.51 for ctc
2024-10-27 17:56:11,444 (beam_search:479) INFO: total log probability: -2.51
2024-10-27 17:56:11,444 (beam_search:480) INFO: normalized log probability: -0.50
2024-10-27 17:56:11,444 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:11,444 (beam_search:483) INFO: best hypo: ▁BECAUSE▁A▁AND

2024-10-27 17:56:11,446 (asr_inference:509) INFO: speech length: 165108
2024-10-27 17:56:17,664 (beam_search:428) INFO: decoder input length: 128
2024-10-27 17:56:17,664 (beam_search:429) INFO: max output length: 128
2024-10-27 17:56:17,664 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:17,793 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:17,793 (beam_search:476) INFO:  -2.24 * 1.0 =  -2.24 for ctc
2024-10-27 17:56:17,793 (beam_search:479) INFO: total log probability: -2.24
2024-10-27 17:56:17,793 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 17:56:17,793 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:17,793 (beam_search:483) INFO: best hypo: ▁THEY▁EACH▁OTHER▁ARE▁AND

2024-10-27 17:56:17,795 (asr_inference:509) INFO: speech length: 121692
2024-10-27 17:56:22,431 (beam_search:428) INFO: decoder input length: 94
2024-10-27 17:56:22,431 (beam_search:429) INFO: max output length: 94
2024-10-27 17:56:22,431 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:22,545 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:22,545 (beam_search:476) INFO:  -2.96 * 1.0 =  -2.96 for ctc
2024-10-27 17:56:22,545 (beam_search:479) INFO: total log probability: -2.96
2024-10-27 17:56:22,545 (beam_search:480) INFO: normalized log probability: -0.37
2024-10-27 17:56:22,545 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:22,545 (beam_search:483) INFO: best hypo: ▁OF▁THAT▁TO▁EACH▁OTHER▁THAT

2024-10-27 17:56:22,547 (asr_inference:509) INFO: speech length: 273849
2024-10-27 17:56:33,830 (beam_search:428) INFO: decoder input length: 213
2024-10-27 17:56:33,831 (beam_search:429) INFO: max output length: 213
2024-10-27 17:56:33,831 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:34,318 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:34,318 (beam_search:476) INFO:  -2.94 * 1.0 =  -2.94 for ctc
2024-10-27 17:56:34,318 (beam_search:479) INFO: total log probability: -2.94
2024-10-27 17:56:34,318 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:56:34,318 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:34,319 (beam_search:483) INFO: best hypo: ▁ARE▁MAGNETS▁AND▁THEY▁LIKE▁AND▁ONE▁OF▁THE▁AND▁THE▁ARE

2024-10-27 17:56:34,322 (asr_inference:509) INFO: speech length: 37719
2024-10-27 17:56:35,770 (beam_search:428) INFO: decoder input length: 28
2024-10-27 17:56:35,770 (beam_search:429) INFO: max output length: 28
2024-10-27 17:56:35,770 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:35,780 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:35,780 (beam_search:476) INFO:  -1.06 * 1.0 =  -1.06 for ctc
2024-10-27 17:56:35,781 (beam_search:479) INFO: total log probability: -1.06
2024-10-27 17:56:35,781 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 17:56:35,781 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:35,781 (beam_search:483) INFO: best hypo: S

2024-10-27 17:56:35,782 (asr_inference:509) INFO: speech length: 75597
2024-10-27 17:56:38,643 (beam_search:428) INFO: decoder input length: 58
2024-10-27 17:56:38,643 (beam_search:429) INFO: max output length: 58
2024-10-27 17:56:38,643 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:38,703 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:38,704 (beam_search:476) INFO:  -3.19 * 1.0 =  -3.19 for ctc
2024-10-27 17:56:38,704 (beam_search:479) INFO: total log probability: -3.19
2024-10-27 17:56:38,704 (beam_search:480) INFO: normalized log probability: -0.40
2024-10-27 17:56:38,704 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:38,704 (beam_search:483) INFO: best hypo: ▁THIS▁IS▁ABOUT▁A▁THES

2024-10-27 17:56:38,706 (asr_inference:509) INFO: speech length: 176241
2024-10-27 17:56:45,359 (beam_search:428) INFO: decoder input length: 137
2024-10-27 17:56:45,360 (beam_search:429) INFO: max output length: 137
2024-10-27 17:56:45,360 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:45,427 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:45,427 (beam_search:476) INFO:  -1.88 * 1.0 =  -1.88 for ctc
2024-10-27 17:56:45,427 (beam_search:479) INFO: total log probability: -1.88
2024-10-27 17:56:45,428 (beam_search:480) INFO: normalized log probability: -0.47
2024-10-27 17:56:45,428 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:45,428 (beam_search:483) INFO: best hypo: ▁BECAUSE▁THE

2024-10-27 17:56:45,430 (asr_inference:509) INFO: speech length: 54500
2024-10-27 17:56:47,432 (beam_search:428) INFO: decoder input length: 42
2024-10-27 17:56:47,433 (beam_search:429) INFO: max output length: 42
2024-10-27 17:56:47,433 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:47,476 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:47,476 (beam_search:476) INFO:  -1.42 * 1.0 =  -1.42 for ctc
2024-10-27 17:56:47,476 (beam_search:479) INFO: total log probability: -1.42
2024-10-27 17:56:47,476 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:56:47,476 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:47,477 (beam_search:483) INFO: best hypo: ▁LEARNING▁ABOUT▁MAGNETS▁AND▁OTHER▁STUFF

2024-10-27 17:56:47,478 (asr_inference:509) INFO: speech length: 100961
2024-10-27 17:56:51,152 (beam_search:428) INFO: decoder input length: 78
2024-10-27 17:56:51,152 (beam_search:429) INFO: max output length: 78
2024-10-27 17:56:51,152 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:51,243 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:51,243 (beam_search:476) INFO:  -0.85 * 1.0 =  -0.85 for ctc
2024-10-27 17:56:51,243 (beam_search:479) INFO: total log probability: -0.85
2024-10-27 17:56:51,243 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:56:51,243 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:51,243 (beam_search:483) INFO: best hypo: ▁WE▁ABOUT▁AND▁AND▁THAT▁STUFF

2024-10-27 17:56:51,245 (asr_inference:509) INFO: speech length: 159110
2024-10-27 17:56:57,223 (beam_search:428) INFO: decoder input length: 123
2024-10-27 17:56:57,223 (beam_search:429) INFO: max output length: 123
2024-10-27 17:56:57,223 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:57,407 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:57,407 (beam_search:476) INFO:  -1.88 * 1.0 =  -1.88 for ctc
2024-10-27 17:56:57,407 (beam_search:479) INFO: total log probability: -1.88
2024-10-27 17:56:57,407 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:56:57,407 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:57,407 (beam_search:483) INFO: best hypo: ▁THE▁GOING▁THROUGH▁THE▁THE▁STICK▁TO▁THE

2024-10-27 17:56:57,410 (asr_inference:509) INFO: speech length: 308233
2024-10-27 17:57:10,474 (beam_search:428) INFO: decoder input length: 240
2024-10-27 17:57:10,474 (beam_search:429) INFO: max output length: 240
2024-10-27 17:57:10,474 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:11,164 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:11,164 (beam_search:476) INFO:  -6.10 * 1.0 =  -6.10 for ctc
2024-10-27 17:57:11,164 (beam_search:479) INFO: total log probability: -6.10
2024-10-27 17:57:11,164 (beam_search:480) INFO: normalized log probability: -0.38
2024-10-27 17:57:11,164 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:11,164 (beam_search:483) INFO: best hypo: ▁WHEN▁THE▁MAGNET▁HAVE▁A▁SO▁IF▁THE▁ENOUGH▁THE▁CAN▁GO▁THROUGH▁IT

2024-10-27 17:57:11,167 (asr_inference:509) INFO: speech length: 175688
2024-10-27 17:57:18,218 (beam_search:428) INFO: decoder input length: 136
2024-10-27 17:57:18,218 (beam_search:429) INFO: max output length: 136
2024-10-27 17:57:18,218 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:18,465 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:18,465 (beam_search:476) INFO:  -3.87 * 1.0 =  -3.87 for ctc
2024-10-27 17:57:18,466 (beam_search:479) INFO: total log probability: -3.87
2024-10-27 17:57:18,466 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 17:57:18,466 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:18,466 (beam_search:483) INFO: best hypo: ▁IS▁A▁SO▁IF▁A▁OR▁STICK▁TO▁THE▁MAGNET

2024-10-27 17:57:18,468 (asr_inference:509) INFO: speech length: 133225
2024-10-27 17:57:23,218 (beam_search:428) INFO: decoder input length: 103
2024-10-27 17:57:23,218 (beam_search:429) INFO: max output length: 103
2024-10-27 17:57:23,218 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:23,417 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:23,417 (beam_search:476) INFO:  -2.12 * 1.0 =  -2.12 for ctc
2024-10-27 17:57:23,417 (beam_search:479) INFO: total log probability: -2.12
2024-10-27 17:57:23,417 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:57:23,417 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:23,417 (beam_search:483) INFO: best hypo: ▁THE▁FIRST▁THE▁POWER▁CAN▁GO▁THROUGH▁BUT▁THE▁IT

2024-10-27 17:57:23,420 (asr_inference:509) INFO: speech length: 198692
2024-10-27 17:57:31,440 (beam_search:428) INFO: decoder input length: 154
2024-10-27 17:57:31,441 (beam_search:429) INFO: max output length: 154
2024-10-27 17:57:31,441 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:31,859 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:31,859 (beam_search:476) INFO:  -2.25 * 1.0 =  -2.25 for ctc
2024-10-27 17:57:31,859 (beam_search:479) INFO: total log probability: -2.25
2024-10-27 17:57:31,859 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:57:31,859 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:31,859 (beam_search:483) INFO: best hypo: ▁THE▁IS▁CAN▁GO▁THROUGH▁THE▁THING▁THE▁THE▁CAN▁ONLY▁GO▁THROUGH▁SOME

2024-10-27 17:57:31,862 (asr_inference:509) INFO: speech length: 33318
2024-10-27 17:57:33,294 (beam_search:428) INFO: decoder input length: 25
2024-10-27 17:57:33,294 (beam_search:429) INFO: max output length: 25
2024-10-27 17:57:33,294 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:33,309 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:33,309 (beam_search:476) INFO:  -0.17 * 1.0 =  -0.17 for ctc
2024-10-27 17:57:33,309 (beam_search:479) INFO: total log probability: -0.17
2024-10-27 17:57:33,309 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:57:33,309 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:33,309 (beam_search:483) INFO: best hypo: ▁IT▁IT

2024-10-27 17:57:33,312 (asr_inference:509) INFO: speech length: 154746
2024-10-27 17:57:38,985 (beam_search:428) INFO: decoder input length: 120
2024-10-27 17:57:38,986 (beam_search:429) INFO: max output length: 120
2024-10-27 17:57:38,986 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:39,185 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:39,186 (beam_search:476) INFO:  -3.63 * 1.0 =  -3.63 for ctc
2024-10-27 17:57:39,186 (beam_search:479) INFO: total log probability: -3.63
2024-10-27 17:57:39,186 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 17:57:39,186 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:39,186 (beam_search:483) INFO: best hypo: ▁THE▁IT▁WILL▁THROUGH▁IT▁WON'T▁GO

2024-10-27 17:57:39,189 (asr_inference:509) INFO: speech length: 68264
2024-10-27 17:57:41,746 (beam_search:428) INFO: decoder input length: 52
2024-10-27 17:57:41,747 (beam_search:429) INFO: max output length: 52
2024-10-27 17:57:41,747 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:41,812 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:41,812 (beam_search:476) INFO:  -2.00 * 1.0 =  -2.00 for ctc
2024-10-27 17:57:41,812 (beam_search:479) INFO: total log probability: -2.00
2024-10-27 17:57:41,812 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:57:41,812 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:41,812 (beam_search:483) INFO: best hypo: ▁THROUGH▁THE▁THE▁CAN▁NOT▁GO▁THROUGH

2024-10-27 17:57:41,814 (asr_inference:509) INFO: speech length: 100901
2024-10-27 17:57:45,488 (beam_search:428) INFO: decoder input length: 78
2024-10-27 17:57:45,488 (beam_search:429) INFO: max output length: 78
2024-10-27 17:57:45,488 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:45,615 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:45,615 (beam_search:476) INFO:  -2.54 * 1.0 =  -2.54 for ctc
2024-10-27 17:57:45,615 (beam_search:479) INFO: total log probability: -2.54
2024-10-27 17:57:45,615 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:57:45,615 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:45,615 (beam_search:483) INFO: best hypo: ▁THE▁IS▁NAIL▁AND▁STEEL▁NAIL▁STICK▁TO▁THE

2024-10-27 17:57:45,617 (asr_inference:509) INFO: speech length: 43212
2024-10-27 17:57:47,194 (beam_search:428) INFO: decoder input length: 33
2024-10-27 17:57:47,195 (beam_search:429) INFO: max output length: 33
2024-10-27 17:57:47,195 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:47,226 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:47,227 (beam_search:476) INFO:  -1.48 * 1.0 =  -1.48 for ctc
2024-10-27 17:57:47,227 (beam_search:479) INFO: total log probability: -1.48
2024-10-27 17:57:47,227 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:57:47,227 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:47,227 (beam_search:483) INFO: best hypo: ▁WE'VE▁LEARNING▁ABOUT

2024-10-27 17:57:47,229 (asr_inference:509) INFO: speech length: 42840
2024-10-27 17:57:48,884 (beam_search:428) INFO: decoder input length: 32
2024-10-27 17:57:48,884 (beam_search:429) INFO: max output length: 32
2024-10-27 17:57:48,884 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:48,896 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:48,896 (beam_search:476) INFO:  -0.52 * 1.0 =  -0.52 for ctc
2024-10-27 17:57:48,896 (beam_search:479) INFO: total log probability: -0.52
2024-10-27 17:57:48,896 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:57:48,896 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:48,897 (beam_search:483) INFO: best hypo: ▁MAGNET

2024-10-27 17:57:48,899 (asr_inference:509) INFO: speech length: 89315
2024-10-27 17:57:52,209 (beam_search:428) INFO: decoder input length: 69
2024-10-27 17:57:52,209 (beam_search:429) INFO: max output length: 69
2024-10-27 17:57:52,209 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:52,266 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:52,266 (beam_search:476) INFO:  -1.10 * 1.0 =  -1.10 for ctc
2024-10-27 17:57:52,266 (beam_search:479) INFO: total log probability: -1.10
2024-10-27 17:57:52,266 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:57:52,266 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:52,266 (beam_search:483) INFO: best hypo: ▁MORE▁TO▁THE▁SIDE

2024-10-27 17:57:52,269 (asr_inference:509) INFO: speech length: 50929
2024-10-27 17:57:54,252 (beam_search:428) INFO: decoder input length: 39
2024-10-27 17:57:54,252 (beam_search:429) INFO: max output length: 39
2024-10-27 17:57:54,252 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:54,261 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:54,261 (beam_search:476) INFO:  -0.12 * 1.0 =  -0.12 for ctc
2024-10-27 17:57:54,261 (beam_search:479) INFO: total log probability: -0.12
2024-10-27 17:57:54,261 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:57:54,262 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:54,262 (beam_search:483) INFO: best hypo: 

2024-10-27 17:57:54,264 (asr_inference:509) INFO: speech length: 182917
2024-10-27 17:58:01,129 (beam_search:428) INFO: decoder input length: 142
2024-10-27 17:58:01,130 (beam_search:429) INFO: max output length: 142
2024-10-27 17:58:01,130 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:01,221 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:01,221 (beam_search:476) INFO:  -1.03 * 1.0 =  -1.03 for ctc
2024-10-27 17:58:01,221 (beam_search:479) INFO: total log probability: -1.03
2024-10-27 17:58:01,221 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:58:01,221 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:01,221 (beam_search:483) INFO: best hypo: ▁SO▁YOU▁IN

2024-10-27 17:58:01,223 (asr_inference:509) INFO: speech length: 170450
2024-10-27 17:58:07,824 (beam_search:428) INFO: decoder input length: 132
2024-10-27 17:58:07,824 (beam_search:429) INFO: max output length: 132
2024-10-27 17:58:07,824 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:07,909 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:07,910 (beam_search:476) INFO:  -1.45 * 1.0 =  -1.45 for ctc
2024-10-27 17:58:07,910 (beam_search:479) INFO: total log probability: -1.45
2024-10-27 17:58:07,910 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 17:58:07,910 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:07,910 (beam_search:483) INFO: best hypo: ▁MAGNETS▁A▁WASHERS

2024-10-27 17:58:07,912 (asr_inference:509) INFO: speech length: 136752
2024-10-27 17:58:12,744 (beam_search:428) INFO: decoder input length: 106
2024-10-27 17:58:12,744 (beam_search:429) INFO: max output length: 106
2024-10-27 17:58:12,744 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:12,897 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:12,897 (beam_search:476) INFO:  -1.37 * 1.0 =  -1.37 for ctc
2024-10-27 17:58:12,897 (beam_search:479) INFO: total log probability: -1.37
2024-10-27 17:58:12,897 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:58:12,897 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:12,897 (beam_search:483) INFO: best hypo: ▁WE▁HAVE▁BEEN▁MAKING▁AND▁OUT▁HOW▁THEY

2024-10-27 17:58:12,899 (asr_inference:509) INFO: speech length: 211031
2024-10-27 17:58:21,790 (beam_search:428) INFO: decoder input length: 164
2024-10-27 17:58:21,790 (beam_search:429) INFO: max output length: 164
2024-10-27 17:58:21,791 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:22,024 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:22,024 (beam_search:476) INFO:  -4.51 * 1.0 =  -4.51 for ctc
2024-10-27 17:58:22,024 (beam_search:479) INFO: total log probability: -4.51
2024-10-27 17:58:22,024 (beam_search:480) INFO: normalized log probability: -0.45
2024-10-27 17:58:22,024 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:22,024 (beam_search:483) INFO: best hypo: ▁THE▁WORKS▁WHEN▁IT▁IS▁IS▁WITH▁A

2024-10-27 17:58:22,026 (asr_inference:509) INFO: speech length: 334901
2024-10-27 17:58:36,977 (beam_search:428) INFO: decoder input length: 261
2024-10-27 17:58:36,977 (beam_search:429) INFO: max output length: 261
2024-10-27 17:58:36,977 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:37,036 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:37,037 (beam_search:476) INFO:  -0.17 * 1.0 =  -0.17 for ctc
2024-10-27 17:58:37,037 (beam_search:479) INFO: total log probability: -0.17
2024-10-27 17:58:37,037 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:58:37,037 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:37,037 (beam_search:483) INFO: best hypo: 

2024-10-27 17:58:37,040 (asr_inference:509) INFO: speech length: 127907
2024-10-27 17:58:41,800 (beam_search:428) INFO: decoder input length: 99
2024-10-27 17:58:41,800 (beam_search:429) INFO: max output length: 99
2024-10-27 17:58:41,800 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:41,881 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:41,881 (beam_search:476) INFO:  -0.67 * 1.0 =  -0.67 for ctc
2024-10-27 17:58:41,881 (beam_search:479) INFO: total log probability: -0.67
2024-10-27 17:58:41,881 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:58:41,881 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:41,881 (beam_search:483) INFO: best hypo: ▁TO▁MAKE▁IT▁A

2024-10-27 17:58:41,884 (asr_inference:509) INFO: speech length: 115174
2024-10-27 17:58:46,156 (beam_search:428) INFO: decoder input length: 89
2024-10-27 17:58:46,157 (beam_search:429) INFO: max output length: 89
2024-10-27 17:58:46,157 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:46,201 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:46,201 (beam_search:476) INFO:  -1.37 * 1.0 =  -1.37 for ctc
2024-10-27 17:58:46,201 (beam_search:479) INFO: total log probability: -1.37
2024-10-27 17:58:46,201 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 17:58:46,201 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:46,201 (beam_search:483) INFO: best hypo: ▁ELECTROMAGNET▁BY

2024-10-27 17:58:46,203 (asr_inference:509) INFO: speech length: 64246
2024-10-27 17:58:48,606 (beam_search:428) INFO: decoder input length: 49
2024-10-27 17:58:48,606 (beam_search:429) INFO: max output length: 49
2024-10-27 17:58:48,606 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:48,667 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:48,667 (beam_search:476) INFO:  -1.07 * 1.0 =  -1.07 for ctc
2024-10-27 17:58:48,667 (beam_search:479) INFO: total log probability: -1.07
2024-10-27 17:58:48,667 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:58:48,667 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:48,668 (beam_search:483) INFO: best hypo: ▁YOU▁HAVE▁TO▁WRAP▁IT▁AROUND▁A

2024-10-27 17:58:48,669 (asr_inference:509) INFO: speech length: 154117
2024-10-27 17:58:54,337 (beam_search:428) INFO: decoder input length: 119
2024-10-27 17:58:54,337 (beam_search:429) INFO: max output length: 119
2024-10-27 17:58:54,337 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:54,401 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:54,401 (beam_search:476) INFO:  -1.36 * 1.0 =  -1.36 for ctc
2024-10-27 17:58:54,401 (beam_search:479) INFO: total log probability: -1.36
2024-10-27 17:58:54,401 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 17:58:54,401 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:54,401 (beam_search:483) INFO: best hypo: ▁MAGNETS▁ON

2024-10-27 17:58:54,404 (asr_inference:509) INFO: speech length: 170472
2024-10-27 17:59:00,852 (beam_search:428) INFO: decoder input length: 132
2024-10-27 17:59:00,852 (beam_search:429) INFO: max output length: 132
2024-10-27 17:59:00,852 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:00,878 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:00,878 (beam_search:476) INFO:  -0.90 * 1.0 =  -0.90 for ctc
2024-10-27 17:59:00,878 (beam_search:479) INFO: total log probability: -0.90
2024-10-27 17:59:00,878 (beam_search:480) INFO: normalized log probability: -0.45
2024-10-27 17:59:00,878 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:00,878 (beam_search:483) INFO: best hypo: 

2024-10-27 17:59:00,881 (asr_inference:509) INFO: speech length: 21000
2024-10-27 17:59:01,817 (beam_search:428) INFO: decoder input length: 15
2024-10-27 17:59:01,817 (beam_search:429) INFO: max output length: 15
2024-10-27 17:59:01,817 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:01,826 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:01,827 (beam_search:476) INFO:  -0.30 * 1.0 =  -0.30 for ctc
2024-10-27 17:59:01,827 (beam_search:479) INFO: total log probability: -0.30
2024-10-27 17:59:01,827 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:59:01,827 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:01,827 (beam_search:483) INFO: best hypo: ▁ON▁AND

2024-10-27 17:59:01,829 (asr_inference:509) INFO: speech length: 36743
2024-10-27 17:59:03,264 (beam_search:428) INFO: decoder input length: 28
2024-10-27 17:59:03,265 (beam_search:429) INFO: max output length: 28
2024-10-27 17:59:03,265 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:03,278 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:03,278 (beam_search:476) INFO:  -0.24 * 1.0 =  -0.24 for ctc
2024-10-27 17:59:03,278 (beam_search:479) INFO: total log probability: -0.24
2024-10-27 17:59:03,278 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:59:03,278 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:03,278 (beam_search:483) INFO: best hypo: ▁ON

2024-10-27 17:59:03,280 (asr_inference:509) INFO: speech length: 22500
2024-10-27 17:59:04,291 (beam_search:428) INFO: decoder input length: 17
2024-10-27 17:59:04,291 (beam_search:429) INFO: max output length: 17
2024-10-27 17:59:04,291 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:04,308 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:04,308 (beam_search:476) INFO:  -0.29 * 1.0 =  -0.29 for ctc
2024-10-27 17:59:04,308 (beam_search:479) INFO: total log probability: -0.29
2024-10-27 17:59:04,308 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:59:04,308 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:04,308 (beam_search:483) INFO: best hypo: ▁I'M▁GOOD

2024-10-27 17:59:04,311 (asr_inference:509) INFO: speech length: 138849
2024-10-27 17:59:09,360 (beam_search:428) INFO: decoder input length: 107
2024-10-27 17:59:09,361 (beam_search:429) INFO: max output length: 107
2024-10-27 17:59:09,361 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:09,562 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:09,562 (beam_search:476) INFO:  -3.08 * 1.0 =  -3.08 for ctc
2024-10-27 17:59:09,562 (beam_search:479) INFO: total log probability: -3.08
2024-10-27 17:59:09,562 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:59:09,562 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:09,562 (beam_search:483) INFO: best hypo: ▁IVE▁BEEN▁ABOUT▁AND▁ENERGY▁THROUGH▁LIGHT▁BULBS▁AND

2024-10-27 17:59:09,564 (asr_inference:509) INFO: speech length: 95614
2024-10-27 17:59:13,100 (beam_search:428) INFO: decoder input length: 74
2024-10-27 17:59:13,100 (beam_search:429) INFO: max output length: 74
2024-10-27 17:59:13,100 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:13,230 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:13,231 (beam_search:476) INFO:  -2.68 * 1.0 =  -2.68 for ctc
2024-10-27 17:59:13,231 (beam_search:479) INFO: total log probability: -2.68
2024-10-27 17:59:13,231 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:59:13,231 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:13,231 (beam_search:483) INFO: best hypo: ▁THEY▁IN▁A▁CIRCUIT▁A▁D▁A▁LIGHT▁BULB▁AND

2024-10-27 17:59:13,233 (asr_inference:509) INFO: speech length: 256427
2024-10-27 17:59:23,685 (beam_search:428) INFO: decoder input length: 199
2024-10-27 17:59:23,685 (beam_search:429) INFO: max output length: 199
2024-10-27 17:59:23,685 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:24,912 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:24,913 (beam_search:476) INFO:  -5.63 * 1.0 =  -5.63 for ctc
2024-10-27 17:59:24,913 (beam_search:479) INFO: total log probability: -5.63
2024-10-27 17:59:24,913 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:59:24,913 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:24,913 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁IN▁A▁CIRCUIT▁PUT▁A▁CIRCUIT▁TOGETHER▁SO▁PART▁A▁THERE'S▁THERE'S▁NO▁THING▁AS▁A▁CIRCUIT▁A▁CIRCUIT▁IS▁JUST▁LIKE▁A▁PATHWAY▁FOR▁THE▁ENERGY▁TO

2024-10-27 17:59:24,916 (asr_inference:509) INFO: speech length: 103826
2024-10-27 17:59:28,725 (beam_search:428) INFO: decoder input length: 80
2024-10-27 17:59:28,725 (beam_search:429) INFO: max output length: 80
2024-10-27 17:59:28,725 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:28,862 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:28,862 (beam_search:476) INFO:  -2.49 * 1.0 =  -2.49 for ctc
2024-10-27 17:59:28,862 (beam_search:479) INFO: total log probability: -2.49
2024-10-27 17:59:28,863 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:59:28,863 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:28,863 (beam_search:483) INFO: best hypo: ▁HAVED▁INSIDE▁THEM▁AND▁ARE▁USED▁TO▁POWER

2024-10-27 17:59:28,865 (asr_inference:509) INFO: speech length: 99526
2024-10-27 17:59:32,357 (beam_search:428) INFO: decoder input length: 77
2024-10-27 17:59:32,357 (beam_search:429) INFO: max output length: 77
2024-10-27 17:59:32,357 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:32,559 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:32,559 (beam_search:476) INFO:  -3.38 * 1.0 =  -3.38 for ctc
2024-10-27 17:59:32,559 (beam_search:479) INFO: total log probability: -3.38
2024-10-27 17:59:32,559 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:59:32,559 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:32,559 (beam_search:483) INFO: best hypo: ▁IF▁THE▁D'T▁THE▁BULB▁WOULD▁NOT▁LIGHT▁UP▁BECAUSE▁THERE▁IS▁NO▁ENERGY

2024-10-27 17:59:32,561 (asr_inference:509) INFO: speech length: 63626
2024-10-27 17:59:34,884 (beam_search:428) INFO: decoder input length: 49
2024-10-27 17:59:34,884 (beam_search:429) INFO: max output length: 49
2024-10-27 17:59:34,884 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:34,945 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:34,946 (beam_search:476) INFO:  -0.73 * 1.0 =  -0.73 for ctc
2024-10-27 17:59:34,946 (beam_search:479) INFO: total log probability: -0.73
2024-10-27 17:59:34,946 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:59:34,946 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:34,946 (beam_search:483) INFO: best hypo: ▁THE▁D▁CELL▁IS▁THE▁OF▁ENERGY

2024-10-27 17:59:34,948 (asr_inference:509) INFO: speech length: 61473
2024-10-27 17:59:37,227 (beam_search:428) INFO: decoder input length: 47
2024-10-27 17:59:37,227 (beam_search:429) INFO: max output length: 47
2024-10-27 17:59:37,227 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:37,286 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:37,286 (beam_search:476) INFO:  -0.90 * 1.0 =  -0.90 for ctc
2024-10-27 17:59:37,286 (beam_search:479) INFO: total log probability: -0.90
2024-10-27 17:59:37,286 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:59:37,286 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:37,286 (beam_search:483) INFO: best hypo: ▁THE▁D▁CELL▁POWER▁INTO▁THE▁CIRCUIT

2024-10-27 17:59:37,289 (asr_inference:509) INFO: speech length: 177107
2024-10-27 17:59:44,284 (beam_search:428) INFO: decoder input length: 137
2024-10-27 17:59:44,284 (beam_search:429) INFO: max output length: 137
2024-10-27 17:59:44,284 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:44,478 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:44,478 (beam_search:476) INFO:  -2.58 * 1.0 =  -2.58 for ctc
2024-10-27 17:59:44,478 (beam_search:479) INFO: total log probability: -2.58
2024-10-27 17:59:44,478 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:59:44,478 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:44,478 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁THE▁THE▁TO▁IT▁THROUGH▁AND

2024-10-27 17:59:44,480 (asr_inference:509) INFO: speech length: 103025
2024-10-27 17:59:48,166 (beam_search:428) INFO: decoder input length: 79
2024-10-27 17:59:48,167 (beam_search:429) INFO: max output length: 79
2024-10-27 17:59:48,167 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:48,246 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:48,247 (beam_search:476) INFO:  -0.92 * 1.0 =  -0.92 for ctc
2024-10-27 17:59:48,247 (beam_search:479) INFO: total log probability: -0.92
2024-10-27 17:59:48,247 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:59:48,247 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:48,247 (beam_search:483) INFO: best hypo: ▁WIRES▁THE▁ELECTRICITY▁OR▁ENERGY

2024-10-27 17:59:48,249 (asr_inference:509) INFO: speech length: 62399
2024-10-27 17:59:50,619 (beam_search:428) INFO: decoder input length: 48
2024-10-27 17:59:50,619 (beam_search:429) INFO: max output length: 48
2024-10-27 17:59:50,620 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:50,663 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:50,664 (beam_search:476) INFO:  -0.93 * 1.0 =  -0.93 for ctc
2024-10-27 17:59:50,664 (beam_search:479) INFO: total log probability: -0.93
2024-10-27 17:59:50,664 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:59:50,664 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:50,664 (beam_search:483) INFO: best hypo: ▁WIRES▁CARRY▁THE▁ENERGY▁TO

2024-10-27 17:59:50,667 (asr_inference:509) INFO: speech length: 126533
2024-10-27 17:59:55,313 (beam_search:428) INFO: decoder input length: 98
2024-10-27 17:59:55,313 (beam_search:429) INFO: max output length: 98
2024-10-27 17:59:55,313 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:55,482 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:55,482 (beam_search:476) INFO:  -1.45 * 1.0 =  -1.45 for ctc
2024-10-27 17:59:55,482 (beam_search:479) INFO: total log probability: -1.45
2024-10-27 17:59:55,482 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:59:55,482 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:55,482 (beam_search:483) INFO: best hypo: ▁THE▁LIGHT▁BULBS▁ARE▁LIKE▁OF▁THE▁ENERGY▁AND▁LIGHT

2024-10-27 17:59:55,485 (asr_inference:509) INFO: speech length: 61139
2024-10-27 17:59:57,752 (beam_search:428) INFO: decoder input length: 47
2024-10-27 17:59:57,752 (beam_search:429) INFO: max output length: 47
2024-10-27 17:59:57,752 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:57,802 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:57,802 (beam_search:476) INFO:  -0.94 * 1.0 =  -0.94 for ctc
2024-10-27 17:59:57,802 (beam_search:479) INFO: total log probability: -0.94
2024-10-27 17:59:57,802 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:59:57,803 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:57,803 (beam_search:483) INFO: best hypo: ▁BULBS▁THE▁ENERGY▁FROM▁THE

2024-10-27 17:59:57,805 (asr_inference:509) INFO: speech length: 127773
2024-10-27 18:00:02,410 (beam_search:428) INFO: decoder input length: 99
2024-10-27 18:00:02,410 (beam_search:429) INFO: max output length: 99
2024-10-27 18:00:02,410 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:02,673 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:02,673 (beam_search:476) INFO:  -3.68 * 1.0 =  -3.68 for ctc
2024-10-27 18:00:02,673 (beam_search:479) INFO: total log probability: -3.68
2024-10-27 18:00:02,673 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:00:02,673 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:02,673 (beam_search:483) INFO: best hypo: ▁THE▁LIGHT▁BULB▁THAT▁ENERGY▁AND▁MAKES▁THE▁IN▁THERE▁GET▁SO▁HOT▁IT▁LIGHT

2024-10-27 18:00:02,675 (asr_inference:509) INFO: speech length: 52627
2024-10-27 18:00:04,654 (beam_search:428) INFO: decoder input length: 40
2024-10-27 18:00:04,654 (beam_search:429) INFO: max output length: 40
2024-10-27 18:00:04,654 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:04,704 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:04,705 (beam_search:476) INFO:  -0.46 * 1.0 =  -0.46 for ctc
2024-10-27 18:00:04,705 (beam_search:479) INFO: total log probability: -0.46
2024-10-27 18:00:04,705 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:00:04,705 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:04,705 (beam_search:483) INFO: best hypo: ▁THAT▁ENERGY▁IS▁FLOWING▁THROUGH▁THE▁CIRCUIT

2024-10-27 18:00:04,707 (asr_inference:509) INFO: speech length: 320754
2024-10-27 18:00:18,900 (beam_search:428) INFO: decoder input length: 250
2024-10-27 18:00:18,900 (beam_search:429) INFO: max output length: 250
2024-10-27 18:00:18,900 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:20,745 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:20,745 (beam_search:476) INFO:  -6.18 * 1.0 =  -6.18 for ctc
2024-10-27 18:00:20,745 (beam_search:479) INFO: total log probability: -6.18
2024-10-27 18:00:20,745 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:00:20,745 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:20,745 (beam_search:483) INFO: best hypo: ▁SEE▁THE▁ENERGY▁FLOWING▁THROUGH▁THE▁WIRES▁THROUGH▁THE▁LIGHT▁BULB▁WELL▁SO▁THE▁ENERGY▁OUT▁OF▁THE▁D▁CELL▁THE▁WIRES▁ITS▁THE▁BULB▁GOES▁THROUGH▁THE▁AND▁OUT▁THROUGH▁THE▁BULB▁INTO▁THE▁POSITIVE▁OF▁THE

2024-10-27 18:00:20,747 (asr_inference:509) INFO: speech length: 103576
2024-10-27 18:00:24,501 (beam_search:428) INFO: decoder input length: 80
2024-10-27 18:00:24,502 (beam_search:429) INFO: max output length: 80
2024-10-27 18:00:24,502 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:24,703 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:24,703 (beam_search:476) INFO:  -1.77 * 1.0 =  -1.77 for ctc
2024-10-27 18:00:24,703 (beam_search:479) INFO: total log probability: -1.77
2024-10-27 18:00:24,703 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:00:24,703 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:24,703 (beam_search:483) INFO: best hypo: ▁SO▁THE▁ENERGY'S▁OUT▁THE▁NEGATIVE▁SIDE▁AND▁IN▁THROUGH▁THE▁POSITIVE▁SIDE

2024-10-27 18:00:24,706 (asr_inference:509) INFO: speech length: 111600
2024-10-27 18:00:28,726 (beam_search:428) INFO: decoder input length: 86
2024-10-27 18:00:28,726 (beam_search:429) INFO: max output length: 86
2024-10-27 18:00:28,726 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:28,907 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:28,907 (beam_search:476) INFO:  -2.73 * 1.0 =  -2.73 for ctc
2024-10-27 18:00:28,907 (beam_search:479) INFO: total log probability: -2.73
2024-10-27 18:00:28,907 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:00:28,907 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:28,907 (beam_search:483) INFO: best hypo: ▁THE▁POSITIVE▁ISING▁THE▁ENERGY▁THAT▁COMES▁OUT▁OF▁THE▁SIDE

2024-10-27 18:00:28,910 (asr_inference:509) INFO: speech length: 118184
2024-10-27 18:00:33,233 (beam_search:428) INFO: decoder input length: 91
2024-10-27 18:00:33,234 (beam_search:429) INFO: max output length: 91
2024-10-27 18:00:33,234 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:33,426 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:33,427 (beam_search:476) INFO:  -2.46 * 1.0 =  -2.46 for ctc
2024-10-27 18:00:33,427 (beam_search:479) INFO: total log probability: -2.46
2024-10-27 18:00:33,427 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:00:33,427 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:33,427 (beam_search:483) INFO: best hypo: ▁WELL▁ELECTRICITY▁INTO▁THE▁D▁CELL▁THE▁POSITIVE▁FROM▁THE▁LIGHT▁BULB

2024-10-27 18:00:33,430 (asr_inference:509) INFO: speech length: 48255
2024-10-27 18:00:35,322 (beam_search:428) INFO: decoder input length: 37
2024-10-27 18:00:35,322 (beam_search:429) INFO: max output length: 37
2024-10-27 18:00:35,322 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:35,346 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:35,346 (beam_search:476) INFO:  -0.87 * 1.0 =  -0.87 for ctc
2024-10-27 18:00:35,346 (beam_search:479) INFO: total log probability: -0.87
2024-10-27 18:00:35,346 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:00:35,346 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:35,346 (beam_search:483) INFO: best hypo: ▁THE▁FLOWING▁THROUGH

2024-10-27 18:00:35,348 (asr_inference:509) INFO: speech length: 193218
2024-10-27 18:00:42,714 (beam_search:428) INFO: decoder input length: 150
2024-10-27 18:00:42,714 (beam_search:429) INFO: max output length: 150
2024-10-27 18:00:42,714 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:43,039 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:43,039 (beam_search:476) INFO:  -1.97 * 1.0 =  -1.97 for ctc
2024-10-27 18:00:43,041 (beam_search:479) INFO: total log probability: -1.97
2024-10-27 18:00:43,041 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:00:43,041 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:43,041 (beam_search:483) INFO: best hypo: ▁WELL▁SO▁IT▁SO▁THE▁NEGATIVE▁THE▁ENERGY▁AND▁THE▁POSITIVE▁THE

2024-10-27 18:00:43,043 (asr_inference:509) INFO: speech length: 123033
2024-10-27 18:00:47,407 (beam_search:428) INFO: decoder input length: 95
2024-10-27 18:00:47,407 (beam_search:429) INFO: max output length: 95
2024-10-27 18:00:47,407 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:47,662 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:47,663 (beam_search:476) INFO:  -1.47 * 1.0 =  -1.47 for ctc
2024-10-27 18:00:47,663 (beam_search:479) INFO: total log probability: -1.47
2024-10-27 18:00:47,663 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:00:47,663 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:47,663 (beam_search:483) INFO: best hypo: ▁ON▁THE▁D▁CELL▁I▁THE▁FLOWING▁OUT▁OF▁THE▁AND▁IN▁THROUGH▁THE▁POSITIVE

2024-10-27 18:00:47,666 (asr_inference:509) INFO: speech length: 146728
2024-10-27 18:00:53,361 (beam_search:428) INFO: decoder input length: 114
2024-10-27 18:00:53,361 (beam_search:429) INFO: max output length: 114
2024-10-27 18:00:53,361 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:53,688 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:53,688 (beam_search:476) INFO:  -5.18 * 1.0 =  -5.18 for ctc
2024-10-27 18:00:53,688 (beam_search:479) INFO: total log probability: -5.18
2024-10-27 18:00:53,688 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:00:53,688 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:53,689 (beam_search:483) INFO: best hypo: ▁WELL▁IF▁YOU▁IT▁I▁IT▁WILL▁THROUGH▁THE▁BULB▁AND▁THE▁BULB▁WILL▁STILL▁LIGHT

2024-10-27 18:00:53,692 (asr_inference:509) INFO: speech length: 124866
2024-10-27 18:00:58,186 (beam_search:428) INFO: decoder input length: 97
2024-10-27 18:00:58,186 (beam_search:429) INFO: max output length: 97
2024-10-27 18:00:58,186 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:58,413 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:58,413 (beam_search:476) INFO:  -2.23 * 1.0 =  -2.23 for ctc
2024-10-27 18:00:58,413 (beam_search:479) INFO: total log probability: -2.23
2024-10-27 18:00:58,413 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:00:58,413 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:58,414 (beam_search:483) INFO: best hypo: ▁WELL▁WHEN▁YOU▁THE▁D▁CELL▁THE▁ENERGY▁I▁THINK▁STILL▁THE▁WAY

2024-10-27 18:00:58,417 (asr_inference:509) INFO: speech length: 260467
2024-10-27 18:01:09,029 (beam_search:428) INFO: decoder input length: 202
2024-10-27 18:01:09,029 (beam_search:429) INFO: max output length: 202
2024-10-27 18:01:09,029 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:10,293 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:10,293 (beam_search:476) INFO:  -6.07 * 1.0 =  -6.07 for ctc
2024-10-27 18:01:10,293 (beam_search:479) INFO: total log probability: -6.07
2024-10-27 18:01:10,294 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:01:10,294 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:10,294 (beam_search:483) INFO: best hypo: ▁THE▁IS▁THAT▁A▁WIRES▁THE▁OF▁THE▁D▁CELL▁AND▁THEN▁THAT▁WIRES▁THE▁BULB▁OF▁THE▁BULB▁AND▁THEN▁WITH▁THE▁POSITIVE▁ANOTHER▁WIRES▁AND▁THAT▁SAMES▁THE

2024-10-27 18:01:10,296 (asr_inference:509) INFO: speech length: 229315
2024-10-27 18:01:19,363 (beam_search:428) INFO: decoder input length: 178
2024-10-27 18:01:19,363 (beam_search:429) INFO: max output length: 178
2024-10-27 18:01:19,363 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:20,192 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:20,193 (beam_search:476) INFO:  -2.86 * 1.0 =  -2.86 for ctc
2024-10-27 18:01:20,193 (beam_search:479) INFO: total log probability: -2.86
2024-10-27 18:01:20,193 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:01:20,193 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:20,193 (beam_search:483) INFO: best hypo: ▁WELL▁IT▁LOOKS▁LIKE▁THAT▁THE▁ENERGY▁IS▁FLOWING▁THROUGH▁THE▁NEGATIVE▁OF▁THE▁D▁THROUGH▁THE▁BULB▁AND▁BACK▁THROUGH▁THE▁D▁CELL▁POSITIVE

2024-10-27 18:01:20,195 (asr_inference:509) INFO: speech length: 173196
2024-10-27 18:01:26,523 (beam_search:428) INFO: decoder input length: 134
2024-10-27 18:01:26,523 (beam_search:429) INFO: max output length: 134
2024-10-27 18:01:26,523 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:26,903 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:26,903 (beam_search:476) INFO:  -3.99 * 1.0 =  -3.99 for ctc
2024-10-27 18:01:26,903 (beam_search:479) INFO: total log probability: -3.99
2024-10-27 18:01:26,904 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:01:26,904 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:26,904 (beam_search:483) INFO: best hypo: ▁THE▁HAVE▁TO▁BE▁CONNECTED▁TO▁ALL▁THE▁POINTS▁TO▁GET▁THE▁TO▁THE▁RIGHT▁WAY

2024-10-27 18:01:26,906 (asr_inference:509) INFO: speech length: 108678
2024-10-27 18:01:30,927 (beam_search:428) INFO: decoder input length: 84
2024-10-27 18:01:30,927 (beam_search:429) INFO: max output length: 84
2024-10-27 18:01:30,927 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:31,113 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:31,113 (beam_search:476) INFO:  -2.91 * 1.0 =  -2.91 for ctc
2024-10-27 18:01:31,113 (beam_search:479) INFO: total log probability: -2.91
2024-10-27 18:01:31,113 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:01:31,113 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:31,113 (beam_search:483) INFO: best hypo: ▁THAT▁ONE▁OF▁THE▁WOULD▁NOT▁GETED▁AND▁THE▁BULB▁WOULD▁TURN

2024-10-27 18:01:31,115 (asr_inference:509) INFO: speech length: 37501
2024-10-27 18:01:32,518 (beam_search:428) INFO: decoder input length: 28
2024-10-27 18:01:32,518 (beam_search:429) INFO: max output length: 28
2024-10-27 18:01:32,518 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:32,548 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:32,548 (beam_search:476) INFO:  -0.15 * 1.0 =  -0.15 for ctc
2024-10-27 18:01:32,549 (beam_search:479) INFO: total log probability: -0.15
2024-10-27 18:01:32,549 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:01:32,549 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:32,549 (beam_search:483) INFO: best hypo: ▁THE▁LIGHT▁BULB▁TURNS▁ON

2024-10-27 18:01:32,551 (asr_inference:509) INFO: speech length: 114727
2024-10-27 18:01:36,774 (beam_search:428) INFO: decoder input length: 89
2024-10-27 18:01:36,775 (beam_search:429) INFO: max output length: 89
2024-10-27 18:01:36,775 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:36,961 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:36,961 (beam_search:476) INFO:  -2.03 * 1.0 =  -2.03 for ctc
2024-10-27 18:01:36,961 (beam_search:479) INFO: total log probability: -2.03
2024-10-27 18:01:36,961 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:01:36,961 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:36,961 (beam_search:483) INFO: best hypo: ▁WE'VE▁BEEN▁LEARNING▁ABOUT▁AND▁SOLAR▁ENERGY▁WITH▁WITH▁A

2024-10-27 18:01:36,964 (asr_inference:509) INFO: speech length: 35023
2024-10-27 18:01:38,356 (beam_search:428) INFO: decoder input length: 26
2024-10-27 18:01:38,356 (beam_search:429) INFO: max output length: 26
2024-10-27 18:01:38,356 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:38,377 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:38,377 (beam_search:476) INFO:  -1.73 * 1.0 =  -1.73 for ctc
2024-10-27 18:01:38,377 (beam_search:479) INFO: total log probability: -1.73
2024-10-27 18:01:38,377 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:01:38,377 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:38,377 (beam_search:483) INFO: best hypo: ▁IS▁POWERING▁THE

2024-10-27 18:01:38,379 (asr_inference:509) INFO: speech length: 149651
2024-10-27 18:01:43,894 (beam_search:428) INFO: decoder input length: 116
2024-10-27 18:01:43,894 (beam_search:429) INFO: max output length: 116
2024-10-27 18:01:43,894 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:44,168 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:44,169 (beam_search:476) INFO:  -2.56 * 1.0 =  -2.56 for ctc
2024-10-27 18:01:44,169 (beam_search:479) INFO: total log probability: -2.56
2024-10-27 18:01:44,169 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:01:44,169 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:44,169 (beam_search:483) INFO: best hypo: ▁THE▁IS▁AND▁THE▁IS▁SO▁THAT▁SO▁THAT▁IT▁SHOWS▁THAT▁THE▁IS

2024-10-27 18:01:44,171 (asr_inference:509) INFO: speech length: 30864
2024-10-27 18:01:45,414 (beam_search:428) INFO: decoder input length: 23
2024-10-27 18:01:45,414 (beam_search:429) INFO: max output length: 23
2024-10-27 18:01:45,414 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:45,427 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:45,427 (beam_search:476) INFO:  -0.82 * 1.0 =  -0.82 for ctc
2024-10-27 18:01:45,427 (beam_search:479) INFO: total log probability: -0.82
2024-10-27 18:01:45,427 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:01:45,427 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:45,427 (beam_search:483) INFO: best hypo: ▁THE▁IS

2024-10-27 18:01:45,429 (asr_inference:509) INFO: speech length: 82171
2024-10-27 18:01:48,335 (beam_search:428) INFO: decoder input length: 63
2024-10-27 18:01:48,335 (beam_search:429) INFO: max output length: 63
2024-10-27 18:01:48,335 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:48,412 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:48,412 (beam_search:476) INFO:  -0.87 * 1.0 =  -0.87 for ctc
2024-10-27 18:01:48,412 (beam_search:479) INFO: total log probability: -0.87
2024-10-27 18:01:48,412 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:01:48,412 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:48,412 (beam_search:483) INFO: best hypo: ▁THE▁IS▁MOVING▁ENERGY▁IS▁THROUGH▁THE

2024-10-27 18:01:48,414 (asr_inference:509) INFO: speech length: 125988
2024-10-27 18:01:52,922 (beam_search:428) INFO: decoder input length: 97
2024-10-27 18:01:52,922 (beam_search:429) INFO: max output length: 97
2024-10-27 18:01:52,922 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:53,135 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:53,135 (beam_search:476) INFO:  -2.67 * 1.0 =  -2.67 for ctc
2024-10-27 18:01:53,135 (beam_search:479) INFO: total log probability: -2.67
2024-10-27 18:01:53,135 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:01:53,135 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:53,135 (beam_search:483) INFO: best hypo: ▁WELL▁THERE▁WAS▁THE▁WIRES▁HAVE▁TO▁BE▁CONNECTED▁TO▁THE▁RIGHT

2024-10-27 18:01:53,137 (asr_inference:509) INFO: speech length: 38931
2024-10-27 18:01:54,643 (beam_search:428) INFO: decoder input length: 29
2024-10-27 18:01:54,643 (beam_search:429) INFO: max output length: 29
2024-10-27 18:01:54,643 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:54,662 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:54,662 (beam_search:476) INFO:  -0.25 * 1.0 =  -0.25 for ctc
2024-10-27 18:01:54,663 (beam_search:479) INFO: total log probability: -0.25
2024-10-27 18:01:54,663 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:01:54,663 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:54,663 (beam_search:483) INFO: best hypo: ▁ENERGY▁TO▁THE

2024-10-27 18:01:54,665 (asr_inference:509) INFO: speech length: 94508
2024-10-27 18:01:58,287 (beam_search:428) INFO: decoder input length: 73
2024-10-27 18:01:58,287 (beam_search:429) INFO: max output length: 73
2024-10-27 18:01:58,287 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:58,424 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:58,424 (beam_search:476) INFO:  -0.84 * 1.0 =  -0.84 for ctc
2024-10-27 18:01:58,424 (beam_search:479) INFO: total log probability: -0.84
2024-10-27 18:01:58,424 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:01:58,424 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:58,425 (beam_search:483) INFO: best hypo: ▁IN▁THE▁CIRCUIT▁THE▁OPEN▁BECAUSE▁THE▁SWITCH▁IS▁NOT▁DOWN

2024-10-27 18:01:58,427 (asr_inference:509) INFO: speech length: 64647
2024-10-27 18:02:00,813 (beam_search:428) INFO: decoder input length: 50
2024-10-27 18:02:00,813 (beam_search:429) INFO: max output length: 50
2024-10-27 18:02:00,813 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:00,869 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:00,869 (beam_search:476) INFO:  -2.10 * 1.0 =  -2.10 for ctc
2024-10-27 18:02:00,869 (beam_search:479) INFO: total log probability: -2.10
2024-10-27 18:02:00,869 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:02:00,869 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:00,869 (beam_search:483) INFO: best hypo: ▁TO▁CAUSE▁ALL▁THE▁ARE▁TOUCHING

2024-10-27 18:02:00,871 (asr_inference:509) INFO: speech length: 85031
2024-10-27 18:02:03,877 (beam_search:428) INFO: decoder input length: 65
2024-10-27 18:02:03,877 (beam_search:429) INFO: max output length: 65
2024-10-27 18:02:03,877 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:03,982 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:03,982 (beam_search:476) INFO:  -2.43 * 1.0 =  -2.43 for ctc
2024-10-27 18:02:03,982 (beam_search:479) INFO: total log probability: -2.43
2024-10-27 18:02:03,982 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:02:03,982 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:03,982 (beam_search:483) INFO: best hypo: ▁IS▁FLOWING▁THROUGH▁THE▁ALL▁THE▁POINTS▁ARE▁TOUCHING

2024-10-27 18:02:03,984 (asr_inference:509) INFO: speech length: 110922
2024-10-27 18:02:07,931 (beam_search:428) INFO: decoder input length: 86
2024-10-27 18:02:07,931 (beam_search:429) INFO: max output length: 86
2024-10-27 18:02:07,931 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:08,102 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:08,102 (beam_search:476) INFO:  -1.68 * 1.0 =  -1.68 for ctc
2024-10-27 18:02:08,102 (beam_search:479) INFO: total log probability: -1.68
2024-10-27 18:02:08,102 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:02:08,102 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:08,102 (beam_search:483) INFO: best hypo: S▁THE▁AND▁ENERGY▁CAN'T▁BACK▁INTO▁THE▁BATTERY

2024-10-27 18:02:08,105 (asr_inference:509) INFO: speech length: 34481
2024-10-27 18:02:09,471 (beam_search:428) INFO: decoder input length: 26
2024-10-27 18:02:09,471 (beam_search:429) INFO: max output length: 26
2024-10-27 18:02:09,471 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:09,495 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:09,495 (beam_search:476) INFO:  -0.14 * 1.0 =  -0.14 for ctc
2024-10-27 18:02:09,495 (beam_search:479) INFO: total log probability: -0.14
2024-10-27 18:02:09,495 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:02:09,495 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:09,496 (beam_search:483) INFO: best hypo: ▁YOU▁HAVE▁TO▁THE▁SWITCH

2024-10-27 18:02:09,498 (asr_inference:509) INFO: speech length: 152584
2024-10-27 18:02:15,013 (beam_search:428) INFO: decoder input length: 118
2024-10-27 18:02:15,013 (beam_search:429) INFO: max output length: 118
2024-10-27 18:02:15,013 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:15,370 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:15,370 (beam_search:476) INFO:  -3.03 * 1.0 =  -3.03 for ctc
2024-10-27 18:02:15,370 (beam_search:479) INFO: total log probability: -3.03
2024-10-27 18:02:15,370 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:02:15,370 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:15,370 (beam_search:483) INFO: best hypo: ▁ITS▁AND▁THE▁WHEN▁THE▁SWITCH▁IS▁THE▁MOTOR▁WILL▁RUN▁WHEN▁THE▁IS▁OPEN▁THE▁MOTOR

2024-10-27 18:02:15,373 (asr_inference:509) INFO: speech length: 189858
2024-10-27 18:02:23,067 (beam_search:428) INFO: decoder input length: 147
2024-10-27 18:02:23,067 (beam_search:429) INFO: max output length: 147
2024-10-27 18:02:23,067 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:23,422 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:23,422 (beam_search:476) INFO:  -3.64 * 1.0 =  -3.64 for ctc
2024-10-27 18:02:23,422 (beam_search:479) INFO: total log probability: -3.64
2024-10-27 18:02:23,422 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:02:23,422 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:23,423 (beam_search:483) INFO: best hypo: ▁SOLAR▁THES▁ENERGY▁AND▁IT▁INTO▁THE▁ELECTRICITY▁FOR▁A▁CIRCUIT

2024-10-27 18:02:23,426 (asr_inference:509) INFO: speech length: 90236
2024-10-27 18:02:26,698 (beam_search:428) INFO: decoder input length: 69
2024-10-27 18:02:26,698 (beam_search:429) INFO: max output length: 69
2024-10-27 18:02:26,698 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:26,845 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:26,845 (beam_search:476) INFO:  -3.93 * 1.0 =  -3.93 for ctc
2024-10-27 18:02:26,845 (beam_search:479) INFO: total log probability: -3.93
2024-10-27 18:02:26,845 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:02:26,845 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:26,845 (beam_search:483) INFO: best hypo: ▁THE▁SOLAR▁CELL▁IS▁TAKING▁THES▁ENERGY▁AND▁THE▁MOTOR▁RUN

2024-10-27 18:02:26,848 (asr_inference:509) INFO: speech length: 44589
2024-10-27 18:02:28,548 (beam_search:428) INFO: decoder input length: 34
2024-10-27 18:02:28,548 (beam_search:429) INFO: max output length: 34
2024-10-27 18:02:28,548 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:28,589 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:28,589 (beam_search:476) INFO:  -1.08 * 1.0 =  -1.08 for ctc
2024-10-27 18:02:28,589 (beam_search:479) INFO: total log probability: -1.08
2024-10-27 18:02:28,589 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:02:28,589 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:28,589 (beam_search:483) INFO: best hypo: ▁ENERGY▁HAS▁TO▁FLOW▁THROUGH▁THE▁CIRCUIT

2024-10-27 18:02:28,592 (asr_inference:509) INFO: speech length: 61115
2024-10-27 18:02:30,749 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:02:30,749 (beam_search:429) INFO: max output length: 47
2024-10-27 18:02:30,749 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:30,814 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:30,814 (beam_search:476) INFO:  -2.08 * 1.0 =  -2.08 for ctc
2024-10-27 18:02:30,814 (beam_search:479) INFO: total log probability: -2.08
2024-10-27 18:02:30,814 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:02:30,814 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:30,814 (beam_search:483) INFO: best hypo: ▁THE▁SUNS▁LIGHT▁IS▁THELAR▁CELL

2024-10-27 18:02:30,817 (asr_inference:509) INFO: speech length: 266725
2024-10-27 18:02:41,988 (beam_search:428) INFO: decoder input length: 207
2024-10-27 18:02:41,988 (beam_search:429) INFO: max output length: 207
2024-10-27 18:02:41,988 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:43,319 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:43,319 (beam_search:476) INFO:  -4.00 * 1.0 =  -4.00 for ctc
2024-10-27 18:02:43,320 (beam_search:479) INFO: total log probability: -4.00
2024-10-27 18:02:43,320 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:02:43,320 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:43,320 (beam_search:483) INFO: best hypo: ▁WELL▁IF▁THE▁OUT▁EVERYTHING▁WOULD▁BECAUSE▁THEN▁THERE▁WOULD▁BE▁NO▁ENERGY▁FOR▁THE▁CIRCUIT▁SO▁SO▁IF▁THE▁IF▁IT▁GOT▁THERE▁WOULD▁BE▁NO▁ENERGY▁TO▁GO▁INTO▁THAT▁SOLAR▁CELL

2024-10-27 18:02:43,323 (asr_inference:509) INFO: speech length: 68084
2024-10-27 18:02:45,714 (beam_search:428) INFO: decoder input length: 52
2024-10-27 18:02:45,714 (beam_search:429) INFO: max output length: 52
2024-10-27 18:02:45,714 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:45,786 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:45,786 (beam_search:476) INFO:  -1.49 * 1.0 =  -1.49 for ctc
2024-10-27 18:02:45,786 (beam_search:479) INFO: total log probability: -1.49
2024-10-27 18:02:45,787 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:02:45,787 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:45,787 (beam_search:483) INFO: best hypo: ▁THE▁WHEN▁YOU▁ON▁THE▁IT▁MAKES▁THE

2024-10-27 18:02:45,789 (asr_inference:509) INFO: speech length: 68879
2024-10-27 18:02:48,360 (beam_search:428) INFO: decoder input length: 53
2024-10-27 18:02:48,361 (beam_search:429) INFO: max output length: 53
2024-10-27 18:02:48,361 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:48,439 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:48,439 (beam_search:476) INFO:  -1.75 * 1.0 =  -1.75 for ctc
2024-10-27 18:02:48,440 (beam_search:479) INFO: total log probability: -1.75
2024-10-27 18:02:48,440 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:02:48,440 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:48,440 (beam_search:483) INFO: best hypo: ▁CAUSE▁THERE▁IS▁NO▁ENERGY▁FOR▁TO▁RUN

2024-10-27 18:02:48,442 (asr_inference:509) INFO: speech length: 57536
2024-10-27 18:02:50,621 (beam_search:428) INFO: decoder input length: 44
2024-10-27 18:02:50,621 (beam_search:429) INFO: max output length: 44
2024-10-27 18:02:50,621 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:50,676 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:50,676 (beam_search:476) INFO:  -2.26 * 1.0 =  -2.26 for ctc
2024-10-27 18:02:50,676 (beam_search:479) INFO: total log probability: -2.26
2024-10-27 18:02:50,676 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:02:50,676 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:50,676 (beam_search:483) INFO: best hypo: ▁THE▁MOTOR▁STARTS▁NOW▁THERE▁IS▁ENERGY

2024-10-27 18:02:50,679 (asr_inference:509) INFO: speech length: 90795
2024-10-27 18:02:54,040 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:02:54,040 (beam_search:429) INFO: max output length: 70
2024-10-27 18:02:54,040 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:54,130 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:54,131 (beam_search:476) INFO:  -0.99 * 1.0 =  -0.99 for ctc
2024-10-27 18:02:54,131 (beam_search:479) INFO: total log probability: -0.99
2024-10-27 18:02:54,131 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:02:54,131 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:54,131 (beam_search:483) INFO: best hypo: ▁THAT▁THAT▁THAT▁MAKES▁THE▁MOTOR▁RUN

2024-10-27 18:02:54,134 (asr_inference:509) INFO: speech length: 105887
2024-10-27 18:02:57,858 (beam_search:428) INFO: decoder input length: 82
2024-10-27 18:02:57,858 (beam_search:429) INFO: max output length: 82
2024-10-27 18:02:57,858 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:58,030 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:58,030 (beam_search:476) INFO:  -1.69 * 1.0 =  -1.69 for ctc
2024-10-27 18:02:58,030 (beam_search:479) INFO: total log probability: -1.69
2024-10-27 18:02:58,030 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:02:58,030 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:58,030 (beam_search:483) INFO: best hypo: ▁IS▁THE▁ENERGY▁FROM▁THE▁TO▁THE▁AND▁BACK▁TO▁THE▁BATTERY

2024-10-27 18:02:58,033 (asr_inference:509) INFO: speech length: 62771
2024-10-27 18:03:00,321 (beam_search:428) INFO: decoder input length: 48
2024-10-27 18:03:00,321 (beam_search:429) INFO: max output length: 48
2024-10-27 18:03:00,321 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:00,371 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:00,371 (beam_search:476) INFO:  -2.36 * 1.0 =  -2.36 for ctc
2024-10-27 18:03:00,371 (beam_search:479) INFO: total log probability: -2.36
2024-10-27 18:03:00,371 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 18:03:00,371 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:00,371 (beam_search:483) INFO: best hypo: ▁WEVE▁LEARNING▁ABOUT▁AND▁INSULATORS

2024-10-27 18:03:00,373 (asr_inference:509) INFO: speech length: 143070
2024-10-27 18:03:05,797 (beam_search:428) INFO: decoder input length: 111
2024-10-27 18:03:05,797 (beam_search:429) INFO: max output length: 111
2024-10-27 18:03:05,797 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:06,046 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:06,046 (beam_search:476) INFO:  -2.25 * 1.0 =  -2.25 for ctc
2024-10-27 18:03:06,046 (beam_search:479) INFO: total log probability: -2.25
2024-10-27 18:03:06,046 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:03:06,046 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:06,047 (beam_search:483) INFO: best hypo: ▁THE▁OPEN▁BUT▁THERE'S▁A▁OF▁THINGS▁ON▁THE▁SIDE

2024-10-27 18:03:06,049 (asr_inference:509) INFO: speech length: 187658
2024-10-27 18:03:13,146 (beam_search:428) INFO: decoder input length: 146
2024-10-27 18:03:13,146 (beam_search:429) INFO: max output length: 146
2024-10-27 18:03:13,146 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:13,610 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:13,610 (beam_search:476) INFO:  -4.38 * 1.0 =  -4.38 for ctc
2024-10-27 18:03:13,610 (beam_search:479) INFO: total log probability: -4.38
2024-10-27 18:03:13,610 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:03:13,610 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:13,610 (beam_search:483) INFO: best hypo: ▁WHEN▁YOU▁USE▁A▁THE▁WILL▁BECAUSE▁ENERGY▁IS▁FLOWING▁IF▁YOU▁USE▁AN▁THE▁MOTOR▁WILL▁NOT

2024-10-27 18:03:13,613 (asr_inference:509) INFO: speech length: 98639
2024-10-27 18:03:17,259 (beam_search:428) INFO: decoder input length: 76
2024-10-27 18:03:17,259 (beam_search:429) INFO: max output length: 76
2024-10-27 18:03:17,259 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:17,358 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:17,358 (beam_search:476) INFO:  -0.45 * 1.0 =  -0.45 for ctc
2024-10-27 18:03:17,358 (beam_search:479) INFO: total log probability: -0.45
2024-10-27 18:03:17,358 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:03:17,358 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:17,358 (beam_search:483) INFO: best hypo: ▁ALL▁THE▁MAKE▁THE▁THE▁MOTOR▁RUN

2024-10-27 18:03:17,360 (asr_inference:509) INFO: speech length: 61864
2024-10-27 18:03:19,653 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:03:19,654 (beam_search:429) INFO: max output length: 47
2024-10-27 18:03:19,654 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:19,706 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:19,706 (beam_search:476) INFO:  -1.26 * 1.0 =  -1.26 for ctc
2024-10-27 18:03:19,706 (beam_search:479) INFO: total log probability: -1.26
2024-10-27 18:03:19,706 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:03:19,706 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:19,706 (beam_search:483) INFO: best hypo: ▁WELL▁IF▁YOU▁PUT▁AN▁ON

2024-10-27 18:03:19,709 (asr_inference:509) INFO: speech length: 219161
2024-10-27 18:03:28,315 (beam_search:428) INFO: decoder input length: 170
2024-10-27 18:03:28,315 (beam_search:429) INFO: max output length: 170
2024-10-27 18:03:28,315 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:28,741 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:28,741 (beam_search:476) INFO:  -2.63 * 1.0 =  -2.63 for ctc
2024-10-27 18:03:28,742 (beam_search:479) INFO: total log probability: -2.63
2024-10-27 18:03:28,742 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:03:28,742 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:28,742 (beam_search:483) INFO: best hypo: ▁WELL▁YOU▁COULD▁LIKE▁A▁STEEL▁NAIL▁OR▁YOU▁COULD▁USE▁LIKE▁A▁OR▁OR

2024-10-27 18:03:28,744 (asr_inference:509) INFO: speech length: 223307
2024-10-27 18:03:37,800 (beam_search:428) INFO: decoder input length: 173
2024-10-27 18:03:37,800 (beam_search:429) INFO: max output length: 173
2024-10-27 18:03:37,800 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:38,486 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:38,486 (beam_search:476) INFO:  -4.63 * 1.0 =  -4.63 for ctc
2024-10-27 18:03:38,486 (beam_search:479) INFO: total log probability: -4.63
2024-10-27 18:03:38,486 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:03:38,486 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:38,486 (beam_search:483) INFO: best hypo: ▁THE▁OF▁ARE▁LIKE▁THEY▁A▁PATHWAY▁THROUGH▁THE▁SWITCH▁AND▁INSULATORS▁ARE▁JUST▁LIKE▁A▁THAT▁ELECTRICITY▁FROM▁FLOWING▁THROUGH▁THE

2024-10-27 18:03:38,490 (asr_inference:509) INFO: speech length: 19064
2024-10-27 18:03:39,440 (beam_search:428) INFO: decoder input length: 14
2024-10-27 18:03:39,440 (beam_search:429) INFO: max output length: 14
2024-10-27 18:03:39,440 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:39,449 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:39,449 (beam_search:476) INFO:  -0.78 * 1.0 =  -0.78 for ctc
2024-10-27 18:03:39,449 (beam_search:479) INFO: total log probability: -0.78
2024-10-27 18:03:39,449 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:03:39,449 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:39,449 (beam_search:483) INFO: best hypo: ▁TO

2024-10-27 18:03:39,451 (asr_inference:509) INFO: speech length: 95296
2024-10-27 18:03:42,961 (beam_search:428) INFO: decoder input length: 73
2024-10-27 18:03:42,961 (beam_search:429) INFO: max output length: 73
2024-10-27 18:03:42,961 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:43,064 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:43,064 (beam_search:476) INFO:  -2.88 * 1.0 =  -2.88 for ctc
2024-10-27 18:03:43,064 (beam_search:479) INFO: total log probability: -2.88
2024-10-27 18:03:43,064 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:03:43,064 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:43,065 (beam_search:483) INFO: best hypo: ▁IT▁IS▁BECAUSE▁STICK▁DOESN'T

2024-10-27 18:03:43,067 (asr_inference:509) INFO: speech length: 153000
2024-10-27 18:03:48,783 (beam_search:428) INFO: decoder input length: 119
2024-10-27 18:03:48,783 (beam_search:429) INFO: max output length: 119
2024-10-27 18:03:48,783 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:49,039 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:49,039 (beam_search:476) INFO:  -2.97 * 1.0 =  -2.97 for ctc
2024-10-27 18:03:49,039 (beam_search:479) INFO: total log probability: -2.97
2024-10-27 18:03:49,039 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:03:49,039 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:49,039 (beam_search:483) INFO: best hypo: ▁AND▁DON'T▁THE▁ELECTRICITYN'T▁THROUGH▁AN

2024-10-27 18:03:49,043 (asr_inference:509) INFO: speech length: 138637
2024-10-27 18:03:54,141 (beam_search:428) INFO: decoder input length: 107
2024-10-27 18:03:54,141 (beam_search:429) INFO: max output length: 107
2024-10-27 18:03:54,141 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:54,361 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:54,362 (beam_search:476) INFO:  -2.01 * 1.0 =  -2.01 for ctc
2024-10-27 18:03:54,362 (beam_search:479) INFO: total log probability: -2.01
2024-10-27 18:03:54,362 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:03:54,362 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:54,362 (beam_search:483) INFO: best hypo: ▁KIND▁OF▁LIKE▁LIKE▁AING▁NO▁ELECTRICITY▁TO▁FLOW▁THROUGH

2024-10-27 18:03:54,364 (asr_inference:509) INFO: speech length: 105305
2024-10-27 18:03:58,140 (beam_search:428) INFO: decoder input length: 81
2024-10-27 18:03:58,141 (beam_search:429) INFO: max output length: 81
2024-10-27 18:03:58,141 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:58,285 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:58,285 (beam_search:476) INFO:  -2.02 * 1.0 =  -2.02 for ctc
2024-10-27 18:03:58,285 (beam_search:479) INFO: total log probability: -2.02
2024-10-27 18:03:58,285 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:03:58,285 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:58,285 (beam_search:483) INFO: best hypo: ▁THE▁ARE▁ALL▁THE▁PARTS▁THAT▁DO▁NOT▁CONDUCT▁ELECTRICITY

2024-10-27 18:03:58,288 (asr_inference:509) INFO: speech length: 39904
2024-10-27 18:03:59,758 (beam_search:428) INFO: decoder input length: 30
2024-10-27 18:03:59,759 (beam_search:429) INFO: max output length: 30
2024-10-27 18:03:59,759 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:59,766 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:59,766 (beam_search:476) INFO:  -0.49 * 1.0 =  -0.49 for ctc
2024-10-27 18:03:59,766 (beam_search:479) INFO: total log probability: -0.49
2024-10-27 18:03:59,766 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:03:59,766 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:59,766 (beam_search:483) INFO: best hypo: 

2024-10-27 18:03:59,768 (asr_inference:509) INFO: speech length: 88494
2024-10-27 18:04:02,972 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:04:02,972 (beam_search:429) INFO: max output length: 68
2024-10-27 18:04:02,972 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:03,072 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:03,072 (beam_search:476) INFO:  -1.70 * 1.0 =  -1.70 for ctc
2024-10-27 18:04:03,072 (beam_search:479) INFO: total log probability: -1.70
2024-10-27 18:04:03,072 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:04:03,072 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:03,072 (beam_search:483) INFO: best hypo: ▁ARE▁ALL▁THE▁PARTS▁THAT▁ELECTRICITY▁LIKE▁A▁PATHWAY

2024-10-27 18:04:03,075 (asr_inference:509) INFO: speech length: 25497
2024-10-27 18:04:04,168 (beam_search:428) INFO: decoder input length: 19
2024-10-27 18:04:04,168 (beam_search:429) INFO: max output length: 19
2024-10-27 18:04:04,168 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:04,182 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:04,182 (beam_search:476) INFO:  -1.32 * 1.0 =  -1.32 for ctc
2024-10-27 18:04:04,182 (beam_search:479) INFO: total log probability: -1.32
2024-10-27 18:04:04,182 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:04:04,182 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:04,182 (beam_search:483) INFO: best hypo: 'RE▁ALL

2024-10-27 18:04:04,184 (asr_inference:509) INFO: speech length: 202053
2024-10-27 18:04:12,421 (beam_search:428) INFO: decoder input length: 157
2024-10-27 18:04:12,421 (beam_search:429) INFO: max output length: 157
2024-10-27 18:04:12,421 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:12,828 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:12,828 (beam_search:476) INFO:  -2.71 * 1.0 =  -2.71 for ctc
2024-10-27 18:04:12,828 (beam_search:479) INFO: total log probability: -2.71
2024-10-27 18:04:12,828 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:04:12,828 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:12,828 (beam_search:483) INFO: best hypo: ▁THE▁STEEL▁NAIL▁THE▁THE▁THE▁THE▁THE▁NAIL▁AND▁THE▁THE▁BRASS

2024-10-27 18:04:12,830 (asr_inference:509) INFO: speech length: 178063
2024-10-27 18:04:19,762 (beam_search:428) INFO: decoder input length: 138
2024-10-27 18:04:19,762 (beam_search:429) INFO: max output length: 138
2024-10-27 18:04:19,762 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:19,963 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:19,964 (beam_search:476) INFO:  -1.54 * 1.0 =  -1.54 for ctc
2024-10-27 18:04:19,964 (beam_search:479) INFO: total log probability: -1.54
2024-10-27 18:04:19,964 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:04:19,964 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:19,964 (beam_search:483) INFO: best hypo: ▁WE▁HAVE▁BEEN▁DOING▁WITH▁LIKE▁AND▁LIKE

2024-10-27 18:04:19,966 (asr_inference:509) INFO: speech length: 116393
2024-10-27 18:04:24,262 (beam_search:428) INFO: decoder input length: 90
2024-10-27 18:04:24,262 (beam_search:429) INFO: max output length: 90
2024-10-27 18:04:24,262 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:24,463 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:24,463 (beam_search:476) INFO:  -1.71 * 1.0 =  -1.71 for ctc
2024-10-27 18:04:24,465 (beam_search:479) INFO: total log probability: -1.71
2024-10-27 18:04:24,465 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:04:24,465 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:24,465 (beam_search:483) INFO: best hypo: ▁ENERGY▁IS▁POWER▁THAT▁WE▁USED▁TO▁PUT▁THINGS▁INTO▁OR▁LIGHT▁OR

2024-10-27 18:04:24,467 (asr_inference:509) INFO: speech length: 60089
2024-10-27 18:04:26,591 (beam_search:428) INFO: decoder input length: 46
2024-10-27 18:04:26,591 (beam_search:429) INFO: max output length: 46
2024-10-27 18:04:26,591 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:26,608 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:26,609 (beam_search:476) INFO:  -0.28 * 1.0 =  -0.28 for ctc
2024-10-27 18:04:26,609 (beam_search:479) INFO: total log probability: -0.28
2024-10-27 18:04:26,609 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:04:26,609 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:26,609 (beam_search:483) INFO: best hypo: ▁A

2024-10-27 18:04:26,611 (asr_inference:509) INFO: speech length: 31686
2024-10-27 18:04:27,920 (beam_search:428) INFO: decoder input length: 24
2024-10-27 18:04:27,920 (beam_search:429) INFO: max output length: 24
2024-10-27 18:04:27,920 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:27,930 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:27,930 (beam_search:476) INFO:  -0.11 * 1.0 =  -0.11 for ctc
2024-10-27 18:04:27,930 (beam_search:479) INFO: total log probability: -0.11
2024-10-27 18:04:27,930 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 18:04:27,930 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:27,930 (beam_search:483) INFO: best hypo: ▁AND

2024-10-27 18:04:27,932 (asr_inference:509) INFO: speech length: 85578
2024-10-27 18:04:31,061 (beam_search:428) INFO: decoder input length: 66
2024-10-27 18:04:31,061 (beam_search:429) INFO: max output length: 66
2024-10-27 18:04:31,061 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:31,111 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:31,111 (beam_search:476) INFO:  -0.83 * 1.0 =  -0.83 for ctc
2024-10-27 18:04:31,111 (beam_search:479) INFO: total log probability: -0.83
2024-10-27 18:04:31,111 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:04:31,111 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:31,111 (beam_search:483) INFO: best hypo: S▁AND▁YOU▁ALSO

2024-10-27 18:04:31,113 (asr_inference:509) INFO: speech length: 139049
2024-10-27 18:04:36,342 (beam_search:428) INFO: decoder input length: 108
2024-10-27 18:04:36,342 (beam_search:429) INFO: max output length: 108
2024-10-27 18:04:36,342 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:36,597 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:36,597 (beam_search:476) INFO:  -2.84 * 1.0 =  -2.84 for ctc
2024-10-27 18:04:36,597 (beam_search:479) INFO: total log probability: -2.84
2024-10-27 18:04:36,597 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:04:36,597 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:36,597 (beam_search:483) INFO: best hypo: ▁THE▁IS▁FROM▁THE▁THE▁AND▁AND▁IT'S▁ALSOING▁LIGHT

2024-10-27 18:04:36,600 (asr_inference:509) INFO: speech length: 73444
2024-10-27 18:04:39,178 (beam_search:428) INFO: decoder input length: 56
2024-10-27 18:04:39,178 (beam_search:429) INFO: max output length: 56
2024-10-27 18:04:39,178 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:39,220 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:39,220 (beam_search:476) INFO:  -0.85 * 1.0 =  -0.85 for ctc
2024-10-27 18:04:39,220 (beam_search:479) INFO: total log probability: -0.85
2024-10-27 18:04:39,220 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:04:39,220 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:39,220 (beam_search:483) INFO: best hypo: ▁IT▁ISING▁AND

2024-10-27 18:04:39,222 (asr_inference:509) INFO: speech length: 168376
2024-10-27 18:04:45,771 (beam_search:428) INFO: decoder input length: 131
2024-10-27 18:04:45,771 (beam_search:429) INFO: max output length: 131
2024-10-27 18:04:45,771 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:45,942 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:45,942 (beam_search:476) INFO:  -1.18 * 1.0 =  -1.18 for ctc
2024-10-27 18:04:45,942 (beam_search:479) INFO: total log probability: -1.18
2024-10-27 18:04:45,942 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:04:45,943 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:45,943 (beam_search:483) INFO: best hypo: ▁THE▁ISING▁THE▁IS▁IS▁ENERGY

2024-10-27 18:04:45,945 (asr_inference:509) INFO: speech length: 29209
2024-10-27 18:04:47,117 (beam_search:428) INFO: decoder input length: 22
2024-10-27 18:04:47,117 (beam_search:429) INFO: max output length: 22
2024-10-27 18:04:47,117 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:47,134 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:47,134 (beam_search:476) INFO:  -0.70 * 1.0 =  -0.70 for ctc
2024-10-27 18:04:47,134 (beam_search:479) INFO: total log probability: -0.70
2024-10-27 18:04:47,134 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:04:47,134 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:47,134 (beam_search:483) INFO: best hypo: ▁SHE▁IS▁MOVING

2024-10-27 18:04:47,137 (asr_inference:509) INFO: speech length: 148839
2024-10-27 18:04:52,541 (beam_search:428) INFO: decoder input length: 115
2024-10-27 18:04:52,541 (beam_search:429) INFO: max output length: 115
2024-10-27 18:04:52,541 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:52,789 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:52,790 (beam_search:476) INFO:  -2.12 * 1.0 =  -2.12 for ctc
2024-10-27 18:04:52,790 (beam_search:479) INFO: total log probability: -2.12
2024-10-27 18:04:52,790 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:04:52,790 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:52,790 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁A▁OR▁KIND▁OF▁THAT▁THAT▁SAYS▁ENERGY▁IS

2024-10-27 18:04:52,792 (asr_inference:509) INFO: speech length: 119702
2024-10-27 18:04:57,099 (beam_search:428) INFO: decoder input length: 93
2024-10-27 18:04:57,099 (beam_search:429) INFO: max output length: 93
2024-10-27 18:04:57,099 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:57,309 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:57,309 (beam_search:476) INFO:  -0.72 * 1.0 =  -0.72 for ctc
2024-10-27 18:04:57,309 (beam_search:479) INFO: total log probability: -0.72
2024-10-27 18:04:57,309 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:04:57,309 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:57,310 (beam_search:483) INFO: best hypo: ▁YES▁YOU▁CAN▁BECAUSE▁ENERGY▁IS▁GOING▁THROUGH▁THE▁TO▁MAKE▁IT▁A

2024-10-27 18:04:57,312 (asr_inference:509) INFO: speech length: 95822
2024-10-27 18:05:00,894 (beam_search:428) INFO: decoder input length: 74
2024-10-27 18:05:00,894 (beam_search:429) INFO: max output length: 74
2024-10-27 18:05:00,894 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:01,005 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:01,006 (beam_search:476) INFO:  -2.02 * 1.0 =  -2.02 for ctc
2024-10-27 18:05:01,006 (beam_search:479) INFO: total log probability: -2.02
2024-10-27 18:05:01,006 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:05:01,006 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:01,006 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁PERSON'SS▁UP▁A

2024-10-27 18:05:01,008 (asr_inference:509) INFO: speech length: 123075
2024-10-27 18:05:05,532 (beam_search:428) INFO: decoder input length: 95
2024-10-27 18:05:05,533 (beam_search:429) INFO: max output length: 95
2024-10-27 18:05:05,533 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:05,566 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:05,567 (beam_search:476) INFO:  -1.04 * 1.0 =  -1.04 for ctc
2024-10-27 18:05:05,567 (beam_search:479) INFO: total log probability: -1.04
2024-10-27 18:05:05,567 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:05:05,567 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:05,567 (beam_search:483) INFO: best hypo: ▁AND

2024-10-27 18:05:05,569 (asr_inference:509) INFO: speech length: 150558
2024-10-27 18:05:11,140 (beam_search:428) INFO: decoder input length: 117
2024-10-27 18:05:11,141 (beam_search:429) INFO: max output length: 117
2024-10-27 18:05:11,141 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:11,506 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:11,506 (beam_search:476) INFO:  -3.00 * 1.0 =  -3.00 for ctc
2024-10-27 18:05:11,506 (beam_search:479) INFO: total log probability: -3.00
2024-10-27 18:05:11,506 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:05:11,506 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:11,506 (beam_search:483) INFO: best hypo: ▁IT▁BE▁SO▁THAT'S▁WHY▁WE▁SHOULD▁USE▁LIKE▁SOLAR▁ENERGY▁AND▁ENERGY▁AND▁WATER

2024-10-27 18:05:11,508 (asr_inference:509) INFO: speech length: 57758
2024-10-27 18:05:13,634 (beam_search:428) INFO: decoder input length: 44
2024-10-27 18:05:13,634 (beam_search:429) INFO: max output length: 44
2024-10-27 18:05:13,634 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:13,667 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:13,667 (beam_search:476) INFO:  -0.32 * 1.0 =  -0.32 for ctc
2024-10-27 18:05:13,667 (beam_search:479) INFO: total log probability: -0.32
2024-10-27 18:05:13,667 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:05:13,667 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:13,667 (beam_search:483) INFO: best hypo: ▁ITS▁INTO▁THE

2024-10-27 18:05:13,669 (asr_inference:509) INFO: speech length: 51118
2024-10-27 18:05:15,613 (beam_search:428) INFO: decoder input length: 39
2024-10-27 18:05:15,613 (beam_search:429) INFO: max output length: 39
2024-10-27 18:05:15,613 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:15,639 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:15,639 (beam_search:476) INFO:  -0.30 * 1.0 =  -0.30 for ctc
2024-10-27 18:05:15,639 (beam_search:479) INFO: total log probability: -0.30
2024-10-27 18:05:15,639 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:05:15,639 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:15,639 (beam_search:483) INFO: best hypo: ▁IT▁WILL▁BE

2024-10-27 18:05:15,642 (asr_inference:509) INFO: speech length: 58480
2024-10-27 18:05:17,937 (beam_search:428) INFO: decoder input length: 45
2024-10-27 18:05:17,937 (beam_search:429) INFO: max output length: 45
2024-10-27 18:05:17,937 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:17,970 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:17,970 (beam_search:476) INFO:  -0.62 * 1.0 =  -0.62 for ctc
2024-10-27 18:05:17,970 (beam_search:479) INFO: total log probability: -0.62
2024-10-27 18:05:17,970 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:05:17,970 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:17,970 (beam_search:483) INFO: best hypo: ▁THE▁WILL▁NOT▁BE

2024-10-27 18:05:17,972 (asr_inference:509) INFO: speech length: 206390
2024-10-27 18:05:26,180 (beam_search:428) INFO: decoder input length: 160
2024-10-27 18:05:26,180 (beam_search:429) INFO: max output length: 160
2024-10-27 18:05:26,180 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:26,699 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:26,699 (beam_search:476) INFO:  -2.05 * 1.0 =  -2.05 for ctc
2024-10-27 18:05:26,699 (beam_search:479) INFO: total log probability: -2.05
2024-10-27 18:05:26,699 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:05:26,699 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:26,699 (beam_search:483) INFO: best hypo: ▁THE▁IS▁AND▁FOOD▁WIND▁WATER▁AND▁SOLAR▁ENERGY▁DO▁NOT▁LIKE▁THEY▁DON'T

2024-10-27 18:05:26,702 (asr_inference:509) INFO: speech length: 34177
2024-10-27 18:05:28,131 (beam_search:428) INFO: decoder input length: 26
2024-10-27 18:05:28,132 (beam_search:429) INFO: max output length: 26
2024-10-27 18:05:28,132 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:28,158 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:28,158 (beam_search:476) INFO:  -0.98 * 1.0 =  -0.98 for ctc
2024-10-27 18:05:28,158 (beam_search:479) INFO: total log probability: -0.98
2024-10-27 18:05:28,158 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:05:28,158 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:28,158 (beam_search:483) INFO: best hypo: ▁I'M▁NOT▁SURE

2024-10-27 18:05:28,160 (asr_inference:509) INFO: speech length: 176780
2024-10-27 18:05:35,090 (beam_search:428) INFO: decoder input length: 137
2024-10-27 18:05:35,090 (beam_search:429) INFO: max output length: 137
2024-10-27 18:05:35,090 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:35,438 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:35,438 (beam_search:476) INFO:  -3.21 * 1.0 =  -3.21 for ctc
2024-10-27 18:05:35,438 (beam_search:479) INFO: total log probability: -3.21
2024-10-27 18:05:35,438 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:05:35,438 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:35,438 (beam_search:483) INFO: best hypo: ▁THEY'▁BOTH▁AND▁THEY▁CAN▁BE▁AND▁AND▁COAL▁ARE▁AND▁RUN▁OUT

2024-10-27 18:05:35,441 (asr_inference:509) INFO: speech length: 79361
2024-10-27 18:05:38,355 (beam_search:428) INFO: decoder input length: 61
2024-10-27 18:05:38,355 (beam_search:429) INFO: max output length: 61
2024-10-27 18:05:38,355 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:38,434 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:38,434 (beam_search:476) INFO:  -1.48 * 1.0 =  -1.48 for ctc
2024-10-27 18:05:38,434 (beam_search:479) INFO: total log probability: -1.48
2024-10-27 18:05:38,434 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:05:38,434 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:38,434 (beam_search:483) INFO: best hypo: ▁THEY'RE▁BOTH▁AND▁THEY▁BOTH▁NEED

2024-10-27 18:05:38,436 (asr_inference:509) INFO: speech length: 110466
2024-10-27 18:05:42,334 (beam_search:428) INFO: decoder input length: 85
2024-10-27 18:05:42,334 (beam_search:429) INFO: max output length: 85
2024-10-27 18:05:42,334 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:42,457 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:42,458 (beam_search:476) INFO:  -1.17 * 1.0 =  -1.17 for ctc
2024-10-27 18:05:42,458 (beam_search:479) INFO: total log probability: -1.17
2024-10-27 18:05:42,458 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:05:42,458 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:42,458 (beam_search:483) INFO: best hypo: ▁THEY▁WILL▁THE▁AIR▁AND▁UP▁THE▁AND

2024-10-27 18:05:42,460 (asr_inference:509) INFO: speech length: 89676
2024-10-27 18:05:45,676 (beam_search:428) INFO: decoder input length: 69
2024-10-27 18:05:45,676 (beam_search:429) INFO: max output length: 69
2024-10-27 18:05:45,676 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:45,775 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:45,775 (beam_search:476) INFO:  -1.61 * 1.0 =  -1.61 for ctc
2024-10-27 18:05:45,775 (beam_search:479) INFO: total log probability: -1.61
2024-10-27 18:05:45,775 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:05:45,775 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:45,775 (beam_search:483) INFO: best hypo: ▁THEY▁THE▁SOLAR▁PANELS▁DO▁NOT▁THE

2024-10-27 18:05:45,777 (asr_inference:509) INFO: speech length: 106976
2024-10-27 18:05:49,721 (beam_search:428) INFO: decoder input length: 83
2024-10-27 18:05:49,721 (beam_search:429) INFO: max output length: 83
2024-10-27 18:05:49,721 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:49,848 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:49,849 (beam_search:476) INFO:  -0.52 * 1.0 =  -0.52 for ctc
2024-10-27 18:05:49,849 (beam_search:479) INFO: total log probability: -0.52
2024-10-27 18:05:49,849 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:05:49,849 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:49,849 (beam_search:483) INFO: best hypo: ▁WELL▁WE'VE▁BEEN▁LEARNING▁ABOUT▁HOW▁ABOUT

2024-10-27 18:05:49,851 (asr_inference:509) INFO: speech length: 91949
2024-10-27 18:05:53,481 (beam_search:428) INFO: decoder input length: 71
2024-10-27 18:05:53,481 (beam_search:429) INFO: max output length: 71
2024-10-27 18:05:53,481 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:53,561 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:53,561 (beam_search:476) INFO:  -1.66 * 1.0 =  -1.66 for ctc
2024-10-27 18:05:53,561 (beam_search:479) INFO: total log probability: -1.66
2024-10-27 18:05:53,561 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:05:53,561 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:53,561 (beam_search:483) INFO: best hypo: ▁A▁IS▁A▁FROM▁THE▁THAT

2024-10-27 18:05:53,564 (asr_inference:509) INFO: speech length: 161538
2024-10-27 18:05:59,722 (beam_search:428) INFO: decoder input length: 125
2024-10-27 18:05:59,723 (beam_search:429) INFO: max output length: 125
2024-10-27 18:05:59,723 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:00,198 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:00,198 (beam_search:476) INFO:  -1.87 * 1.0 =  -1.87 for ctc
2024-10-27 18:06:00,198 (beam_search:479) INFO: total log probability: -1.87
2024-10-27 18:06:00,198 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:06:00,198 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:00,198 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁ENERGY▁FLOWS▁FROM▁THE▁BATTERY▁THROUGH▁THE▁WIRES▁THROUGH▁THE▁AND▁BACK▁THROUGH▁THE▁WIRES▁TO▁THE▁D▁CELL

2024-10-27 18:06:00,201 (asr_inference:509) INFO: speech length: 287210
2024-10-27 18:06:12,476 (beam_search:428) INFO: decoder input length: 223
2024-10-27 18:06:12,476 (beam_search:429) INFO: max output length: 223
2024-10-27 18:06:12,476 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:13,802 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:13,802 (beam_search:476) INFO:  -5.72 * 1.0 =  -5.72 for ctc
2024-10-27 18:06:13,802 (beam_search:479) INFO: total log probability: -5.72
2024-10-27 18:06:13,802 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:06:13,802 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:13,802 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁ARE▁FLOWING▁OUT▁THE▁NEGATIVE▁SIDE▁OF▁THE▁THE▁TERMINAL▁OF▁THE▁THROUGH▁THE▁WIRE▁INTO▁THE▁BULB▁OUT▁OF▁THE▁AND▁INTO▁THE▁THE▁POSITIVE▁TERMINAL▁OF▁THE▁D▁CELL

2024-10-27 18:06:13,805 (asr_inference:509) INFO: speech length: 107239
2024-10-27 18:06:17,685 (beam_search:428) INFO: decoder input length: 83
2024-10-27 18:06:17,685 (beam_search:429) INFO: max output length: 83
2024-10-27 18:06:17,685 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:17,830 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:17,831 (beam_search:476) INFO:  -1.86 * 1.0 =  -1.86 for ctc
2024-10-27 18:06:17,831 (beam_search:479) INFO: total log probability: -1.86
2024-10-27 18:06:17,831 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:06:17,831 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:17,831 (beam_search:483) INFO: best hypo: ▁WELL▁ELECTRICITY▁OUT▁OF▁THE▁NEGATIVE▁AND▁INTO▁THE▁POSITIVE

2024-10-27 18:06:17,833 (asr_inference:509) INFO: speech length: 129875
2024-10-27 18:06:22,538 (beam_search:428) INFO: decoder input length: 100
2024-10-27 18:06:22,538 (beam_search:429) INFO: max output length: 100
2024-10-27 18:06:22,538 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:22,720 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:22,721 (beam_search:476) INFO:  -2.96 * 1.0 =  -2.96 for ctc
2024-10-27 18:06:22,721 (beam_search:479) INFO: total log probability: -2.96
2024-10-27 18:06:22,721 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:06:22,721 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:22,721 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁ELECTRICITY▁HAS▁TO▁THROUGH▁BOTH▁OF▁ALL▁THE

2024-10-27 18:06:22,723 (asr_inference:509) INFO: speech length: 174405
2024-10-27 18:06:29,609 (beam_search:428) INFO: decoder input length: 135
2024-10-27 18:06:29,610 (beam_search:429) INFO: max output length: 135
2024-10-27 18:06:29,610 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:30,084 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:30,084 (beam_search:476) INFO:  -3.09 * 1.0 =  -3.09 for ctc
2024-10-27 18:06:30,084 (beam_search:479) INFO: total log probability: -3.09
2024-10-27 18:06:30,084 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:06:30,084 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:30,084 (beam_search:483) INFO: best hypo: ▁THEY▁ALL▁MAKE▁THEY▁ALL▁MAKE▁THE▁CIRCUIT▁A▁CIRCUIT▁BECAUSE▁WITHOUT▁IN▁THE▁CIRCUIT▁THEN▁IT'S

2024-10-27 18:06:30,087 (asr_inference:509) INFO: speech length: 121399
2024-10-27 18:06:34,520 (beam_search:428) INFO: decoder input length: 94
2024-10-27 18:06:34,520 (beam_search:429) INFO: max output length: 94
2024-10-27 18:06:34,520 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:34,809 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:34,809 (beam_search:476) INFO:  -2.05 * 1.0 =  -2.05 for ctc
2024-10-27 18:06:34,809 (beam_search:479) INFO: total log probability: -2.05
2024-10-27 18:06:34,809 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:06:34,809 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:34,809 (beam_search:483) INFO: best hypo: ▁THIS▁IS▁A▁PATHWAY▁THROUGH▁THE▁LIGHT▁BULB▁FROM▁THE▁BATTERY▁THROUGH▁THE▁LIGHT▁BULB▁AND▁A▁CIRCUIT

2024-10-27 18:06:34,811 (asr_inference:509) INFO: speech length: 129231
2024-10-27 18:06:39,508 (beam_search:428) INFO: decoder input length: 100
2024-10-27 18:06:39,508 (beam_search:429) INFO: max output length: 100
2024-10-27 18:06:39,508 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:39,695 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:39,695 (beam_search:476) INFO:  -2.76 * 1.0 =  -2.76 for ctc
2024-10-27 18:06:39,695 (beam_search:479) INFO: total log probability: -2.76
2024-10-27 18:06:39,695 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:06:39,695 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:39,695 (beam_search:483) INFO: best hypo: ▁ELECTRICITY▁THROUGH▁A▁CIRCUIT▁THROUGH▁BOTH▁AND▁BACK▁THROUGH▁THE

2024-10-27 18:06:39,697 (asr_inference:509) INFO: speech length: 186586
2024-10-27 18:06:47,323 (beam_search:428) INFO: decoder input length: 145
2024-10-27 18:06:47,323 (beam_search:429) INFO: max output length: 145
2024-10-27 18:06:47,324 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:47,884 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:47,884 (beam_search:476) INFO:  -3.27 * 1.0 =  -3.27 for ctc
2024-10-27 18:06:47,884 (beam_search:479) INFO: total log probability: -3.27
2024-10-27 18:06:47,884 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:06:47,884 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:47,884 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁ELECTRICITY▁FLOWING▁OUT▁OF▁THE▁SOLAR▁PANEL▁OR▁THE▁D▁THROUGH▁THE▁LIGHT▁BULBS▁AND▁BACK▁THROUGH▁THE▁POWER

2024-10-27 18:06:47,887 (asr_inference:509) INFO: speech length: 349040
2024-10-27 18:07:03,659 (beam_search:428) INFO: decoder input length: 272
2024-10-27 18:07:03,659 (beam_search:429) INFO: max output length: 272
2024-10-27 18:07:03,659 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:05,718 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:05,718 (beam_search:476) INFO:  -5.61 * 1.0 =  -5.61 for ctc
2024-10-27 18:07:05,719 (beam_search:479) INFO: total log probability: -5.61
2024-10-27 18:07:05,719 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:07:05,719 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:05,719 (beam_search:483) INFO: best hypo: ▁IT'S▁FLOWING▁OUT▁OF▁THE▁THE▁D▁CELL▁AND▁THEN▁THROUGH▁THE▁BULB▁THEN▁OUT▁OF▁THE▁OUT▁OF▁THE▁AND▁THEN▁THROUGH▁THE▁BULB▁OF▁THE▁OTHER▁BULB▁OUT▁OF▁THE▁AND▁BACK▁TO▁THE▁POSITIVE▁THE▁D▁CELL

2024-10-27 18:07:05,721 (asr_inference:509) INFO: speech length: 61954
2024-10-27 18:07:08,071 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:07:08,071 (beam_search:429) INFO: max output length: 47
2024-10-27 18:07:08,071 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:08,131 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:08,131 (beam_search:476) INFO:  -1.37 * 1.0 =  -1.37 for ctc
2024-10-27 18:07:08,131 (beam_search:479) INFO: total log probability: -1.37
2024-10-27 18:07:08,131 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:07:08,131 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:08,131 (beam_search:483) INFO: best hypo: ▁THOSE▁ARE▁THE▁FLOWING▁THROUGH▁THE▁CIRCUIT

2024-10-27 18:07:08,134 (asr_inference:509) INFO: speech length: 76751
2024-10-27 18:07:11,072 (beam_search:428) INFO: decoder input length: 59
2024-10-27 18:07:11,072 (beam_search:429) INFO: max output length: 59
2024-10-27 18:07:11,072 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:11,179 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:11,179 (beam_search:476) INFO:  -2.06 * 1.0 =  -2.06 for ctc
2024-10-27 18:07:11,179 (beam_search:479) INFO: total log probability: -2.06
2024-10-27 18:07:11,179 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:07:11,179 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:11,179 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁THE▁BLUE▁DOTS▁ARE▁THE▁ELECTRICITY▁FLOWING▁THROUGH▁THE

2024-10-27 18:07:11,181 (asr_inference:509) INFO: speech length: 162611
2024-10-27 18:07:17,542 (beam_search:428) INFO: decoder input length: 126
2024-10-27 18:07:17,542 (beam_search:429) INFO: max output length: 126
2024-10-27 18:07:17,542 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:17,905 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:17,906 (beam_search:476) INFO:  -2.09 * 1.0 =  -2.09 for ctc
2024-10-27 18:07:17,906 (beam_search:479) INFO: total log probability: -2.09
2024-10-27 18:07:17,906 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:07:17,906 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:17,906 (beam_search:483) INFO: best hypo: ▁OUT▁OF▁THE▁AND▁THEN▁THROUGH▁BOTH▁OF▁THE▁BULB▁THROUGH▁THE▁AND▁GOES▁BACK▁TO

2024-10-27 18:07:17,909 (asr_inference:509) INFO: speech length: 288833
2024-10-27 18:07:30,075 (beam_search:428) INFO: decoder input length: 225
2024-10-27 18:07:30,076 (beam_search:429) INFO: max output length: 225
2024-10-27 18:07:30,076 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:31,685 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:31,686 (beam_search:476) INFO:  -4.72 * 1.0 =  -4.72 for ctc
2024-10-27 18:07:31,686 (beam_search:479) INFO: total log probability: -4.72
2024-10-27 18:07:31,686 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:07:31,686 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:31,686 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁THE▁ELECTRICITY▁IS▁OUT▁OF▁THE▁THROUGH▁THE▁WIRE▁INTO▁THE▁BULB▁OUT▁OF▁THE▁BULB▁INTO▁THROUGH▁THE▁WIRE▁INTO▁THE▁OTHER▁BULB▁OUT▁THE▁OTHER▁BULB▁AND▁BACK▁INTO▁THE▁POSITIVE▁THE▁D▁CELL

2024-10-27 18:07:31,688 (asr_inference:509) INFO: speech length: 108519
2024-10-27 18:07:35,573 (beam_search:428) INFO: decoder input length: 84
2024-10-27 18:07:35,573 (beam_search:429) INFO: max output length: 84
2024-10-27 18:07:35,573 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:35,839 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:35,839 (beam_search:476) INFO:  -2.46 * 1.0 =  -2.46 for ctc
2024-10-27 18:07:35,839 (beam_search:479) INFO: total log probability: -2.46
2024-10-27 18:07:35,839 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:07:35,839 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:35,839 (beam_search:483) INFO: best hypo: ▁THE▁LIGHT▁WILL▁GET▁BECAUSE▁THERE▁IS▁NOT▁THERE'S▁NOT▁ENOUGH▁ENERGY▁TO▁POWER▁ALL▁OF▁THEM

2024-10-27 18:07:35,841 (asr_inference:509) INFO: speech length: 95855
2024-10-27 18:07:39,406 (beam_search:428) INFO: decoder input length: 74
2024-10-27 18:07:39,406 (beam_search:429) INFO: max output length: 74
2024-10-27 18:07:39,406 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:39,578 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:39,579 (beam_search:476) INFO:  -3.61 * 1.0 =  -3.61 for ctc
2024-10-27 18:07:39,579 (beam_search:479) INFO: total log probability: -3.61
2024-10-27 18:07:39,579 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:07:39,579 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:39,579 (beam_search:483) INFO: best hypo: ▁THE▁LIGHT▁WILL▁GET▁BRIGHTER▁THEN▁THERE'S▁MORE▁POWER▁TO▁THE▁ENERGY

2024-10-27 18:07:39,581 (asr_inference:509) INFO: speech length: 212706
2024-10-27 18:07:47,852 (beam_search:428) INFO: decoder input length: 165
2024-10-27 18:07:47,852 (beam_search:429) INFO: max output length: 165
2024-10-27 18:07:47,852 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:48,303 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:48,303 (beam_search:476) INFO:  -1.51 * 1.0 =  -1.51 for ctc
2024-10-27 18:07:48,303 (beam_search:479) INFO: total log probability: -1.51
2024-10-27 18:07:48,303 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:07:48,303 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:48,303 (beam_search:483) INFO: best hypo: ▁WHEN▁YOU▁MORE▁THEN▁THE▁LIGHTS▁TO▁OUT▁MORE▁BECAUSE▁THEY▁GET▁MORE▁ENERGY

2024-10-27 18:07:48,306 (asr_inference:509) INFO: speech length: 85004
2024-10-27 18:07:51,398 (beam_search:428) INFO: decoder input length: 65
2024-10-27 18:07:51,398 (beam_search:429) INFO: max output length: 65
2024-10-27 18:07:51,398 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:51,466 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:51,466 (beam_search:476) INFO:  -1.45 * 1.0 =  -1.45 for ctc
2024-10-27 18:07:51,466 (beam_search:479) INFO: total log probability: -1.45
2024-10-27 18:07:51,466 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:07:51,466 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:51,467 (beam_search:483) INFO: best hypo: ▁BECAUSE▁THE▁POSITIVES▁ARE▁TOGETHER

2024-10-27 18:07:51,469 (asr_inference:509) INFO: speech length: 272545
2024-10-27 18:08:03,118 (beam_search:428) INFO: decoder input length: 212
2024-10-27 18:08:03,118 (beam_search:429) INFO: max output length: 212
2024-10-27 18:08:03,118 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:04,239 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:04,239 (beam_search:476) INFO:  -5.04 * 1.0 =  -5.04 for ctc
2024-10-27 18:08:04,239 (beam_search:479) INFO: total log probability: -5.04
2024-10-27 18:08:04,239 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:08:04,239 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:04,240 (beam_search:483) INFO: best hypo: ▁WHEN▁YOU▁THE▁D▁IT▁GOES▁TO▁POSITIVE▁TO▁NEGATIVE▁AND▁D▁HAVE▁TO▁BE▁THAT▁WAY▁JUST▁LIKE▁MAGNETS▁HAVE▁TO▁BE▁JUST▁LIKE▁THE▁HAVE▁TO▁THE

2024-10-27 18:08:04,243 (asr_inference:509) INFO: speech length: 93028
2024-10-27 18:08:07,667 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:08:07,667 (beam_search:429) INFO: max output length: 72
2024-10-27 18:08:07,667 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:07,737 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:07,737 (beam_search:476) INFO:  -1.29 * 1.0 =  -1.29 for ctc
2024-10-27 18:08:07,737 (beam_search:479) INFO: total log probability: -1.29
2024-10-27 18:08:07,737 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:08:07,737 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:07,737 (beam_search:483) INFO: best hypo: ▁THE▁D▁CELL▁THE▁THE

2024-10-27 18:08:07,740 (asr_inference:509) INFO: speech length: 131630
2024-10-27 18:08:12,736 (beam_search:428) INFO: decoder input length: 102
2024-10-27 18:08:12,736 (beam_search:429) INFO: max output length: 102
2024-10-27 18:08:12,736 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:13,015 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:13,015 (beam_search:476) INFO:  -2.32 * 1.0 =  -2.32 for ctc
2024-10-27 18:08:13,015 (beam_search:479) INFO: total log probability: -2.32
2024-10-27 18:08:13,015 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:08:13,015 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:13,015 (beam_search:483) INFO: best hypo: ▁THE▁D▁TO▁MAKE▁IT▁SO▁THAT▁THERE▁IS▁POSITIVE▁TO▁NEGATIVE▁OR▁NEGATIVE▁TO

2024-10-27 18:08:13,017 (asr_inference:509) INFO: speech length: 68952
2024-10-27 18:08:15,580 (beam_search:428) INFO: decoder input length: 53
2024-10-27 18:08:15,580 (beam_search:429) INFO: max output length: 53
2024-10-27 18:08:15,580 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:15,644 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:15,644 (beam_search:476) INFO:  -2.65 * 1.0 =  -2.65 for ctc
2024-10-27 18:08:15,644 (beam_search:479) INFO: total log probability: -2.65
2024-10-27 18:08:15,644 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:08:15,644 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:15,644 (beam_search:483) INFO: best hypo: ▁I'VE▁BEEN▁LEARNING▁ABOUT▁AND

2024-10-27 18:08:15,646 (asr_inference:509) INFO: speech length: 146772
2024-10-27 18:08:21,067 (beam_search:428) INFO: decoder input length: 114
2024-10-27 18:08:21,067 (beam_search:429) INFO: max output length: 114
2024-10-27 18:08:21,067 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:21,380 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:21,380 (beam_search:476) INFO:  -3.25 * 1.0 =  -3.25 for ctc
2024-10-27 18:08:21,380 (beam_search:479) INFO: total log probability: -3.25
2024-10-27 18:08:21,380 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:08:21,380 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:21,380 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁OUT▁TWO▁IN▁THE▁NEGATIVE▁AND▁THEN▁FLOWS▁IN▁TWO▁INTO▁THE▁POSITIVE

2024-10-27 18:08:21,382 (asr_inference:509) INFO: speech length: 46049
2024-10-27 18:08:23,210 (beam_search:428) INFO: decoder input length: 35
2024-10-27 18:08:23,210 (beam_search:429) INFO: max output length: 35
2024-10-27 18:08:23,210 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:23,245 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:23,245 (beam_search:476) INFO:  -2.05 * 1.0 =  -2.05 for ctc
2024-10-27 18:08:23,245 (beam_search:479) INFO: total log probability: -2.05
2024-10-27 18:08:23,245 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:08:23,245 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:23,245 (beam_search:483) INFO: best hypo: ▁HAS▁ITS▁OWN▁PATHWAY

2024-10-27 18:08:23,248 (asr_inference:509) INFO: speech length: 174974
2024-10-27 18:08:30,015 (beam_search:428) INFO: decoder input length: 136
2024-10-27 18:08:30,015 (beam_search:429) INFO: max output length: 136
2024-10-27 18:08:30,015 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:30,595 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:30,595 (beam_search:476) INFO:  -6.02 * 1.0 =  -6.02 for ctc
2024-10-27 18:08:30,595 (beam_search:479) INFO: total log probability: -6.02
2024-10-27 18:08:30,596 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:08:30,596 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:30,596 (beam_search:483) INFO: best hypo: ▁IN▁A▁CIRCUIT▁SO▁ONE▁OF▁WIRES▁TO▁OF▁THE▁LIGHT▁AND▁THE▁OTHER▁OF▁WIRES▁GOES▁TO▁THE▁OTHER▁LIGHT▁BULB

2024-10-27 18:08:30,599 (asr_inference:509) INFO: speech length: 52505
2024-10-27 18:08:32,597 (beam_search:428) INFO: decoder input length: 40
2024-10-27 18:08:32,597 (beam_search:429) INFO: max output length: 40
2024-10-27 18:08:32,597 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:32,640 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:32,640 (beam_search:476) INFO:  -2.23 * 1.0 =  -2.23 for ctc
2024-10-27 18:08:32,640 (beam_search:479) INFO: total log probability: -2.23
2024-10-27 18:08:32,640 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:08:32,640 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:32,640 (beam_search:483) INFO: best hypo: ▁THERE▁IS▁ONE▁D▁AND▁TWO

2024-10-27 18:08:32,642 (asr_inference:509) INFO: speech length: 125311
2024-10-27 18:08:37,283 (beam_search:428) INFO: decoder input length: 97
2024-10-27 18:08:37,283 (beam_search:429) INFO: max output length: 97
2024-10-27 18:08:37,283 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:37,555 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:37,555 (beam_search:476) INFO:  -5.48 * 1.0 =  -5.48 for ctc
2024-10-27 18:08:37,557 (beam_search:479) INFO: total log probability: -5.48
2024-10-27 18:08:37,557 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:08:37,557 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:37,557 (beam_search:483) INFO: best hypo: ▁THIS▁IS▁NOT▁A▁PARALLEL▁CIRCUIT▁IT'S▁A▁CIRCUIT▁BECAUSE▁EACH▁IS▁CONNECTED▁IN▁THE

2024-10-27 18:08:37,559 (asr_inference:509) INFO: speech length: 31217
2024-10-27 18:08:38,839 (beam_search:428) INFO: decoder input length: 23
2024-10-27 18:08:38,839 (beam_search:429) INFO: max output length: 23
2024-10-27 18:08:38,839 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:38,850 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:38,850 (beam_search:476) INFO:  -0.08 * 1.0 =  -0.08 for ctc
2024-10-27 18:08:38,850 (beam_search:479) INFO: total log probability: -0.08
2024-10-27 18:08:38,850 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:08:38,850 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:38,850 (beam_search:483) INFO: best hypo: ▁AND

2024-10-27 18:08:38,853 (asr_inference:509) INFO: speech length: 259134
2024-10-27 18:08:49,460 (beam_search:428) INFO: decoder input length: 201
2024-10-27 18:08:49,461 (beam_search:429) INFO: max output length: 201
2024-10-27 18:08:49,461 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:49,977 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:49,977 (beam_search:476) INFO:  -5.94 * 1.0 =  -5.94 for ctc
2024-10-27 18:08:49,977 (beam_search:479) INFO: total log probability: -5.94
2024-10-27 18:08:49,977 (beam_search:480) INFO: normalized log probability: -0.37
2024-10-27 18:08:49,977 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:49,977 (beam_search:483) INFO: best hypo: ▁BOTH▁THE▁THE▁HAVE▁MORE▁ENERGY▁FOR▁THE▁BULBS▁THE▁BECAUSE▁IT▁THROUGH▁OF

2024-10-27 18:08:49,980 (asr_inference:509) INFO: speech length: 237295
2024-10-27 18:09:00,079 (beam_search:428) INFO: decoder input length: 184
2024-10-27 18:09:00,080 (beam_search:429) INFO: max output length: 184
2024-10-27 18:09:00,080 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:00,964 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:00,964 (beam_search:476) INFO:  -7.44 * 1.0 =  -7.44 for ctc
2024-10-27 18:09:00,964 (beam_search:479) INFO: total log probability: -7.44
2024-10-27 18:09:00,964 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:09:00,964 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:00,965 (beam_search:483) INFO: best hypo: ▁BECAUSE▁THERE▁TWO▁IN▁IN▁A▁CIRCUIT▁THERE'S▁IN▁A▁CIRCUIT▁AND▁THERE▁IS▁STILL▁ONE▁IN▁A▁EXCEPT▁THERE'S▁LESS

2024-10-27 18:09:00,967 (asr_inference:509) INFO: speech length: 247212
2024-10-27 18:09:10,938 (beam_search:428) INFO: decoder input length: 192
2024-10-27 18:09:10,939 (beam_search:429) INFO: max output length: 192
2024-10-27 18:09:10,939 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:11,672 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:11,672 (beam_search:476) INFO:  -6.35 * 1.0 =  -6.35 for ctc
2024-10-27 18:09:11,672 (beam_search:479) INFO: total log probability: -6.35
2024-10-27 18:09:11,672 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:09:11,672 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:11,672 (beam_search:483) INFO: best hypo: ▁THE▁HAS▁LESS▁OF▁ENERGY▁BECAUSE▁BECAUSE▁LIGHT▁ARE▁CONNECTED▁IN▁ONE▁THE▁CIRCUIT▁HAS▁MORE▁ENERGY▁BECAUSE▁THEY▁CONNECTED▁TWO

2024-10-27 18:09:11,675 (asr_inference:509) INFO: speech length: 35889
2024-10-27 18:09:13,029 (beam_search:428) INFO: decoder input length: 27
2024-10-27 18:09:13,029 (beam_search:429) INFO: max output length: 27
2024-10-27 18:09:13,029 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:13,048 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:13,048 (beam_search:476) INFO:  -0.80 * 1.0 =  -0.80 for ctc
2024-10-27 18:09:13,048 (beam_search:479) INFO: total log probability: -0.80
2024-10-27 18:09:13,048 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:09:13,048 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:13,048 (beam_search:483) INFO: best hypo: ▁THE▁WILL▁GET

2024-10-27 18:09:13,050 (asr_inference:509) INFO: speech length: 83867
2024-10-27 18:09:16,077 (beam_search:428) INFO: decoder input length: 65
2024-10-27 18:09:16,077 (beam_search:429) INFO: max output length: 65
2024-10-27 18:09:16,077 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:16,213 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:16,213 (beam_search:476) INFO:  -2.54 * 1.0 =  -2.54 for ctc
2024-10-27 18:09:16,213 (beam_search:479) INFO: total log probability: -2.54
2024-10-27 18:09:16,213 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:09:16,213 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:16,213 (beam_search:483) INFO: best hypo: ▁IT▁WOULD▁MAKE▁THE▁BRIGHTER▁BECAUSE▁THEN▁THERE▁WOULD▁MORE▁ENERGY▁TO

2024-10-27 18:09:16,215 (asr_inference:509) INFO: speech length: 160221
2024-10-27 18:09:22,286 (beam_search:428) INFO: decoder input length: 124
2024-10-27 18:09:22,286 (beam_search:429) INFO: max output length: 124
2024-10-27 18:09:22,286 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:22,651 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:22,651 (beam_search:476) INFO:  -3.18 * 1.0 =  -3.18 for ctc
2024-10-27 18:09:22,651 (beam_search:479) INFO: total log probability: -3.18
2024-10-27 18:09:22,651 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:09:22,651 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:22,652 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁OUT▁OF▁THE▁TERMINAL▁OF▁THE▁BATTERY▁AND▁INTO▁THE▁POSITIVE▁TERMINAL▁OF▁THE

2024-10-27 18:09:22,654 (asr_inference:509) INFO: speech length: 101694
2024-10-27 18:09:26,747 (beam_search:428) INFO: decoder input length: 78
2024-10-27 18:09:26,747 (beam_search:429) INFO: max output length: 78
2024-10-27 18:09:26,747 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:26,914 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:26,915 (beam_search:476) INFO:  -2.84 * 1.0 =  -2.84 for ctc
2024-10-27 18:09:26,915 (beam_search:479) INFO: total log probability: -2.84
2024-10-27 18:09:26,915 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:09:26,915 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:26,915 (beam_search:483) INFO: best hypo: ▁THE▁IS▁FLOWING▁OUT▁OF▁THE▁NEGATIVE▁AND▁INTO▁THE▁POSITIVE▁TERMINAL

2024-10-27 18:09:26,918 (asr_inference:509) INFO: speech length: 340531
2024-10-27 18:09:42,023 (beam_search:428) INFO: decoder input length: 265
2024-10-27 18:09:42,023 (beam_search:429) INFO: max output length: 265
2024-10-27 18:09:42,023 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:44,203 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:44,203 (beam_search:476) INFO:  -7.98 * 1.0 =  -7.98 for ctc
2024-10-27 18:09:44,203 (beam_search:479) INFO: total log probability: -7.98
2024-10-27 18:09:44,203 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:09:44,203 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:44,204 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁IS▁FLOWING▁OUT▁OF▁THE▁NEGATIVE▁THROUGH▁THE▁FIRST▁LIGHT▁BULB▁AND▁THEN▁INTO▁THE▁SECOND▁LIGHT▁BULB▁BACK▁THROUGH▁FROM▁THE▁TO▁THE▁OTHER▁BULB▁AND▁BACK▁INTO▁THE▁POSITIVE▁THE▁IT'S▁ONE▁SO▁IT'S▁A

2024-10-27 18:09:44,206 (asr_inference:509) INFO: speech length: 75422
2024-10-27 18:09:46,963 (beam_search:428) INFO: decoder input length: 58
2024-10-27 18:09:46,963 (beam_search:429) INFO: max output length: 58
2024-10-27 18:09:46,963 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:47,028 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:47,028 (beam_search:476) INFO:  -2.25 * 1.0 =  -2.25 for ctc
2024-10-27 18:09:47,028 (beam_search:479) INFO: total log probability: -2.25
2024-10-27 18:09:47,028 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:09:47,028 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:47,029 (beam_search:483) INFO: best hypo: ▁EACH▁BULB▁HAS▁ITS▁PATHWAY

2024-10-27 18:09:47,031 (asr_inference:509) INFO: speech length: 175885
2024-10-27 18:09:53,669 (beam_search:428) INFO: decoder input length: 136
2024-10-27 18:09:53,670 (beam_search:429) INFO: max output length: 136
2024-10-27 18:09:53,670 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:54,153 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:54,153 (beam_search:476) INFO:  -6.06 * 1.0 =  -6.06 for ctc
2024-10-27 18:09:54,153 (beam_search:479) INFO: total log probability: -6.06
2024-10-27 18:09:54,153 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:09:54,153 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:54,154 (beam_search:483) INFO: best hypo: ▁SO▁THE▁PATHWAYS▁BOTH▁COME▁OUT▁OF▁THE▁BATTERY▁THROUGH▁IN▁SEPARATE▁TO▁THE▁LIGHT▁AND▁BACK▁INTO▁THE

2024-10-27 18:09:54,156 (asr_inference:509) INFO: speech length: 61916
2024-10-27 18:09:56,448 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:09:56,448 (beam_search:429) INFO: max output length: 47
2024-10-27 18:09:56,448 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:56,483 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:56,483 (beam_search:476) INFO:  -0.59 * 1.0 =  -0.59 for ctc
2024-10-27 18:09:56,484 (beam_search:479) INFO: total log probability: -0.59
2024-10-27 18:09:56,484 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:09:56,484 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:56,484 (beam_search:483) INFO: best hypo: ▁IT▁ONLY▁ONE▁BULB

2024-10-27 18:09:56,486 (asr_inference:509) INFO: speech length: 35737
2024-10-27 18:09:57,872 (beam_search:428) INFO: decoder input length: 27
2024-10-27 18:09:57,872 (beam_search:429) INFO: max output length: 27
2024-10-27 18:09:57,872 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:57,894 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:57,894 (beam_search:476) INFO:  -1.68 * 1.0 =  -1.68 for ctc
2024-10-27 18:09:57,894 (beam_search:479) INFO: total log probability: -1.68
2024-10-27 18:09:57,894 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:09:57,894 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:57,894 (beam_search:483) INFO: best hypo: ▁THERE'S▁TWO

2024-10-27 18:09:57,896 (asr_inference:509) INFO: speech length: 57702
2024-10-27 18:10:00,007 (beam_search:428) INFO: decoder input length: 44
2024-10-27 18:10:00,007 (beam_search:429) INFO: max output length: 44
2024-10-27 18:10:00,007 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:00,059 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:00,059 (beam_search:476) INFO:  -2.92 * 1.0 =  -2.92 for ctc
2024-10-27 18:10:00,059 (beam_search:479) INFO: total log probability: -2.92
2024-10-27 18:10:00,059 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 18:10:00,059 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:00,059 (beam_search:483) INFO: best hypo: ▁THE▁ARE▁BECAUSE▁THERE'S▁MORE

2024-10-27 18:10:00,062 (asr_inference:509) INFO: speech length: 34937
2024-10-27 18:10:01,427 (beam_search:428) INFO: decoder input length: 26
2024-10-27 18:10:01,427 (beam_search:429) INFO: max output length: 26
2024-10-27 18:10:01,427 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:01,453 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:01,453 (beam_search:476) INFO:  -1.87 * 1.0 =  -1.87 for ctc
2024-10-27 18:10:01,453 (beam_search:479) INFO: total log probability: -1.87
2024-10-27 18:10:01,453 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:10:01,453 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:01,454 (beam_search:483) INFO: best hypo: ▁CAUSE▁THERE▁IS▁LESS▁ENERGY

2024-10-27 18:10:01,456 (asr_inference:509) INFO: speech length: 25500
2024-10-27 18:10:02,552 (beam_search:428) INFO: decoder input length: 19
2024-10-27 18:10:02,552 (beam_search:429) INFO: max output length: 19
2024-10-27 18:10:02,552 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:02,564 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:02,564 (beam_search:476) INFO:  -0.99 * 1.0 =  -0.99 for ctc
2024-10-27 18:10:02,564 (beam_search:479) INFO: total log probability: -0.99
2024-10-27 18:10:02,564 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:10:02,564 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:02,565 (beam_search:483) INFO: best hypo: ▁ARE▁TWO

2024-10-27 18:10:02,566 (asr_inference:509) INFO: speech length: 203151
2024-10-27 18:10:10,733 (beam_search:428) INFO: decoder input length: 158
2024-10-27 18:10:10,733 (beam_search:429) INFO: max output length: 158
2024-10-27 18:10:10,733 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:11,350 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:11,350 (beam_search:476) INFO:  -4.43 * 1.0 =  -4.43 for ctc
2024-10-27 18:10:11,350 (beam_search:479) INFO: total log probability: -4.43
2024-10-27 18:10:11,350 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:10:11,350 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:11,350 (beam_search:483) INFO: best hypo: ▁OF▁LIGHTS▁ARED▁TO▁BE▁CONNECTED▁IN▁PARALLEL▁SO▁THAT▁THEY▁DON'T▁ALL▁THE▁BULBS▁WHEN▁BULB▁OUT

2024-10-27 18:10:11,353 (asr_inference:509) INFO: speech length: 177026
2024-10-27 18:10:18,087 (beam_search:428) INFO: decoder input length: 137
2024-10-27 18:10:18,087 (beam_search:429) INFO: max output length: 137
2024-10-27 18:10:18,087 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:18,555 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:18,555 (beam_search:476) INFO:  -2.44 * 1.0 =  -2.44 for ctc
2024-10-27 18:10:18,555 (beam_search:479) INFO: total log probability: -2.44
2024-10-27 18:10:18,555 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:10:18,555 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:18,555 (beam_search:483) INFO: best hypo: ▁ARE▁IN▁THE▁OF▁LIGHTS▁AND▁WHEN▁A▁OUT▁THEN▁ALL▁THE▁LIGHTS▁GO▁OUT▁ON▁THE

2024-10-27 18:10:18,558 (asr_inference:509) INFO: speech length: 88332
2024-10-27 18:10:21,731 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:10:21,731 (beam_search:429) INFO: max output length: 68
2024-10-27 18:10:21,731 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:21,857 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:21,857 (beam_search:476) INFO:  -1.94 * 1.0 =  -1.94 for ctc
2024-10-27 18:10:21,857 (beam_search:479) INFO: total log probability: -1.94
2024-10-27 18:10:21,857 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:10:21,857 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:21,857 (beam_search:483) INFO: best hypo: ▁IT'S▁A▁CIRCUIT▁AND▁AND▁THE▁SWITCH▁IS▁OPEN

2024-10-27 18:10:21,860 (asr_inference:509) INFO: speech length: 44472
2024-10-27 18:10:23,587 (beam_search:428) INFO: decoder input length: 34
2024-10-27 18:10:23,587 (beam_search:429) INFO: max output length: 34
2024-10-27 18:10:23,587 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:23,611 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:23,612 (beam_search:476) INFO:  -0.71 * 1.0 =  -0.71 for ctc
2024-10-27 18:10:23,612 (beam_search:479) INFO: total log probability: -0.71
2024-10-27 18:10:23,612 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:10:23,612 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:23,612 (beam_search:483) INFO: best hypo: ▁NO▁ENERGY▁IS

2024-10-27 18:10:23,614 (asr_inference:509) INFO: speech length: 37500
2024-10-27 18:10:25,079 (beam_search:428) INFO: decoder input length: 28
2024-10-27 18:10:25,079 (beam_search:429) INFO: max output length: 28
2024-10-27 18:10:25,079 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:25,114 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:25,114 (beam_search:476) INFO:  -0.75 * 1.0 =  -0.75 for ctc
2024-10-27 18:10:25,114 (beam_search:479) INFO: total log probability: -0.75
2024-10-27 18:10:25,114 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:10:25,114 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:25,114 (beam_search:483) INFO: best hypo: ▁THE▁LIGHT▁TURNS▁ON▁BECAUSE▁THE

2024-10-27 18:10:25,116 (asr_inference:509) INFO: speech length: 61288
2024-10-27 18:10:27,402 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:10:27,403 (beam_search:429) INFO: max output length: 47
2024-10-27 18:10:27,403 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:27,497 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:27,498 (beam_search:476) INFO:  -3.29 * 1.0 =  -3.29 for ctc
2024-10-27 18:10:27,498 (beam_search:479) INFO: total log probability: -3.29
2024-10-27 18:10:27,498 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:10:27,498 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:27,498 (beam_search:483) INFO: best hypo: ▁THEN▁ALL▁THE▁BULBS▁GO▁OUT▁IF▁THEY'RE▁IN▁A

2024-10-27 18:10:27,500 (asr_inference:509) INFO: speech length: 221910
2024-10-27 18:10:36,280 (beam_search:428) INFO: decoder input length: 172
2024-10-27 18:10:36,280 (beam_search:429) INFO: max output length: 172
2024-10-27 18:10:36,280 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:37,294 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:37,294 (beam_search:476) INFO:  -5.93 * 1.0 =  -5.93 for ctc
2024-10-27 18:10:37,294 (beam_search:479) INFO: total log probability: -5.93
2024-10-27 18:10:37,294 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:10:37,294 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:37,294 (beam_search:483) INFO: best hypo: ▁OF▁LIGHTS▁AND▁ARE▁CONNECTED▁IN▁ONE▁PATHWAY▁SO▁IF▁THE▁FILAMENT▁IN▁A▁BULB▁OUT▁IT'S▁LIKE▁AN▁OPEN▁SWITCH▁SO▁NO▁ENERGY▁CAN▁FLOW▁THROUGH▁THE▁CIRCUIT

2024-10-27 18:10:37,297 (asr_inference:509) INFO: speech length: 38320
2024-10-27 18:10:38,887 (beam_search:428) INFO: decoder input length: 29
2024-10-27 18:10:38,887 (beam_search:429) INFO: max output length: 29
2024-10-27 18:10:38,887 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:38,904 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:38,904 (beam_search:476) INFO:  -0.10 * 1.0 =  -0.10 for ctc
2024-10-27 18:10:38,904 (beam_search:479) INFO: total log probability: -0.10
2024-10-27 18:10:38,904 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:10:38,904 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:38,904 (beam_search:483) INFO: best hypo: ▁THE▁OTHER

2024-10-27 18:10:38,906 (asr_inference:509) INFO: speech length: 127552
2024-10-27 18:10:43,563 (beam_search:428) INFO: decoder input length: 99
2024-10-27 18:10:43,563 (beam_search:429) INFO: max output length: 99
2024-10-27 18:10:43,563 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:43,884 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:43,885 (beam_search:476) INFO:  -3.14 * 1.0 =  -3.14 for ctc
2024-10-27 18:10:43,885 (beam_search:479) INFO: total log probability: -3.14
2024-10-27 18:10:43,885 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:10:43,885 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:43,885 (beam_search:483) INFO: best hypo: ▁THE▁OTHER▁GOES▁OUT▁TOO▁BECAUSE▁ONE▁OF▁IS▁UP▁SO▁IT'S▁LIKE▁AN▁OPEN▁SWITCH

2024-10-27 18:10:43,887 (asr_inference:509) INFO: speech length: 268870
2024-10-27 18:10:55,312 (beam_search:428) INFO: decoder input length: 209
2024-10-27 18:10:55,312 (beam_search:429) INFO: max output length: 209
2024-10-27 18:10:55,312 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:55,818 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:55,818 (beam_search:476) INFO:  -6.47 * 1.0 =  -6.47 for ctc
2024-10-27 18:10:55,818 (beam_search:479) INFO: total log probability: -6.47
2024-10-27 18:10:55,818 (beam_search:480) INFO: normalized log probability: -0.43
2024-10-27 18:10:55,818 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:55,818 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁IS▁FLOWING▁ONE▁BULB▁IS▁SO▁THE▁OF▁ARE▁U▁YOU

2024-10-27 18:10:55,821 (asr_inference:509) INFO: speech length: 77127
2024-10-27 18:10:58,632 (beam_search:428) INFO: decoder input length: 59
2024-10-27 18:10:58,632 (beam_search:429) INFO: max output length: 59
2024-10-27 18:10:58,632 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:58,703 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:58,704 (beam_search:476) INFO:  -0.34 * 1.0 =  -0.34 for ctc
2024-10-27 18:10:58,704 (beam_search:479) INFO: total log probability: -0.34
2024-10-27 18:10:58,704 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 18:10:58,704 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:58,704 (beam_search:483) INFO: best hypo: ▁ALL▁THE▁ARE▁CONNECTED▁IN▁ONE▁PATHWAY

2024-10-27 18:10:58,706 (asr_inference:509) INFO: speech length: 131177
2024-10-27 18:11:03,376 (beam_search:428) INFO: decoder input length: 101
2024-10-27 18:11:03,377 (beam_search:429) INFO: max output length: 101
2024-10-27 18:11:03,377 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:03,672 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:03,672 (beam_search:476) INFO:  -2.36 * 1.0 =  -2.36 for ctc
2024-10-27 18:11:03,672 (beam_search:479) INFO: total log probability: -2.36
2024-10-27 18:11:03,672 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:11:03,672 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:03,672 (beam_search:483) INFO: best hypo: ▁A▁SERIES▁CIRCUIT▁BE▁LIKE▁THIS▁BECAUSE▁IT'S▁ONE▁PATHWAY▁THATS▁THROUGH▁ALL▁THE

2024-10-27 18:11:03,675 (asr_inference:509) INFO: speech length: 185587
2024-10-27 18:11:10,770 (beam_search:428) INFO: decoder input length: 144
2024-10-27 18:11:10,770 (beam_search:429) INFO: max output length: 144
2024-10-27 18:11:10,771 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:11,129 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:11,129 (beam_search:476) INFO:  -5.55 * 1.0 =  -5.55 for ctc
2024-10-27 18:11:11,129 (beam_search:479) INFO: total log probability: -5.55
2024-10-27 18:11:11,129 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:11:11,129 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:11,129 (beam_search:483) INFO: best hypo: ▁WELL▁WHEN▁THE▁FILAMENT▁IT▁IT▁LIKES▁THE▁SWITCH▁LIKE▁A▁LIKE▁OPEN

2024-10-27 18:11:11,132 (asr_inference:509) INFO: speech length: 80818
2024-10-27 18:11:14,211 (beam_search:428) INFO: decoder input length: 62
2024-10-27 18:11:14,211 (beam_search:429) INFO: max output length: 62
2024-10-27 18:11:14,211 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:14,300 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:14,300 (beam_search:476) INFO:  -1.84 * 1.0 =  -1.84 for ctc
2024-10-27 18:11:14,301 (beam_search:479) INFO: total log probability: -1.84
2024-10-27 18:11:14,301 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:11:14,301 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:14,301 (beam_search:483) INFO: best hypo: ▁ONE▁IS▁OUT▁SO▁THE▁OTHER▁BULB▁IS

2024-10-27 18:11:14,303 (asr_inference:509) INFO: speech length: 207435
2024-10-27 18:11:22,433 (beam_search:428) INFO: decoder input length: 161
2024-10-27 18:11:22,433 (beam_search:429) INFO: max output length: 161
2024-10-27 18:11:22,433 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:22,967 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:22,967 (beam_search:476) INFO:  -4.09 * 1.0 =  -4.09 for ctc
2024-10-27 18:11:22,967 (beam_search:479) INFO: total log probability: -4.09
2024-10-27 18:11:22,967 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:11:22,967 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:22,967 (beam_search:483) INFO: best hypo: ▁THIS▁IS▁A▁CIRCUIT▁WITH▁ENERGY▁FLOWING▁THE▁FILAMENT▁OUT▁SO▁BECAUSE▁THE▁OUT▁THEN▁THE▁OTHER▁TURNS

2024-10-27 18:11:22,970 (asr_inference:509) INFO: speech length: 63831
2024-10-27 18:11:25,361 (beam_search:428) INFO: decoder input length: 49
2024-10-27 18:11:25,361 (beam_search:429) INFO: max output length: 49
2024-10-27 18:11:25,361 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:25,407 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:25,408 (beam_search:476) INFO:  -0.65 * 1.0 =  -0.65 for ctc
2024-10-27 18:11:25,408 (beam_search:479) INFO: total log probability: -0.65
2024-10-27 18:11:25,408 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:11:25,408 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:25,408 (beam_search:483) INFO: best hypo: ▁A▁ANDS▁THE▁PATHWAY

2024-10-27 18:11:25,411 (asr_inference:509) INFO: speech length: 88405
2024-10-27 18:11:28,665 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:11:28,666 (beam_search:429) INFO: max output length: 68
2024-10-27 18:11:28,666 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:28,755 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:28,756 (beam_search:476) INFO:  -0.98 * 1.0 =  -0.98 for ctc
2024-10-27 18:11:28,756 (beam_search:479) INFO: total log probability: -0.98
2024-10-27 18:11:28,756 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:11:28,756 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:28,756 (beam_search:483) INFO: best hypo: ▁WELL▁WHEN▁A▁BULB▁GOES▁OUT▁THE▁ELECTRICITY

2024-10-27 18:11:28,758 (asr_inference:509) INFO: speech length: 86083
2024-10-27 18:11:31,771 (beam_search:428) INFO: decoder input length: 66
2024-10-27 18:11:31,771 (beam_search:429) INFO: max output length: 66
2024-10-27 18:11:31,771 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:31,858 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:31,858 (beam_search:476) INFO:  -0.73 * 1.0 =  -0.73 for ctc
2024-10-27 18:11:31,858 (beam_search:479) INFO: total log probability: -0.73
2024-10-27 18:11:31,858 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:11:31,858 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:31,858 (beam_search:483) INFO: best hypo: ▁THE▁IS▁NOT▁CONNECTED▁FOR▁ENERGY▁TO

2024-10-27 18:11:31,861 (asr_inference:509) INFO: speech length: 155511
2024-10-27 18:11:37,913 (beam_search:428) INFO: decoder input length: 120
2024-10-27 18:11:37,913 (beam_search:429) INFO: max output length: 120
2024-10-27 18:11:37,913 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:38,220 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:38,220 (beam_search:476) INFO:  -6.07 * 1.0 =  -6.07 for ctc
2024-10-27 18:11:38,220 (beam_search:479) INFO: total log probability: -6.07
2024-10-27 18:11:38,220 (beam_search:480) INFO: normalized log probability: -0.36
2024-10-27 18:11:38,221 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:38,221 (beam_search:483) INFO: best hypo: ▁IT'S▁SO▁ONE▁OUT▁THEN▁THE▁OTHER▁ONE▁KEEPS▁BECAUSE▁THERE▁TWO

2024-10-27 18:11:38,223 (asr_inference:509) INFO: speech length: 85372
2024-10-27 18:11:41,253 (beam_search:428) INFO: decoder input length: 66
2024-10-27 18:11:41,253 (beam_search:429) INFO: max output length: 66
2024-10-27 18:11:41,253 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:41,404 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:41,404 (beam_search:476) INFO:  -1.37 * 1.0 =  -1.37 for ctc
2024-10-27 18:11:41,404 (beam_search:479) INFO: total log probability: -1.37
2024-10-27 18:11:41,404 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:11:41,404 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:41,405 (beam_search:483) INFO: best hypo: ▁IT'S▁TO▁THE▁ONE▁THAT'S▁OUT▁SO▁YOU▁CAN▁IT

2024-10-27 18:11:41,407 (asr_inference:509) INFO: speech length: 120463
2024-10-27 18:11:45,802 (beam_search:428) INFO: decoder input length: 93
2024-10-27 18:11:45,802 (beam_search:429) INFO: max output length: 93
2024-10-27 18:11:45,802 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:46,053 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:46,053 (beam_search:476) INFO:  -2.47 * 1.0 =  -2.47 for ctc
2024-10-27 18:11:46,053 (beam_search:479) INFO: total log probability: -2.47
2024-10-27 18:11:46,053 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:11:46,053 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:46,053 (beam_search:483) INFO: best hypo: ▁THE▁OTHER▁BULB▁GETS▁BECAUSE▁THE▁ENERGY▁DOESN'T▁HAVE▁TO▁AND▁IT▁ON

2024-10-27 18:11:46,055 (asr_inference:509) INFO: speech length: 270583
2024-10-27 18:11:57,160 (beam_search:428) INFO: decoder input length: 210
2024-10-27 18:11:57,160 (beam_search:429) INFO: max output length: 210
2024-10-27 18:11:57,160 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:58,235 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:58,235 (beam_search:476) INFO:  -2.58 * 1.0 =  -2.58 for ctc
2024-10-27 18:11:58,235 (beam_search:479) INFO: total log probability: -2.58
2024-10-27 18:11:58,235 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:11:58,235 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:58,235 (beam_search:483) INFO: best hypo: ▁IT'S▁A▁IT'S▁A▁CIRCUIT▁IT'S▁NOT▁IN▁SO▁WHEN▁ONE▁OUT▁THE▁BECAUSE▁ONE▁PATHWAY▁AND▁THE▁OF▁THE▁ARE▁STILL

2024-10-27 18:11:58,238 (asr_inference:509) INFO: speech length: 98516
2024-10-27 18:12:01,923 (beam_search:428) INFO: decoder input length: 76
2024-10-27 18:12:01,923 (beam_search:429) INFO: max output length: 76
2024-10-27 18:12:01,923 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:02,086 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:02,087 (beam_search:476) INFO:  -1.72 * 1.0 =  -1.72 for ctc
2024-10-27 18:12:02,087 (beam_search:479) INFO: total log probability: -1.72
2024-10-27 18:12:02,087 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:12:02,087 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:02,087 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁FLOWING▁BECAUSE▁THE▁BULB▁IS▁WHICH▁IS▁LIKE▁AN▁OPEN

2024-10-27 18:12:02,090 (asr_inference:509) INFO: speech length: 44376
2024-10-27 18:12:03,739 (beam_search:428) INFO: decoder input length: 34
2024-10-27 18:12:03,739 (beam_search:429) INFO: max output length: 34
2024-10-27 18:12:03,739 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:03,775 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:03,776 (beam_search:476) INFO:  -2.02 * 1.0 =  -2.02 for ctc
2024-10-27 18:12:03,776 (beam_search:479) INFO: total log probability: -2.02
2024-10-27 18:12:03,776 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:12:03,776 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:03,776 (beam_search:483) INFO: best hypo: ▁THEY▁ARE▁ALL▁CONNECTED▁IN▁PATHWAYS

2024-10-27 18:12:03,779 (asr_inference:509) INFO: speech length: 135270
2024-10-27 18:12:08,850 (beam_search:428) INFO: decoder input length: 105
2024-10-27 18:12:08,851 (beam_search:429) INFO: max output length: 105
2024-10-27 18:12:08,851 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:09,093 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:09,093 (beam_search:476) INFO:  -4.31 * 1.0 =  -4.31 for ctc
2024-10-27 18:12:09,093 (beam_search:479) INFO: total log probability: -4.31
2024-10-27 18:12:09,093 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 18:12:09,093 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:09,093 (beam_search:483) INFO: best hypo: ▁SO▁SO▁THE▁OTHER▁BULB▁GOES▁OFF▁IF▁IT'S▁IN

2024-10-27 18:12:09,096 (asr_inference:509) INFO: speech length: 96611
2024-10-27 18:12:12,464 (beam_search:428) INFO: decoder input length: 74
2024-10-27 18:12:12,464 (beam_search:429) INFO: max output length: 74
2024-10-27 18:12:12,464 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:12,653 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:12,653 (beam_search:476) INFO:  -3.43 * 1.0 =  -3.43 for ctc
2024-10-27 18:12:12,653 (beam_search:479) INFO: total log probability: -3.43
2024-10-27 18:12:12,653 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:12:12,653 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:12,653 (beam_search:483) INFO: best hypo: ▁THIS▁IS▁A▁SERIES▁CIRCUIT▁AND▁ONE▁BULB▁IS▁OUT▁SO▁THE▁OTHER▁ONE▁GOES

2024-10-27 18:12:12,655 (asr_inference:509) INFO: speech length: 100986
2024-10-27 18:12:16,302 (beam_search:428) INFO: decoder input length: 78
2024-10-27 18:12:16,302 (beam_search:429) INFO: max output length: 78
2024-10-27 18:12:16,302 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:16,467 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:16,467 (beam_search:476) INFO:  -1.40 * 1.0 =  -1.40 for ctc
2024-10-27 18:12:16,467 (beam_search:479) INFO: total log probability: -1.40
2024-10-27 18:12:16,467 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:12:16,467 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:16,467 (beam_search:483) INFO: best hypo: ▁BECAUSE▁THE▁FILAMENT▁BURNS▁OUT▁AND▁LIKES▁THE▁LIKE▁AN▁OPEN

2024-10-27 18:12:16,470 (asr_inference:509) INFO: speech length: 45777
2024-10-27 18:12:18,166 (beam_search:428) INFO: decoder input length: 35
2024-10-27 18:12:18,166 (beam_search:429) INFO: max output length: 35
2024-10-27 18:12:18,166 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:18,206 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:18,206 (beam_search:476) INFO:  -1.05 * 1.0 =  -1.05 for ctc
2024-10-27 18:12:18,206 (beam_search:479) INFO: total log probability: -1.05
2024-10-27 18:12:18,206 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:12:18,206 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:18,206 (beam_search:483) INFO: best hypo: ▁WE'VE▁BEEN▁LEARNING▁ABOUT▁MAGNETS

2024-10-27 18:12:18,208 (asr_inference:509) INFO: speech length: 94678
2024-10-27 18:12:21,684 (beam_search:428) INFO: decoder input length: 73
2024-10-27 18:12:21,685 (beam_search:429) INFO: max output length: 73
2024-10-27 18:12:21,685 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:21,803 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:21,803 (beam_search:476) INFO:  -3.35 * 1.0 =  -3.35 for ctc
2024-10-27 18:12:21,803 (beam_search:479) INFO: total log probability: -3.35
2024-10-27 18:12:21,803 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 18:12:21,803 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:21,803 (beam_search:483) INFO: best hypo: ▁WE▁HOW▁STICK▁TO▁ONLY▁THINGS▁THAT▁ARE▁OR

2024-10-27 18:12:21,806 (asr_inference:509) INFO: speech length: 172329
2024-10-27 18:12:28,257 (beam_search:428) INFO: decoder input length: 134
2024-10-27 18:12:28,257 (beam_search:429) INFO: max output length: 134
2024-10-27 18:12:28,257 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:28,570 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:28,571 (beam_search:476) INFO:  -2.47 * 1.0 =  -2.47 for ctc
2024-10-27 18:12:28,571 (beam_search:479) INFO: total log probability: -2.47
2024-10-27 18:12:28,571 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:12:28,571 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:28,571 (beam_search:483) INFO: best hypo: ▁WELL▁ARE▁MADE▁OUT▁OF▁AND▁THEY▁STICK▁TO▁OTHER▁MAGNETS▁OR

2024-10-27 18:12:28,574 (asr_inference:509) INFO: speech length: 63948
2024-10-27 18:12:30,956 (beam_search:428) INFO: decoder input length: 49
2024-10-27 18:12:30,956 (beam_search:429) INFO: max output length: 49
2024-10-27 18:12:30,956 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:31,012 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:31,012 (beam_search:476) INFO:  -1.19 * 1.0 =  -1.19 for ctc
2024-10-27 18:12:31,012 (beam_search:479) INFO: total log probability: -1.19
2024-10-27 18:12:31,012 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:12:31,012 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:31,012 (beam_search:483) INFO: best hypo: ▁IT▁IS▁MADE▁OF▁OR▁STEEL

2024-10-27 18:12:31,015 (asr_inference:509) INFO: speech length: 111132
2024-10-27 18:12:35,062 (beam_search:428) INFO: decoder input length: 86
2024-10-27 18:12:35,063 (beam_search:429) INFO: max output length: 86
2024-10-27 18:12:35,063 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:35,169 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:35,170 (beam_search:476) INFO:  -5.07 * 1.0 =  -5.07 for ctc
2024-10-27 18:12:35,170 (beam_search:479) INFO: total log probability: -5.07
2024-10-27 18:12:35,170 (beam_search:480) INFO: normalized log probability: -0.56
2024-10-27 18:12:35,170 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:35,170 (beam_search:483) INFO: best hypo: ▁THE▁IS▁THE▁NAIL▁BECAUSE▁IT▁STEEL

2024-10-27 18:12:35,173 (asr_inference:509) INFO: speech length: 115404
2024-10-27 18:12:39,444 (beam_search:428) INFO: decoder input length: 89
2024-10-27 18:12:39,444 (beam_search:429) INFO: max output length: 89
2024-10-27 18:12:39,444 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:39,590 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:39,591 (beam_search:476) INFO:  -2.14 * 1.0 =  -2.14 for ctc
2024-10-27 18:12:39,591 (beam_search:479) INFO: total log probability: -2.14
2024-10-27 18:12:39,591 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:12:39,591 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:39,591 (beam_search:483) INFO: best hypo: ▁BECAUSE▁IT▁IS▁OR▁AND▁ONLY▁THOSE▁STICK▁TO

2024-10-27 18:12:39,593 (asr_inference:509) INFO: speech length: 101887
2024-10-27 18:12:43,302 (beam_search:428) INFO: decoder input length: 79
2024-10-27 18:12:43,302 (beam_search:429) INFO: max output length: 79
2024-10-27 18:12:43,303 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:43,376 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:43,376 (beam_search:476) INFO:  -2.73 * 1.0 =  -2.73 for ctc
2024-10-27 18:12:43,376 (beam_search:479) INFO: total log probability: -2.73
2024-10-27 18:12:43,377 (beam_search:480) INFO: normalized log probability: -0.39
2024-10-27 18:12:43,377 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:43,377 (beam_search:483) INFO: best hypo: S▁WHICH▁TO▁THE▁MAGNET

2024-10-27 18:12:43,379 (asr_inference:509) INFO: speech length: 352199
2024-10-27 18:12:58,913 (beam_search:428) INFO: decoder input length: 274
2024-10-27 18:12:58,913 (beam_search:429) INFO: max output length: 274
2024-10-27 18:12:58,913 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:59,103 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:59,103 (beam_search:476) INFO:  -2.58 * 1.0 =  -2.58 for ctc
2024-10-27 18:12:59,104 (beam_search:479) INFO: total log probability: -2.58
2024-10-27 18:12:59,104 (beam_search:480) INFO: normalized log probability: -0.52
2024-10-27 18:12:59,104 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:59,104 (beam_search:483) INFO: best hypo: ▁WOOD▁AND▁EVEN

2024-10-27 18:12:59,107 (asr_inference:509) INFO: speech length: 296875
2024-10-27 18:13:12,533 (beam_search:428) INFO: decoder input length: 231
2024-10-27 18:13:12,533 (beam_search:429) INFO: max output length: 231
2024-10-27 18:13:12,533 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:13,396 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:13,396 (beam_search:476) INFO:  -5.40 * 1.0 =  -5.40 for ctc
2024-10-27 18:13:13,396 (beam_search:479) INFO: total log probability: -5.40
2024-10-27 18:13:13,396 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:13:13,396 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:13,396 (beam_search:483) INFO: best hypo: ▁ONLY▁STICK▁TO▁THAT▁HAVES▁IN▁LIKE▁WOOD▁HAS▁OF▁THAT▁WOOD▁NO▁SO▁IT▁WON'T▁STICK

2024-10-27 18:13:13,399 (asr_inference:509) INFO: speech length: 153627
2024-10-27 18:13:19,617 (beam_search:428) INFO: decoder input length: 119
2024-10-27 18:13:19,617 (beam_search:429) INFO: max output length: 119
2024-10-27 18:13:19,617 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:19,914 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:19,915 (beam_search:476) INFO:  -6.94 * 1.0 =  -6.94 for ctc
2024-10-27 18:13:19,915 (beam_search:479) INFO: total log probability: -6.94
2024-10-27 18:13:19,915 (beam_search:480) INFO: normalized log probability: -0.43
2024-10-27 18:13:19,915 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:19,915 (beam_search:483) INFO: best hypo: ▁IS▁MADE▁OF▁THE▁NAIL▁IS▁STEEL▁WHICH▁TO▁AND▁THE▁IS▁NOT▁STICK

2024-10-27 18:13:19,917 (asr_inference:509) INFO: speech length: 149264
2024-10-27 18:13:25,889 (beam_search:428) INFO: decoder input length: 116
2024-10-27 18:13:25,890 (beam_search:429) INFO: max output length: 116
2024-10-27 18:13:25,890 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:26,132 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:26,132 (beam_search:476) INFO:  -4.24 * 1.0 =  -4.24 for ctc
2024-10-27 18:13:26,132 (beam_search:479) INFO: total log probability: -4.24
2024-10-27 18:13:26,132 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 18:13:26,132 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:26,132 (beam_search:483) INFO: best hypo: ▁ONLY▁STICK▁TO▁TO▁WITH▁OR▁ANYTHING▁THAT▁HAS▁IN▁IT

2024-10-27 18:13:26,134 (asr_inference:509) INFO: speech length: 149403
2024-10-27 18:13:32,026 (beam_search:428) INFO: decoder input length: 116
2024-10-27 18:13:32,026 (beam_search:429) INFO: max output length: 116
2024-10-27 18:13:32,026 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:32,363 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:32,363 (beam_search:476) INFO:  -5.71 * 1.0 =  -5.71 for ctc
2024-10-27 18:13:32,363 (beam_search:479) INFO: total log probability: -5.71
2024-10-27 18:13:32,364 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 18:13:32,364 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:32,364 (beam_search:483) INFO: best hypo: ▁SO▁THE▁NO▁IN▁IT▁AND▁IT'S▁NOT▁IRON▁OR▁STEEL▁SO▁NOT▁STICK

2024-10-27 18:13:32,366 (asr_inference:509) INFO: speech length: 167678
2024-10-27 18:13:38,475 (beam_search:428) INFO: decoder input length: 130
2024-10-27 18:13:38,475 (beam_search:429) INFO: max output length: 130
2024-10-27 18:13:38,475 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:38,816 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:38,816 (beam_search:476) INFO:  -5.58 * 1.0 =  -5.58 for ctc
2024-10-27 18:13:38,816 (beam_search:479) INFO: total log probability: -5.58
2024-10-27 18:13:38,816 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:13:38,816 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:38,816 (beam_search:483) INFO: best hypo: ▁STICK▁TO▁THEY▁MADE▁OF▁AND▁IS▁MAGNETIC▁STEEL▁HASS▁OF▁IN▁IT

2024-10-27 18:13:38,820 (asr_inference:509) INFO: speech length: 250474
2024-10-27 18:13:49,099 (beam_search:428) INFO: decoder input length: 195
2024-10-27 18:13:49,099 (beam_search:429) INFO: max output length: 195
2024-10-27 18:13:49,099 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:49,966 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:49,966 (beam_search:476) INFO:  -8.07 * 1.0 =  -8.07 for ctc
2024-10-27 18:13:49,966 (beam_search:479) INFO: total log probability: -8.07
2024-10-27 18:13:49,966 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 18:13:49,966 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:49,966 (beam_search:483) INFO: best hypo: ▁ONE▁IS▁ONE▁IS▁STICK▁TO▁BECAUSE▁THEY▁HAVES▁OF▁IRON▁IN▁THEM▁NOT▁STICK▁TO▁THE▁BECAUSE▁IT▁HAS▁NO▁OF▁IN▁IT

2024-10-27 18:13:49,968 (asr_inference:509) INFO: speech length: 143063
2024-10-27 18:13:55,381 (beam_search:428) INFO: decoder input length: 111
2024-10-27 18:13:55,381 (beam_search:429) INFO: max output length: 111
2024-10-27 18:13:55,381 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:55,518 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:55,518 (beam_search:476) INFO:  -2.38 * 1.0 =  -2.38 for ctc
2024-10-27 18:13:55,518 (beam_search:479) INFO: total log probability: -2.38
2024-10-27 18:13:55,518 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 18:13:55,518 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:55,518 (beam_search:483) INFO: best hypo: ▁STICK▁TO▁NOT▁STICK▁TO▁MAGNETS

2024-10-27 18:13:55,520 (asr_inference:509) INFO: speech length: 358940
2024-10-27 18:14:11,904 (beam_search:428) INFO: decoder input length: 279
2024-10-27 18:14:11,904 (beam_search:429) INFO: max output length: 279
2024-10-27 18:14:11,904 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:13,022 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:13,022 (beam_search:476) INFO:  -8.48 * 1.0 =  -8.48 for ctc
2024-10-27 18:14:13,022 (beam_search:479) INFO: total log probability: -8.48
2024-10-27 18:14:13,023 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:14:13,023 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:13,023 (beam_search:483) INFO: best hypo: ▁MAKES▁MAGNET▁STICK▁TO▁IN▁IT▁IS▁LIKE▁THE▁IT▁THE▁THE▁MAGNET▁TO▁LIKE▁IT▁BECAUSE▁THE▁MAGNET▁REALLY▁LIKES

2024-10-27 18:14:13,025 (asr_inference:509) INFO: speech length: 233302
2024-10-27 18:14:22,371 (beam_search:428) INFO: decoder input length: 181
2024-10-27 18:14:22,371 (beam_search:429) INFO: max output length: 181
2024-10-27 18:14:22,371 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:22,869 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:22,870 (beam_search:476) INFO:  -3.00 * 1.0 =  -3.00 for ctc
2024-10-27 18:14:22,870 (beam_search:479) INFO: total log probability: -3.00
2024-10-27 18:14:22,870 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:14:22,870 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:22,870 (beam_search:483) INFO: best hypo: ▁STICK▁TO▁AND▁BECAUSE▁THEY▁BECAUSE▁STICK▁TO▁STEEL▁IS▁WITH▁AND▁HAS▁NO▁IRON

2024-10-27 18:14:22,872 (asr_inference:509) INFO: speech length: 226359
2024-10-27 18:14:31,870 (beam_search:428) INFO: decoder input length: 176
2024-10-27 18:14:31,870 (beam_search:429) INFO: max output length: 176
2024-10-27 18:14:31,870 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:32,622 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:32,622 (beam_search:476) INFO:  -2.94 * 1.0 =  -2.94 for ctc
2024-10-27 18:14:32,623 (beam_search:479) INFO: total log probability: -2.94
2024-10-27 18:14:32,623 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:14:32,623 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:32,623 (beam_search:483) INFO: best hypo: ▁SO▁THE▁ONLY▁THAT▁DID▁NOT▁STICK▁WERE▁THE▁THE▁BRASS▁RING▁AND▁THE▁BECAUSE▁THEY▁HAVE▁NOS▁OF▁IRON▁IN▁THEM

2024-10-27 18:14:32,625 (asr_inference:509) INFO: speech length: 92955
2024-10-27 18:14:35,894 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:14:35,894 (beam_search:429) INFO: max output length: 72
2024-10-27 18:14:35,894 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:36,071 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:36,071 (beam_search:476) INFO:  -3.63 * 1.0 =  -3.63 for ctc
2024-10-27 18:14:36,071 (beam_search:479) INFO: total log probability: -3.63
2024-10-27 18:14:36,071 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:14:36,071 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:36,071 (beam_search:483) INFO: best hypo: ▁IT▁WILL▁DO▁BECAUSE▁IT▁ITS▁IT▁HAS▁NO▁IN▁IT▁BUT▁THE▁BLACK

2024-10-27 18:14:36,073 (asr_inference:509) INFO: speech length: 88906
2024-10-27 18:14:39,272 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:14:39,272 (beam_search:429) INFO: max output length: 68
2024-10-27 18:14:39,272 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:39,324 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:39,324 (beam_search:476) INFO:  -2.00 * 1.0 =  -2.00 for ctc
2024-10-27 18:14:39,324 (beam_search:479) INFO: total log probability: -2.00
2024-10-27 18:14:39,324 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 18:14:39,324 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:39,324 (beam_search:483) INFO: best hypo: ▁STICK▁BECAUSE▁IT▁IS

2024-10-27 18:14:39,326 (asr_inference:509) INFO: speech length: 57233
2024-10-27 18:14:41,417 (beam_search:428) INFO: decoder input length: 44
2024-10-27 18:14:41,417 (beam_search:429) INFO: max output length: 44
2024-10-27 18:14:41,418 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:41,457 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:41,457 (beam_search:476) INFO:  -2.02 * 1.0 =  -2.02 for ctc
2024-10-27 18:14:41,457 (beam_search:479) INFO: total log probability: -2.02
2024-10-27 18:14:41,457 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:14:41,457 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:41,457 (beam_search:483) INFO: best hypo: ▁IT▁HAVE▁IRON▁OR▁AND

2024-10-27 18:14:41,460 (asr_inference:509) INFO: speech length: 52851
2024-10-27 18:14:43,386 (beam_search:428) INFO: decoder input length: 40
2024-10-27 18:14:43,386 (beam_search:429) INFO: max output length: 40
2024-10-27 18:14:43,386 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:43,452 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:43,452 (beam_search:476) INFO:  -1.60 * 1.0 =  -1.60 for ctc
2024-10-27 18:14:43,452 (beam_search:479) INFO: total log probability: -1.60
2024-10-27 18:14:43,452 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:14:43,452 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:43,452 (beam_search:483) INFO: best hypo: ▁IT▁WILL▁BECAUSE▁IT'S▁MADE▁OUT▁OF▁STEEL

2024-10-27 18:14:43,454 (asr_inference:509) INFO: speech length: 90554
2024-10-27 18:14:46,896 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:14:46,896 (beam_search:429) INFO: max output length: 70
2024-10-27 18:14:46,896 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:47,061 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:47,062 (beam_search:476) INFO:  -3.84 * 1.0 =  -3.84 for ctc
2024-10-27 18:14:47,062 (beam_search:479) INFO: total log probability: -3.84
2024-10-27 18:14:47,062 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:14:47,062 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:47,062 (beam_search:483) INFO: best hypo: ▁IT▁BECAUSE▁IT▁IT'S▁MADE▁OUT▁OF▁STEEL▁WHICH▁IRON▁IN▁IT

2024-10-27 18:14:47,064 (asr_inference:509) INFO: speech length: 87082
2024-10-27 18:14:50,417 (beam_search:428) INFO: decoder input length: 67
2024-10-27 18:14:50,417 (beam_search:429) INFO: max output length: 67
2024-10-27 18:14:50,417 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:50,510 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:50,510 (beam_search:476) INFO:  -4.33 * 1.0 =  -4.33 for ctc
2024-10-27 18:14:50,510 (beam_search:479) INFO: total log probability: -4.33
2024-10-27 18:14:50,510 (beam_search:480) INFO: normalized log probability: -0.43
2024-10-27 18:14:50,510 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:50,510 (beam_search:483) INFO: best hypo: ▁ITS▁BECAUSE▁IT▁IN▁IT▁AND▁STICKS

2024-10-27 18:14:50,513 (asr_inference:509) INFO: speech length: 165348
2024-10-27 18:14:57,074 (beam_search:428) INFO: decoder input length: 128
2024-10-27 18:14:57,074 (beam_search:429) INFO: max output length: 128
2024-10-27 18:14:57,074 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:57,676 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:57,677 (beam_search:476) INFO:  -4.73 * 1.0 =  -4.73 for ctc
2024-10-27 18:14:57,677 (beam_search:479) INFO: total log probability: -4.73
2024-10-27 18:14:57,677 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:14:57,677 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:57,677 (beam_search:483) INFO: best hypo: ▁IT▁HAS▁TO▁HAVE▁IRON▁IN▁IT▁SO▁THAT▁SO▁IF▁THERE'▁NO▁IRON▁IN▁IT▁IT'T▁HAVE▁BUT▁IT▁HAS▁IRON▁SO▁IT

2024-10-27 18:14:57,679 (asr_inference:509) INFO: speech length: 325445
2024-10-27 18:15:12,149 (beam_search:428) INFO: decoder input length: 253
2024-10-27 18:15:12,149 (beam_search:429) INFO: max output length: 253
2024-10-27 18:15:12,149 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:13,337 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:13,337 (beam_search:476) INFO:  -9.33 * 1.0 =  -9.33 for ctc
2024-10-27 18:15:13,337 (beam_search:479) INFO: total log probability: -9.33
2024-10-27 18:15:13,337 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 18:15:13,337 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:13,337 (beam_search:483) INFO: best hypo: ▁WE▁TALKED▁ABOUT▁HOW▁STICK▁TO▁AND▁BECAUSE▁THEY▁HAVE▁IN▁IT▁MADE▁OUT▁OF▁SO▁SO▁STICK▁TO▁MAGNET▁BECAUSE▁ONLY▁ATTRACT▁IRON▁OR

2024-10-27 18:15:13,340 (asr_inference:509) INFO: speech length: 94213
2024-10-27 18:15:16,701 (beam_search:428) INFO: decoder input length: 73
2024-10-27 18:15:16,701 (beam_search:429) INFO: max output length: 73
2024-10-27 18:15:16,701 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:16,786 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:16,786 (beam_search:476) INFO:  -2.01 * 1.0 =  -2.01 for ctc
2024-10-27 18:15:16,786 (beam_search:479) INFO: total log probability: -2.01
2024-10-27 18:15:16,786 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:15:16,786 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:16,786 (beam_search:483) INFO: best hypo: ▁WE▁BEEN▁LEARNING▁ABOUT▁WHAT▁MAGNETS

2024-10-27 18:15:16,788 (asr_inference:509) INFO: speech length: 61827
2024-10-27 18:15:19,052 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:15:19,052 (beam_search:429) INFO: max output length: 47
2024-10-27 18:15:19,052 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:19,082 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:19,082 (beam_search:476) INFO:  -1.32 * 1.0 =  -1.32 for ctc
2024-10-27 18:15:19,082 (beam_search:479) INFO: total log probability: -1.32
2024-10-27 18:15:19,082 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:15:19,082 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:19,082 (beam_search:483) INFO: best hypo: ▁THE▁AND▁ARE

2024-10-27 18:15:19,085 (asr_inference:509) INFO: speech length: 130461
2024-10-27 18:15:24,012 (beam_search:428) INFO: decoder input length: 101
2024-10-27 18:15:24,013 (beam_search:429) INFO: max output length: 101
2024-10-27 18:15:24,013 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:24,209 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:24,209 (beam_search:476) INFO:  -2.98 * 1.0 =  -2.98 for ctc
2024-10-27 18:15:24,209 (beam_search:479) INFO: total log probability: -2.98
2024-10-27 18:15:24,209 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:15:24,209 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:24,209 (beam_search:483) INFO: best hypo: ▁THEY▁WILL▁EACH▁OTHER▁BECAUSE▁IT▁WILL▁BET▁ORT

2024-10-27 18:15:24,211 (asr_inference:509) INFO: speech length: 80516
2024-10-27 18:15:27,213 (beam_search:428) INFO: decoder input length: 62
2024-10-27 18:15:27,213 (beam_search:429) INFO: max output length: 62
2024-10-27 18:15:27,213 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:27,303 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:27,304 (beam_search:476) INFO:  -2.15 * 1.0 =  -2.15 for ctc
2024-10-27 18:15:27,304 (beam_search:479) INFO: total log probability: -2.15
2024-10-27 18:15:27,304 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:15:27,304 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:27,304 (beam_search:483) INFO: best hypo: ▁THE▁ARE▁EACH▁OTHER▁BECAUSE▁THE▁ARE▁THE

2024-10-27 18:15:27,306 (asr_inference:509) INFO: speech length: 171966
2024-10-27 18:15:33,575 (beam_search:428) INFO: decoder input length: 133
2024-10-27 18:15:33,575 (beam_search:429) INFO: max output length: 133
2024-10-27 18:15:33,575 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:33,886 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:33,886 (beam_search:476) INFO:  -4.93 * 1.0 =  -4.93 for ctc
2024-10-27 18:15:33,886 (beam_search:479) INFO: total log probability: -4.93
2024-10-27 18:15:33,886 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 18:15:33,886 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:33,886 (beam_search:483) INFO: best hypo: ▁THE▁ARE▁TO▁OTHER▁THEY▁ATTRACT▁BUT▁THE▁ARE▁TO▁EACH▁OTHER▁THEY

2024-10-27 18:15:33,889 (asr_inference:509) INFO: speech length: 196248
2024-10-27 18:15:41,527 (beam_search:428) INFO: decoder input length: 152
2024-10-27 18:15:41,527 (beam_search:429) INFO: max output length: 152
2024-10-27 18:15:41,527 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:41,824 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:41,824 (beam_search:476) INFO:  -3.71 * 1.0 =  -3.71 for ctc
2024-10-27 18:15:41,824 (beam_search:479) INFO: total log probability: -3.71
2024-10-27 18:15:41,824 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:15:41,824 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:41,824 (beam_search:483) INFO: best hypo: ▁THE▁AND▁ARE▁THE▁AND▁AND▁AND▁ON▁THE▁MAGNETS▁ARE

2024-10-27 18:15:41,827 (asr_inference:509) INFO: speech length: 117300
2024-10-27 18:15:46,066 (beam_search:428) INFO: decoder input length: 91
2024-10-27 18:15:46,066 (beam_search:429) INFO: max output length: 91
2024-10-27 18:15:46,066 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:46,223 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:46,223 (beam_search:476) INFO:  -2.64 * 1.0 =  -2.64 for ctc
2024-10-27 18:15:46,223 (beam_search:479) INFO: total log probability: -2.64
2024-10-27 18:15:46,223 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:15:46,223 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:46,223 (beam_search:483) INFO: best hypo: ▁ARE▁THE▁THAT▁ARE▁TO▁EACH▁OTHER▁ARE▁THE

2024-10-27 18:15:46,225 (asr_inference:509) INFO: speech length: 200496
2024-10-27 18:15:53,966 (beam_search:428) INFO: decoder input length: 156
2024-10-27 18:15:53,966 (beam_search:429) INFO: max output length: 156
2024-10-27 18:15:53,966 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:54,328 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:54,328 (beam_search:476) INFO:  -2.36 * 1.0 =  -2.36 for ctc
2024-10-27 18:15:54,328 (beam_search:479) INFO: total log probability: -2.36
2024-10-27 18:15:54,328 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:15:54,328 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:54,329 (beam_search:483) INFO: best hypo: ▁WELL▁THEY▁HAVE▁AND▁AND▁THOSE▁ARE▁JUST▁THEY▁EACH▁OTHER▁THEY

2024-10-27 18:15:54,331 (asr_inference:509) INFO: speech length: 161972
2024-10-27 18:16:00,388 (beam_search:428) INFO: decoder input length: 126
2024-10-27 18:16:00,388 (beam_search:429) INFO: max output length: 126
2024-10-27 18:16:00,388 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:00,711 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:00,711 (beam_search:476) INFO:  -4.98 * 1.0 =  -4.98 for ctc
2024-10-27 18:16:00,711 (beam_search:479) INFO: total log probability: -4.98
2024-10-27 18:16:00,711 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:16:00,711 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:00,711 (beam_search:483) INFO: best hypo: ▁ARE▁BECAUSE▁THEY'RE▁THEY▁DON'T▁TO▁THE▁OTHER▁THEY'RE

2024-10-27 18:16:00,713 (asr_inference:509) INFO: speech length: 84844
2024-10-27 18:16:03,743 (beam_search:428) INFO: decoder input length: 65
2024-10-27 18:16:03,743 (beam_search:429) INFO: max output length: 65
2024-10-27 18:16:03,743 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:03,820 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:03,820 (beam_search:476) INFO:  -1.24 * 1.0 =  -1.24 for ctc
2024-10-27 18:16:03,820 (beam_search:479) INFO: total log probability: -1.24
2024-10-27 18:16:03,820 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:16:03,820 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:03,820 (beam_search:483) INFO: best hypo: ▁IT'S▁A▁ON▁THE▁EARTH

2024-10-27 18:16:03,822 (asr_inference:509) INFO: speech length: 81385
2024-10-27 18:16:06,929 (beam_search:428) INFO: decoder input length: 63
2024-10-27 18:16:06,930 (beam_search:429) INFO: max output length: 63
2024-10-27 18:16:06,930 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:07,014 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:07,014 (beam_search:476) INFO:  -2.07 * 1.0 =  -2.07 for ctc
2024-10-27 18:16:07,014 (beam_search:479) INFO: total log probability: -2.07
2024-10-27 18:16:07,014 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:16:07,014 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:07,014 (beam_search:483) INFO: best hypo: ▁USING▁A▁TO▁WHAT▁THEYRE▁GOING

2024-10-27 18:16:07,016 (asr_inference:509) INFO: speech length: 114164
2024-10-27 18:16:11,450 (beam_search:428) INFO: decoder input length: 88
2024-10-27 18:16:11,450 (beam_search:429) INFO: max output length: 88
2024-10-27 18:16:11,450 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:11,597 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:11,598 (beam_search:476) INFO:  -2.09 * 1.0 =  -2.09 for ctc
2024-10-27 18:16:11,598 (beam_search:479) INFO: total log probability: -2.09
2024-10-27 18:16:11,598 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:16:11,598 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:11,598 (beam_search:483) INFO: best hypo: ▁IT▁THE▁FLOW▁OF▁THE▁THAT▁IS▁INSIDE▁THE

2024-10-27 18:16:11,600 (asr_inference:509) INFO: speech length: 92264
2024-10-27 18:16:14,876 (beam_search:428) INFO: decoder input length: 71
2024-10-27 18:16:14,876 (beam_search:429) INFO: max output length: 71
2024-10-27 18:16:14,876 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:14,968 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:14,968 (beam_search:476) INFO:  -2.23 * 1.0 =  -2.23 for ctc
2024-10-27 18:16:14,968 (beam_search:479) INFO: total log probability: -2.23
2024-10-27 18:16:14,968 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:16:14,968 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:14,968 (beam_search:483) INFO: best hypo: ▁WE▁HAVE▁BEEN▁LEARNING▁ABOUT▁A▁OF

2024-10-27 18:16:14,970 (asr_inference:509) INFO: speech length: 72190
2024-10-27 18:16:17,581 (beam_search:428) INFO: decoder input length: 55
2024-10-27 18:16:17,581 (beam_search:429) INFO: max output length: 55
2024-10-27 18:16:17,581 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:17,615 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:17,615 (beam_search:476) INFO:  -0.82 * 1.0 =  -0.82 for ctc
2024-10-27 18:16:17,615 (beam_search:479) INFO: total log probability: -0.82
2024-10-27 18:16:17,615 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:16:17,615 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:17,615 (beam_search:483) INFO: best hypo: ▁THEY▁HAVE▁AND

2024-10-27 18:16:17,617 (asr_inference:509) INFO: speech length: 262109
2024-10-27 18:16:28,577 (beam_search:428) INFO: decoder input length: 204
2024-10-27 18:16:28,577 (beam_search:429) INFO: max output length: 204
2024-10-27 18:16:28,577 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:29,261 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:29,261 (beam_search:476) INFO:  -7.56 * 1.0 =  -7.56 for ctc
2024-10-27 18:16:29,261 (beam_search:479) INFO: total log probability: -7.56
2024-10-27 18:16:29,261 (beam_search:480) INFO: normalized log probability: -0.38
2024-10-27 18:16:29,261 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:29,261 (beam_search:483) INFO: best hypo: ▁MAGNETS▁HAVE▁A▁AND▁A▁POLE▁WHEN▁OTHER▁THEY▁ATTRACT▁BUT▁WHEN▁TWO▁OF▁THE▁SAME▁OTHER▁THEY

2024-10-27 18:16:29,264 (asr_inference:509) INFO: speech length: 93670
2024-10-27 18:16:32,668 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:16:32,668 (beam_search:429) INFO: max output length: 72
2024-10-27 18:16:32,668 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:32,719 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:32,720 (beam_search:476) INFO:  -0.86 * 1.0 =  -0.86 for ctc
2024-10-27 18:16:32,720 (beam_search:479) INFO: total log probability: -0.86
2024-10-27 18:16:32,720 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:16:32,720 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:32,720 (beam_search:483) INFO: best hypo: ▁THE▁IS▁THE

2024-10-27 18:16:32,722 (asr_inference:509) INFO: speech length: 126746
2024-10-27 18:16:37,740 (beam_search:428) INFO: decoder input length: 98
2024-10-27 18:16:37,740 (beam_search:429) INFO: max output length: 98
2024-10-27 18:16:37,740 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:37,837 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:37,837 (beam_search:476) INFO:  -3.99 * 1.0 =  -3.99 for ctc
2024-10-27 18:16:37,837 (beam_search:479) INFO: total log probability: -3.99
2024-10-27 18:16:37,837 (beam_search:480) INFO: normalized log probability: -0.57
2024-10-27 18:16:37,837 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:37,837 (beam_search:483) INFO: best hypo: ▁IS▁TO▁BOTH▁TOGETHER▁ATTRACT

2024-10-27 18:16:37,840 (asr_inference:509) INFO: speech length: 173626
2024-10-27 18:16:44,261 (beam_search:428) INFO: decoder input length: 135
2024-10-27 18:16:44,261 (beam_search:429) INFO: max output length: 135
2024-10-27 18:16:44,261 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:44,474 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:44,475 (beam_search:476) INFO:  -3.65 * 1.0 =  -3.65 for ctc
2024-10-27 18:16:44,475 (beam_search:479) INFO: total log probability: -3.65
2024-10-27 18:16:44,475 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 18:16:44,475 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:44,475 (beam_search:483) INFO: best hypo: ▁THE▁OF▁THE▁IS▁LIKE▁THAT▁MAGNETS▁ARE▁THROUGH

2024-10-27 18:16:44,477 (asr_inference:509) INFO: speech length: 273295
2024-10-27 18:16:56,296 (beam_search:428) INFO: decoder input length: 213
2024-10-27 18:16:56,297 (beam_search:429) INFO: max output length: 213
2024-10-27 18:16:56,297 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:56,891 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:56,891 (beam_search:476) INFO:  -5.94 * 1.0 =  -5.94 for ctc
2024-10-27 18:16:56,891 (beam_search:479) INFO: total log probability: -5.94
2024-10-27 18:16:56,891 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:16:56,891 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:56,891 (beam_search:483) INFO: best hypo: ▁WELL▁IT▁CAN▁GO▁THROUGH▁LIKE▁AS▁AS▁AS▁THE▁MAGNET▁OR▁THE▁NOT▁TOO

2024-10-27 18:16:56,894 (asr_inference:509) INFO: speech length: 249027
2024-10-27 18:17:07,251 (beam_search:428) INFO: decoder input length: 194
2024-10-27 18:17:07,251 (beam_search:429) INFO: max output length: 194
2024-10-27 18:17:07,251 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:07,888 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:07,888 (beam_search:476) INFO: -10.72 * 1.0 = -10.72 for ctc
2024-10-27 18:17:07,888 (beam_search:479) INFO: total log probability: -10.72
2024-10-27 18:17:07,888 (beam_search:480) INFO: normalized log probability: -0.56
2024-10-27 18:17:07,888 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:07,889 (beam_search:483) INFO: best hypo: ▁ITS▁IF▁THE▁NOT▁ABLE▁TO▁SOMETHING▁ELSE▁THATS▁BUT▁ITS▁WILL▁BE▁TO

2024-10-27 18:17:07,891 (asr_inference:509) INFO: speech length: 44753
2024-10-27 18:17:09,555 (beam_search:428) INFO: decoder input length: 34
2024-10-27 18:17:09,556 (beam_search:429) INFO: max output length: 34
2024-10-27 18:17:09,556 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:09,586 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:09,586 (beam_search:476) INFO:  -0.31 * 1.0 =  -0.31 for ctc
2024-10-27 18:17:09,587 (beam_search:479) INFO: total log probability: -0.31
2024-10-27 18:17:09,587 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 18:17:09,587 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:09,587 (beam_search:483) INFO: best hypo: ▁BECAUSE▁IT'S▁TOO

2024-10-27 18:17:09,590 (asr_inference:509) INFO: speech length: 220941
2024-10-27 18:17:18,664 (beam_search:428) INFO: decoder input length: 172
2024-10-27 18:17:18,664 (beam_search:429) INFO: max output length: 172
2024-10-27 18:17:18,664 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:19,216 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:19,216 (beam_search:476) INFO:  -5.72 * 1.0 =  -5.72 for ctc
2024-10-27 18:17:19,216 (beam_search:479) INFO: total log probability: -5.72
2024-10-27 18:17:19,216 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:17:19,216 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:19,216 (beam_search:483) INFO: best hypo: ▁A▁IS▁A▁NAIL▁WHICH▁IS▁THE▁STEEL▁NAIL▁SO▁BECAUSE▁ITS▁IT▁CAN▁PICK▁UP▁A

2024-10-27 18:17:19,218 (asr_inference:509) INFO: speech length: 102298
2024-10-27 18:17:22,813 (beam_search:428) INFO: decoder input length: 79
2024-10-27 18:17:22,814 (beam_search:429) INFO: max output length: 79
2024-10-27 18:17:22,814 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:22,867 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:22,867 (beam_search:476) INFO:  -2.10 * 1.0 =  -2.10 for ctc
2024-10-27 18:17:22,867 (beam_search:479) INFO: total log probability: -2.10
2024-10-27 18:17:22,867 (beam_search:480) INFO: normalized log probability: -0.42
2024-10-27 18:17:22,867 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:22,867 (beam_search:483) INFO: best hypo: ▁THE▁THROUGH▁NAIL

2024-10-27 18:17:22,870 (asr_inference:509) INFO: speech length: 122610
2024-10-27 18:17:27,419 (beam_search:428) INFO: decoder input length: 95
2024-10-27 18:17:27,419 (beam_search:429) INFO: max output length: 95
2024-10-27 18:17:27,419 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:27,522 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:27,522 (beam_search:476) INFO:  -3.33 * 1.0 =  -3.33 for ctc
2024-10-27 18:17:27,522 (beam_search:479) INFO: total log probability: -3.33
2024-10-27 18:17:27,522 (beam_search:480) INFO: normalized log probability: -0.42
2024-10-27 18:17:27,522 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:27,522 (beam_search:483) INFO: best hypo: ▁IS▁THE▁THROUGH▁THE▁NAIL▁WHICH

2024-10-27 18:17:27,525 (asr_inference:509) INFO: speech length: 171267
2024-10-27 18:17:34,510 (beam_search:428) INFO: decoder input length: 133
2024-10-27 18:17:34,510 (beam_search:429) INFO: max output length: 133
2024-10-27 18:17:34,510 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:34,785 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:34,785 (beam_search:476) INFO:  -3.65 * 1.0 =  -3.65 for ctc
2024-10-27 18:17:34,785 (beam_search:479) INFO: total log probability: -3.65
2024-10-27 18:17:34,785 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 18:17:34,786 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:34,786 (beam_search:483) INFO: best hypo: ▁MAGNET▁IS▁MAKING▁THE▁TEMPORARY▁MAGNET▁BYING▁THE▁THROUGH

2024-10-27 18:17:34,788 (asr_inference:509) INFO: speech length: 196199
2024-10-27 18:17:42,103 (beam_search:428) INFO: decoder input length: 152
2024-10-27 18:17:42,103 (beam_search:429) INFO: max output length: 152
2024-10-27 18:17:42,103 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:42,518 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:42,518 (beam_search:476) INFO:  -3.95 * 1.0 =  -3.95 for ctc
2024-10-27 18:17:42,518 (beam_search:479) INFO: total log probability: -3.95
2024-10-27 18:17:42,518 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:17:42,518 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:42,518 (beam_search:483) INFO: best hypo: ▁THE▁TO▁BE▁A▁MAGNET▁WHICH▁IS▁WHICH▁IT▁A▁TEMPORARY▁MAGNET▁WHICH▁UP▁LIKE

2024-10-27 18:17:42,520 (asr_inference:509) INFO: speech length: 54425
2024-10-27 18:17:44,555 (beam_search:428) INFO: decoder input length: 42
2024-10-27 18:17:44,555 (beam_search:429) INFO: max output length: 42
2024-10-27 18:17:44,555 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:44,609 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:44,609 (beam_search:476) INFO:  -0.67 * 1.0 =  -0.67 for ctc
2024-10-27 18:17:44,609 (beam_search:479) INFO: total log probability: -0.67
2024-10-27 18:17:44,609 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:17:44,609 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:44,609 (beam_search:483) INFO: best hypo: ▁IT▁HAS▁TO▁BE▁TOUCHING▁THE▁NAIL

2024-10-27 18:17:44,612 (asr_inference:509) INFO: speech length: 92724
2024-10-27 18:17:47,925 (beam_search:428) INFO: decoder input length: 71
2024-10-27 18:17:47,925 (beam_search:429) INFO: max output length: 71
2024-10-27 18:17:47,925 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:47,993 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:47,993 (beam_search:476) INFO:  -2.80 * 1.0 =  -2.80 for ctc
2024-10-27 18:17:47,993 (beam_search:479) INFO: total log probability: -2.80
2024-10-27 18:17:47,993 (beam_search:480) INFO: normalized log probability: -0.40
2024-10-27 18:17:47,993 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:47,993 (beam_search:483) INFO: best hypo: ▁IS▁TOUCHING▁NAIL▁THEN▁THE

2024-10-27 18:17:47,995 (asr_inference:509) INFO: speech length: 115464
2024-10-27 18:17:52,328 (beam_search:428) INFO: decoder input length: 89
2024-10-27 18:17:52,328 (beam_search:429) INFO: max output length: 89
2024-10-27 18:17:52,328 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:52,399 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:52,399 (beam_search:476) INFO:  -1.68 * 1.0 =  -1.68 for ctc
2024-10-27 18:17:52,399 (beam_search:479) INFO: total log probability: -1.68
2024-10-27 18:17:52,399 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:17:52,399 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:52,399 (beam_search:483) INFO: best hypo: ▁THE▁IS▁TO▁UP

2024-10-27 18:17:52,402 (asr_inference:509) INFO: speech length: 163148
2024-10-27 18:17:59,035 (beam_search:428) INFO: decoder input length: 126
2024-10-27 18:17:59,035 (beam_search:429) INFO: max output length: 126
2024-10-27 18:17:59,035 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:59,445 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:59,445 (beam_search:476) INFO:  -9.06 * 1.0 =  -9.06 for ctc
2024-10-27 18:17:59,445 (beam_search:479) INFO: total log probability: -9.06
2024-10-27 18:17:59,445 (beam_search:480) INFO: normalized log probability: -0.45
2024-10-27 18:17:59,445 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:59,445 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁IS▁THE▁MAGNET▁THE▁TEMPORARY▁IT▁WONT▁WORK▁BECAUSE▁THERE▁NO▁MAGNETIC▁IN▁THE▁TEMPORARY

2024-10-27 18:17:59,449 (asr_inference:509) INFO: speech length: 72508
2024-10-27 18:18:02,325 (beam_search:428) INFO: decoder input length: 56
2024-10-27 18:18:02,325 (beam_search:429) INFO: max output length: 56
2024-10-27 18:18:02,325 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:02,361 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:02,361 (beam_search:476) INFO:  -1.40 * 1.0 =  -1.40 for ctc
2024-10-27 18:18:02,361 (beam_search:479) INFO: total log probability: -1.40
2024-10-27 18:18:02,361 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:18:02,361 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:02,361 (beam_search:483) INFO: best hypo: ▁THE▁IS▁EVERYTHING

2024-10-27 18:18:02,363 (asr_inference:509) INFO: speech length: 145077
2024-10-27 18:18:08,015 (beam_search:428) INFO: decoder input length: 112
2024-10-27 18:18:08,015 (beam_search:429) INFO: max output length: 112
2024-10-27 18:18:08,015 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:08,172 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:08,172 (beam_search:476) INFO:  -4.19 * 1.0 =  -4.19 for ctc
2024-10-27 18:18:08,172 (beam_search:479) INFO: total log probability: -4.19
2024-10-27 18:18:08,172 (beam_search:480) INFO: normalized log probability: -0.42
2024-10-27 18:18:08,172 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:08,172 (beam_search:483) INFO: best hypo: ▁WELL▁BY▁THE▁MAGNET▁IT▁EVERYTHING▁THAT▁MAGNET

2024-10-27 18:18:08,175 (asr_inference:509) INFO: speech length: 96491
2024-10-27 18:18:11,597 (beam_search:428) INFO: decoder input length: 74
2024-10-27 18:18:11,597 (beam_search:429) INFO: max output length: 74
2024-10-27 18:18:11,597 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:11,723 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:11,723 (beam_search:476) INFO:  -3.62 * 1.0 =  -3.62 for ctc
2024-10-27 18:18:11,723 (beam_search:479) INFO: total log probability: -3.62
2024-10-27 18:18:11,723 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 18:18:11,723 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:11,723 (beam_search:483) INFO: best hypo: ▁BECAUSE▁IS▁NO▁MAGNETIC▁FORCE▁IT▁WILL▁NOT▁WORK

2024-10-27 18:18:11,725 (asr_inference:509) INFO: speech length: 18548
2024-10-27 18:18:12,582 (beam_search:428) INFO: decoder input length: 13
2024-10-27 18:18:12,582 (beam_search:429) INFO: max output length: 13
2024-10-27 18:18:12,582 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:12,593 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:12,593 (beam_search:476) INFO:  -0.39 * 1.0 =  -0.39 for ctc
2024-10-27 18:18:12,593 (beam_search:479) INFO: total log probability: -0.39
2024-10-27 18:18:12,593 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:18:12,593 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:12,593 (beam_search:483) INFO: best hypo: ▁IT'S

2024-10-27 18:18:12,595 (asr_inference:509) INFO: speech length: 68536
2024-10-27 18:18:15,161 (beam_search:428) INFO: decoder input length: 53
2024-10-27 18:18:15,161 (beam_search:429) INFO: max output length: 53
2024-10-27 18:18:15,161 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:15,172 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:15,172 (beam_search:476) INFO:  -0.37 * 1.0 =  -0.37 for ctc
2024-10-27 18:18:15,172 (beam_search:479) INFO: total log probability: -0.37
2024-10-27 18:18:15,172 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:18:15,172 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:15,172 (beam_search:483) INFO: best hypo: 

2024-10-27 18:18:15,174 (asr_inference:509) INFO: speech length: 88904
2024-10-27 18:18:18,480 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:18:18,480 (beam_search:429) INFO: max output length: 68
2024-10-27 18:18:18,480 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:18,575 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:18,575 (beam_search:476) INFO:  -2.47 * 1.0 =  -2.47 for ctc
2024-10-27 18:18:18,575 (beam_search:479) INFO: total log probability: -2.47
2024-10-27 18:18:18,575 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:18:18,575 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:18,575 (beam_search:483) INFO: best hypo: ▁THE▁IS▁THE▁MAGNET▁TO▁NAIL▁TO▁THE

2024-10-27 18:18:18,577 (asr_inference:509) INFO: speech length: 116162
2024-10-27 18:18:22,921 (beam_search:428) INFO: decoder input length: 90
2024-10-27 18:18:22,921 (beam_search:429) INFO: max output length: 90
2024-10-27 18:18:22,921 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:23,037 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:23,037 (beam_search:476) INFO:  -2.00 * 1.0 =  -2.00 for ctc
2024-10-27 18:18:23,037 (beam_search:479) INFO: total log probability: -2.00
2024-10-27 18:18:23,037 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:18:23,037 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:23,037 (beam_search:483) INFO: best hypo: ▁WEVE▁BEEN▁ABOUT▁THE▁OF▁WITH

2024-10-27 18:18:23,040 (asr_inference:509) INFO: speech length: 81147
2024-10-27 18:18:26,087 (beam_search:428) INFO: decoder input length: 62
2024-10-27 18:18:26,087 (beam_search:429) INFO: max output length: 62
2024-10-27 18:18:26,087 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:26,155 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:26,156 (beam_search:476) INFO:  -1.59 * 1.0 =  -1.59 for ctc
2024-10-27 18:18:26,156 (beam_search:479) INFO: total log probability: -1.59
2024-10-27 18:18:26,156 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:18:26,156 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:26,156 (beam_search:483) INFO: best hypo: ▁THE▁MAGNET▁IS▁THE▁THROUGH▁THE

2024-10-27 18:18:26,158 (asr_inference:509) INFO: speech length: 85452
2024-10-27 18:18:29,218 (beam_search:428) INFO: decoder input length: 66
2024-10-27 18:18:29,218 (beam_search:429) INFO: max output length: 66
2024-10-27 18:18:29,218 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:29,302 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:29,302 (beam_search:476) INFO:  -2.26 * 1.0 =  -2.26 for ctc
2024-10-27 18:18:29,302 (beam_search:479) INFO: total log probability: -2.26
2024-10-27 18:18:29,302 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:18:29,302 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:29,302 (beam_search:483) INFO: best hypo: ▁IS▁NOT▁ENOUGH▁BECAUSE▁IT▁IS▁TOO

2024-10-27 18:18:29,304 (asr_inference:509) INFO: speech length: 124706
2024-10-27 18:18:33,908 (beam_search:428) INFO: decoder input length: 96
2024-10-27 18:18:33,908 (beam_search:429) INFO: max output length: 96
2024-10-27 18:18:33,908 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:34,106 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:34,106 (beam_search:476) INFO:  -2.85 * 1.0 =  -2.85 for ctc
2024-10-27 18:18:34,106 (beam_search:479) INFO: total log probability: -2.85
2024-10-27 18:18:34,106 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:18:34,106 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:34,106 (beam_search:483) INFO: best hypo: ▁SO▁ARE▁BEING▁INTO▁A▁WHICH▁IS▁BREAKING▁THE▁OF▁WITH

2024-10-27 18:18:34,108 (asr_inference:509) INFO: speech length: 164185
2024-10-27 18:18:40,572 (beam_search:428) INFO: decoder input length: 127
2024-10-27 18:18:40,573 (beam_search:429) INFO: max output length: 127
2024-10-27 18:18:40,573 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:40,907 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:40,907 (beam_search:476) INFO:  -6.21 * 1.0 =  -6.21 for ctc
2024-10-27 18:18:40,907 (beam_search:479) INFO: total log probability: -6.21
2024-10-27 18:18:40,907 (beam_search:480) INFO: normalized log probability: -0.37
2024-10-27 18:18:40,907 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:40,907 (beam_search:483) INFO: best hypo: ▁WHEN▁THERE▁IS▁TOO▁ON▁THE▁OTHER▁SIDE▁OF▁THE▁THE▁IS▁TO▁TO▁THE

2024-10-27 18:18:40,909 (asr_inference:509) INFO: speech length: 289709
2024-10-27 18:18:53,563 (beam_search:428) INFO: decoder input length: 225
2024-10-27 18:18:53,564 (beam_search:429) INFO: max output length: 225
2024-10-27 18:18:53,564 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:54,335 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:54,336 (beam_search:476) INFO:  -7.14 * 1.0 =  -7.14 for ctc
2024-10-27 18:18:54,336 (beam_search:479) INFO: total log probability: -7.14
2024-10-27 18:18:54,336 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 18:18:54,336 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:54,336 (beam_search:483) INFO: best hypo: ▁YOU▁DROP▁AND▁THEN▁IS▁DOWN▁THE▁WHICH▁IS▁MAKING▁BECAUSE▁THE▁IS▁THAN▁THE▁IT▁THE▁EACH▁OTHER

2024-10-27 18:18:54,338 (asr_inference:509) INFO: speech length: 163300
2024-10-27 18:19:01,061 (beam_search:428) INFO: decoder input length: 127
2024-10-27 18:19:01,062 (beam_search:429) INFO: max output length: 127
2024-10-27 18:19:01,062 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:01,270 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:01,270 (beam_search:476) INFO:  -3.93 * 1.0 =  -3.93 for ctc
2024-10-27 18:19:01,270 (beam_search:479) INFO: total log probability: -3.93
2024-10-27 18:19:01,270 (beam_search:480) INFO: normalized log probability: -0.36
2024-10-27 18:19:01,270 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:01,270 (beam_search:483) INFO: best hypo: ▁YOU▁SEPARATE▁THE▁IF▁THERE▁THEN▁THET▁WORK

2024-10-27 18:19:01,272 (asr_inference:509) INFO: speech length: 227702
2024-10-27 18:19:10,521 (beam_search:428) INFO: decoder input length: 177
2024-10-27 18:19:10,521 (beam_search:429) INFO: max output length: 177
2024-10-27 18:19:10,521 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:10,719 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:10,719 (beam_search:476) INFO:  -3.10 * 1.0 =  -3.10 for ctc
2024-10-27 18:19:10,719 (beam_search:479) INFO: total log probability: -3.10
2024-10-27 18:19:10,719 (beam_search:480) INFO: normalized log probability: -0.39
2024-10-27 18:19:10,719 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:10,719 (beam_search:483) INFO: best hypo: ▁ARE▁AND▁WASHERS▁YOU▁THE▁AWAY

2024-10-27 18:19:10,723 (asr_inference:509) INFO: speech length: 130565
2024-10-27 18:19:15,863 (beam_search:428) INFO: decoder input length: 101
2024-10-27 18:19:15,864 (beam_search:429) INFO: max output length: 101
2024-10-27 18:19:15,864 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:16,058 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:16,058 (beam_search:476) INFO:  -3.60 * 1.0 =  -3.60 for ctc
2024-10-27 18:19:16,058 (beam_search:479) INFO: total log probability: -3.60
2024-10-27 18:19:16,058 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:19:16,058 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:16,058 (beam_search:483) INFO: best hypo: ▁WHEN▁YOU▁ADD▁IT▁WITH▁THE▁MAGNETIC▁IT▁BECAUSE▁ITS

2024-10-27 18:19:16,060 (asr_inference:509) INFO: speech length: 277108
2024-10-27 18:19:27,985 (beam_search:428) INFO: decoder input length: 215
2024-10-27 18:19:27,985 (beam_search:429) INFO: max output length: 215
2024-10-27 18:19:27,985 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:28,223 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:28,223 (beam_search:476) INFO:  -2.82 * 1.0 =  -2.82 for ctc
2024-10-27 18:19:28,223 (beam_search:479) INFO: total log probability: -2.82
2024-10-27 18:19:28,223 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:19:28,223 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:28,223 (beam_search:483) INFO: best hypo: ▁THE▁IN▁IS▁THE▁IS▁THE

2024-10-27 18:19:28,226 (asr_inference:509) INFO: speech length: 87375
2024-10-27 18:19:31,378 (beam_search:428) INFO: decoder input length: 67
2024-10-27 18:19:31,378 (beam_search:429) INFO: max output length: 67
2024-10-27 18:19:31,378 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:31,441 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:31,441 (beam_search:476) INFO:  -2.31 * 1.0 =  -2.31 for ctc
2024-10-27 18:19:31,441 (beam_search:479) INFO: total log probability: -2.31
2024-10-27 18:19:31,441 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 18:19:31,441 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:31,441 (beam_search:483) INFO: best hypo: ▁YOU▁ADD▁THENS▁THE

2024-10-27 18:19:31,443 (asr_inference:509) INFO: speech length: 157855
2024-10-27 18:19:37,321 (beam_search:428) INFO: decoder input length: 122
2024-10-27 18:19:37,321 (beam_search:429) INFO: max output length: 122
2024-10-27 18:19:37,321 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:37,439 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:37,440 (beam_search:476) INFO:  -2.48 * 1.0 =  -2.48 for ctc
2024-10-27 18:19:37,440 (beam_search:479) INFO: total log probability: -2.48
2024-10-27 18:19:37,440 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:19:37,440 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:37,440 (beam_search:483) INFO: best hypo: ▁THE▁THE▁BUT▁THE▁THE

2024-10-27 18:19:37,442 (asr_inference:509) INFO: speech length: 35754
2024-10-27 18:19:38,857 (beam_search:428) INFO: decoder input length: 27
2024-10-27 18:19:38,857 (beam_search:429) INFO: max output length: 27
2024-10-27 18:19:38,857 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:38,871 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:38,872 (beam_search:476) INFO:  -0.69 * 1.0 =  -0.69 for ctc
2024-10-27 18:19:38,872 (beam_search:479) INFO: total log probability: -0.69
2024-10-27 18:19:38,872 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:19:38,872 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:38,872 (beam_search:483) INFO: best hypo: ▁IT▁THE

2024-10-27 18:19:38,873 (asr_inference:509) INFO: speech length: 145557
2024-10-27 18:19:44,422 (beam_search:428) INFO: decoder input length: 113
2024-10-27 18:19:44,422 (beam_search:429) INFO: max output length: 113
2024-10-27 18:19:44,422 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:44,717 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:44,717 (beam_search:476) INFO:  -5.49 * 1.0 =  -5.49 for ctc
2024-10-27 18:19:44,717 (beam_search:479) INFO: total log probability: -5.49
2024-10-27 18:19:44,717 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 18:19:44,717 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:44,717 (beam_search:483) INFO: best hypo: ▁THE▁FORCE▁MAGNETISM▁IS▁BUT▁AS▁YOU▁PUT▁IN▁MORE▁OF▁ITS▁THE▁BECAUSE

2024-10-27 18:19:44,721 (asr_inference:509) INFO: speech length: 59204
2024-10-27 18:19:46,861 (beam_search:428) INFO: decoder input length: 45
2024-10-27 18:19:46,861 (beam_search:429) INFO: max output length: 45
2024-10-27 18:19:46,861 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:46,903 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:46,903 (beam_search:476) INFO:  -1.24 * 1.0 =  -1.24 for ctc
2024-10-27 18:19:46,903 (beam_search:479) INFO: total log probability: -1.24
2024-10-27 18:19:46,903 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:19:46,903 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:46,903 (beam_search:483) INFO: best hypo: ▁WELL▁TO▁THE▁YOU▁ADD

2024-10-27 18:19:46,906 (asr_inference:509) INFO: speech length: 91289
2024-10-27 18:19:50,226 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:19:50,226 (beam_search:429) INFO: max output length: 70
2024-10-27 18:19:50,226 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:50,293 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:50,294 (beam_search:476) INFO:  -2.81 * 1.0 =  -2.81 for ctc
2024-10-27 18:19:50,294 (beam_search:479) INFO: total log probability: -2.81
2024-10-27 18:19:50,294 (beam_search:480) INFO: normalized log probability: -0.40
2024-10-27 18:19:50,294 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:50,294 (beam_search:483) INFO: best hypo: ▁IT▁TAKE▁TO▁THE▁OF

2024-10-27 18:19:50,296 (asr_inference:509) INFO: speech length: 60102
2024-10-27 18:19:52,610 (beam_search:428) INFO: decoder input length: 46
2024-10-27 18:19:52,611 (beam_search:429) INFO: max output length: 46
2024-10-27 18:19:52,611 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:52,658 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:52,659 (beam_search:476) INFO:  -1.93 * 1.0 =  -1.93 for ctc
2024-10-27 18:19:52,659 (beam_search:479) INFO: total log probability: -1.93
2024-10-27 18:19:52,659 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:19:52,659 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:52,659 (beam_search:483) INFO: best hypo: ▁WE'VE▁HOW▁TO▁MAKE

2024-10-27 18:19:52,661 (asr_inference:509) INFO: speech length: 226549
2024-10-27 18:20:01,870 (beam_search:428) INFO: decoder input length: 176
2024-10-27 18:20:01,871 (beam_search:429) INFO: max output length: 176
2024-10-27 18:20:01,871 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:02,446 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:02,446 (beam_search:476) INFO:  -6.87 * 1.0 =  -6.87 for ctc
2024-10-27 18:20:02,446 (beam_search:479) INFO: total log probability: -6.87
2024-10-27 18:20:02,446 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 18:20:02,446 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:02,446 (beam_search:483) INFO: best hypo: ▁ITS▁A▁IS▁A▁STEEL▁NAIL▁RIVET▁KIND▁OF▁LIKE▁A▁THAT▁HAS▁WIRED▁AROUND▁IT

2024-10-27 18:20:02,448 (asr_inference:509) INFO: speech length: 62868
2024-10-27 18:20:04,779 (beam_search:428) INFO: decoder input length: 48
2024-10-27 18:20:04,780 (beam_search:429) INFO: max output length: 48
2024-10-27 18:20:04,780 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:04,828 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:04,829 (beam_search:476) INFO:  -1.68 * 1.0 =  -1.68 for ctc
2024-10-27 18:20:04,829 (beam_search:479) INFO: total log probability: -1.68
2024-10-27 18:20:04,829 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:20:04,829 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:04,829 (beam_search:483) INFO: best hypo: ▁YOU▁THE▁WITH▁WIRE▁IT▁THE

2024-10-27 18:20:04,831 (asr_inference:509) INFO: speech length: 189690
2024-10-27 18:20:11,978 (beam_search:428) INFO: decoder input length: 147
2024-10-27 18:20:11,978 (beam_search:429) INFO: max output length: 147
2024-10-27 18:20:11,978 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:12,372 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:12,372 (beam_search:476) INFO:  -2.37 * 1.0 =  -2.37 for ctc
2024-10-27 18:20:12,372 (beam_search:479) INFO: total log probability: -2.37
2024-10-27 18:20:12,373 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:20:12,373 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:12,373 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁GOES▁THROUGH▁THOSES▁AND▁THATS▁MAGNETIC▁ENERGY▁WHICH▁CAN▁PICK▁UP

2024-10-27 18:20:12,375 (asr_inference:509) INFO: speech length: 260743
2024-10-27 18:20:23,634 (beam_search:428) INFO: decoder input length: 203
2024-10-27 18:20:23,634 (beam_search:429) INFO: max output length: 203
2024-10-27 18:20:23,634 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:24,323 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:24,323 (beam_search:476) INFO:  -4.08 * 1.0 =  -4.08 for ctc
2024-10-27 18:20:24,323 (beam_search:479) INFO: total log probability: -4.08
2024-10-27 18:20:24,323 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:20:24,323 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:24,323 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁THROUGH▁THOSES▁GOES▁REALLY▁FAST▁SO▁THAT▁CREATES▁A▁OF▁ENERGY▁WHICHS▁THE

2024-10-27 18:20:24,326 (asr_inference:509) INFO: speech length: 168778
2024-10-27 18:20:30,459 (beam_search:428) INFO: decoder input length: 131
2024-10-27 18:20:30,459 (beam_search:429) INFO: max output length: 131
2024-10-27 18:20:30,459 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:30,844 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:30,845 (beam_search:476) INFO:  -3.16 * 1.0 =  -3.16 for ctc
2024-10-27 18:20:30,845 (beam_search:479) INFO: total log probability: -3.16
2024-10-27 18:20:30,845 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:20:30,845 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:30,845 (beam_search:483) INFO: best hypo: ▁SO▁IT'S▁A▁CIRCUIT▁AND▁IT'S▁THE▁IS▁WITH▁A▁OF▁WIRE

2024-10-27 18:20:30,848 (asr_inference:509) INFO: speech length: 164962
2024-10-27 18:20:37,715 (beam_search:428) INFO: decoder input length: 128
2024-10-27 18:20:37,715 (beam_search:429) INFO: max output length: 128
2024-10-27 18:20:37,715 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:38,225 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:38,225 (beam_search:476) INFO:  -6.56 * 1.0 =  -6.56 for ctc
2024-10-27 18:20:38,225 (beam_search:479) INFO: total log probability: -6.56
2024-10-27 18:20:38,225 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:20:38,225 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:38,225 (beam_search:483) INFO: best hypo: ▁WELL▁RIGHT▁NOW▁THERE▁IS▁NO▁ENERGY▁SO▁ITS▁JUST▁A▁YOU▁CANT▁ANYTHING▁UP▁ITS▁NOT▁A▁MAGNET

2024-10-27 18:20:38,227 (asr_inference:509) INFO: speech length: 32432
2024-10-27 18:20:39,539 (beam_search:428) INFO: decoder input length: 24
2024-10-27 18:20:39,540 (beam_search:429) INFO: max output length: 24
2024-10-27 18:20:39,540 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:39,553 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:39,553 (beam_search:476) INFO:  -0.09 * 1.0 =  -0.09 for ctc
2024-10-27 18:20:39,553 (beam_search:479) INFO: total log probability: -0.09
2024-10-27 18:20:39,553 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:20:39,553 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:39,554 (beam_search:483) INFO: best hypo: ▁IT▁THE

2024-10-27 18:20:39,556 (asr_inference:509) INFO: speech length: 262376
2024-10-27 18:20:50,423 (beam_search:428) INFO: decoder input length: 204
2024-10-27 18:20:50,423 (beam_search:429) INFO: max output length: 204
2024-10-27 18:20:50,423 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:51,179 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:51,179 (beam_search:476) INFO:  -3.51 * 1.0 =  -3.51 for ctc
2024-10-27 18:20:51,179 (beam_search:479) INFO: total log probability: -3.51
2024-10-27 18:20:51,179 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:20:51,179 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:51,179 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁IS▁FLOWING▁FROM▁THE▁NEGATIVE▁GOING▁THROUGH▁THE▁SWITCH▁THROUGH▁THE▁AND▁THAT▁ENERGY▁ISING▁REALLY▁WHICHS

2024-10-27 18:20:51,181 (asr_inference:509) INFO: speech length: 72659
2024-10-27 18:20:53,917 (beam_search:428) INFO: decoder input length: 56
2024-10-27 18:20:53,918 (beam_search:429) INFO: max output length: 56
2024-10-27 18:20:53,918 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:53,954 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:53,954 (beam_search:476) INFO:  -1.24 * 1.0 =  -1.24 for ctc
2024-10-27 18:20:53,954 (beam_search:479) INFO: total log probability: -1.24
2024-10-27 18:20:53,954 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:20:53,954 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:53,954 (beam_search:483) INFO: best hypo: ▁ARE▁INTO▁THE

2024-10-27 18:20:53,956 (asr_inference:509) INFO: speech length: 313793
2024-10-27 18:21:07,602 (beam_search:428) INFO: decoder input length: 244
2024-10-27 18:21:07,602 (beam_search:429) INFO: max output length: 244
2024-10-27 18:21:07,602 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:08,712 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:08,713 (beam_search:476) INFO:  -5.95 * 1.0 =  -5.95 for ctc
2024-10-27 18:21:08,713 (beam_search:479) INFO: total log probability: -5.95
2024-10-27 18:21:08,713 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:21:08,713 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:08,713 (beam_search:483) INFO: best hypo: ▁THE▁IS▁IMPORTANT▁BECAUSE▁THE▁WIRE▁MAKES▁THAT▁ENERGY▁INTO▁THE▁IS▁ALSO▁IMPORTANT▁BECAUSE▁IT▁NEEDS▁TO▁BE▁SO▁THES▁CAN▁DO▁THEIR

2024-10-27 18:21:08,716 (asr_inference:509) INFO: speech length: 242478
2024-10-27 18:21:18,619 (beam_search:428) INFO: decoder input length: 188
2024-10-27 18:21:18,619 (beam_search:429) INFO: max output length: 188
2024-10-27 18:21:18,619 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:19,151 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:19,151 (beam_search:476) INFO:  -3.29 * 1.0 =  -3.29 for ctc
2024-10-27 18:21:19,151 (beam_search:479) INFO: total log probability: -3.29
2024-10-27 18:21:19,151 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:21:19,151 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:19,151 (beam_search:483) INFO: best hypo: ▁WELL▁THERE▁TO▁BE▁A▁BECAUSE▁OTHER▁THE▁BATTERY▁ALL▁ITS▁ENERGY▁A▁OF▁ENERGY

2024-10-27 18:21:19,154 (asr_inference:509) INFO: speech length: 232712
2024-10-27 18:21:28,622 (beam_search:428) INFO: decoder input length: 181
2024-10-27 18:21:28,622 (beam_search:429) INFO: max output length: 181
2024-10-27 18:21:28,622 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:29,410 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:29,410 (beam_search:476) INFO:  -6.67 * 1.0 =  -6.67 for ctc
2024-10-27 18:21:29,410 (beam_search:479) INFO: total log probability: -6.67
2024-10-27 18:21:29,410 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:21:29,410 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:29,410 (beam_search:483) INFO: best hypo: ▁YOU▁THIS▁TOGETHER▁WITH▁A▁CIRCUIT▁THE▁ENERGY▁FROM▁THE▁BATTERY▁THROUGH▁THE▁AND▁THROUGH▁ALL▁THOSES▁BACK▁TO▁THE▁TERMINAL▁WHICH▁IS▁A

2024-10-27 18:21:29,412 (asr_inference:509) INFO: speech length: 114872
2024-10-27 18:21:33,580 (beam_search:428) INFO: decoder input length: 89
2024-10-27 18:21:33,581 (beam_search:429) INFO: max output length: 89
2024-10-27 18:21:33,581 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:33,725 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:33,725 (beam_search:476) INFO:  -1.20 * 1.0 =  -1.20 for ctc
2024-10-27 18:21:33,725 (beam_search:479) INFO: total log probability: -1.20
2024-10-27 18:21:33,725 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:21:33,725 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:33,725 (beam_search:483) INFO: best hypo: ▁WHICH▁THE▁BATTERY▁FROM▁BECAUSE▁USE▁A▁OF▁ENERGY

2024-10-27 18:21:33,728 (asr_inference:509) INFO: speech length: 152543
2024-10-27 18:21:39,168 (beam_search:428) INFO: decoder input length: 118
2024-10-27 18:21:39,168 (beam_search:429) INFO: max output length: 118
2024-10-27 18:21:39,168 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:39,495 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:39,496 (beam_search:476) INFO:  -2.17 * 1.0 =  -2.17 for ctc
2024-10-27 18:21:39,496 (beam_search:479) INFO: total log probability: -2.17
2024-10-27 18:21:39,496 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:21:39,496 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:39,496 (beam_search:483) INFO: best hypo: ▁I▁JUST▁TOLD▁YOU▁BUT▁THE▁THE▁SWITCH▁MAKES▁THAT▁THE▁ENERGY▁AND▁THE▁IS▁NOT

2024-10-27 18:21:39,498 (asr_inference:509) INFO: speech length: 263818
2024-10-27 18:21:50,507 (beam_search:428) INFO: decoder input length: 205
2024-10-27 18:21:50,507 (beam_search:429) INFO: max output length: 205
2024-10-27 18:21:50,507 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:51,326 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:51,326 (beam_search:476) INFO:  -7.01 * 1.0 =  -7.01 for ctc
2024-10-27 18:21:51,326 (beam_search:479) INFO: total log probability: -7.01
2024-10-27 18:21:51,326 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:21:51,326 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:51,326 (beam_search:483) INFO: best hypo: ▁WHEN▁YOU▁THE▁SWITCH▁IT▁THE▁BUT▁WHEN▁YOU'▁NOT▁IT▁YOU▁TO▁OFF▁THE▁CIRCUIT▁BY▁THE▁SO▁THET

2024-10-27 18:21:51,329 (asr_inference:509) INFO: speech length: 248778
2024-10-27 18:22:01,376 (beam_search:428) INFO: decoder input length: 193
2024-10-27 18:22:01,376 (beam_search:429) INFO: max output length: 193
2024-10-27 18:22:01,376 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:02,027 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:02,028 (beam_search:476) INFO:  -3.39 * 1.0 =  -3.39 for ctc
2024-10-27 18:22:02,028 (beam_search:479) INFO: total log probability: -3.39
2024-10-27 18:22:02,028 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:22:02,028 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:02,028 (beam_search:483) INFO: best hypo: ▁THE▁SWITCHS▁THE▁BATTERY▁FROM▁AND▁ITS▁THE▁OF▁ENERGYING▁THE▁AND▁IT▁BACK▁ON

2024-10-27 18:22:02,030 (asr_inference:509) INFO: speech length: 340017
2024-10-27 18:22:17,122 (beam_search:428) INFO: decoder input length: 265
2024-10-27 18:22:17,122 (beam_search:429) INFO: max output length: 265
2024-10-27 18:22:17,122 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:18,522 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:18,522 (beam_search:476) INFO:  -5.07 * 1.0 =  -5.07 for ctc
2024-10-27 18:22:18,522 (beam_search:479) INFO: total log probability: -5.07
2024-10-27 18:22:18,522 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:22:18,522 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:18,522 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁LIKE▁WHEN▁THE▁SWITCH▁IS▁THE▁SWITCH▁IS▁THE▁MAGNETICS▁AND▁PICKS▁UP▁WASHERS▁WHEN▁YOU▁OPEN▁THE▁SWITCH▁THE▁MAGNETIC▁AND▁THE▁WASHERS

2024-10-27 18:22:18,525 (asr_inference:509) INFO: speech length: 144046
2024-10-27 18:22:23,788 (beam_search:428) INFO: decoder input length: 112
2024-10-27 18:22:23,788 (beam_search:429) INFO: max output length: 112
2024-10-27 18:22:23,788 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:23,971 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:23,971 (beam_search:476) INFO:  -2.20 * 1.0 =  -2.20 for ctc
2024-10-27 18:22:23,971 (beam_search:479) INFO: total log probability: -2.20
2024-10-27 18:22:23,971 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:22:23,971 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:23,971 (beam_search:483) INFO: best hypo: ▁ELECTRICITY▁IS▁FLOWING▁THROUGH▁THAT▁WHICHS▁A▁MAGNETIC

2024-10-27 18:22:23,973 (asr_inference:509) INFO: speech length: 254760
2024-10-27 18:22:34,621 (beam_search:428) INFO: decoder input length: 198
2024-10-27 18:22:34,621 (beam_search:429) INFO: max output length: 198
2024-10-27 18:22:34,621 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:35,320 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:35,320 (beam_search:476) INFO:  -3.69 * 1.0 =  -3.69 for ctc
2024-10-27 18:22:35,320 (beam_search:479) INFO: total log probability: -3.69
2024-10-27 18:22:35,320 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:22:35,320 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:35,320 (beam_search:483) INFO: best hypo: ▁THE▁IS▁A▁THATS▁THE▁BATTERY▁FROM▁AND▁IT▁MAKES▁THE▁MAGNETING▁UP▁STUFF▁OR▁OR▁OFF

2024-10-27 18:22:35,323 (asr_inference:509) INFO: speech length: 62960
2024-10-27 18:22:37,716 (beam_search:428) INFO: decoder input length: 48
2024-10-27 18:22:37,716 (beam_search:429) INFO: max output length: 48
2024-10-27 18:22:37,716 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:37,753 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:37,753 (beam_search:476) INFO:  -1.54 * 1.0 =  -1.54 for ctc
2024-10-27 18:22:37,753 (beam_search:479) INFO: total log probability: -1.54
2024-10-27 18:22:37,753 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:22:37,753 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:37,753 (beam_search:483) INFO: best hypo: ▁ARE▁DON'T

2024-10-27 18:22:37,756 (asr_inference:509) INFO: speech length: 34969
2024-10-27 18:22:39,120 (beam_search:428) INFO: decoder input length: 26
2024-10-27 18:22:39,121 (beam_search:429) INFO: max output length: 26
2024-10-27 18:22:39,121 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:39,134 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:39,134 (beam_search:476) INFO:  -0.13 * 1.0 =  -0.13 for ctc
2024-10-27 18:22:39,135 (beam_search:479) INFO: total log probability: -0.13
2024-10-27 18:22:39,135 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:22:39,135 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:39,135 (beam_search:483) INFO: best hypo: ▁IT▁THE

2024-10-27 18:22:39,137 (asr_inference:509) INFO: speech length: 66247
2024-10-27 18:22:41,522 (beam_search:428) INFO: decoder input length: 51
2024-10-27 18:22:41,522 (beam_search:429) INFO: max output length: 51
2024-10-27 18:22:41,522 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:41,577 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:41,578 (beam_search:476) INFO:  -1.49 * 1.0 =  -1.49 for ctc
2024-10-27 18:22:41,578 (beam_search:479) INFO: total log probability: -1.49
2024-10-27 18:22:41,578 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:22:41,578 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:41,578 (beam_search:483) INFO: best hypo: ▁IS▁FLOWING▁THROUGH▁THE▁TO▁IT

2024-10-27 18:22:41,580 (asr_inference:509) INFO: speech length: 228574
2024-10-27 18:22:50,971 (beam_search:428) INFO: decoder input length: 178
2024-10-27 18:22:50,971 (beam_search:429) INFO: max output length: 178
2024-10-27 18:22:50,971 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:51,479 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:51,479 (beam_search:476) INFO:  -3.27 * 1.0 =  -3.27 for ctc
2024-10-27 18:22:51,479 (beam_search:479) INFO: total log probability: -3.27
2024-10-27 18:22:51,479 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:22:51,479 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:51,479 (beam_search:483) INFO: best hypo: ▁THE▁IS▁NOT▁FLOWING▁THERE▁IS▁NO▁MAGNETIC▁BECAUSE▁THERE▁IS▁NO▁IN▁THE▁THE

2024-10-27 18:22:51,482 (asr_inference:509) INFO: speech length: 108409
2024-10-27 18:22:55,474 (beam_search:428) INFO: decoder input length: 84
2024-10-27 18:22:55,474 (beam_search:429) INFO: max output length: 84
2024-10-27 18:22:55,474 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:55,619 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:55,620 (beam_search:476) INFO:  -1.44 * 1.0 =  -1.44 for ctc
2024-10-27 18:22:55,620 (beam_search:479) INFO: total log probability: -1.44
2024-10-27 18:22:55,620 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:22:55,620 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:55,620 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁HAS▁TO▁BE▁FLOWING▁THROUGH▁THAT▁TO▁THE

2024-10-27 18:22:55,622 (asr_inference:509) INFO: speech length: 91719
2024-10-27 18:22:58,796 (beam_search:428) INFO: decoder input length: 71
2024-10-27 18:22:58,796 (beam_search:429) INFO: max output length: 71
2024-10-27 18:22:58,797 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:58,921 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:58,922 (beam_search:476) INFO:  -2.92 * 1.0 =  -2.92 for ctc
2024-10-27 18:22:58,922 (beam_search:479) INFO: total log probability: -2.92
2024-10-27 18:22:58,922 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:22:58,922 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:58,922 (beam_search:483) INFO: best hypo: ▁TO▁BE▁FROM▁OR▁IRON▁BECAUSE▁THOSE▁ARE▁THE▁ONLY

2024-10-27 18:22:58,924 (asr_inference:509) INFO: speech length: 88669
2024-10-27 18:23:02,050 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:23:02,050 (beam_search:429) INFO: max output length: 68
2024-10-27 18:23:02,050 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:02,152 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:02,152 (beam_search:476) INFO:  -3.09 * 1.0 =  -3.09 for ctc
2024-10-27 18:23:02,152 (beam_search:479) INFO: total log probability: -3.09
2024-10-27 18:23:02,152 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:23:02,152 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:02,152 (beam_search:483) INFO: best hypo: ▁HAVE▁TO▁BED▁AROUND▁OR▁STEEL▁TO▁THE

2024-10-27 18:23:02,154 (asr_inference:509) INFO: speech length: 120150
2024-10-27 18:23:06,570 (beam_search:428) INFO: decoder input length: 93
2024-10-27 18:23:06,571 (beam_search:429) INFO: max output length: 93
2024-10-27 18:23:06,571 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:06,716 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:06,716 (beam_search:476) INFO:  -0.58 * 1.0 =  -0.58 for ctc
2024-10-27 18:23:06,716 (beam_search:479) INFO: total log probability: -0.58
2024-10-27 18:23:06,716 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:23:06,716 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:06,716 (beam_search:483) INFO: best hypo: ▁WE'VE▁BEEN▁LEARNING▁ABOUT▁CIRCUITS▁AND▁ENERGY

2024-10-27 18:23:06,719 (asr_inference:509) INFO: speech length: 94770
2024-10-27 18:23:10,166 (beam_search:428) INFO: decoder input length: 73
2024-10-27 18:23:10,166 (beam_search:429) INFO: max output length: 73
2024-10-27 18:23:10,166 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:10,317 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:10,317 (beam_search:476) INFO:  -0.87 * 1.0 =  -0.87 for ctc
2024-10-27 18:23:10,317 (beam_search:479) INFO: total log probability: -0.87
2024-10-27 18:23:10,317 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:23:10,317 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:10,317 (beam_search:483) INFO: best hypo: ▁UM▁THERE▁IS▁A▁LIGHT▁BULB▁A▁WIRE▁AND▁A▁D▁CELL

2024-10-27 18:23:10,320 (asr_inference:509) INFO: speech length: 61867
2024-10-27 18:23:12,569 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:23:12,569 (beam_search:429) INFO: max output length: 47
2024-10-27 18:23:12,569 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:12,600 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:12,600 (beam_search:476) INFO:  -0.84 * 1.0 =  -0.84 for ctc
2024-10-27 18:23:12,600 (beam_search:479) INFO: total log probability: -0.84
2024-10-27 18:23:12,601 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:23:12,601 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:12,601 (beam_search:483) INFO: best hypo: ▁THESE▁ARE▁WIRES

2024-10-27 18:23:12,603 (asr_inference:509) INFO: speech length: 106514
2024-10-27 18:23:16,380 (beam_search:428) INFO: decoder input length: 82
2024-10-27 18:23:16,380 (beam_search:429) INFO: max output length: 82
2024-10-27 18:23:16,380 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:16,570 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:16,570 (beam_search:476) INFO:  -3.44 * 1.0 =  -3.44 for ctc
2024-10-27 18:23:16,570 (beam_search:479) INFO: total log probability: -3.44
2024-10-27 18:23:16,570 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:23:16,570 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:16,571 (beam_search:483) INFO: best hypo: ▁THEY▁THEY▁HAVED▁ENERGY▁IN▁THEM▁AND▁THEY▁THE▁LIGHT▁BULB▁LIGHT

2024-10-27 18:23:16,573 (asr_inference:509) INFO: speech length: 47775
2024-10-27 18:23:18,318 (beam_search:428) INFO: decoder input length: 36
2024-10-27 18:23:18,319 (beam_search:429) INFO: max output length: 36
2024-10-27 18:23:18,319 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:18,353 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:18,354 (beam_search:476) INFO:  -0.84 * 1.0 =  -0.84 for ctc
2024-10-27 18:23:18,354 (beam_search:479) INFO: total log probability: -0.84
2024-10-27 18:23:18,354 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:23:18,354 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:18,354 (beam_search:483) INFO: best hypo: ▁THEY▁HAVED▁ENERGY▁IN

2024-10-27 18:23:18,356 (asr_inference:509) INFO: speech length: 34562
2024-10-27 18:23:19,754 (beam_search:428) INFO: decoder input length: 26
2024-10-27 18:23:19,754 (beam_search:429) INFO: max output length: 26
2024-10-27 18:23:19,754 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:19,773 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:19,774 (beam_search:476) INFO:  -0.10 * 1.0 =  -0.10 for ctc
2024-10-27 18:23:19,774 (beam_search:479) INFO: total log probability: -0.10
2024-10-27 18:23:19,774 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:23:19,774 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:19,774 (beam_search:483) INFO: best hypo: ▁IT▁IS▁A

2024-10-27 18:23:19,776 (asr_inference:509) INFO: speech length: 59190
2024-10-27 18:23:21,991 (beam_search:428) INFO: decoder input length: 45
2024-10-27 18:23:21,991 (beam_search:429) INFO: max output length: 45
2024-10-27 18:23:21,991 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:22,039 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:22,039 (beam_search:476) INFO:  -0.58 * 1.0 =  -0.58 for ctc
2024-10-27 18:23:22,039 (beam_search:479) INFO: total log probability: -0.58
2024-10-27 18:23:22,039 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:23:22,039 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:22,039 (beam_search:483) INFO: best hypo: ▁IT▁ENERGY▁TO▁LIGHT▁THE▁BULB

2024-10-27 18:23:22,041 (asr_inference:509) INFO: speech length: 75741
2024-10-27 18:23:24,763 (beam_search:428) INFO: decoder input length: 58
2024-10-27 18:23:24,764 (beam_search:429) INFO: max output length: 58
2024-10-27 18:23:24,764 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:24,845 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:24,845 (beam_search:476) INFO:  -1.31 * 1.0 =  -1.31 for ctc
2024-10-27 18:23:24,845 (beam_search:479) INFO: total log probability: -1.31
2024-10-27 18:23:24,845 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:23:24,845 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:24,845 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁THE▁BULB▁WOULD▁NOT▁LIGHT

2024-10-27 18:23:24,848 (asr_inference:509) INFO: speech length: 69735
2024-10-27 18:23:27,352 (beam_search:428) INFO: decoder input length: 53
2024-10-27 18:23:27,352 (beam_search:429) INFO: max output length: 53
2024-10-27 18:23:27,352 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:27,402 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:27,402 (beam_search:476) INFO:  -0.44 * 1.0 =  -0.44 for ctc
2024-10-27 18:23:27,402 (beam_search:479) INFO: total log probability: -0.44
2024-10-27 18:23:27,402 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:23:27,402 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:27,402 (beam_search:483) INFO: best hypo: ▁THE▁LIGHT▁BULB▁TO▁THE

2024-10-27 18:23:27,404 (asr_inference:509) INFO: speech length: 72767
2024-10-27 18:23:30,051 (beam_search:428) INFO: decoder input length: 56
2024-10-27 18:23:30,052 (beam_search:429) INFO: max output length: 56
2024-10-27 18:23:30,052 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:30,137 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:30,137 (beam_search:476) INFO:  -3.17 * 1.0 =  -3.17 for ctc
2024-10-27 18:23:30,137 (beam_search:479) INFO: total log probability: -3.17
2024-10-27 18:23:30,137 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:23:30,137 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:30,137 (beam_search:483) INFO: best hypo: ▁THE▁WIRES▁ARE▁A▁FROM▁THE▁TO▁THE▁BULB

2024-10-27 18:23:30,140 (asr_inference:509) INFO: speech length: 66523
2024-10-27 18:23:32,667 (beam_search:428) INFO: decoder input length: 51
2024-10-27 18:23:32,667 (beam_search:429) INFO: max output length: 51
2024-10-27 18:23:32,667 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:32,706 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:32,706 (beam_search:476) INFO:  -1.04 * 1.0 =  -1.04 for ctc
2024-10-27 18:23:32,706 (beam_search:479) INFO: total log probability: -1.04
2024-10-27 18:23:32,706 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:23:32,706 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:32,706 (beam_search:483) INFO: best hypo: ▁I▁THAT▁ELECTRICITY▁IN

2024-10-27 18:23:32,709 (asr_inference:509) INFO: speech length: 57493
2024-10-27 18:23:34,865 (beam_search:428) INFO: decoder input length: 44
2024-10-27 18:23:34,865 (beam_search:429) INFO: max output length: 44
2024-10-27 18:23:34,865 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:34,887 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:34,887 (beam_search:476) INFO:  -0.94 * 1.0 =  -0.94 for ctc
2024-10-27 18:23:34,887 (beam_search:479) INFO: total log probability: -0.94
2024-10-27 18:23:34,887 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:23:34,887 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:34,887 (beam_search:483) INFO: best hypo: ▁IN▁A

2024-10-27 18:23:34,889 (asr_inference:509) INFO: speech length: 91190
2024-10-27 18:23:38,223 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:23:38,223 (beam_search:429) INFO: max output length: 70
2024-10-27 18:23:38,223 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:38,289 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:38,289 (beam_search:476) INFO:  -0.66 * 1.0 =  -0.66 for ctc
2024-10-27 18:23:38,289 (beam_search:479) INFO: total log probability: -0.66
2024-10-27 18:23:38,289 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:23:38,289 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:38,289 (beam_search:483) INFO: best hypo: ▁THEY▁AND▁THEY▁USE▁ELECTRICITY

2024-10-27 18:23:38,292 (asr_inference:509) INFO: speech length: 107729
2024-10-27 18:23:42,206 (beam_search:428) INFO: decoder input length: 83
2024-10-27 18:23:42,206 (beam_search:429) INFO: max output length: 83
2024-10-27 18:23:42,206 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:42,303 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:42,303 (beam_search:476) INFO:  -1.16 * 1.0 =  -1.16 for ctc
2024-10-27 18:23:42,303 (beam_search:479) INFO: total log probability: -1.16
2024-10-27 18:23:42,303 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:23:42,304 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:42,304 (beam_search:483) INFO: best hypo: S▁ITS▁ELECTRICITY▁AND▁LIGHT

2024-10-27 18:23:42,307 (asr_inference:509) INFO: speech length: 126897
2024-10-27 18:23:47,022 (beam_search:428) INFO: decoder input length: 98
2024-10-27 18:23:47,022 (beam_search:429) INFO: max output length: 98
2024-10-27 18:23:47,022 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:47,229 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:47,229 (beam_search:476) INFO:  -1.54 * 1.0 =  -1.54 for ctc
2024-10-27 18:23:47,229 (beam_search:479) INFO: total log probability: -1.54
2024-10-27 18:23:47,229 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:23:47,230 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:47,230 (beam_search:483) INFO: best hypo: ▁THE▁AND▁IT▁INTO▁THE▁LIGHT▁BULB▁TO▁MAKE▁IT▁LIGHT

2024-10-27 18:23:47,232 (asr_inference:509) INFO: speech length: 261840
2024-10-27 18:23:57,914 (beam_search:428) INFO: decoder input length: 204
2024-10-27 18:23:57,914 (beam_search:429) INFO: max output length: 204
2024-10-27 18:23:57,914 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:59,000 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:59,000 (beam_search:476) INFO:  -3.76 * 1.0 =  -3.76 for ctc
2024-10-27 18:23:59,000 (beam_search:479) INFO: total log probability: -3.76
2024-10-27 18:23:59,000 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:23:59,000 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:59,000 (beam_search:483) INFO: best hypo: ▁THE▁ELECTRICITY▁IS▁GOING▁THROUGH▁THE▁WIRES▁TO▁THE▁LIGHT▁BULB▁AND▁BACK▁DOWN▁TO▁THE▁TO▁THE▁AND▁IT▁KEEPS▁GOING▁IT▁MAKES▁THE▁LIGHT▁BULB▁LIGHT

2024-10-27 18:23:59,002 (asr_inference:509) INFO: speech length: 249700
2024-10-27 18:24:09,064 (beam_search:428) INFO: decoder input length: 194
2024-10-27 18:24:09,064 (beam_search:429) INFO: max output length: 194
2024-10-27 18:24:09,064 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:10,268 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:10,268 (beam_search:476) INFO:  -4.15 * 1.0 =  -4.15 for ctc
2024-10-27 18:24:10,268 (beam_search:479) INFO: total log probability: -4.15
2024-10-27 18:24:10,268 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:24:10,268 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:10,268 (beam_search:483) INFO: best hypo: ▁ONE▁IS▁THE▁POSITIVE▁SIDE▁AND▁ONE▁SIDE▁IS▁THE▁NEGATIVE▁SIDE▁AND▁THE▁POSITIVE▁HAS▁THE▁IT▁AND▁IT▁GOES▁INTO▁THE▁ELECTRICITY▁GOES▁INTO▁THE▁POSITIVE▁SIDE▁AND▁OUT▁THE▁NEGATIVE▁SIDE

2024-10-27 18:24:10,271 (asr_inference:509) INFO: speech length: 202458
2024-10-27 18:24:18,494 (beam_search:428) INFO: decoder input length: 157
2024-10-27 18:24:18,494 (beam_search:429) INFO: max output length: 157
2024-10-27 18:24:18,494 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:19,219 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:19,220 (beam_search:476) INFO:  -2.97 * 1.0 =  -2.97 for ctc
2024-10-27 18:24:19,220 (beam_search:479) INFO: total log probability: -2.97
2024-10-27 18:24:19,220 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:24:19,220 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:19,220 (beam_search:483) INFO: best hypo: ▁IT▁GOES▁IT▁GOES▁INTO▁THE▁POSITIVE▁SIDE▁AND▁OUT▁THROUGH▁THE▁NEGATIVE▁SIDE▁AND▁THEN▁GOES▁AROUND▁AND▁THEN▁BACK▁IN▁THROUGH▁THE▁POSITIVE▁SIDE

2024-10-27 18:24:19,223 (asr_inference:509) INFO: speech length: 40975
2024-10-27 18:24:20,923 (beam_search:428) INFO: decoder input length: 31
2024-10-27 18:24:20,923 (beam_search:429) INFO: max output length: 31
2024-10-27 18:24:20,923 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:20,945 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:20,945 (beam_search:476) INFO:  -0.62 * 1.0 =  -0.62 for ctc
2024-10-27 18:24:20,945 (beam_search:479) INFO: total log probability: -0.62
2024-10-27 18:24:20,945 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:24:20,945 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:20,945 (beam_search:483) INFO: best hypo: ▁SIDE▁THE▁SIDE

2024-10-27 18:24:20,948 (asr_inference:509) INFO: speech length: 65727
2024-10-27 18:24:23,432 (beam_search:428) INFO: decoder input length: 50
2024-10-27 18:24:23,432 (beam_search:429) INFO: max output length: 50
2024-10-27 18:24:23,432 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:23,518 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:23,518 (beam_search:476) INFO:  -1.07 * 1.0 =  -1.07 for ctc
2024-10-27 18:24:23,518 (beam_search:479) INFO: total log probability: -1.07
2024-10-27 18:24:23,518 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:24:23,518 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:23,518 (beam_search:483) INFO: best hypo: ▁IS▁THE▁POSITIVE▁SIDE▁AND▁IT▁HAS▁A▁ON▁IT

2024-10-27 18:24:23,521 (asr_inference:509) INFO: speech length: 131623
2024-10-27 18:24:28,371 (beam_search:428) INFO: decoder input length: 102
2024-10-27 18:24:28,371 (beam_search:429) INFO: max output length: 102
2024-10-27 18:24:28,371 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:28,564 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:28,564 (beam_search:476) INFO:  -2.02 * 1.0 =  -2.02 for ctc
2024-10-27 18:24:28,564 (beam_search:479) INFO: total log probability: -2.02
2024-10-27 18:24:28,564 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:24:28,565 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:28,565 (beam_search:483) INFO: best hypo: ▁THE▁ELECTRICITY▁TOWARDS▁THE▁AND▁GOES▁INTO▁THE▁D▁CELL

2024-10-27 18:24:28,567 (asr_inference:509) INFO: speech length: 39215
2024-10-27 18:24:30,058 (beam_search:428) INFO: decoder input length: 30
2024-10-27 18:24:30,058 (beam_search:429) INFO: max output length: 30
2024-10-27 18:24:30,058 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:30,070 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:30,070 (beam_search:476) INFO:  -1.10 * 1.0 =  -1.10 for ctc
2024-10-27 18:24:30,070 (beam_search:479) INFO: total log probability: -1.10
2024-10-27 18:24:30,070 (beam_search:480) INFO: normalized log probability: -0.37
2024-10-27 18:24:30,070 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:30,070 (beam_search:483) INFO: best hypo: ▁ELECTRICITY

2024-10-27 18:24:30,072 (asr_inference:509) INFO: speech length: 60942
2024-10-27 18:24:32,378 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:24:32,378 (beam_search:429) INFO: max output length: 47
2024-10-27 18:24:32,378 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:32,430 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:32,430 (beam_search:476) INFO:  -1.31 * 1.0 =  -1.31 for ctc
2024-10-27 18:24:32,430 (beam_search:479) INFO: total log probability: -1.31
2024-10-27 18:24:32,430 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:24:32,430 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:32,430 (beam_search:483) INFO: best hypo: ▁THE▁ELECTRICITY▁IS▁THE▁POSITIVE▁SIDE

2024-10-27 18:24:32,433 (asr_inference:509) INFO: speech length: 207738
2024-10-27 18:24:41,008 (beam_search:428) INFO: decoder input length: 161
2024-10-27 18:24:41,008 (beam_search:429) INFO: max output length: 161
2024-10-27 18:24:41,008 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:41,589 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:41,589 (beam_search:476) INFO:  -3.02 * 1.0 =  -3.02 for ctc
2024-10-27 18:24:41,589 (beam_search:479) INFO: total log probability: -3.02
2024-10-27 18:24:41,590 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:24:41,590 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:41,590 (beam_search:483) INFO: best hypo: ▁I▁THAT▁IT▁AROUND▁AND▁THE▁ELECTRICITY▁IS▁STILL▁FLOWING▁INTO▁THE▁POSITIVE▁SIDE▁AND▁OUT▁THE▁NEGATIVE▁SIDE

2024-10-27 18:24:41,593 (asr_inference:509) INFO: speech length: 105899
2024-10-27 18:24:45,385 (beam_search:428) INFO: decoder input length: 82
2024-10-27 18:24:45,385 (beam_search:429) INFO: max output length: 82
2024-10-27 18:24:45,385 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:45,465 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:45,465 (beam_search:476) INFO:  -1.19 * 1.0 =  -1.19 for ctc
2024-10-27 18:24:45,465 (beam_search:479) INFO: total log probability: -1.19
2024-10-27 18:24:45,465 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:24:45,465 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:45,465 (beam_search:483) INFO: best hypo: ▁THE▁THE▁GOES▁THE▁OTHER

2024-10-27 18:24:45,468 (asr_inference:509) INFO: speech length: 100672
2024-10-27 18:24:49,104 (beam_search:428) INFO: decoder input length: 78
2024-10-27 18:24:49,105 (beam_search:429) INFO: max output length: 78
2024-10-27 18:24:49,105 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:49,265 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:49,265 (beam_search:476) INFO:  -1.68 * 1.0 =  -1.68 for ctc
2024-10-27 18:24:49,265 (beam_search:479) INFO: total log probability: -1.68
2024-10-27 18:24:49,265 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:24:49,265 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:49,265 (beam_search:483) INFO: best hypo: ▁THE▁OF▁THE▁FLOW▁TURNS▁JUST▁AS▁THE▁D▁CELL

2024-10-27 18:24:49,267 (asr_inference:509) INFO: speech length: 114634
2024-10-27 18:24:53,386 (beam_search:428) INFO: decoder input length: 89
2024-10-27 18:24:53,386 (beam_search:429) INFO: max output length: 89
2024-10-27 18:24:53,386 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:53,537 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:53,537 (beam_search:476) INFO:  -2.65 * 1.0 =  -2.65 for ctc
2024-10-27 18:24:53,537 (beam_search:479) INFO: total log probability: -2.65
2024-10-27 18:24:53,537 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:24:53,537 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:53,537 (beam_search:483) INFO: best hypo: ▁IS▁FLOWING▁FROM▁THE▁NEGATIVE▁SIDE▁OF▁THE▁D

2024-10-27 18:24:53,539 (asr_inference:509) INFO: speech length: 254768
2024-10-27 18:25:03,863 (beam_search:428) INFO: decoder input length: 198
2024-10-27 18:25:03,864 (beam_search:429) INFO: max output length: 198
2024-10-27 18:25:03,864 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:04,798 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:04,799 (beam_search:476) INFO:  -7.63 * 1.0 =  -7.63 for ctc
2024-10-27 18:25:04,799 (beam_search:479) INFO: total log probability: -7.63
2024-10-27 18:25:04,799 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:25:04,799 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:04,799 (beam_search:483) INFO: best hypo: ▁THE▁POSITIVE▁SIDE▁THE▁THATS▁THE▁POSITIVE▁IS▁ALSO▁THE▁BULB▁AND▁THE▁WIRE▁THAT▁IS▁TOUCHING▁THE▁NEGATIVE▁SIDE▁OF▁THE▁D▁CELL▁THE

2024-10-27 18:25:04,801 (asr_inference:509) INFO: speech length: 328155
2024-10-27 18:25:19,024 (beam_search:428) INFO: decoder input length: 255
2024-10-27 18:25:19,025 (beam_search:429) INFO: max output length: 255
2024-10-27 18:25:19,025 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:19,746 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:19,747 (beam_search:476) INFO:  -6.87 * 1.0 =  -6.87 for ctc
2024-10-27 18:25:19,747 (beam_search:479) INFO: total log probability: -6.87
2024-10-27 18:25:19,747 (beam_search:480) INFO: normalized log probability: -0.38
2024-10-27 18:25:19,747 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:19,747 (beam_search:483) INFO: best hypo: ▁THE▁THE▁THAT▁THE▁POSITIVE▁IS▁THE▁AND▁THE▁THAT'S▁THE▁SIDE▁IS▁THE

2024-10-27 18:25:19,749 (asr_inference:509) INFO: speech length: 328886
2024-10-27 18:25:34,208 (beam_search:428) INFO: decoder input length: 256
2024-10-27 18:25:34,208 (beam_search:429) INFO: max output length: 256
2024-10-27 18:25:34,208 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:35,365 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:35,365 (beam_search:476) INFO:  -7.72 * 1.0 =  -7.72 for ctc
2024-10-27 18:25:35,365 (beam_search:479) INFO: total log probability: -7.72
2024-10-27 18:25:35,365 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:25:35,365 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:35,365 (beam_search:483) INFO: best hypo: ▁IT▁HAS▁TO▁BE▁A▁COMPLETE▁ONE▁HAS▁TO▁TOUCH▁THE▁SIDE▁AND▁ONE▁HAS▁THE▁POSITIVE▁AND▁ONE▁TO▁TOUCH▁THE▁AND▁THE▁OTHER▁THE

2024-10-27 18:25:35,368 (asr_inference:509) INFO: speech length: 53467
2024-10-27 18:25:37,340 (beam_search:428) INFO: decoder input length: 41
2024-10-27 18:25:37,340 (beam_search:429) INFO: max output length: 41
2024-10-27 18:25:37,340 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:37,368 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:37,369 (beam_search:476) INFO:  -1.35 * 1.0 =  -1.35 for ctc
2024-10-27 18:25:37,369 (beam_search:479) INFO: total log probability: -1.35
2024-10-27 18:25:37,369 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:25:37,369 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:37,369 (beam_search:483) INFO: best hypo: ▁THE▁ELECTRICITY▁WILL

2024-10-27 18:25:37,371 (asr_inference:509) INFO: speech length: 379041
2024-10-27 18:25:54,297 (beam_search:428) INFO: decoder input length: 295
2024-10-27 18:25:54,297 (beam_search:429) INFO: max output length: 295
2024-10-27 18:25:54,297 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:56,488 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:56,488 (beam_search:476) INFO:  -8.78 * 1.0 =  -8.78 for ctc
2024-10-27 18:25:56,488 (beam_search:479) INFO: total log probability: -8.78
2024-10-27 18:25:56,489 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:25:56,489 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:56,489 (beam_search:483) INFO: best hypo: ▁I▁THAT▁IF▁ONE▁ONE▁WIRE▁IS▁ON▁THE▁BULB▁ON▁THE▁BASE▁THEN▁THE▁LIGHT▁BULB▁LIGHTS▁BUT▁IF▁BOTH▁IF▁BOTH▁OF▁THE▁WIRES▁ARE▁ON▁THE▁BULB▁THEN▁THE▁LIGHT▁BULB▁WON'T▁LIGHT

2024-10-27 18:25:56,491 (asr_inference:509) INFO: speech length: 84462
2024-10-27 18:25:59,600 (beam_search:428) INFO: decoder input length: 65
2024-10-27 18:25:59,601 (beam_search:429) INFO: max output length: 65
2024-10-27 18:25:59,601 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:59,757 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:59,757 (beam_search:476) INFO:  -1.98 * 1.0 =  -1.98 for ctc
2024-10-27 18:25:59,757 (beam_search:479) INFO: total log probability: -1.98
2024-10-27 18:25:59,757 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:25:59,757 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:59,757 (beam_search:483) INFO: best hypo: ▁WE'VE▁BEEN▁DOING▁WE'VE▁BEEN▁LEARNING▁ABOUT▁CIRCUITS▁AND▁ENERGY

2024-10-27 18:25:59,760 (asr_inference:509) INFO: speech length: 79016
2024-10-27 18:26:02,616 (beam_search:428) INFO: decoder input length: 61
2024-10-27 18:26:02,616 (beam_search:429) INFO: max output length: 61
2024-10-27 18:26:02,616 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:02,673 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:02,673 (beam_search:476) INFO:  -1.68 * 1.0 =  -1.68 for ctc
2024-10-27 18:26:02,673 (beam_search:479) INFO: total log probability: -1.68
2024-10-27 18:26:02,673 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:26:02,673 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:02,673 (beam_search:483) INFO: best hypo: ▁THE▁ISING▁THE▁MOTOR

2024-10-27 18:26:02,675 (asr_inference:509) INFO: speech length: 104096
2024-10-27 18:26:06,361 (beam_search:428) INFO: decoder input length: 80
2024-10-27 18:26:06,361 (beam_search:429) INFO: max output length: 80
2024-10-27 18:26:06,361 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:06,605 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:06,606 (beam_search:476) INFO:  -2.22 * 1.0 =  -2.22 for ctc
2024-10-27 18:26:06,606 (beam_search:479) INFO: total log probability: -2.22
2024-10-27 18:26:06,606 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:26:06,606 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:06,606 (beam_search:483) INFO: best hypo: ▁I▁THAT▁THE▁IS▁MOVING▁AND▁IT'S▁IMPORTANT▁BECAUSE▁THEN▁I▁KNOW▁THAT▁THE▁MOTOR▁IS

2024-10-27 18:26:06,608 (asr_inference:509) INFO: speech length: 73283
2024-10-27 18:26:09,321 (beam_search:428) INFO: decoder input length: 56
2024-10-27 18:26:09,321 (beam_search:429) INFO: max output length: 56
2024-10-27 18:26:09,321 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:09,364 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:09,365 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 18:26:09,365 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 18:26:09,365 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:26:09,365 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:09,365 (beam_search:483) INFO: best hypo: ▁IT▁MEANS▁THAT▁THE

2024-10-27 18:26:09,367 (asr_inference:509) INFO: speech length: 133702
2024-10-27 18:26:14,251 (beam_search:428) INFO: decoder input length: 103
2024-10-27 18:26:14,251 (beam_search:429) INFO: max output length: 103
2024-10-27 18:26:14,251 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:14,419 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:14,419 (beam_search:476) INFO:  -0.29 * 1.0 =  -0.29 for ctc
2024-10-27 18:26:14,419 (beam_search:479) INFO: total log probability: -0.29
2024-10-27 18:26:14,419 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:26:14,419 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:14,420 (beam_search:483) INFO: best hypo: ▁THE▁THE▁D▁CELL▁BATTERY▁AND▁THE▁WIRES

2024-10-27 18:26:14,423 (asr_inference:509) INFO: speech length: 99681
2024-10-27 18:26:18,222 (beam_search:428) INFO: decoder input length: 77
2024-10-27 18:26:18,222 (beam_search:429) INFO: max output length: 77
2024-10-27 18:26:18,222 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:18,331 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:18,331 (beam_search:476) INFO:  -1.02 * 1.0 =  -1.02 for ctc
2024-10-27 18:26:18,331 (beam_search:479) INFO: total log probability: -1.02
2024-10-27 18:26:18,331 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:26:18,331 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:18,331 (beam_search:483) INFO: best hypo: ▁THAT▁MEANS▁THAT▁THES▁ENERGY▁AND▁IT

2024-10-27 18:26:18,333 (asr_inference:509) INFO: speech length: 90106
2024-10-27 18:26:21,586 (beam_search:428) INFO: decoder input length: 69
2024-10-27 18:26:21,586 (beam_search:429) INFO: max output length: 69
2024-10-27 18:26:21,587 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:21,640 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:21,641 (beam_search:476) INFO:  -1.28 * 1.0 =  -1.28 for ctc
2024-10-27 18:26:21,641 (beam_search:479) INFO: total log probability: -1.28
2024-10-27 18:26:21,641 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:26:21,641 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:21,641 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁AND▁ELECTRICITY

2024-10-27 18:26:21,643 (asr_inference:509) INFO: speech length: 209471
2024-10-27 18:26:29,857 (beam_search:428) INFO: decoder input length: 163
2024-10-27 18:26:29,857 (beam_search:429) INFO: max output length: 163
2024-10-27 18:26:29,857 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:30,265 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:30,265 (beam_search:476) INFO:  -4.67 * 1.0 =  -4.67 for ctc
2024-10-27 18:26:30,265 (beam_search:479) INFO: total log probability: -4.67
2024-10-27 18:26:30,265 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 18:26:30,265 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:30,265 (beam_search:483) INFO: best hypo: ▁THE▁MOTOR▁HAVE▁A▁BATTERY▁AND▁IT▁HAVE▁A▁BATTERY▁AND▁TWO▁WIRES

2024-10-27 18:26:30,267 (asr_inference:509) INFO: speech length: 217832
2024-10-27 18:26:38,672 (beam_search:428) INFO: decoder input length: 169
2024-10-27 18:26:38,672 (beam_search:429) INFO: max output length: 169
2024-10-27 18:26:38,672 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:39,055 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:39,055 (beam_search:476) INFO:  -1.48 * 1.0 =  -1.48 for ctc
2024-10-27 18:26:39,055 (beam_search:479) INFO: total log probability: -1.48
2024-10-27 18:26:39,055 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:26:39,055 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:39,055 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁THE▁D▁I▁THINK▁THAT▁THE▁ENERGY▁IS▁IS

2024-10-27 18:26:39,058 (asr_inference:509) INFO: speech length: 277823
2024-10-27 18:26:50,780 (beam_search:428) INFO: decoder input length: 216
2024-10-27 18:26:50,780 (beam_search:429) INFO: max output length: 216
2024-10-27 18:26:50,780 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:51,351 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:51,351 (beam_search:476) INFO:  -1.27 * 1.0 =  -1.27 for ctc
2024-10-27 18:26:51,351 (beam_search:479) INFO: total log probability: -1.27
2024-10-27 18:26:51,351 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:26:51,351 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:51,351 (beam_search:483) INFO: best hypo: ▁THE▁THE▁BATTERY▁ENERGY▁TO▁THE▁MOTOR▁AND▁THEN▁AND▁THEN▁ENERGY▁GOES▁BACK

2024-10-27 18:26:51,354 (asr_inference:509) INFO: speech length: 342604
2024-10-27 18:27:06,322 (beam_search:428) INFO: decoder input length: 267
2024-10-27 18:27:06,322 (beam_search:429) INFO: max output length: 267
2024-10-27 18:27:06,322 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:07,987 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:07,987 (beam_search:476) INFO:  -3.86 * 1.0 =  -3.86 for ctc
2024-10-27 18:27:07,987 (beam_search:479) INFO: total log probability: -3.86
2024-10-27 18:27:07,987 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:27:07,987 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:07,987 (beam_search:483) INFO: best hypo: ▁THERE▁IS▁THE▁THE▁MOTOR▁AND▁THE▁BATTERY▁CONNECTED▁TO▁A▁SWITCH▁AND▁WHEN▁THE▁SWITCH▁IS▁UP▁THE▁MOTOR▁DOES▁NOT▁RUN▁BUT▁IF▁THE▁IS▁DOWN▁THEN▁THE▁MOTOR▁DOES

2024-10-27 18:27:07,990 (asr_inference:509) INFO: speech length: 118830
2024-10-27 18:27:12,378 (beam_search:428) INFO: decoder input length: 92
2024-10-27 18:27:12,379 (beam_search:429) INFO: max output length: 92
2024-10-27 18:27:12,379 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:12,484 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:12,484 (beam_search:476) INFO:  -1.26 * 1.0 =  -1.26 for ctc
2024-10-27 18:27:12,484 (beam_search:479) INFO: total log probability: -1.26
2024-10-27 18:27:12,484 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:27:12,484 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:12,484 (beam_search:483) INFO: best hypo: ▁THE▁IS▁BECAUSE▁THE▁IS▁DOWN

2024-10-27 18:27:12,486 (asr_inference:509) INFO: speech length: 49001
2024-10-27 18:27:14,320 (beam_search:428) INFO: decoder input length: 37
2024-10-27 18:27:14,320 (beam_search:429) INFO: max output length: 37
2024-10-27 18:27:14,320 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:14,355 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:14,355 (beam_search:476) INFO:  -0.01 * 1.0 =  -0.01 for ctc
2024-10-27 18:27:14,355 (beam_search:479) INFO: total log probability: -0.01
2024-10-27 18:27:14,355 (beam_search:480) INFO: normalized log probability: -0.00
2024-10-27 18:27:14,355 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:14,355 (beam_search:483) INFO: best hypo: ▁I▁DON'T▁KNOW

2024-10-27 18:27:14,358 (asr_inference:509) INFO: speech length: 98813
2024-10-27 18:27:18,063 (beam_search:428) INFO: decoder input length: 76
2024-10-27 18:27:18,064 (beam_search:429) INFO: max output length: 76
2024-10-27 18:27:18,064 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:18,134 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:18,135 (beam_search:476) INFO:  -0.32 * 1.0 =  -0.32 for ctc
2024-10-27 18:27:18,135 (beam_search:479) INFO: total log probability: -0.32
2024-10-27 18:27:18,135 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:27:18,135 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:18,135 (beam_search:483) INFO: best hypo: ▁IN▁THIS▁IS▁UP▁AND

2024-10-27 18:27:18,138 (asr_inference:509) INFO: speech length: 125750
2024-10-27 18:27:22,829 (beam_search:428) INFO: decoder input length: 97
2024-10-27 18:27:22,829 (beam_search:429) INFO: max output length: 97
2024-10-27 18:27:22,829 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:22,996 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:22,996 (beam_search:476) INFO:  -1.95 * 1.0 =  -1.95 for ctc
2024-10-27 18:27:22,996 (beam_search:479) INFO: total log probability: -1.95
2024-10-27 18:27:22,996 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:27:22,996 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:22,996 (beam_search:483) INFO: best hypo: ▁THE▁THE▁SWITCH▁GOES▁DOWN▁AND▁THE▁MOTOR▁STARTS

2024-10-27 18:27:22,998 (asr_inference:509) INFO: speech length: 222181
2024-10-27 18:27:31,777 (beam_search:428) INFO: decoder input length: 173
2024-10-27 18:27:31,777 (beam_search:429) INFO: max output length: 173
2024-10-27 18:27:31,777 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:32,251 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:32,251 (beam_search:476) INFO:  -3.19 * 1.0 =  -3.19 for ctc
2024-10-27 18:27:32,251 (beam_search:479) INFO: total log probability: -3.19
2024-10-27 18:27:32,251 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:27:32,251 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:32,251 (beam_search:483) INFO: best hypo: ▁THE▁CIRCUIT▁IN▁THIS▁IS▁IS▁OPEN▁BECAUSE▁THE▁NOT▁IS▁NOT▁NOT▁IS▁CONNECTED

2024-10-27 18:27:32,253 (asr_inference:509) INFO: speech length: 151551
2024-10-27 18:27:37,882 (beam_search:428) INFO: decoder input length: 117
2024-10-27 18:27:37,882 (beam_search:429) INFO: max output length: 117
2024-10-27 18:27:37,882 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:38,079 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:38,079 (beam_search:476) INFO:  -1.22 * 1.0 =  -1.22 for ctc
2024-10-27 18:27:38,079 (beam_search:479) INFO: total log probability: -1.22
2024-10-27 18:27:38,079 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:27:38,079 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:38,079 (beam_search:483) INFO: best hypo: ▁IT▁MEANS▁THAT▁THE▁THAT▁THE▁ENERGY▁IS▁NOT

2024-10-27 18:27:38,082 (asr_inference:509) INFO: speech length: 94951
2024-10-27 18:27:41,707 (beam_search:428) INFO: decoder input length: 73
2024-10-27 18:27:41,707 (beam_search:429) INFO: max output length: 73
2024-10-27 18:27:41,707 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:41,813 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:41,813 (beam_search:476) INFO:  -1.65 * 1.0 =  -1.65 for ctc
2024-10-27 18:27:41,813 (beam_search:479) INFO: total log probability: -1.65
2024-10-27 18:27:41,813 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:27:41,813 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:41,813 (beam_search:483) INFO: best hypo: ▁I▁THAT▁WHEN▁THE▁SWITCH▁IS▁THE▁RUNNING

2024-10-27 18:27:41,815 (asr_inference:509) INFO: speech length: 129590
2024-10-27 18:27:46,659 (beam_search:428) INFO: decoder input length: 100
2024-10-27 18:27:46,659 (beam_search:429) INFO: max output length: 100
2024-10-27 18:27:46,659 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:46,864 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:46,865 (beam_search:476) INFO:  -3.21 * 1.0 =  -3.21 for ctc
2024-10-27 18:27:46,865 (beam_search:479) INFO: total log probability: -3.21
2024-10-27 18:27:46,865 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:27:46,865 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:46,865 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THE▁IS▁BECAUSE▁IT▁IS▁A▁CIRCUIT▁AND▁ENERGY

2024-10-27 18:27:46,868 (asr_inference:509) INFO: speech length: 80370
2024-10-27 18:27:49,720 (beam_search:428) INFO: decoder input length: 62
2024-10-27 18:27:49,720 (beam_search:429) INFO: max output length: 62
2024-10-27 18:27:49,720 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:49,788 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:49,788 (beam_search:476) INFO:  -1.26 * 1.0 =  -1.26 for ctc
2024-10-27 18:27:49,788 (beam_search:479) INFO: total log probability: -1.26
2024-10-27 18:27:49,788 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:27:49,788 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:49,788 (beam_search:483) INFO: best hypo: ▁THE▁WHEN▁THE▁SWITCH▁IS▁OPEN

2024-10-27 18:27:49,791 (asr_inference:509) INFO: speech length: 168075
2024-10-27 18:27:56,457 (beam_search:428) INFO: decoder input length: 130
2024-10-27 18:27:56,457 (beam_search:429) INFO: max output length: 130
2024-10-27 18:27:56,457 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:56,839 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:56,839 (beam_search:476) INFO:  -1.68 * 1.0 =  -1.68 for ctc
2024-10-27 18:27:56,839 (beam_search:479) INFO: total log probability: -1.68
2024-10-27 18:27:56,839 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:27:56,839 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:56,839 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁THE▁THE▁SWITCH▁HAS▁TO▁BE▁SO▁THAT▁THE▁THE▁THE▁MOTOR▁CAN▁RUN

2024-10-27 18:27:56,842 (asr_inference:509) INFO: speech length: 66759
2024-10-27 18:27:59,293 (beam_search:428) INFO: decoder input length: 51
2024-10-27 18:27:59,293 (beam_search:429) INFO: max output length: 51
2024-10-27 18:27:59,293 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:59,372 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:59,373 (beam_search:476) INFO:  -1.69 * 1.0 =  -1.69 for ctc
2024-10-27 18:27:59,373 (beam_search:479) INFO: total log probability: -1.69
2024-10-27 18:27:59,373 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:27:59,373 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:59,373 (beam_search:483) INFO: best hypo: ▁I▁THAT▁THE▁MOTOR▁RUNNING▁WHEN▁THE▁SWITCH▁IS

2024-10-27 18:27:59,375 (asr_inference:509) INFO: speech length: 38192
2024-10-27 18:28:00,910 (beam_search:428) INFO: decoder input length: 29
2024-10-27 18:28:00,910 (beam_search:429) INFO: max output length: 29
2024-10-27 18:28:00,910 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:00,931 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:00,931 (beam_search:476) INFO:  -0.57 * 1.0 =  -0.57 for ctc
2024-10-27 18:28:00,931 (beam_search:479) INFO: total log probability: -0.57
2024-10-27 18:28:00,931 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:28:00,931 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:00,931 (beam_search:483) INFO: best hypo: ▁THE▁SWITCH▁IS

2024-10-27 18:28:00,934 (asr_inference:509) INFO: speech length: 88558
2024-10-27 18:28:04,030 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:28:04,030 (beam_search:429) INFO: max output length: 68
2024-10-27 18:28:04,030 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:04,084 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:04,084 (beam_search:476) INFO:  -1.04 * 1.0 =  -1.04 for ctc
2024-10-27 18:28:04,084 (beam_search:479) INFO: total log probability: -1.04
2024-10-27 18:28:04,084 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:28:04,085 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:04,085 (beam_search:483) INFO: best hypo: ▁CIRCUIT▁HAS▁TO▁BE

2024-10-27 18:28:04,087 (asr_inference:509) INFO: speech length: 149223
2024-10-27 18:28:09,676 (beam_search:428) INFO: decoder input length: 116
2024-10-27 18:28:09,676 (beam_search:429) INFO: max output length: 116
2024-10-27 18:28:09,676 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:10,006 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:10,007 (beam_search:476) INFO:  -2.69 * 1.0 =  -2.69 for ctc
2024-10-27 18:28:10,007 (beam_search:479) INFO: total log probability: -2.69
2024-10-27 18:28:10,007 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:28:10,007 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:10,007 (beam_search:483) INFO: best hypo: ▁WHEN▁THE▁SWITCH▁IS▁OPEN▁THE▁THE▁ELECTRICITY▁NOT▁FLOW▁SO▁THE▁MOTOR▁DOES▁NOT▁RUN

2024-10-27 18:28:10,009 (asr_inference:509) INFO: speech length: 221924
2024-10-27 18:28:19,072 (beam_search:428) INFO: decoder input length: 172
2024-10-27 18:28:19,072 (beam_search:429) INFO: max output length: 172
2024-10-27 18:28:19,072 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:19,680 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:19,680 (beam_search:476) INFO:  -3.43 * 1.0 =  -3.43 for ctc
2024-10-27 18:28:19,680 (beam_search:479) INFO: total log probability: -3.43
2024-10-27 18:28:19,680 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:28:19,680 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:19,680 (beam_search:483) INFO: best hypo: ▁THEY▁GET▁THES▁AND▁THEN▁THEY▁THEY▁THEY▁IS▁THE▁ENERGY▁AND▁THEN▁ITS▁IT▁TO▁THE

2024-10-27 18:28:19,683 (asr_inference:509) INFO: speech length: 196482
2024-10-27 18:28:27,434 (beam_search:428) INFO: decoder input length: 153
2024-10-27 18:28:27,434 (beam_search:429) INFO: max output length: 153
2024-10-27 18:28:27,434 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:28,005 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:28,005 (beam_search:476) INFO:  -1.72 * 1.0 =  -1.72 for ctc
2024-10-27 18:28:28,006 (beam_search:479) INFO: total log probability: -1.72
2024-10-27 18:28:28,006 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:28:28,006 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:28,006 (beam_search:483) INFO: best hypo: ▁THE▁SOLAR▁PANEL▁LIGHT▁SO▁AND▁THE▁LIGHT▁IS▁THE▁ENERGY▁SO▁THEN▁IT▁IT▁TO▁THE▁AND▁THE▁MOTOR

2024-10-27 18:28:28,008 (asr_inference:509) INFO: speech length: 228533
2024-10-27 18:28:37,288 (beam_search:428) INFO: decoder input length: 178
2024-10-27 18:28:37,288 (beam_search:429) INFO: max output length: 178
2024-10-27 18:28:37,288 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:37,990 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:37,990 (beam_search:476) INFO:  -1.14 * 1.0 =  -1.14 for ctc
2024-10-27 18:28:37,990 (beam_search:479) INFO: total log probability: -1.14
2024-10-27 18:28:37,990 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:28:37,990 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:37,990 (beam_search:483) INFO: best hypo: ▁THE▁SOLAR▁CELL▁HAS▁TO▁BE▁CONNECTED▁TO▁THE▁AND▁THE▁SOLAR▁CELL▁HAS▁TO▁BE▁HAS▁TO▁HAVE▁LIGHT

2024-10-27 18:28:37,992 (asr_inference:509) INFO: speech length: 255089
2024-10-27 18:28:48,953 (beam_search:428) INFO: decoder input length: 198
2024-10-27 18:28:48,953 (beam_search:429) INFO: max output length: 198
2024-10-27 18:28:48,953 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:49,786 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:49,786 (beam_search:476) INFO:  -3.07 * 1.0 =  -3.07 for ctc
2024-10-27 18:28:49,786 (beam_search:479) INFO: total log probability: -3.07
2024-10-27 18:28:49,786 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:28:49,786 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:49,786 (beam_search:483) INFO: best hypo: ▁THE▁SOLAR▁CELL▁IS▁THE▁SUNS▁ENERGY▁AND▁THEN▁IT▁GETS▁TO▁AND▁IT▁INTO▁THE▁MOTOR▁SO▁THE▁MOTOR

2024-10-27 18:28:49,789 (asr_inference:509) INFO: speech length: 93964
2024-10-27 18:28:53,123 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:28:53,123 (beam_search:429) INFO: max output length: 72
2024-10-27 18:28:53,123 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:53,226 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:53,226 (beam_search:476) INFO:  -0.33 * 1.0 =  -0.33 for ctc
2024-10-27 18:28:53,226 (beam_search:479) INFO: total log probability: -0.33
2024-10-27 18:28:53,226 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:28:53,226 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:53,226 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁THE▁I▁DON'T

2024-10-27 18:28:53,230 (asr_inference:509) INFO: speech length: 188916
2024-10-27 18:29:00,326 (beam_search:428) INFO: decoder input length: 147
2024-10-27 18:29:00,327 (beam_search:429) INFO: max output length: 147
2024-10-27 18:29:00,327 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:00,991 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:00,992 (beam_search:476) INFO:  -2.38 * 1.0 =  -2.38 for ctc
2024-10-27 18:29:00,992 (beam_search:479) INFO: total log probability: -2.38
2024-10-27 18:29:00,992 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:29:00,992 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:00,992 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁THE▁SOLAR▁CELL▁WILL▁NOT▁GET▁ENERGY▁BECAUSE▁THE▁WILL▁NOT▁GET▁TO▁THE▁SOLAR▁CELL▁BECAUSE▁THE▁AREING▁IT

2024-10-27 18:29:00,994 (asr_inference:509) INFO: speech length: 83457
2024-10-27 18:29:03,996 (beam_search:428) INFO: decoder input length: 64
2024-10-27 18:29:03,996 (beam_search:429) INFO: max output length: 64
2024-10-27 18:29:03,996 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:04,142 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:04,142 (beam_search:476) INFO:  -3.10 * 1.0 =  -3.10 for ctc
2024-10-27 18:29:04,142 (beam_search:479) INFO: total log probability: -3.10
2024-10-27 18:29:04,142 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:29:04,142 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:04,142 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THELAR▁CELL▁WILL▁NOT▁BE▁TO▁PRODUCE▁ELECTRICITY▁ON▁THE▁DAY

2024-10-27 18:29:04,144 (asr_inference:509) INFO: speech length: 159461
2024-10-27 18:29:10,032 (beam_search:428) INFO: decoder input length: 124
2024-10-27 18:29:10,032 (beam_search:429) INFO: max output length: 124
2024-10-27 18:29:10,032 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:10,319 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:10,319 (beam_search:476) INFO:  -2.83 * 1.0 =  -2.83 for ctc
2024-10-27 18:29:10,319 (beam_search:479) INFO: total log probability: -2.83
2024-10-27 18:29:10,319 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:29:10,319 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:10,319 (beam_search:483) INFO: best hypo: ▁WHEN▁I▁CLICK▁ON▁THE▁THE▁THE▁THE▁MOTOR▁BUT▁ON▁IT▁AGAIN

2024-10-27 18:29:10,321 (asr_inference:509) INFO: speech length: 83258
2024-10-27 18:29:13,422 (beam_search:428) INFO: decoder input length: 64
2024-10-27 18:29:13,423 (beam_search:429) INFO: max output length: 64
2024-10-27 18:29:13,423 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:13,514 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:13,514 (beam_search:476) INFO:  -2.42 * 1.0 =  -2.42 for ctc
2024-10-27 18:29:13,514 (beam_search:479) INFO: total log probability: -2.42
2024-10-27 18:29:13,514 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:29:13,514 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:13,514 (beam_search:483) INFO: best hypo: ▁WHEN▁THE▁IS▁OUT▁THE▁THE▁MOTOR▁STARTS

2024-10-27 18:29:13,516 (asr_inference:509) INFO: speech length: 158774
2024-10-27 18:29:19,432 (beam_search:428) INFO: decoder input length: 123
2024-10-27 18:29:19,433 (beam_search:429) INFO: max output length: 123
2024-10-27 18:29:19,433 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:19,838 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:19,838 (beam_search:476) INFO:  -3.62 * 1.0 =  -3.62 for ctc
2024-10-27 18:29:19,838 (beam_search:479) INFO: total log probability: -3.62
2024-10-27 18:29:19,838 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:29:19,838 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:19,838 (beam_search:483) INFO: best hypo: ▁A▁SOLAR▁CELL▁DOES▁NOT▁GET▁ENERGY▁FROM▁THE▁BECAUSE▁IT'SED▁SO▁THE▁MOTOR▁WILL▁NOT

2024-10-27 18:29:19,840 (asr_inference:509) INFO: speech length: 65559
2024-10-27 18:29:22,265 (beam_search:428) INFO: decoder input length: 50
2024-10-27 18:29:22,265 (beam_search:429) INFO: max output length: 50
2024-10-27 18:29:22,265 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:22,361 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:22,361 (beam_search:476) INFO:  -1.58 * 1.0 =  -1.58 for ctc
2024-10-27 18:29:22,361 (beam_search:479) INFO: total log probability: -1.58
2024-10-27 18:29:22,361 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:29:22,361 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:22,361 (beam_search:483) INFO: best hypo: ▁I▁THAT▁THE▁MOTOR▁STARTS▁RUNNING▁WHEN▁THE▁SUN▁IS▁OUT

2024-10-27 18:29:22,364 (asr_inference:509) INFO: speech length: 43129
2024-10-27 18:29:24,072 (beam_search:428) INFO: decoder input length: 33
2024-10-27 18:29:24,072 (beam_search:429) INFO: max output length: 33
2024-10-27 18:29:24,072 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:24,099 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:24,099 (beam_search:476) INFO:  -0.46 * 1.0 =  -0.46 for ctc
2024-10-27 18:29:24,099 (beam_search:479) INFO: total log probability: -0.46
2024-10-27 18:29:24,099 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:29:24,099 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:24,099 (beam_search:483) INFO: best hypo: ▁THAT▁MAKES▁THE▁MOTOR

2024-10-27 18:29:24,101 (asr_inference:509) INFO: speech length: 410980
2024-10-27 18:29:43,351 (beam_search:428) INFO: decoder input length: 320
2024-10-27 18:29:43,351 (beam_search:429) INFO: max output length: 320
2024-10-27 18:29:43,351 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:46,076 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:46,076 (beam_search:476) INFO:  -8.77 * 1.0 =  -8.77 for ctc
2024-10-27 18:29:46,076 (beam_search:479) INFO: total log probability: -8.77
2024-10-27 18:29:46,076 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:29:46,076 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:46,076 (beam_search:483) INFO: best hypo: ▁THIS▁IS▁SHOWING▁THAT▁THE▁THE▁BATTERY▁THE▁BATTERY▁TO▁A▁WIRE▁AND▁IT▁GOES▁TO▁THE▁MOTOR▁AND▁THEN▁THE▁MOTOR▁HAS▁A▁THAT▁TO▁THE▁NEGATIVE▁AND▁THEN▁THE▁THE▁ENERGY▁OR▁ELECTRICITY▁TO▁THE▁MOTOR▁AND▁THE▁MOTOR▁STARTS▁RUNNING

2024-10-27 18:29:46,080 (asr_inference:509) INFO: speech length: 32500
2024-10-27 18:29:47,403 (beam_search:428) INFO: decoder input length: 24
2024-10-27 18:29:47,404 (beam_search:429) INFO: max output length: 24
2024-10-27 18:29:47,404 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:47,423 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:47,423 (beam_search:476) INFO:  -0.14 * 1.0 =  -0.14 for ctc
2024-10-27 18:29:47,423 (beam_search:479) INFO: total log probability: -0.14
2024-10-27 18:29:47,423 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:29:47,423 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:47,423 (beam_search:483) INFO: best hypo: ▁GOOD▁HOW▁ARE▁YOU

2024-10-27 18:29:47,425 (asr_inference:509) INFO: speech length: 152422
2024-10-27 18:29:52,908 (beam_search:428) INFO: decoder input length: 118
2024-10-27 18:29:52,908 (beam_search:429) INFO: max output length: 118
2024-10-27 18:29:52,908 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:53,104 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:53,104 (beam_search:476) INFO:  -1.96 * 1.0 =  -1.96 for ctc
2024-10-27 18:29:53,104 (beam_search:479) INFO: total log probability: -1.96
2024-10-27 18:29:53,104 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:29:53,104 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:53,104 (beam_search:483) INFO: best hypo: ▁WE'VE▁BEEN▁WITH▁AND▁AND▁SOLAR

2024-10-27 18:29:53,107 (asr_inference:509) INFO: speech length: 228009
2024-10-27 18:30:02,257 (beam_search:428) INFO: decoder input length: 177
2024-10-27 18:30:02,257 (beam_search:429) INFO: max output length: 177
2024-10-27 18:30:02,257 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:02,676 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:02,676 (beam_search:476) INFO:  -1.71 * 1.0 =  -1.71 for ctc
2024-10-27 18:30:02,676 (beam_search:479) INFO: total log probability: -1.71
2024-10-27 18:30:02,676 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:30:02,676 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:02,676 (beam_search:483) INFO: best hypo: ▁WE▁AND▁WE▁WE▁USED▁SOLAR▁TO▁TO▁MAKE▁A▁RUN

2024-10-27 18:30:02,679 (asr_inference:509) INFO: speech length: 73578
2024-10-27 18:30:05,479 (beam_search:428) INFO: decoder input length: 56
2024-10-27 18:30:05,479 (beam_search:429) INFO: max output length: 56
2024-10-27 18:30:05,479 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:05,506 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:05,507 (beam_search:476) INFO:  -0.39 * 1.0 =  -0.39 for ctc
2024-10-27 18:30:05,507 (beam_search:479) INFO: total log probability: -0.39
2024-10-27 18:30:05,507 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:30:05,507 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:05,507 (beam_search:483) INFO: best hypo: ▁THE▁IS

2024-10-27 18:30:05,509 (asr_inference:509) INFO: speech length: 354497
2024-10-27 18:30:21,465 (beam_search:428) INFO: decoder input length: 276
2024-10-27 18:30:21,465 (beam_search:429) INFO: max output length: 276
2024-10-27 18:30:21,465 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:22,355 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:22,355 (beam_search:476) INFO:  -4.63 * 1.0 =  -4.63 for ctc
2024-10-27 18:30:22,355 (beam_search:479) INFO: total log probability: -4.63
2024-10-27 18:30:22,355 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:30:22,355 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:22,355 (beam_search:483) INFO: best hypo: ▁THERE'S▁A▁NAIL▁AND▁TWO▁WIRES▁THERE'S▁A▁NAIL▁A▁AND▁A▁WIRE

2024-10-27 18:30:22,358 (asr_inference:509) INFO: speech length: 196085
2024-10-27 18:30:30,178 (beam_search:428) INFO: decoder input length: 152
2024-10-27 18:30:30,178 (beam_search:429) INFO: max output length: 152
2024-10-27 18:30:30,178 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:30,516 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:30,516 (beam_search:476) INFO:  -2.52 * 1.0 =  -2.52 for ctc
2024-10-27 18:30:30,516 (beam_search:479) INFO: total log probability: -2.52
2024-10-27 18:30:30,516 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:30:30,516 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:30,516 (beam_search:483) INFO: best hypo: ▁THERE▁IS▁A▁WIRE▁A▁SWITCH▁A▁NAIL▁A▁MOTOR▁AND▁A

2024-10-27 18:30:30,520 (asr_inference:509) INFO: speech length: 141262
2024-10-27 18:30:35,832 (beam_search:428) INFO: decoder input length: 109
2024-10-27 18:30:35,832 (beam_search:429) INFO: max output length: 109
2024-10-27 18:30:35,832 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:35,975 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:35,976 (beam_search:476) INFO:  -2.64 * 1.0 =  -2.64 for ctc
2024-10-27 18:30:35,976 (beam_search:479) INFO: total log probability: -2.64
2024-10-27 18:30:35,976 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:30:35,976 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:35,976 (beam_search:483) INFO: best hypo: ▁THE▁MOTORS▁WHEN▁THE▁THE▁OPEN

2024-10-27 18:30:35,978 (asr_inference:509) INFO: speech length: 88939
2024-10-27 18:30:39,254 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:30:39,255 (beam_search:429) INFO: max output length: 68
2024-10-27 18:30:39,255 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:39,360 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:39,360 (beam_search:476) INFO:  -1.75 * 1.0 =  -1.75 for ctc
2024-10-27 18:30:39,360 (beam_search:479) INFO: total log probability: -1.75
2024-10-27 18:30:39,360 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:30:39,360 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:39,360 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁IT'S▁BECAUSE▁THE▁IS

2024-10-27 18:30:39,362 (asr_inference:509) INFO: speech length: 116617
2024-10-27 18:30:43,603 (beam_search:428) INFO: decoder input length: 90
2024-10-27 18:30:43,603 (beam_search:429) INFO: max output length: 90
2024-10-27 18:30:43,603 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:43,827 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:43,827 (beam_search:476) INFO:  -1.85 * 1.0 =  -1.85 for ctc
2024-10-27 18:30:43,827 (beam_search:479) INFO: total log probability: -1.85
2024-10-27 18:30:43,827 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:30:43,827 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:43,827 (beam_search:483) INFO: best hypo: ▁THE▁IS▁THERE▁AND▁IT'S▁NOT▁METAL▁SO▁THE▁MOTOR▁IS▁NOT

2024-10-27 18:30:43,830 (asr_inference:509) INFO: speech length: 263976
2024-10-27 18:30:54,699 (beam_search:428) INFO: decoder input length: 205
2024-10-27 18:30:54,699 (beam_search:429) INFO: max output length: 205
2024-10-27 18:30:54,699 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:55,234 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:55,234 (beam_search:476) INFO:  -3.99 * 1.0 =  -3.99 for ctc
2024-10-27 18:30:55,234 (beam_search:479) INFO: total log probability: -3.99
2024-10-27 18:30:55,234 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:30:55,234 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:55,234 (beam_search:483) INFO: best hypo: ▁THE▁IS▁AN▁INSULATOR▁IS▁A▁THING▁THAT▁CAN▁LIKE▁A▁THAT▁IS▁WOOD

2024-10-27 18:30:55,236 (asr_inference:509) INFO: speech length: 134736
2024-10-27 18:31:00,386 (beam_search:428) INFO: decoder input length: 104
2024-10-27 18:31:00,386 (beam_search:429) INFO: max output length: 104
2024-10-27 18:31:00,386 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:00,525 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:00,525 (beam_search:476) INFO:  -2.64 * 1.0 =  -2.64 for ctc
2024-10-27 18:31:00,525 (beam_search:479) INFO: total log probability: -2.64
2024-10-27 18:31:00,525 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:31:00,525 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:00,525 (beam_search:483) INFO: best hypo: ▁INSULATORS▁CAN▁AND▁THEY▁ARE▁NOT▁OF

2024-10-27 18:31:00,527 (asr_inference:509) INFO: speech length: 109390
2024-10-27 18:31:04,474 (beam_search:428) INFO: decoder input length: 84
2024-10-27 18:31:04,475 (beam_search:429) INFO: max output length: 84
2024-10-27 18:31:04,475 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:04,607 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:04,607 (beam_search:476) INFO:  -1.46 * 1.0 =  -1.46 for ctc
2024-10-27 18:31:04,607 (beam_search:479) INFO: total log probability: -1.46
2024-10-27 18:31:04,607 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:31:04,607 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:04,607 (beam_search:483) INFO: best hypo: ▁THE▁INSULATORS▁ARE▁NOT▁METAL▁BUT▁THE▁ARE

2024-10-27 18:31:04,610 (asr_inference:509) INFO: speech length: 132973
2024-10-27 18:31:09,445 (beam_search:428) INFO: decoder input length: 103
2024-10-27 18:31:09,445 (beam_search:429) INFO: max output length: 103
2024-10-27 18:31:09,445 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:09,615 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:09,615 (beam_search:476) INFO:  -3.90 * 1.0 =  -3.90 for ctc
2024-10-27 18:31:09,615 (beam_search:479) INFO: total log probability: -3.90
2024-10-27 18:31:09,615 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:31:09,615 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:09,615 (beam_search:483) INFO: best hypo: ▁THE▁ARE▁NOT▁AND▁THEYT▁NOT▁CONDUCT▁ELECTRICITY

2024-10-27 18:31:09,617 (asr_inference:509) INFO: speech length: 85115
2024-10-27 18:31:12,624 (beam_search:428) INFO: decoder input length: 65
2024-10-27 18:31:12,624 (beam_search:429) INFO: max output length: 65
2024-10-27 18:31:12,624 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:12,655 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:12,656 (beam_search:476) INFO:  -0.74 * 1.0 =  -0.74 for ctc
2024-10-27 18:31:12,656 (beam_search:479) INFO: total log probability: -0.74
2024-10-27 18:31:12,656 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:31:12,656 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:12,656 (beam_search:483) INFO: best hypo: ▁THE▁MOTOR

2024-10-27 18:31:12,658 (asr_inference:509) INFO: speech length: 74507
2024-10-27 18:31:15,366 (beam_search:428) INFO: decoder input length: 57
2024-10-27 18:31:15,366 (beam_search:429) INFO: max output length: 57
2024-10-27 18:31:15,366 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:15,443 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:15,443 (beam_search:476) INFO:  -1.31 * 1.0 =  -1.31 for ctc
2024-10-27 18:31:15,443 (beam_search:479) INFO: total log probability: -1.31
2024-10-27 18:31:15,443 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:31:15,443 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:15,443 (beam_search:483) INFO: best hypo: ▁I▁WOULD▁SAY▁IT▁IS▁I▁IS▁OPEN

2024-10-27 18:31:15,446 (asr_inference:509) INFO: speech length: 59113
2024-10-27 18:31:17,792 (beam_search:428) INFO: decoder input length: 45
2024-10-27 18:31:17,792 (beam_search:429) INFO: max output length: 45
2024-10-27 18:31:17,792 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:17,816 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:17,817 (beam_search:476) INFO:  -0.10 * 1.0 =  -0.10 for ctc
2024-10-27 18:31:17,817 (beam_search:479) INFO: total log probability: -0.10
2024-10-27 18:31:17,817 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:31:17,817 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:17,817 (beam_search:483) INFO: best hypo: ▁IT▁IS

2024-10-27 18:31:17,819 (asr_inference:509) INFO: speech length: 85173
2024-10-27 18:31:20,875 (beam_search:428) INFO: decoder input length: 66
2024-10-27 18:31:20,875 (beam_search:429) INFO: max output length: 66
2024-10-27 18:31:20,875 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:20,907 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:20,907 (beam_search:476) INFO:  -0.53 * 1.0 =  -0.53 for ctc
2024-10-27 18:31:20,907 (beam_search:479) INFO: total log probability: -0.53
2024-10-27 18:31:20,907 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:31:20,907 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:20,907 (beam_search:483) INFO: best hypo: ▁ARE▁METAL

2024-10-27 18:31:20,909 (asr_inference:509) INFO: speech length: 39426
2024-10-27 18:31:22,414 (beam_search:428) INFO: decoder input length: 30
2024-10-27 18:31:22,414 (beam_search:429) INFO: max output length: 30
2024-10-27 18:31:22,414 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:22,430 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:22,430 (beam_search:476) INFO:  -1.07 * 1.0 =  -1.07 for ctc
2024-10-27 18:31:22,430 (beam_search:479) INFO: total log probability: -1.07
2024-10-27 18:31:22,430 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:31:22,430 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:22,431 (beam_search:483) INFO: best hypo: ▁ARE▁A

2024-10-27 18:31:22,433 (asr_inference:509) INFO: speech length: 51000
2024-10-27 18:31:24,395 (beam_search:428) INFO: decoder input length: 39
2024-10-27 18:31:24,395 (beam_search:429) INFO: max output length: 39
2024-10-27 18:31:24,395 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:24,421 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:24,421 (beam_search:476) INFO:  -0.61 * 1.0 =  -0.61 for ctc
2024-10-27 18:31:24,421 (beam_search:479) INFO: total log probability: -0.61
2024-10-27 18:31:24,421 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:31:24,421 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:24,421 (beam_search:483) INFO: best hypo: ▁HOW▁ARE▁YOU

2024-10-27 18:31:24,424 (asr_inference:509) INFO: speech length: 127242
2024-10-27 18:31:29,156 (beam_search:428) INFO: decoder input length: 98
2024-10-27 18:31:29,156 (beam_search:429) INFO: max output length: 98
2024-10-27 18:31:29,156 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:29,220 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:29,220 (beam_search:476) INFO:  -0.98 * 1.0 =  -0.98 for ctc
2024-10-27 18:31:29,220 (beam_search:479) INFO: total log probability: -0.98
2024-10-27 18:31:29,221 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:31:29,221 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:29,221 (beam_search:483) INFO: best hypo: ▁WE▁ABOUT▁CIRCUITS

2024-10-27 18:31:29,224 (asr_inference:509) INFO: speech length: 31397
2024-10-27 18:31:30,541 (beam_search:428) INFO: decoder input length: 24
2024-10-27 18:31:30,541 (beam_search:429) INFO: max output length: 24
2024-10-27 18:31:30,541 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:30,557 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:30,557 (beam_search:476) INFO:  -0.35 * 1.0 =  -0.35 for ctc
2024-10-27 18:31:30,557 (beam_search:479) INFO: total log probability: -0.35
2024-10-27 18:31:30,557 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:31:30,557 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:30,557 (beam_search:483) INFO: best hypo: ▁I▁SEE▁A

2024-10-27 18:31:30,559 (asr_inference:509) INFO: speech length: 61938
2024-10-27 18:31:32,782 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:31:32,782 (beam_search:429) INFO: max output length: 47
2024-10-27 18:31:32,782 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:32,831 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:32,831 (beam_search:476) INFO:  -0.48 * 1.0 =  -0.48 for ctc
2024-10-27 18:31:32,831 (beam_search:479) INFO: total log probability: -0.48
2024-10-27 18:31:32,831 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:31:32,831 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:32,831 (beam_search:483) INFO: best hypo: ▁THINK▁THAT▁THE▁LIGHT▁IS▁IMPORTANT

2024-10-27 18:31:32,833 (asr_inference:509) INFO: speech length: 31735
2024-10-27 18:31:34,185 (beam_search:428) INFO: decoder input length: 24
2024-10-27 18:31:34,185 (beam_search:429) INFO: max output length: 24
2024-10-27 18:31:34,185 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:34,202 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:34,202 (beam_search:476) INFO:  -0.53 * 1.0 =  -0.53 for ctc
2024-10-27 18:31:34,202 (beam_search:479) INFO: total log probability: -0.53
2024-10-27 18:31:34,202 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:31:34,203 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:34,203 (beam_search:483) INFO: best hypo: ▁BECAUSE▁WE▁NEED

2024-10-27 18:31:34,205 (asr_inference:509) INFO: speech length: 63826
2024-10-27 18:31:36,646 (beam_search:428) INFO: decoder input length: 49
2024-10-27 18:31:36,646 (beam_search:429) INFO: max output length: 49
2024-10-27 18:31:36,646 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:36,688 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:36,688 (beam_search:476) INFO:  -0.95 * 1.0 =  -0.95 for ctc
2024-10-27 18:31:36,688 (beam_search:479) INFO: total log probability: -0.95
2024-10-27 18:31:36,688 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:31:36,688 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:36,688 (beam_search:483) INFO: best hypo: ▁I▁THAT▁THE▁THE▁IS

2024-10-27 18:31:36,690 (asr_inference:509) INFO: speech length: 82379
2024-10-27 18:31:39,604 (beam_search:428) INFO: decoder input length: 63
2024-10-27 18:31:39,604 (beam_search:429) INFO: max output length: 63
2024-10-27 18:31:39,604 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:39,667 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:39,667 (beam_search:476) INFO:  -1.05 * 1.0 =  -1.05 for ctc
2024-10-27 18:31:39,667 (beam_search:479) INFO: total log probability: -1.05
2024-10-27 18:31:39,667 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:31:39,667 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:39,667 (beam_search:483) INFO: best hypo: ▁I▁SEE▁THAT▁THE▁IS

2024-10-27 18:31:39,671 (asr_inference:509) INFO: speech length: 47721
2024-10-27 18:31:41,514 (beam_search:428) INFO: decoder input length: 36
2024-10-27 18:31:41,514 (beam_search:429) INFO: max output length: 36
2024-10-27 18:31:41,514 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:41,548 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:41,548 (beam_search:476) INFO:  -0.69 * 1.0 =  -0.69 for ctc
2024-10-27 18:31:41,548 (beam_search:479) INFO: total log probability: -0.69
2024-10-27 18:31:41,548 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:31:41,548 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:41,548 (beam_search:483) INFO: best hypo: ▁I▁SEE▁THAT▁THE▁IS

2024-10-27 18:31:41,551 (asr_inference:509) INFO: speech length: 54555
2024-10-27 18:31:43,683 (beam_search:428) INFO: decoder input length: 42
2024-10-27 18:31:43,683 (beam_search:429) INFO: max output length: 42
2024-10-27 18:31:43,683 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:43,710 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:43,711 (beam_search:476) INFO:  -0.05 * 1.0 =  -0.05 for ctc
2024-10-27 18:31:43,711 (beam_search:479) INFO: total log probability: -0.05
2024-10-27 18:31:43,711 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 18:31:43,711 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:43,711 (beam_search:483) INFO: best hypo: ▁I▁SEE▁A

2024-10-27 18:31:43,714 (asr_inference:509) INFO: speech length: 57166
2024-10-27 18:31:45,856 (beam_search:428) INFO: decoder input length: 44
2024-10-27 18:31:45,857 (beam_search:429) INFO: max output length: 44
2024-10-27 18:31:45,857 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:45,884 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:45,884 (beam_search:476) INFO:  -0.75 * 1.0 =  -0.75 for ctc
2024-10-27 18:31:45,884 (beam_search:479) INFO: total log probability: -0.75
2024-10-27 18:31:45,884 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:31:45,884 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:45,884 (beam_search:483) INFO: best hypo: ▁IT▁IS▁THE

2024-10-27 18:31:45,886 (asr_inference:509) INFO: speech length: 94002
2024-10-27 18:31:49,211 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:31:49,211 (beam_search:429) INFO: max output length: 72
2024-10-27 18:31:49,211 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:49,236 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:49,236 (beam_search:476) INFO:  -0.13 * 1.0 =  -0.13 for ctc
2024-10-27 18:31:49,236 (beam_search:479) INFO: total log probability: -0.13
2024-10-27 18:31:49,236 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 18:31:49,236 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:49,236 (beam_search:483) INFO: best hypo: ▁IS

2024-10-27 18:31:49,239 (asr_inference:509) INFO: speech length: 41116
2024-10-27 18:31:50,841 (beam_search:428) INFO: decoder input length: 31
2024-10-27 18:31:50,841 (beam_search:429) INFO: max output length: 31
2024-10-27 18:31:50,841 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:50,858 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:50,858 (beam_search:476) INFO:  -0.49 * 1.0 =  -0.49 for ctc
2024-10-27 18:31:50,858 (beam_search:479) INFO: total log probability: -0.49
2024-10-27 18:31:50,858 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:31:50,858 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:50,858 (beam_search:483) INFO: best hypo: ▁IS▁AND

2024-10-27 18:31:50,861 (asr_inference:509) INFO: speech length: 58059
2024-10-27 18:31:53,224 (beam_search:428) INFO: decoder input length: 44
2024-10-27 18:31:53,225 (beam_search:429) INFO: max output length: 44
2024-10-27 18:31:53,225 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:53,247 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:53,248 (beam_search:476) INFO:  -0.81 * 1.0 =  -0.81 for ctc
2024-10-27 18:31:53,248 (beam_search:479) INFO: total log probability: -0.81
2024-10-27 18:31:53,248 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:31:53,248 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:53,248 (beam_search:483) INFO: best hypo: ▁I▁CAN

2024-10-27 18:31:53,250 (asr_inference:509) INFO: speech length: 133144
2024-10-27 18:31:58,148 (beam_search:428) INFO: decoder input length: 103
2024-10-27 18:31:58,148 (beam_search:429) INFO: max output length: 103
2024-10-27 18:31:58,148 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:58,386 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:58,386 (beam_search:476) INFO:  -3.43 * 1.0 =  -3.43 for ctc
2024-10-27 18:31:58,386 (beam_search:479) INFO: total log probability: -3.43
2024-10-27 18:31:58,386 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:31:58,386 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:58,386 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁THE▁IS▁GETTING▁ENERGY▁FROM▁THE▁SHE'S▁TO

2024-10-27 18:31:58,389 (asr_inference:509) INFO: speech length: 108261
2024-10-27 18:32:02,355 (beam_search:428) INFO: decoder input length: 84
2024-10-27 18:32:02,355 (beam_search:429) INFO: max output length: 84
2024-10-27 18:32:02,355 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:02,465 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:02,465 (beam_search:476) INFO:  -2.18 * 1.0 =  -2.18 for ctc
2024-10-27 18:32:02,465 (beam_search:479) INFO: total log probability: -2.18
2024-10-27 18:32:02,465 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:32:02,465 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:02,465 (beam_search:483) INFO: best hypo: ▁THE▁IS▁THAT▁IS▁HAS▁ENERGY▁TO

2024-10-27 18:32:02,469 (asr_inference:509) INFO: speech length: 172272
2024-10-27 18:32:09,162 (beam_search:428) INFO: decoder input length: 134
2024-10-27 18:32:09,162 (beam_search:429) INFO: max output length: 134
2024-10-27 18:32:09,162 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:09,443 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:09,443 (beam_search:476) INFO:  -3.09 * 1.0 =  -3.09 for ctc
2024-10-27 18:32:09,443 (beam_search:479) INFO: total log probability: -3.09
2024-10-27 18:32:09,443 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:32:09,443 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:09,444 (beam_search:483) INFO: best hypo: ▁I▁A▁A▁AND▁I▁DON'T▁KNOW▁WHAT▁OTHER

2024-10-27 18:32:09,446 (asr_inference:509) INFO: speech length: 50067
2024-10-27 18:32:11,351 (beam_search:428) INFO: decoder input length: 38
2024-10-27 18:32:11,351 (beam_search:429) INFO: max output length: 38
2024-10-27 18:32:11,351 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:11,380 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:11,380 (beam_search:476) INFO:  -1.20 * 1.0 =  -1.20 for ctc
2024-10-27 18:32:11,380 (beam_search:479) INFO: total log probability: -1.20
2024-10-27 18:32:11,380 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:32:11,381 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:11,381 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁IS

2024-10-27 18:32:11,383 (asr_inference:509) INFO: speech length: 53417
2024-10-27 18:32:13,373 (beam_search:428) INFO: decoder input length: 41
2024-10-27 18:32:13,373 (beam_search:429) INFO: max output length: 41
2024-10-27 18:32:13,373 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:13,389 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:13,389 (beam_search:476) INFO:  -0.55 * 1.0 =  -0.55 for ctc
2024-10-27 18:32:13,389 (beam_search:479) INFO: total log probability: -0.55
2024-10-27 18:32:13,389 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:32:13,389 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:13,389 (beam_search:483) INFO: best hypo: ▁THE

2024-10-27 18:32:13,392 (asr_inference:509) INFO: speech length: 49328
2024-10-27 18:32:15,289 (beam_search:428) INFO: decoder input length: 38
2024-10-27 18:32:15,289 (beam_search:429) INFO: max output length: 38
2024-10-27 18:32:15,289 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:15,304 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:15,304 (beam_search:476) INFO:  -0.93 * 1.0 =  -0.93 for ctc
2024-10-27 18:32:15,304 (beam_search:479) INFO: total log probability: -0.93
2024-10-27 18:32:15,304 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 18:32:15,304 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:15,304 (beam_search:483) INFO: best hypo: ▁IT

2024-10-27 18:32:15,307 (asr_inference:509) INFO: speech length: 23568
2024-10-27 18:32:16,314 (beam_search:428) INFO: decoder input length: 17
2024-10-27 18:32:16,314 (beam_search:429) INFO: max output length: 17
2024-10-27 18:32:16,314 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:16,332 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:16,332 (beam_search:476) INFO:  -0.02 * 1.0 =  -0.02 for ctc
2024-10-27 18:32:16,332 (beam_search:479) INFO: total log probability: -0.02
2024-10-27 18:32:16,332 (beam_search:480) INFO: normalized log probability: -0.00
2024-10-27 18:32:16,332 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:16,332 (beam_search:483) INFO: best hypo: ▁I▁DON'T▁KNOW

2024-10-27 18:32:16,334 (asr_inference:509) INFO: speech length: 104561
2024-10-27 18:32:20,062 (beam_search:428) INFO: decoder input length: 81
2024-10-27 18:32:20,062 (beam_search:429) INFO: max output length: 81
2024-10-27 18:32:20,062 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:20,117 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:20,117 (beam_search:476) INFO:  -1.77 * 1.0 =  -1.77 for ctc
2024-10-27 18:32:20,117 (beam_search:479) INFO: total log probability: -1.77
2024-10-27 18:32:20,117 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:32:20,117 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:20,117 (beam_search:483) INFO: best hypo: ▁THELAR▁THE

2024-10-27 18:32:20,120 (asr_inference:509) INFO: speech length: 61189
2024-10-27 18:32:22,427 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:32:22,428 (beam_search:429) INFO: max output length: 47
2024-10-27 18:32:22,428 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:22,454 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:22,454 (beam_search:476) INFO:  -0.55 * 1.0 =  -0.55 for ctc
2024-10-27 18:32:22,454 (beam_search:479) INFO: total log probability: -0.55
2024-10-27 18:32:22,454 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:32:22,454 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:22,454 (beam_search:483) INFO: best hypo: ▁THEY▁ARE

2024-10-27 18:32:22,457 (asr_inference:509) INFO: speech length: 27836
2024-10-27 18:32:23,771 (beam_search:428) INFO: decoder input length: 21
2024-10-27 18:32:23,771 (beam_search:429) INFO: max output length: 21
2024-10-27 18:32:23,771 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:23,795 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:23,795 (beam_search:476) INFO:  -0.04 * 1.0 =  -0.04 for ctc
2024-10-27 18:32:23,795 (beam_search:479) INFO: total log probability: -0.04
2024-10-27 18:32:23,795 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 18:32:23,795 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:23,795 (beam_search:483) INFO: best hypo: ▁I▁DON'T▁KNOW

2024-10-27 18:32:23,797 (asr_inference:509) INFO: speech length: 61616
2024-10-27 18:32:26,046 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:32:26,046 (beam_search:429) INFO: max output length: 47
2024-10-27 18:32:26,046 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:26,082 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:26,083 (beam_search:476) INFO:  -2.38 * 1.0 =  -2.38 for ctc
2024-10-27 18:32:26,083 (beam_search:479) INFO: total log probability: -2.38
2024-10-27 18:32:26,083 (beam_search:480) INFO: normalized log probability: -0.40
2024-10-27 18:32:26,083 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:26,083 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁BE

2024-10-27 18:32:26,085 (asr_inference:509) INFO: speech length: 68100
2024-10-27 18:32:28,549 (beam_search:428) INFO: decoder input length: 52
2024-10-27 18:32:28,549 (beam_search:429) INFO: max output length: 52
2024-10-27 18:32:28,549 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:28,584 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:28,584 (beam_search:476) INFO:  -0.55 * 1.0 =  -0.55 for ctc
2024-10-27 18:32:28,584 (beam_search:479) INFO: total log probability: -0.55
2024-10-27 18:32:28,584 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:32:28,584 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:28,584 (beam_search:483) INFO: best hypo: ▁THE▁THE▁AIR

2024-10-27 18:32:28,586 (asr_inference:509) INFO: speech length: 47233
2024-10-27 18:32:30,428 (beam_search:428) INFO: decoder input length: 36
2024-10-27 18:32:30,428 (beam_search:429) INFO: max output length: 36
2024-10-27 18:32:30,428 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:30,451 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:30,451 (beam_search:476) INFO:  -1.09 * 1.0 =  -1.09 for ctc
2024-10-27 18:32:30,451 (beam_search:479) INFO: total log probability: -1.09
2024-10-27 18:32:30,451 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:32:30,451 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:30,451 (beam_search:483) INFO: best hypo: ▁AND▁THE▁AIR

2024-10-27 18:32:30,453 (asr_inference:509) INFO: speech length: 40358
2024-10-27 18:32:31,948 (beam_search:428) INFO: decoder input length: 31
2024-10-27 18:32:31,948 (beam_search:429) INFO: max output length: 31
2024-10-27 18:32:31,948 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:31,976 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:31,977 (beam_search:476) INFO:  -0.02 * 1.0 =  -0.02 for ctc
2024-10-27 18:32:31,977 (beam_search:479) INFO: total log probability: -0.02
2024-10-27 18:32:31,977 (beam_search:480) INFO: normalized log probability: -0.00
2024-10-27 18:32:31,977 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:31,977 (beam_search:483) INFO: best hypo: ▁I▁DON'T▁KNOW

2024-10-27 18:32:31,979 (asr_inference:509) INFO: speech length: 47000
2024-10-27 18:32:33,783 (beam_search:428) INFO: decoder input length: 36
2024-10-27 18:32:33,783 (beam_search:429) INFO: max output length: 36
2024-10-27 18:32:33,783 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:33,817 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:33,817 (beam_search:476) INFO:  -0.62 * 1.0 =  -0.62 for ctc
2024-10-27 18:32:33,817 (beam_search:479) INFO: total log probability: -0.62
2024-10-27 18:32:33,817 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:32:33,817 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:33,817 (beam_search:483) INFO: best hypo: ▁I▁GOOD▁HOW▁ARE▁YOU

2024-10-27 18:32:33,819 (asr_inference:509) INFO: speech length: 139131
2024-10-27 18:32:38,924 (beam_search:428) INFO: decoder input length: 108
2024-10-27 18:32:38,924 (beam_search:429) INFO: max output length: 108
2024-10-27 18:32:38,924 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:39,123 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:39,123 (beam_search:476) INFO:  -4.01 * 1.0 =  -4.01 for ctc
2024-10-27 18:32:39,123 (beam_search:479) INFO: total log probability: -4.01
2024-10-27 18:32:39,123 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 18:32:39,123 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:39,123 (beam_search:483) INFO: best hypo: ▁WE'VE▁BEEN▁WE▁BEEN▁LEARNING▁ABOUT▁AND▁PARALLEL

2024-10-27 18:32:39,125 (asr_inference:509) INFO: speech length: 353084
2024-10-27 18:32:55,317 (beam_search:428) INFO: decoder input length: 275
2024-10-27 18:32:55,317 (beam_search:429) INFO: max output length: 275
2024-10-27 18:32:55,317 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:55,501 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:55,501 (beam_search:476) INFO:  -1.25 * 1.0 =  -1.25 for ctc
2024-10-27 18:32:55,501 (beam_search:479) INFO: total log probability: -1.25
2024-10-27 18:32:55,501 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:32:55,501 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:55,501 (beam_search:483) INFO: best hypo: ▁A▁IS▁SOMETHING

2024-10-27 18:32:55,504 (asr_inference:509) INFO: speech length: 152622
2024-10-27 18:33:01,286 (beam_search:428) INFO: decoder input length: 118
2024-10-27 18:33:01,287 (beam_search:429) INFO: max output length: 118
2024-10-27 18:33:01,287 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:01,427 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:01,427 (beam_search:476) INFO:  -0.34 * 1.0 =  -0.34 for ctc
2024-10-27 18:33:01,427 (beam_search:479) INFO: total log probability: -0.34
2024-10-27 18:33:01,427 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 18:33:01,427 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:01,427 (beam_search:483) INFO: best hypo: ▁THE▁ELECTRICITY▁IS▁AND▁IT▁IS

2024-10-27 18:33:01,429 (asr_inference:509) INFO: speech length: 107084
2024-10-27 18:33:05,481 (beam_search:428) INFO: decoder input length: 83
2024-10-27 18:33:05,481 (beam_search:429) INFO: max output length: 83
2024-10-27 18:33:05,481 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:05,610 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:05,611 (beam_search:476) INFO:  -1.04 * 1.0 =  -1.04 for ctc
2024-10-27 18:33:05,611 (beam_search:479) INFO: total log probability: -1.04
2024-10-27 18:33:05,611 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:33:05,611 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:05,611 (beam_search:483) INFO: best hypo: ▁THE▁ELECTRICITY▁HAS▁TO▁AND▁IT▁CAN'T

2024-10-27 18:33:05,613 (asr_inference:509) INFO: speech length: 62858
2024-10-27 18:33:07,975 (beam_search:428) INFO: decoder input length: 48
2024-10-27 18:33:07,975 (beam_search:429) INFO: max output length: 48
2024-10-27 18:33:07,975 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:08,007 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:08,008 (beam_search:476) INFO:  -0.36 * 1.0 =  -0.36 for ctc
2024-10-27 18:33:08,008 (beam_search:479) INFO: total log probability: -0.36
2024-10-27 18:33:08,008 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:33:08,008 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:08,008 (beam_search:483) INFO: best hypo: ▁ELECTRICITY▁IN▁A

2024-10-27 18:33:08,011 (asr_inference:509) INFO: speech length: 198827
2024-10-27 18:33:15,765 (beam_search:428) INFO: decoder input length: 154
2024-10-27 18:33:15,765 (beam_search:429) INFO: max output length: 154
2024-10-27 18:33:15,765 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:16,194 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:16,194 (beam_search:476) INFO:  -4.07 * 1.0 =  -4.07 for ctc
2024-10-27 18:33:16,194 (beam_search:479) INFO: total log probability: -4.07
2024-10-27 18:33:16,194 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:33:16,194 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:16,195 (beam_search:483) INFO: best hypo: ▁THE▁HASD▁ENERGY▁THAT▁CAN▁THE▁BULB▁AND▁THE▁WIRES▁THE▁THE▁TO▁THE▁BULB

2024-10-27 18:33:16,197 (asr_inference:509) INFO: speech length: 230042
2024-10-27 18:33:25,840 (beam_search:428) INFO: decoder input length: 179
2024-10-27 18:33:25,840 (beam_search:429) INFO: max output length: 179
2024-10-27 18:33:25,840 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:26,351 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:26,351 (beam_search:476) INFO:  -3.10 * 1.0 =  -3.10 for ctc
2024-10-27 18:33:26,351 (beam_search:479) INFO: total log probability: -3.10
2024-10-27 18:33:26,351 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:33:26,351 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:26,351 (beam_search:483) INFO: best hypo: ▁THE▁THE▁WIRES▁MAKE▁A▁FROM▁THE▁THE▁CIRCUIT▁THE▁WIRES▁MAKE▁A▁THE▁TO▁THE

2024-10-27 18:33:26,353 (asr_inference:509) INFO: speech length: 59610
2024-10-27 18:33:28,570 (beam_search:428) INFO: decoder input length: 46
2024-10-27 18:33:28,570 (beam_search:429) INFO: max output length: 46
2024-10-27 18:33:28,571 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:28,607 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:28,607 (beam_search:476) INFO:  -1.10 * 1.0 =  -1.10 for ctc
2024-10-27 18:33:28,607 (beam_search:479) INFO: total log probability: -1.10
2024-10-27 18:33:28,607 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:33:28,607 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:28,607 (beam_search:483) INFO: best hypo: ▁OF▁ELECTRICITY▁GOES▁TO

2024-10-27 18:33:28,610 (asr_inference:509) INFO: speech length: 133858
2024-10-27 18:33:33,583 (beam_search:428) INFO: decoder input length: 104
2024-10-27 18:33:33,583 (beam_search:429) INFO: max output length: 104
2024-10-27 18:33:33,583 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:33,790 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:33,790 (beam_search:476) INFO:  -1.65 * 1.0 =  -1.65 for ctc
2024-10-27 18:33:33,790 (beam_search:479) INFO: total log probability: -1.65
2024-10-27 18:33:33,790 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:33:33,790 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:33,790 (beam_search:483) INFO: best hypo: ▁ELECTRICITY▁IS▁FLOWING▁THROUGH▁THE▁WIRES▁THE▁AND▁THE▁LIGHT▁BULB

2024-10-27 18:33:33,793 (asr_inference:509) INFO: speech length: 387438
2024-10-27 18:33:52,002 (beam_search:428) INFO: decoder input length: 302
2024-10-27 18:33:52,003 (beam_search:429) INFO: max output length: 302
2024-10-27 18:33:52,003 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:54,523 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:54,523 (beam_search:476) INFO:  -5.95 * 1.0 =  -5.95 for ctc
2024-10-27 18:33:54,523 (beam_search:479) INFO: total log probability: -5.95
2024-10-27 18:33:54,523 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:33:54,523 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:54,523 (beam_search:483) INFO: best hypo: ▁THE▁THE▁ELECTRICITY▁GOES▁THROUGH▁THE▁D▁AND▁THEN▁THROUGH▁THE▁WIRES▁TO▁THE▁LIGHT▁TO▁THE▁TO▁THE▁LIGHT▁BULB▁AND▁GOES▁BACK▁TO▁THE▁WIRES▁AND▁INTO▁THE▁SECOND▁LIGHT▁BULB▁AND▁GOES▁TO▁THE▁AND▁THEN▁TO▁THE▁D

2024-10-27 18:33:54,526 (asr_inference:509) INFO: speech length: 120018
2024-10-27 18:33:58,868 (beam_search:428) INFO: decoder input length: 93
2024-10-27 18:33:58,868 (beam_search:429) INFO: max output length: 93
2024-10-27 18:33:58,868 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:59,052 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:59,053 (beam_search:476) INFO:  -1.91 * 1.0 =  -1.91 for ctc
2024-10-27 18:33:59,053 (beam_search:479) INFO: total log probability: -1.91
2024-10-27 18:33:59,053 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:33:59,053 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:59,053 (beam_search:483) INFO: best hypo: ▁THE▁BLUE▁DOTS▁ARE▁ELECTRICITY▁IT'S▁HOW▁THE▁ELECTRICITY

2024-10-27 18:33:59,056 (asr_inference:509) INFO: speech length: 221138
2024-10-27 18:34:07,673 (beam_search:428) INFO: decoder input length: 172
2024-10-27 18:34:07,673 (beam_search:429) INFO: max output length: 172
2024-10-27 18:34:07,673 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:08,188 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:08,188 (beam_search:476) INFO:  -2.24 * 1.0 =  -2.24 for ctc
2024-10-27 18:34:08,188 (beam_search:479) INFO: total log probability: -2.24
2024-10-27 18:34:08,188 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:34:08,188 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:08,189 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁THE▁THE▁ELECTRICITY▁AND▁THE▁BLUE▁ARE▁LIKE▁ELECTRICITY▁AND▁THEY▁FLOW▁THROUGH

2024-10-27 18:34:08,192 (asr_inference:509) INFO: speech length: 209069
2024-10-27 18:34:16,405 (beam_search:428) INFO: decoder input length: 162
2024-10-27 18:34:16,405 (beam_search:429) INFO: max output length: 162
2024-10-27 18:34:16,405 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:16,868 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:16,868 (beam_search:476) INFO:  -2.20 * 1.0 =  -2.20 for ctc
2024-10-27 18:34:16,868 (beam_search:479) INFO: total log probability: -2.20
2024-10-27 18:34:16,868 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:34:16,868 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:16,868 (beam_search:483) INFO: best hypo: ▁THE▁ELECTRICITY▁HAS▁TO▁FLOW▁INTO▁THE▁POSITIVE▁AND▁AND▁OUT▁THE▁NEGATIVE▁ANDT▁THE

2024-10-27 18:34:16,870 (asr_inference:509) INFO: speech length: 60056
2024-10-27 18:34:19,029 (beam_search:428) INFO: decoder input length: 46
2024-10-27 18:34:19,029 (beam_search:429) INFO: max output length: 46
2024-10-27 18:34:19,029 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:19,053 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:19,053 (beam_search:476) INFO:  -1.05 * 1.0 =  -1.05 for ctc
2024-10-27 18:34:19,053 (beam_search:479) INFO: total log probability: -1.05
2024-10-27 18:34:19,053 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:34:19,053 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:19,053 (beam_search:483) INFO: best hypo: ▁THE▁ELECTRICITY

2024-10-27 18:34:19,056 (asr_inference:509) INFO: speech length: 31141
2024-10-27 18:34:20,433 (beam_search:428) INFO: decoder input length: 23
2024-10-27 18:34:20,433 (beam_search:429) INFO: max output length: 23
2024-10-27 18:34:20,433 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:20,443 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:20,443 (beam_search:476) INFO:  -0.54 * 1.0 =  -0.54 for ctc
2024-10-27 18:34:20,443 (beam_search:479) INFO: total log probability: -0.54
2024-10-27 18:34:20,443 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:34:20,443 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:20,443 (beam_search:483) INFO: best hypo: ▁I

2024-10-27 18:34:20,445 (asr_inference:509) INFO: speech length: 113683
2024-10-27 18:34:24,696 (beam_search:428) INFO: decoder input length: 88
2024-10-27 18:34:24,697 (beam_search:429) INFO: max output length: 88
2024-10-27 18:34:24,697 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:24,903 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:24,904 (beam_search:476) INFO:  -2.25 * 1.0 =  -2.25 for ctc
2024-10-27 18:34:24,904 (beam_search:479) INFO: total log probability: -2.25
2024-10-27 18:34:24,904 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:34:24,904 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:24,904 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁THES▁WILL▁GET▁DIMMER▁AND▁WELL▁NEED▁MORE▁WIRES

2024-10-27 18:34:24,906 (asr_inference:509) INFO: speech length: 83228
2024-10-27 18:34:28,039 (beam_search:428) INFO: decoder input length: 64
2024-10-27 18:34:28,039 (beam_search:429) INFO: max output length: 64
2024-10-27 18:34:28,039 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:28,128 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:28,128 (beam_search:476) INFO:  -1.27 * 1.0 =  -1.27 for ctc
2024-10-27 18:34:28,128 (beam_search:479) INFO: total log probability: -1.27
2024-10-27 18:34:28,128 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:34:28,128 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:28,128 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁THES▁WILL▁GET▁BRIGHTER

2024-10-27 18:34:28,130 (asr_inference:509) INFO: speech length: 66743
2024-10-27 18:34:30,535 (beam_search:428) INFO: decoder input length: 51
2024-10-27 18:34:30,536 (beam_search:429) INFO: max output length: 51
2024-10-27 18:34:30,536 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:30,561 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:30,561 (beam_search:476) INFO:  -0.80 * 1.0 =  -0.80 for ctc
2024-10-27 18:34:30,561 (beam_search:479) INFO: total log probability: -0.80
2024-10-27 18:34:30,561 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:34:30,561 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:30,561 (beam_search:483) INFO: best hypo: ▁THES

2024-10-27 18:34:30,563 (asr_inference:509) INFO: speech length: 97116
2024-10-27 18:34:34,076 (beam_search:428) INFO: decoder input length: 75
2024-10-27 18:34:34,076 (beam_search:429) INFO: max output length: 75
2024-10-27 18:34:34,076 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:34,147 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:34,147 (beam_search:476) INFO:  -1.65 * 1.0 =  -1.65 for ctc
2024-10-27 18:34:34,147 (beam_search:479) INFO: total log probability: -1.65
2024-10-27 18:34:34,147 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:34:34,147 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:34,147 (beam_search:483) INFO: best hypo: ▁THAT▁THE▁LIGHTS▁IN

2024-10-27 18:34:34,149 (asr_inference:509) INFO: speech length: 88979
2024-10-27 18:34:37,364 (beam_search:428) INFO: decoder input length: 69
2024-10-27 18:34:37,364 (beam_search:429) INFO: max output length: 69
2024-10-27 18:34:37,364 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:37,410 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:37,410 (beam_search:476) INFO:  -0.09 * 1.0 =  -0.09 for ctc
2024-10-27 18:34:37,410 (beam_search:479) INFO: total log probability: -0.09
2024-10-27 18:34:37,410 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:34:37,410 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:37,410 (beam_search:483) INFO: best hypo: ▁I▁SEE▁THAT

2024-10-27 18:34:37,412 (asr_inference:509) INFO: speech length: 195306
2024-10-27 18:34:45,104 (beam_search:428) INFO: decoder input length: 152
2024-10-27 18:34:45,104 (beam_search:429) INFO: max output length: 152
2024-10-27 18:34:45,104 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:45,643 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:45,643 (beam_search:476) INFO:  -3.17 * 1.0 =  -3.17 for ctc
2024-10-27 18:34:45,643 (beam_search:479) INFO: total log probability: -3.17
2024-10-27 18:34:45,643 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:34:45,643 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:45,643 (beam_search:483) INFO: best hypo: ▁IN▁TWO▁D▁LIGHT▁BULB▁OUT▁OF▁LIGHT▁OUT▁REALLY▁BUT▁WITH▁ONE▁D▁CELL▁THEY▁DON'T

2024-10-27 18:34:45,646 (asr_inference:509) INFO: speech length: 304158
2024-10-27 18:34:59,214 (beam_search:428) INFO: decoder input length: 237
2024-10-27 18:34:59,214 (beam_search:429) INFO: max output length: 237
2024-10-27 18:34:59,214 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:59,984 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:59,984 (beam_search:476) INFO:  -7.73 * 1.0 =  -7.73 for ctc
2024-10-27 18:34:59,984 (beam_search:479) INFO: total log probability: -7.73
2024-10-27 18:34:59,984 (beam_search:480) INFO: normalized log probability: -0.39
2024-10-27 18:34:59,984 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:59,984 (beam_search:483) INFO: best hypo: ▁ELECTRICITY▁ISN'T▁BECAUSE▁THE▁ON▁BULB▁THE▁IS▁GOING▁THE▁THE▁BECAUSE▁THE▁TWO▁ARE▁TOGETHER

2024-10-27 18:34:59,987 (asr_inference:509) INFO: speech length: 71470
2024-10-27 18:35:02,721 (beam_search:428) INFO: decoder input length: 55
2024-10-27 18:35:02,721 (beam_search:429) INFO: max output length: 55
2024-10-27 18:35:02,721 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:02,771 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:02,771 (beam_search:476) INFO:  -0.39 * 1.0 =  -0.39 for ctc
2024-10-27 18:35:02,771 (beam_search:479) INFO: total log probability: -0.39
2024-10-27 18:35:02,771 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:35:02,771 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:02,771 (beam_search:483) INFO: best hypo: ▁YOU▁ARE▁THE▁IN▁THE

2024-10-27 18:35:02,773 (asr_inference:509) INFO: speech length: 52500
2024-10-27 18:35:04,782 (beam_search:428) INFO: decoder input length: 40
2024-10-27 18:35:04,782 (beam_search:429) INFO: max output length: 40
2024-10-27 18:35:04,782 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:04,830 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:04,830 (beam_search:476) INFO:  -0.63 * 1.0 =  -0.63 for ctc
2024-10-27 18:35:04,830 (beam_search:479) INFO: total log probability: -0.63
2024-10-27 18:35:04,830 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:35:04,830 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:04,830 (beam_search:483) INFO: best hypo: ▁I'M▁GOOD▁HOW▁ARE▁YOU

2024-10-27 18:35:04,834 (asr_inference:509) INFO: speech length: 72780
2024-10-27 18:35:07,409 (beam_search:428) INFO: decoder input length: 56
2024-10-27 18:35:07,409 (beam_search:429) INFO: max output length: 56
2024-10-27 18:35:07,409 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:07,485 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:07,485 (beam_search:476) INFO:  -2.09 * 1.0 =  -2.09 for ctc
2024-10-27 18:35:07,485 (beam_search:479) INFO: total log probability: -2.09
2024-10-27 18:35:07,485 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:35:07,485 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:07,485 (beam_search:483) INFO: best hypo: ▁WE'VE▁BEEN▁ABOUT▁AND▁PARALLEL▁CIRCUITS

2024-10-27 18:35:07,487 (asr_inference:509) INFO: speech length: 150061
2024-10-27 18:35:12,989 (beam_search:428) INFO: decoder input length: 116
2024-10-27 18:35:12,989 (beam_search:429) INFO: max output length: 116
2024-10-27 18:35:12,989 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:13,253 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:13,253 (beam_search:476) INFO:  -1.82 * 1.0 =  -1.82 for ctc
2024-10-27 18:35:13,253 (beam_search:479) INFO: total log probability: -1.82
2024-10-27 18:35:13,253 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:35:13,253 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:13,253 (beam_search:483) INFO: best hypo: ▁THE▁THERE▁ARE▁TWOS▁AND▁ONE▁BATTERY▁AND▁THE▁LIGHT▁ARE▁BRIGHT

2024-10-27 18:35:13,256 (asr_inference:509) INFO: speech length: 298948
2024-10-27 18:35:26,337 (beam_search:428) INFO: decoder input length: 233
2024-10-27 18:35:26,337 (beam_search:429) INFO: max output length: 233
2024-10-27 18:35:26,337 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:26,723 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:26,724 (beam_search:476) INFO:  -3.49 * 1.0 =  -3.49 for ctc
2024-10-27 18:35:26,724 (beam_search:479) INFO: total log probability: -3.49
2024-10-27 18:35:26,724 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 18:35:26,724 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:26,724 (beam_search:483) INFO: best hypo: ▁THE▁CIRCUITS▁ITS▁AND▁THE▁LIGHTS▁ARE

2024-10-27 18:35:26,726 (asr_inference:509) INFO: speech length: 26774
2024-10-27 18:35:27,914 (beam_search:428) INFO: decoder input length: 20
2024-10-27 18:35:27,915 (beam_search:429) INFO: max output length: 20
2024-10-27 18:35:27,915 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:27,929 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:27,929 (beam_search:476) INFO:  -0.44 * 1.0 =  -0.44 for ctc
2024-10-27 18:35:27,929 (beam_search:479) INFO: total log probability: -0.44
2024-10-27 18:35:27,929 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:35:27,929 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:27,929 (beam_search:483) INFO: best hypo: ▁IT▁TURNS

2024-10-27 18:35:27,931 (asr_inference:509) INFO: speech length: 55089
2024-10-27 18:35:30,004 (beam_search:428) INFO: decoder input length: 42
2024-10-27 18:35:30,004 (beam_search:429) INFO: max output length: 42
2024-10-27 18:35:30,004 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:30,024 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:30,025 (beam_search:476) INFO:  -0.74 * 1.0 =  -0.74 for ctc
2024-10-27 18:35:30,025 (beam_search:479) INFO: total log probability: -0.74
2024-10-27 18:35:30,025 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:35:30,025 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:30,025 (beam_search:483) INFO: best hypo: ▁THAT▁ARE

2024-10-27 18:35:30,027 (asr_inference:509) INFO: speech length: 245674
2024-10-27 18:35:40,016 (beam_search:428) INFO: decoder input length: 191
2024-10-27 18:35:40,016 (beam_search:429) INFO: max output length: 191
2024-10-27 18:35:40,016 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:40,507 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:40,507 (beam_search:476) INFO:  -4.09 * 1.0 =  -4.09 for ctc
2024-10-27 18:35:40,507 (beam_search:479) INFO: total log probability: -4.09
2024-10-27 18:35:40,507 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:35:40,507 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:40,507 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁THE▁BULBS▁WILL▁BE▁VERY▁VERY▁VERY▁AND▁AND▁THE▁IN

2024-10-27 18:35:40,510 (asr_inference:509) INFO: speech length: 70764
2024-10-27 18:35:43,150 (beam_search:428) INFO: decoder input length: 54
2024-10-27 18:35:43,150 (beam_search:429) INFO: max output length: 54
2024-10-27 18:35:43,150 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:43,201 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:43,201 (beam_search:476) INFO:  -0.33 * 1.0 =  -0.33 for ctc
2024-10-27 18:35:43,201 (beam_search:479) INFO: total log probability: -0.33
2024-10-27 18:35:43,201 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:35:43,201 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:43,201 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁THE▁WILL

2024-10-27 18:35:43,203 (asr_inference:509) INFO: speech length: 98008
2024-10-27 18:35:46,713 (beam_search:428) INFO: decoder input length: 76
2024-10-27 18:35:46,713 (beam_search:429) INFO: max output length: 76
2024-10-27 18:35:46,713 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:46,811 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:46,811 (beam_search:476) INFO:  -2.08 * 1.0 =  -2.08 for ctc
2024-10-27 18:35:46,811 (beam_search:479) INFO: total log probability: -2.08
2024-10-27 18:35:46,811 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:35:46,811 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:46,811 (beam_search:483) INFO: best hypo: ▁GOES▁THROUGH▁THE▁BATTERY▁THROUGH▁THE▁POSITIVE

2024-10-27 18:35:46,814 (asr_inference:509) INFO: speech length: 199421
2024-10-27 18:35:54,769 (beam_search:428) INFO: decoder input length: 155
2024-10-27 18:35:54,769 (beam_search:429) INFO: max output length: 155
2024-10-27 18:35:54,769 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:55,263 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:55,263 (beam_search:476) INFO:  -3.78 * 1.0 =  -3.78 for ctc
2024-10-27 18:35:55,263 (beam_search:479) INFO: total log probability: -3.78
2024-10-27 18:35:55,263 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:35:55,263 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:55,263 (beam_search:483) INFO: best hypo: ▁IT▁GOING▁THROUGH▁AND▁IT▁AND▁IT'S▁GOING▁INTO▁THE▁POSITIVE▁AND▁OUT▁THROUGH▁THE

2024-10-27 18:35:55,266 (asr_inference:509) INFO: speech length: 201024
2024-10-27 18:36:03,494 (beam_search:428) INFO: decoder input length: 156
2024-10-27 18:36:03,494 (beam_search:429) INFO: max output length: 156
2024-10-27 18:36:03,494 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:03,669 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:03,669 (beam_search:476) INFO:  -1.11 * 1.0 =  -1.11 for ctc
2024-10-27 18:36:03,669 (beam_search:479) INFO: total log probability: -1.11
2024-10-27 18:36:03,669 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:36:03,669 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:03,669 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁GOES▁TO▁TO▁IT

2024-10-27 18:36:03,672 (asr_inference:509) INFO: speech length: 193133
2024-10-27 18:36:11,310 (beam_search:428) INFO: decoder input length: 150
2024-10-27 18:36:11,310 (beam_search:429) INFO: max output length: 150
2024-10-27 18:36:11,310 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:11,683 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:11,684 (beam_search:476) INFO:  -2.84 * 1.0 =  -2.84 for ctc
2024-10-27 18:36:11,684 (beam_search:479) INFO: total log probability: -2.84
2024-10-27 18:36:11,684 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:36:11,684 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:11,684 (beam_search:483) INFO: best hypo: ▁THE▁IT▁IS▁GOING▁IN▁THROUGH▁THE▁POSITIVE▁AND▁OUT▁THROUGH▁THE▁NEGATIVE

2024-10-27 18:36:11,687 (asr_inference:509) INFO: speech length: 25123
2024-10-27 18:36:12,802 (beam_search:428) INFO: decoder input length: 19
2024-10-27 18:36:12,802 (beam_search:429) INFO: max output length: 19
2024-10-27 18:36:12,802 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:12,819 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:12,819 (beam_search:476) INFO:  -0.14 * 1.0 =  -0.14 for ctc
2024-10-27 18:36:12,819 (beam_search:479) INFO: total log probability: -0.14
2024-10-27 18:36:12,819 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:36:12,819 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:12,819 (beam_search:483) INFO: best hypo: ▁WHAT▁DID▁YOU▁SAY

2024-10-27 18:36:12,821 (asr_inference:509) INFO: speech length: 121425
2024-10-27 18:36:17,247 (beam_search:428) INFO: decoder input length: 94
2024-10-27 18:36:17,247 (beam_search:429) INFO: max output length: 94
2024-10-27 18:36:17,247 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:17,479 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:17,479 (beam_search:476) INFO:  -2.29 * 1.0 =  -2.29 for ctc
2024-10-27 18:36:17,479 (beam_search:479) INFO: total log probability: -2.29
2024-10-27 18:36:17,479 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:36:17,480 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:17,480 (beam_search:483) INFO: best hypo: ▁I▁THAT▁IT▁GOES▁IN▁THROUGH▁THE▁AND▁OUT▁THROUGH▁THE▁YOU'RE▁THE

2024-10-27 18:36:17,483 (asr_inference:509) INFO: speech length: 122257
2024-10-27 18:36:21,854 (beam_search:428) INFO: decoder input length: 95
2024-10-27 18:36:21,855 (beam_search:429) INFO: max output length: 95
2024-10-27 18:36:21,855 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:21,915 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:21,916 (beam_search:476) INFO:  -1.08 * 1.0 =  -1.08 for ctc
2024-10-27 18:36:21,916 (beam_search:479) INFO: total log probability: -1.08
2024-10-27 18:36:21,916 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:36:21,916 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:21,916 (beam_search:483) INFO: best hypo: ▁THERE▁ARE▁TWO

2024-10-27 18:36:21,918 (asr_inference:509) INFO: speech length: 62439
2024-10-27 18:36:24,220 (beam_search:428) INFO: decoder input length: 48
2024-10-27 18:36:24,220 (beam_search:429) INFO: max output length: 48
2024-10-27 18:36:24,220 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:24,258 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:24,258 (beam_search:476) INFO:  -1.07 * 1.0 =  -1.07 for ctc
2024-10-27 18:36:24,258 (beam_search:479) INFO: total log probability: -1.07
2024-10-27 18:36:24,258 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:36:24,258 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:24,258 (beam_search:483) INFO: best hypo: ▁I▁THAT▁THERE▁ARE

2024-10-27 18:36:24,260 (asr_inference:509) INFO: speech length: 168121
2024-10-27 18:36:30,649 (beam_search:428) INFO: decoder input length: 130
2024-10-27 18:36:30,649 (beam_search:429) INFO: max output length: 130
2024-10-27 18:36:30,649 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:30,838 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:30,838 (beam_search:476) INFO:  -1.79 * 1.0 =  -1.79 for ctc
2024-10-27 18:36:30,838 (beam_search:479) INFO: total log probability: -1.79
2024-10-27 18:36:30,838 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:36:30,838 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:30,838 (beam_search:483) INFO: best hypo: ▁I▁TOLD▁YOU▁A▁A▁TIMES▁ARE▁TWO

2024-10-27 18:36:30,840 (asr_inference:509) INFO: speech length: 68331
2024-10-27 18:36:33,392 (beam_search:428) INFO: decoder input length: 52
2024-10-27 18:36:33,392 (beam_search:429) INFO: max output length: 52
2024-10-27 18:36:33,392 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:33,418 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:33,418 (beam_search:476) INFO:  -0.38 * 1.0 =  -0.38 for ctc
2024-10-27 18:36:33,418 (beam_search:479) INFO: total log probability: -0.38
2024-10-27 18:36:33,418 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:36:33,418 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:33,418 (beam_search:483) INFO: best hypo: ▁IT▁OUT

2024-10-27 18:36:33,420 (asr_inference:509) INFO: speech length: 80324
2024-10-27 18:36:36,374 (beam_search:428) INFO: decoder input length: 62
2024-10-27 18:36:36,374 (beam_search:429) INFO: max output length: 62
2024-10-27 18:36:36,375 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:36,407 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:36,408 (beam_search:476) INFO:  -0.09 * 1.0 =  -0.09 for ctc
2024-10-27 18:36:36,408 (beam_search:479) INFO: total log probability: -0.09
2024-10-27 18:36:36,408 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:36:36,408 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:36,408 (beam_search:483) INFO: best hypo: ▁THEY▁ARE

2024-10-27 18:36:36,410 (asr_inference:509) INFO: speech length: 139617
2024-10-27 18:36:41,493 (beam_search:428) INFO: decoder input length: 108
2024-10-27 18:36:41,494 (beam_search:429) INFO: max output length: 108
2024-10-27 18:36:41,494 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:41,845 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:41,846 (beam_search:476) INFO:  -2.05 * 1.0 =  -2.05 for ctc
2024-10-27 18:36:41,847 (beam_search:479) INFO: total log probability: -2.05
2024-10-27 18:36:41,848 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:36:41,848 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:41,848 (beam_search:483) INFO: best hypo: ▁BECAUSE▁IT▁HAS▁TO▁WITH▁BOTH▁OF▁THEM▁AND▁IT▁DOESN'T▁GO▁IT▁ONLY▁ONE

2024-10-27 18:36:41,850 (asr_inference:509) INFO: speech length: 93220
2024-10-27 18:36:45,292 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:36:45,292 (beam_search:429) INFO: max output length: 72
2024-10-27 18:36:45,292 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:45,361 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:45,361 (beam_search:476) INFO:  -1.84 * 1.0 =  -1.84 for ctc
2024-10-27 18:36:45,361 (beam_search:479) INFO: total log probability: -1.84
2024-10-27 18:36:45,361 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:36:45,361 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:45,361 (beam_search:483) INFO: best hypo: ▁THEY▁DO▁NOT▁HAVE▁A

2024-10-27 18:36:45,363 (asr_inference:509) INFO: speech length: 70136
2024-10-27 18:36:47,980 (beam_search:428) INFO: decoder input length: 54
2024-10-27 18:36:47,981 (beam_search:429) INFO: max output length: 54
2024-10-27 18:36:47,981 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:48,001 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:48,001 (beam_search:476) INFO:  -0.43 * 1.0 =  -0.43 for ctc
2024-10-27 18:36:48,001 (beam_search:479) INFO: total log probability: -0.43
2024-10-27 18:36:48,001 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:36:48,001 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:48,001 (beam_search:483) INFO: best hypo: ▁THE

2024-10-27 18:36:48,004 (asr_inference:509) INFO: speech length: 32500
2024-10-27 18:36:49,265 (beam_search:428) INFO: decoder input length: 24
2024-10-27 18:36:49,265 (beam_search:429) INFO: max output length: 24
2024-10-27 18:36:49,265 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:49,288 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:49,288 (beam_search:476) INFO:  -0.63 * 1.0 =  -0.63 for ctc
2024-10-27 18:36:49,289 (beam_search:479) INFO: total log probability: -0.63
2024-10-27 18:36:49,289 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:36:49,289 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:49,289 (beam_search:483) INFO: best hypo: ▁I▁GOOD▁HOW▁ARE▁YOU

2024-10-27 18:36:49,291 (asr_inference:509) INFO: speech length: 43423
2024-10-27 18:36:51,019 (beam_search:428) INFO: decoder input length: 33
2024-10-27 18:36:51,019 (beam_search:429) INFO: max output length: 33
2024-10-27 18:36:51,019 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:51,062 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:51,062 (beam_search:476) INFO:  -2.92 * 1.0 =  -2.92 for ctc
2024-10-27 18:36:51,062 (beam_search:479) INFO: total log probability: -2.92
2024-10-27 18:36:51,062 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 18:36:51,062 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:51,062 (beam_search:483) INFO: best hypo: ▁THEY▁THEY▁ARE▁TWOS▁AND▁THEY

2024-10-27 18:36:51,065 (asr_inference:509) INFO: speech length: 64245
2024-10-27 18:36:53,393 (beam_search:428) INFO: decoder input length: 49
2024-10-27 18:36:53,394 (beam_search:429) INFO: max output length: 49
2024-10-27 18:36:53,394 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:53,469 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:53,470 (beam_search:476) INFO:  -2.94 * 1.0 =  -2.94 for ctc
2024-10-27 18:36:53,470 (beam_search:479) INFO: total log probability: -2.94
2024-10-27 18:36:53,470 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:36:53,470 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:53,470 (beam_search:483) INFO: best hypo: ▁THERE▁IS▁ONE▁THERE▁IS▁ONE▁D▁AND▁TWO

2024-10-27 18:36:53,472 (asr_inference:509) INFO: speech length: 51616
2024-10-27 18:36:55,490 (beam_search:428) INFO: decoder input length: 39
2024-10-27 18:36:55,490 (beam_search:429) INFO: max output length: 39
2024-10-27 18:36:55,490 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:55,544 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:55,545 (beam_search:476) INFO:  -2.73 * 1.0 =  -2.73 for ctc
2024-10-27 18:36:55,545 (beam_search:479) INFO: total log probability: -2.73
2024-10-27 18:36:55,545 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:36:55,545 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:55,545 (beam_search:483) INFO: best hypo: ▁EACH▁LIGHT▁BULB▁HAS▁ITS▁PATHWAY▁ARE

2024-10-27 18:36:55,547 (asr_inference:509) INFO: speech length: 67303
2024-10-27 18:36:58,110 (beam_search:428) INFO: decoder input length: 52
2024-10-27 18:36:58,110 (beam_search:429) INFO: max output length: 52
2024-10-27 18:36:58,110 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:58,146 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:58,146 (beam_search:476) INFO:  -0.93 * 1.0 =  -0.93 for ctc
2024-10-27 18:36:58,146 (beam_search:479) INFO: total log probability: -0.93
2024-10-27 18:36:58,146 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:36:58,146 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:58,146 (beam_search:483) INFO: best hypo: ▁THEY▁HAVE▁THEIR

2024-10-27 18:36:58,148 (asr_inference:509) INFO: speech length: 149585
2024-10-27 18:37:03,611 (beam_search:428) INFO: decoder input length: 116
2024-10-27 18:37:03,611 (beam_search:429) INFO: max output length: 116
2024-10-27 18:37:03,611 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:03,685 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:03,685 (beam_search:476) INFO:  -2.08 * 1.0 =  -2.08 for ctc
2024-10-27 18:37:03,685 (beam_search:479) INFO: total log probability: -2.08
2024-10-27 18:37:03,685 (beam_search:480) INFO: normalized log probability: -0.42
2024-10-27 18:37:03,685 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:03,685 (beam_search:483) INFO: best hypo: ▁THEY▁ARE▁ALL

2024-10-27 18:37:03,687 (asr_inference:509) INFO: speech length: 65098
2024-10-27 18:37:06,060 (beam_search:428) INFO: decoder input length: 50
2024-10-27 18:37:06,060 (beam_search:429) INFO: max output length: 50
2024-10-27 18:37:06,060 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:06,082 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:06,082 (beam_search:476) INFO:  -0.82 * 1.0 =  -0.82 for ctc
2024-10-27 18:37:06,082 (beam_search:479) INFO: total log probability: -0.82
2024-10-27 18:37:06,082 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:37:06,082 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:06,082 (beam_search:483) INFO: best hypo: ▁IS

2024-10-27 18:37:06,085 (asr_inference:509) INFO: speech length: 182669
2024-10-27 18:37:13,459 (beam_search:428) INFO: decoder input length: 142
2024-10-27 18:37:13,460 (beam_search:429) INFO: max output length: 142
2024-10-27 18:37:13,460 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:13,929 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:13,929 (beam_search:476) INFO:  -3.79 * 1.0 =  -3.79 for ctc
2024-10-27 18:37:13,929 (beam_search:479) INFO: total log probability: -3.79
2024-10-27 18:37:13,929 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:37:13,929 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:13,929 (beam_search:483) INFO: best hypo: ▁WE'VE▁BEEN▁WITH▁AND▁CIRCUITS▁I'VE▁TOLD▁YOU▁LIKE▁A▁TIMES▁YOU▁ARE▁THE▁MOST

2024-10-27 18:37:13,931 (asr_inference:509) INFO: speech length: 61371
2024-10-27 18:37:16,304 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:37:16,305 (beam_search:429) INFO: max output length: 47
2024-10-27 18:37:16,305 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:16,362 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:16,362 (beam_search:476) INFO:  -1.54 * 1.0 =  -1.54 for ctc
2024-10-27 18:37:16,362 (beam_search:479) INFO: total log probability: -1.54
2024-10-27 18:37:16,362 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:37:16,362 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:16,362 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁IT▁WILL▁BECOME▁BRIGHTER

2024-10-27 18:37:16,364 (asr_inference:509) INFO: speech length: 56894
2024-10-27 18:37:18,701 (beam_search:428) INFO: decoder input length: 43
2024-10-27 18:37:18,701 (beam_search:429) INFO: max output length: 43
2024-10-27 18:37:18,701 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:18,751 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:18,751 (beam_search:476) INFO:  -1.12 * 1.0 =  -1.12 for ctc
2024-10-27 18:37:18,751 (beam_search:479) INFO: total log probability: -1.12
2024-10-27 18:37:18,751 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:37:18,751 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:18,751 (beam_search:483) INFO: best hypo: ▁PARALLEL▁CIRCUITS▁ARE▁YOU▁ARE▁THE

2024-10-27 18:37:18,754 (asr_inference:509) INFO: speech length: 66717
2024-10-27 18:37:21,423 (beam_search:428) INFO: decoder input length: 51
2024-10-27 18:37:21,423 (beam_search:429) INFO: max output length: 51
2024-10-27 18:37:21,423 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:21,463 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:21,463 (beam_search:476) INFO:  -0.76 * 1.0 =  -0.76 for ctc
2024-10-27 18:37:21,463 (beam_search:479) INFO: total log probability: -0.76
2024-10-27 18:37:21,463 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:37:21,463 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:21,463 (beam_search:483) INFO: best hypo: ▁THEY▁ARE▁GOOD▁THEY

2024-10-27 18:37:21,466 (asr_inference:509) INFO: speech length: 71637
2024-10-27 18:37:24,260 (beam_search:428) INFO: decoder input length: 55
2024-10-27 18:37:24,260 (beam_search:429) INFO: max output length: 55
2024-10-27 18:37:24,260 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:24,331 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:24,331 (beam_search:476) INFO:  -0.84 * 1.0 =  -0.84 for ctc
2024-10-27 18:37:24,331 (beam_search:479) INFO: total log probability: -0.84
2024-10-27 18:37:24,331 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:37:24,331 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:24,331 (beam_search:483) INFO: best hypo: ▁THEY▁ARE▁LIKE▁THE▁FLOW▁OF▁ELECTRICITY

2024-10-27 18:37:24,334 (asr_inference:509) INFO: speech length: 112694
2024-10-27 18:37:28,579 (beam_search:428) INFO: decoder input length: 87
2024-10-27 18:37:28,579 (beam_search:429) INFO: max output length: 87
2024-10-27 18:37:28,579 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:28,623 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:28,623 (beam_search:476) INFO:  -0.18 * 1.0 =  -0.18 for ctc
2024-10-27 18:37:28,623 (beam_search:479) INFO: total log probability: -0.18
2024-10-27 18:37:28,623 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 18:37:28,623 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:28,623 (beam_search:483) INFO: best hypo: ▁IT▁IS

2024-10-27 18:37:28,625 (asr_inference:509) INFO: speech length: 109478
2024-10-27 18:37:32,767 (beam_search:428) INFO: decoder input length: 85
2024-10-27 18:37:32,767 (beam_search:429) INFO: max output length: 85
2024-10-27 18:37:32,767 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:32,924 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:32,924 (beam_search:476) INFO:  -1.52 * 1.0 =  -1.52 for ctc
2024-10-27 18:37:32,924 (beam_search:479) INFO: total log probability: -1.52
2024-10-27 18:37:32,924 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:37:32,924 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:32,924 (beam_search:483) INFO: best hypo: ▁THE▁IS▁GOING▁IN▁THROUGH▁THE▁AND▁OUT▁THROUGH▁THE

2024-10-27 18:37:32,927 (asr_inference:509) INFO: speech length: 27513
2024-10-27 18:37:34,165 (beam_search:428) INFO: decoder input length: 20
2024-10-27 18:37:34,166 (beam_search:429) INFO: max output length: 20
2024-10-27 18:37:34,166 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:34,185 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:34,185 (beam_search:476) INFO:  -0.15 * 1.0 =  -0.15 for ctc
2024-10-27 18:37:34,185 (beam_search:479) INFO: total log probability: -0.15
2024-10-27 18:37:34,185 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:37:34,185 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:34,185 (beam_search:483) INFO: best hypo: ▁WHAT▁DID▁YOU▁SAY

2024-10-27 18:37:34,187 (asr_inference:509) INFO: speech length: 117593
2024-10-27 18:37:38,625 (beam_search:428) INFO: decoder input length: 91
2024-10-27 18:37:38,626 (beam_search:429) INFO: max output length: 91
2024-10-27 18:37:38,626 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:38,831 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:38,831 (beam_search:476) INFO:  -2.18 * 1.0 =  -2.18 for ctc
2024-10-27 18:37:38,831 (beam_search:479) INFO: total log probability: -2.18
2024-10-27 18:37:38,831 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:37:38,831 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:38,831 (beam_search:483) INFO: best hypo: ▁THE▁IT▁IS▁GOING▁IN▁THROUGH▁THE▁POSITIVE▁AND▁OUT▁THROUGH▁THE

2024-10-27 18:37:38,834 (asr_inference:509) INFO: speech length: 59813
2024-10-27 18:37:41,270 (beam_search:428) INFO: decoder input length: 46
2024-10-27 18:37:41,270 (beam_search:429) INFO: max output length: 46
2024-10-27 18:37:41,270 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:41,336 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:41,336 (beam_search:476) INFO:  -0.71 * 1.0 =  -0.71 for ctc
2024-10-27 18:37:41,336 (beam_search:479) INFO: total log probability: -0.71
2024-10-27 18:37:41,336 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:37:41,336 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:41,336 (beam_search:483) INFO: best hypo: ▁I▁TOLD▁YOU▁IT▁GOES▁OUT▁THROUGH▁THE

2024-10-27 18:37:41,339 (asr_inference:509) INFO: speech length: 55747
2024-10-27 18:37:43,719 (beam_search:428) INFO: decoder input length: 43
2024-10-27 18:37:43,719 (beam_search:429) INFO: max output length: 43
2024-10-27 18:37:43,719 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:43,773 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:43,773 (beam_search:476) INFO:  -1.62 * 1.0 =  -1.62 for ctc
2024-10-27 18:37:43,774 (beam_search:479) INFO: total log probability: -1.62
2024-10-27 18:37:43,774 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:37:43,774 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:43,774 (beam_search:483) INFO: best hypo: ▁I▁NOT▁YOU▁SO▁PLEASE▁IT▁AGAIN

2024-10-27 18:37:43,776 (asr_inference:509) INFO: speech length: 88394
2024-10-27 18:37:47,466 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:37:47,466 (beam_search:429) INFO: max output length: 68
2024-10-27 18:37:47,466 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:47,534 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:47,535 (beam_search:476) INFO:  -1.50 * 1.0 =  -1.50 for ctc
2024-10-27 18:37:47,535 (beam_search:479) INFO: total log probability: -1.50
2024-10-27 18:37:47,535 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:37:47,535 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:47,535 (beam_search:483) INFO: best hypo: ▁ARE▁TWO▁AND▁THEY▁ARE

2024-10-27 18:37:47,538 (asr_inference:509) INFO: speech length: 70048
2024-10-27 18:37:50,466 (beam_search:428) INFO: decoder input length: 54
2024-10-27 18:37:50,467 (beam_search:429) INFO: max output length: 54
2024-10-27 18:37:50,467 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:50,531 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:50,531 (beam_search:476) INFO:  -3.44 * 1.0 =  -3.44 for ctc
2024-10-27 18:37:50,531 (beam_search:479) INFO: total log probability: -3.44
2024-10-27 18:37:50,531 (beam_search:480) INFO: normalized log probability: -0.43
2024-10-27 18:37:50,531 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:50,531 (beam_search:483) INFO: best hypo: ▁THERE▁ARE▁PATHWAYS▁IN▁A▁PARALLEL

2024-10-27 18:37:50,533 (asr_inference:509) INFO: speech length: 91043
2024-10-27 18:37:54,439 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:37:54,439 (beam_search:429) INFO: max output length: 70
2024-10-27 18:37:54,439 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:54,469 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:54,470 (beam_search:476) INFO:  -0.67 * 1.0 =  -0.67 for ctc
2024-10-27 18:37:54,470 (beam_search:479) INFO: total log probability: -0.67
2024-10-27 18:37:54,470 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:37:54,470 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:54,470 (beam_search:483) INFO: best hypo: ▁IT

2024-10-27 18:37:54,472 (asr_inference:509) INFO: speech length: 93932
2024-10-27 18:37:58,322 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:37:58,323 (beam_search:429) INFO: max output length: 72
2024-10-27 18:37:58,323 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:58,363 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:58,363 (beam_search:476) INFO:  -0.61 * 1.0 =  -0.61 for ctc
2024-10-27 18:37:58,363 (beam_search:479) INFO: total log probability: -0.61
2024-10-27 18:37:58,363 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:37:58,363 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:58,363 (beam_search:483) INFO: best hypo: ▁ARE▁TWO

2024-10-27 18:37:58,366 (asr_inference:509) INFO: speech length: 92708
2024-10-27 18:38:02,876 (beam_search:428) INFO: decoder input length: 71
2024-10-27 18:38:02,876 (beam_search:429) INFO: max output length: 71
2024-10-27 18:38:02,876 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:02,976 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:02,976 (beam_search:476) INFO:  -2.29 * 1.0 =  -2.29 for ctc
2024-10-27 18:38:02,976 (beam_search:479) INFO: total log probability: -2.29
2024-10-27 18:38:02,976 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:38:02,976 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:02,976 (beam_search:483) INFO: best hypo: ▁THE▁ON▁THE▁BOTTOM▁IS▁OUT

2024-10-27 18:38:02,979 (asr_inference:509) INFO: speech length: 91472
2024-10-27 18:38:07,431 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:38:07,431 (beam_search:429) INFO: max output length: 70
2024-10-27 18:38:07,431 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:07,522 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:07,522 (beam_search:476) INFO:  -1.00 * 1.0 =  -1.00 for ctc
2024-10-27 18:38:07,522 (beam_search:479) INFO: total log probability: -1.00
2024-10-27 18:38:07,522 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:38:07,522 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:07,522 (beam_search:483) INFO: best hypo: ▁THE▁ARE▁BRIGHTER▁THAN▁THE▁BOTTOM

2024-10-27 18:38:07,525 (asr_inference:509) INFO: speech length: 79537
2024-10-27 18:38:10,807 (beam_search:428) INFO: decoder input length: 61
2024-10-27 18:38:10,808 (beam_search:429) INFO: max output length: 61
2024-10-27 18:38:10,808 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:10,872 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:10,872 (beam_search:476) INFO:  -0.49 * 1.0 =  -0.49 for ctc
2024-10-27 18:38:10,872 (beam_search:479) INFO: total log probability: -0.49
2024-10-27 18:38:10,872 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:38:10,872 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:10,872 (beam_search:483) INFO: best hypo: ▁BECAUSE▁IT▁HAS▁TO▁THE

2024-10-27 18:38:10,874 (asr_inference:509) INFO: speech length: 69142
2024-10-27 18:38:13,659 (beam_search:428) INFO: decoder input length: 53
2024-10-27 18:38:13,659 (beam_search:429) INFO: max output length: 53
2024-10-27 18:38:13,659 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:13,733 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:13,733 (beam_search:476) INFO:  -0.62 * 1.0 =  -0.62 for ctc
2024-10-27 18:38:13,733 (beam_search:479) INFO: total log probability: -0.62
2024-10-27 18:38:13,733 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:38:13,733 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:13,734 (beam_search:483) INFO: best hypo: ▁THEY▁HAVE▁TO▁A▁PATHWAY▁AND▁A

2024-10-27 18:38:13,736 (asr_inference:509) INFO: speech length: 57462
2024-10-27 18:38:16,291 (beam_search:428) INFO: decoder input length: 44
2024-10-27 18:38:16,291 (beam_search:429) INFO: max output length: 44
2024-10-27 18:38:16,291 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:16,356 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:16,356 (beam_search:476) INFO:  -2.59 * 1.0 =  -2.59 for ctc
2024-10-27 18:38:16,356 (beam_search:479) INFO: total log probability: -2.59
2024-10-27 18:38:16,356 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:38:16,356 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:16,356 (beam_search:483) INFO: best hypo: ▁THERE▁ARE▁TWO▁AND▁BOTH▁OF▁THEM

2024-10-27 18:38:16,359 (asr_inference:509) INFO: speech length: 189147
2024-10-27 18:38:25,965 (beam_search:428) INFO: decoder input length: 147
2024-10-27 18:38:25,965 (beam_search:429) INFO: max output length: 147
2024-10-27 18:38:25,965 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:26,268 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:26,268 (beam_search:476) INFO:  -2.18 * 1.0 =  -2.18 for ctc
2024-10-27 18:38:26,270 (beam_search:479) INFO: total log probability: -2.18
2024-10-27 18:38:26,270 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:38:26,270 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:26,270 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁THE▁BE▁THE▁OF▁OR▁THE▁CIRCUIT▁OF

2024-10-27 18:38:26,283 (asr_inference:509) INFO: speech length: 97688
2024-10-27 18:38:30,165 (beam_search:428) INFO: decoder input length: 75
2024-10-27 18:38:30,165 (beam_search:429) INFO: max output length: 75
2024-10-27 18:38:30,165 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:30,367 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:30,367 (beam_search:476) INFO:  -2.19 * 1.0 =  -2.19 for ctc
2024-10-27 18:38:30,367 (beam_search:479) INFO: total log probability: -2.19
2024-10-27 18:38:30,367 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:38:30,367 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:30,367 (beam_search:483) INFO: best hypo: ▁THINK▁THAT▁WE▁NEED▁TO▁MAKE▁THAT▁THES▁ARE▁SO▁WE▁NEED▁THE▁CIRCUIT

2024-10-27 18:38:30,380 (asr_inference:509) INFO: speech length: 44251
2024-10-27 18:38:32,350 (beam_search:428) INFO: decoder input length: 34
2024-10-27 18:38:32,350 (beam_search:429) INFO: max output length: 34
2024-10-27 18:38:32,350 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:32,366 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:32,366 (beam_search:476) INFO:  -0.27 * 1.0 =  -0.27 for ctc
2024-10-27 18:38:32,366 (beam_search:479) INFO: total log probability: -0.27
2024-10-27 18:38:32,366 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:38:32,366 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:32,367 (beam_search:483) INFO: best hypo: ▁THE

2024-10-27 18:38:32,369 (asr_inference:509) INFO: speech length: 86000
2024-10-27 18:38:35,758 (beam_search:428) INFO: decoder input length: 66
2024-10-27 18:38:35,758 (beam_search:429) INFO: max output length: 66
2024-10-27 18:38:35,758 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:35,810 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:35,810 (beam_search:476) INFO:  -1.63 * 1.0 =  -1.63 for ctc
2024-10-27 18:38:35,810 (beam_search:479) INFO: total log probability: -1.63
2024-10-27 18:38:35,810 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:38:35,810 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:35,810 (beam_search:483) INFO: best hypo: ▁GOOD▁ARE▁YOU▁MY

2024-10-27 18:38:35,813 (asr_inference:509) INFO: speech length: 162552
2024-10-27 18:38:42,645 (beam_search:428) INFO: decoder input length: 126
2024-10-27 18:38:42,645 (beam_search:429) INFO: max output length: 126
2024-10-27 18:38:42,645 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:42,798 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:42,798 (beam_search:476) INFO:  -2.47 * 1.0 =  -2.47 for ctc
2024-10-27 18:38:42,798 (beam_search:479) INFO: total log probability: -2.47
2024-10-27 18:38:42,798 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 18:38:42,798 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:42,798 (beam_search:483) INFO: best hypo: ▁WEVE▁DOING▁STUFF▁LIKE▁AND

2024-10-27 18:38:42,800 (asr_inference:509) INFO: speech length: 124856
2024-10-27 18:38:47,900 (beam_search:428) INFO: decoder input length: 97
2024-10-27 18:38:47,900 (beam_search:429) INFO: max output length: 97
2024-10-27 18:38:47,900 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:47,988 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:47,989 (beam_search:476) INFO:  -1.59 * 1.0 =  -1.59 for ctc
2024-10-27 18:38:47,989 (beam_search:479) INFO: total log probability: -1.59
2024-10-27 18:38:47,989 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:38:47,989 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:47,989 (beam_search:483) INFO: best hypo: ▁I▁OUT▁THAT▁STICK

2024-10-27 18:38:47,991 (asr_inference:509) INFO: speech length: 78215
2024-10-27 18:38:50,983 (beam_search:428) INFO: decoder input length: 60
2024-10-27 18:38:50,983 (beam_search:429) INFO: max output length: 60
2024-10-27 18:38:50,983 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:51,021 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:51,022 (beam_search:476) INFO:  -0.46 * 1.0 =  -0.46 for ctc
2024-10-27 18:38:51,022 (beam_search:479) INFO: total log probability: -0.46
2024-10-27 18:38:51,022 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:38:51,022 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:51,022 (beam_search:483) INFO: best hypo: ▁AND▁THE▁AND

2024-10-27 18:38:51,024 (asr_inference:509) INFO: speech length: 91632
2024-10-27 18:38:54,562 (beam_search:428) INFO: decoder input length: 71
2024-10-27 18:38:54,562 (beam_search:429) INFO: max output length: 71
2024-10-27 18:38:54,562 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:54,650 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:54,650 (beam_search:476) INFO:  -2.12 * 1.0 =  -2.12 for ctc
2024-10-27 18:38:54,651 (beam_search:479) INFO: total log probability: -2.12
2024-10-27 18:38:54,651 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:38:54,651 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:54,651 (beam_search:483) INFO: best hypo: ▁THE▁MAGNETS▁ARE▁STICKING▁TO▁THE

2024-10-27 18:38:54,664 (asr_inference:509) INFO: speech length: 66184
2024-10-27 18:38:57,493 (beam_search:428) INFO: decoder input length: 51
2024-10-27 18:38:57,493 (beam_search:429) INFO: max output length: 51
2024-10-27 18:38:57,493 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:57,519 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:57,519 (beam_search:476) INFO:  -0.37 * 1.0 =  -0.37 for ctc
2024-10-27 18:38:57,519 (beam_search:479) INFO: total log probability: -0.37
2024-10-27 18:38:57,519 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:38:57,520 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:57,520 (beam_search:483) INFO: best hypo: ▁THE▁IS

2024-10-27 18:38:57,522 (asr_inference:509) INFO: speech length: 114592
2024-10-27 18:39:02,426 (beam_search:428) INFO: decoder input length: 89
2024-10-27 18:39:02,426 (beam_search:429) INFO: max output length: 89
2024-10-27 18:39:02,426 (beam_search:430) INFO: min output length: 0
2024-10-27 18:39:02,491 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:39:02,491 (beam_search:476) INFO:  -1.50 * 1.0 =  -1.50 for ctc
2024-10-27 18:39:02,491 (beam_search:479) INFO: total log probability: -1.50
2024-10-27 18:39:02,491 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 18:39:02,491 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:39:02,492 (beam_search:483) INFO: best hypo: ▁IS▁STICKING▁THE

2024-10-27 18:39:02,494 (asr_inference:509) INFO: speech length: 313636
2024-10-27 18:58:01,685 (beam_search:428) INFO: decoder input length: 244
2024-10-27 18:58:01,730 (beam_search:429) INFO: max output length: 244
2024-10-27 18:58:01,730 (beam_search:430) INFO: min output length: 0
2024-10-27 18:58:02,503 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:58:02,503 (beam_search:476) INFO:  -9.74 * 1.0 =  -9.74 for ctc
2024-10-27 18:58:02,503 (beam_search:479) INFO: total log probability: -9.74
2024-10-27 18:58:02,503 (beam_search:480) INFO: normalized log probability: -0.54
2024-10-27 18:58:02,503 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:58:02,503 (beam_search:483) INFO: best hypo: ▁BECAUSE▁IT▁THE▁IS▁AND▁THE▁THEY'RE▁OF▁THEY▁OF▁AND▁THEY▁TO▁EACH

2024-10-27 18:58:02,508 (asr_inference:509) INFO: speech length: 118898
2024-10-27 18:58:07,236 (beam_search:428) INFO: decoder input length: 92
2024-10-27 18:58:07,237 (beam_search:429) INFO: max output length: 92
2024-10-27 18:58:07,237 (beam_search:430) INFO: min output length: 0
2024-10-27 18:58:07,337 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:58:07,337 (beam_search:476) INFO:  -0.19 * 1.0 =  -0.19 for ctc
2024-10-27 18:58:07,337 (beam_search:479) INFO: total log probability: -0.19
2024-10-27 18:58:07,337 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:58:07,338 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:58:07,338 (beam_search:483) INFO: best hypo: ▁IT▁IS▁MADE▁OF▁NOT

2024-10-27 18:58:07,341 (asr_inference:509) INFO: speech length: 156670
2024-10-27 18:58:13,933 (beam_search:428) INFO: decoder input length: 121
2024-10-27 18:58:13,933 (beam_search:429) INFO: max output length: 121
2024-10-27 18:58:13,933 (beam_search:430) INFO: min output length: 0
2024-10-27 18:58:14,014 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:58:14,014 (beam_search:476) INFO:  -1.50 * 1.0 =  -1.50 for ctc
2024-10-27 18:58:14,014 (beam_search:479) INFO: total log probability: -1.50
2024-10-27 18:58:14,014 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 18:58:14,014 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:58:14,014 (beam_search:483) INFO: best hypo: ▁WOOD▁PLASTIC▁YEAH

2024-10-27 18:58:14,017 (asr_inference:509) INFO: speech length: 117056
2024-10-27 18:58:18,561 (beam_search:428) INFO: decoder input length: 90
2024-10-27 18:58:18,562 (beam_search:429) INFO: max output length: 90
2024-10-27 18:58:18,562 (beam_search:430) INFO: min output length: 0
2024-10-27 18:58:18,692 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:58:18,693 (beam_search:476) INFO:  -2.54 * 1.0 =  -2.54 for ctc
2024-10-27 18:58:18,693 (beam_search:479) INFO: total log probability: -2.54
2024-10-27 18:58:18,693 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:58:18,693 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:58:18,693 (beam_search:483) INFO: best hypo: ▁THAT▁ARE▁EITHER▁MADE▁OF▁IRON▁OR

2024-10-27 18:58:18,705 (asr_inference:509) INFO: speech length: 91500
2024-10-27 18:58:22,405 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:58:22,405 (beam_search:429) INFO: max output length: 70
2024-10-27 18:58:22,405 (beam_search:430) INFO: min output length: 0
2024-10-27 18:58:22,512 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:58:22,512 (beam_search:476) INFO:  -4.17 * 1.0 =  -4.17 for ctc
2024-10-27 18:58:22,512 (beam_search:479) INFO: total log probability: -4.17
2024-10-27 18:58:22,512 (beam_search:480) INFO: normalized log probability: -0.38
2024-10-27 18:58:22,512 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:58:22,512 (beam_search:483) INFO: best hypo: ▁THE▁WOOD'▁HAVE▁SO▁STICK▁TO▁THE▁MAGNET

2024-10-27 18:58:22,525 (asr_inference:509) INFO: speech length: 187278
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 843, in inference
    results = speech2text(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 515, in __call__
    enc, enc_olens = self.asr_model.encode(**batch)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 378, in encode
    feats, feats_lengths = self._extract_feats(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 445, in _extract_feats
    feats, feats_lengths = self.frontend(speech, speech_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/frontend/s3prl.py", line 99, in forward
    feats, feats_lens = self.upstream(input, input_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/nn/upstream.py", line 209, in forward
    hidden_states = self.upstream(wavs_list)["hidden_states"]
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/interfaces.py", line 103, in __call__
    result = super().__call__(wavs, *args, **kwargs) or {}
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/expert.py", line 83, in forward
    features, feat_padding_mask = self.model.extract_features(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 389, in extract_features
    x, layer_results = self.encoder(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 592, in forward
    x, layer_results = self.extract_features(x, padding_mask, streaming_mask, layer)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 626, in extract_features
    x, z, pos_bias = layer(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 742, in forward
    x = self.activation_fn(self.fc1(x))
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1498924) is killed by signal: Killed. 
# Accounting: time=5276 threads=1
# Ended (code 1) at Sun Oct 27 18:58:53 EDT 2024, elapsed time 5276 seconds
# python3 -m espnet2.bin.asr_inference --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/keys.8.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.cer_ctc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/output.8 --config conf/decode_asr_ctc_greedy.yaml 
# Started at Sun Oct 27 17:30:57 EDT 2024
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 0 --data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --key_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/keys.8.scp --asr_train_config exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml --asr_model_file exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/valid.cer_ctc.ave.pth --output_dir exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/output.8 --config conf/decode_asr_ctc_greedy.yaml
2024-10-27 17:31:05,875 (abs_task:2300) INFO: config file: exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/config.yaml
2024-10-27 17:31:06,030 (asr:523) INFO: Vocabulary size: 5000
2024-10-27 17:31:06,098 (download:170) INFO: Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
2024-10-27 17:31:06,099 (download:181) INFO: Using URL's local file: ../asr2/ckpt/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
2024-10-27 17:31:06,982 (WavLM:254) INFO: WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
2024-10-27 17:31:16,348 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
2024-10-27 17:31:18,834 (asr_inference:372) INFO: BatchBeamSearch implementation is selected.
2024-10-27 17:31:18,834 (asr_inference:383) INFO: Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict()
)
2024-10-27 17:31:18,834 (asr_inference:384) INFO: Decoding device=cpu, dtype=float32
2024-10-27 17:31:18,836 (asr_inference:462) INFO: Text tokenizer: SentencepiecesTokenizer(model="data/en_token_list/bpe_unigram5000/bpe.model")
2024-10-27 17:31:18,838 (asr:495) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
2024-10-27 17:31:20,135 (asr_inference:509) INFO: speech length: 211248
2024-10-27 17:31:28,809 (beam_search:428) INFO: decoder input length: 164
2024-10-27 17:31:28,809 (beam_search:429) INFO: max output length: 164
2024-10-27 17:31:28,809 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:29,886 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:29,887 (beam_search:476) INFO:  -4.83 * 1.0 =  -4.83 for ctc
2024-10-27 17:31:29,887 (beam_search:479) INFO: total log probability: -4.83
2024-10-27 17:31:29,887 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:31:29,887 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:29,887 (beam_search:483) INFO: best hypo: ▁BECAUSE▁THE▁MAGNET▁HAS▁ENOUGH▁ENERGY▁THAT▁ITS▁ENERGY▁IS▁THAN▁THE▁PAPER▁SO▁WHEN▁YOU▁PUT▁THE▁MAGNET▁ON▁YOU▁PUT▁THE▁PAPER▁IT'▁STILL▁STICK▁BECAUSE▁OF▁THE▁MAGNET'S

2024-10-27 17:31:29,895 (asr_inference:509) INFO: speech length: 17152
2024-10-27 17:31:30,809 (beam_search:428) INFO: decoder input length: 12
2024-10-27 17:31:30,809 (beam_search:429) INFO: max output length: 12
2024-10-27 17:31:30,809 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:30,822 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:30,822 (beam_search:476) INFO:  -0.06 * 1.0 =  -0.06 for ctc
2024-10-27 17:31:30,822 (beam_search:479) INFO: total log probability: -0.06
2024-10-27 17:31:30,822 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 17:31:30,822 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:30,822 (beam_search:483) INFO: best hypo: ▁I▁DON'T▁KNOW

2024-10-27 17:31:30,824 (asr_inference:509) INFO: speech length: 138560
2024-10-27 17:31:36,142 (beam_search:428) INFO: decoder input length: 107
2024-10-27 17:31:36,143 (beam_search:429) INFO: max output length: 107
2024-10-27 17:31:36,143 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:36,498 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:36,498 (beam_search:476) INFO:  -3.28 * 1.0 =  -3.28 for ctc
2024-10-27 17:31:36,498 (beam_search:479) INFO: total log probability: -3.28
2024-10-27 17:31:36,498 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:31:36,498 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:36,498 (beam_search:483) INFO: best hypo: ▁YES▁WE▁TRIED▁TO▁PAPER▁CLIPS▁ON▁UM▁THE▁MAGNET▁TO▁SEE▁IF▁IT▁WOULD▁STICK▁TO▁OTHER▁PAPER

2024-10-27 17:31:36,500 (asr_inference:509) INFO: speech length: 96144
2024-10-27 17:31:39,929 (beam_search:428) INFO: decoder input length: 74
2024-10-27 17:31:39,929 (beam_search:429) INFO: max output length: 74
2024-10-27 17:31:39,929 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:40,152 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:40,152 (beam_search:476) INFO:  -2.79 * 1.0 =  -2.79 for ctc
2024-10-27 17:31:40,152 (beam_search:479) INFO: total log probability: -2.79
2024-10-27 17:31:40,152 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:31:40,152 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:40,152 (beam_search:483) INFO: best hypo: ▁IT▁WAS▁TO▁GET▁ENOUGH▁ENERGY▁IN▁THE▁IN▁THE▁PAPER▁CLIP▁THAT▁IT▁WOULD▁STICK▁TO▁ANOTHER▁ONE

2024-10-27 17:31:40,154 (asr_inference:509) INFO: speech length: 45072
2024-10-27 17:31:41,776 (beam_search:428) INFO: decoder input length: 34
2024-10-27 17:31:41,776 (beam_search:429) INFO: max output length: 34
2024-10-27 17:31:41,776 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:41,815 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:41,816 (beam_search:476) INFO:  -1.07 * 1.0 =  -1.07 for ctc
2024-10-27 17:31:41,816 (beam_search:479) INFO: total log probability: -1.07
2024-10-27 17:31:41,816 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:31:41,816 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:41,816 (beam_search:483) INFO: best hypo: ▁YES▁IT▁DID▁BECAUSE▁OF▁THEING

2024-10-27 17:31:41,818 (asr_inference:509) INFO: speech length: 322384
2024-10-27 17:31:56,347 (beam_search:428) INFO: decoder input length: 251
2024-10-27 17:31:56,347 (beam_search:429) INFO: max output length: 251
2024-10-27 17:31:56,348 (beam_search:430) INFO: min output length: 0
2024-10-27 17:31:58,754 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:31:58,754 (beam_search:476) INFO:  -5.31 * 1.0 =  -5.31 for ctc
2024-10-27 17:31:58,754 (beam_search:479) INFO: total log probability: -5.31
2024-10-27 17:31:58,754 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:31:58,754 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:31:58,754 (beam_search:483) INFO: best hypo: ▁IT▁US▁MORE▁ABOUT▁MAGNETS▁BECAUSE▁IT▁IT▁HELPED▁THAT▁IF▁YOU▁SOMETHING▁THAT▁COULD▁STICK▁TO▁A▁MAGNET▁AND▁IF▁YOU▁IT▁WITH▁THAT▁THE▁SAME▁THING▁LIKE▁A▁NAIL▁IF▁YOU▁IT▁IN▁THE▁MAGNET▁AND▁IF▁YOU▁IT▁WITH▁ANOTHER▁NAIL▁IT▁WOULD▁STILL▁UM▁IT▁WOULD

2024-10-27 17:31:58,756 (asr_inference:509) INFO: speech length: 332416
2024-10-27 17:32:13,361 (beam_search:428) INFO: decoder input length: 259
2024-10-27 17:32:13,362 (beam_search:429) INFO: max output length: 259
2024-10-27 17:32:13,362 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:15,971 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:15,972 (beam_search:476) INFO:  -6.12 * 1.0 =  -6.12 for ctc
2024-10-27 17:32:15,972 (beam_search:479) INFO: total log probability: -6.12
2024-10-27 17:32:15,972 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:32:15,972 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:15,972 (beam_search:483) INFO: best hypo: ▁THE▁MAGNET▁THE▁THE▁MAGNET▁THAT'S▁IS▁MOVING▁THE▁THAT'S▁ON▁SO▁SO▁IF▁IT▁WAS▁MORE▁THEN▁THE▁MAGNET'T▁MOVE▁BECAUSE▁IT'S▁TOO▁AND▁IF▁IT▁IT'S▁LIKE▁YOU▁COULD▁THE▁MAGNET▁COULD▁STILL▁HAVE▁ENOUGH▁ENERGY▁THAT▁IT'LL▁GO

2024-10-27 17:32:15,976 (asr_inference:509) INFO: speech length: 111024
2024-10-27 17:32:20,100 (beam_search:428) INFO: decoder input length: 86
2024-10-27 17:32:20,100 (beam_search:429) INFO: max output length: 86
2024-10-27 17:32:20,100 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:20,357 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:20,357 (beam_search:476) INFO:  -2.72 * 1.0 =  -2.72 for ctc
2024-10-27 17:32:20,357 (beam_search:479) INFO: total log probability: -2.72
2024-10-27 17:32:20,357 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:32:20,357 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:20,357 (beam_search:483) INFO: best hypo: ▁TODAY▁UM▁WE▁LEARNED▁ABOUT▁HOW▁IT'S▁TO▁HAVE▁AND▁NO▁AND▁ALL▁THOSE▁OTHER▁STUFF

2024-10-27 17:32:20,360 (asr_inference:509) INFO: speech length: 138832
2024-10-27 17:32:25,466 (beam_search:428) INFO: decoder input length: 107
2024-10-27 17:32:25,466 (beam_search:429) INFO: max output length: 107
2024-10-27 17:32:25,466 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:25,670 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:25,670 (beam_search:476) INFO:  -2.33 * 1.0 =  -2.33 for ctc
2024-10-27 17:32:25,670 (beam_search:479) INFO: total log probability: -2.33
2024-10-27 17:32:25,670 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:32:25,670 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:25,670 (beam_search:483) INFO: best hypo: ▁AND▁IS▁TOO▁BUT▁IS▁REALLY▁GOOD▁SO▁WE▁MADE▁FOR

2024-10-27 17:32:25,673 (asr_inference:509) INFO: speech length: 24544
2024-10-27 17:32:26,709 (beam_search:428) INFO: decoder input length: 18
2024-10-27 17:32:26,709 (beam_search:429) INFO: max output length: 18
2024-10-27 17:32:26,709 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:26,723 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:26,723 (beam_search:476) INFO:  -0.24 * 1.0 =  -0.24 for ctc
2024-10-27 17:32:26,723 (beam_search:479) INFO: total log probability: -0.24
2024-10-27 17:32:26,723 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:32:26,723 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:26,723 (beam_search:483) INFO: best hypo: ▁I▁REALLY▁WAS

2024-10-27 17:32:26,726 (asr_inference:509) INFO: speech length: 13840
2024-10-27 17:32:27,513 (beam_search:428) INFO: decoder input length: 10
2024-10-27 17:32:27,513 (beam_search:429) INFO: max output length: 10
2024-10-27 17:32:27,513 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:27,523 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:27,523 (beam_search:476) INFO:  -0.21 * 1.0 =  -0.21 for ctc
2024-10-27 17:32:27,523 (beam_search:479) INFO: total log probability: -0.21
2024-10-27 17:32:27,523 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:32:27,523 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:27,523 (beam_search:483) INFO: best hypo: ▁YOU'RE

2024-10-27 17:32:27,526 (asr_inference:509) INFO: speech length: 183392
2024-10-27 17:32:34,491 (beam_search:428) INFO: decoder input length: 142
2024-10-27 17:32:34,491 (beam_search:429) INFO: max output length: 142
2024-10-27 17:32:34,491 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:35,300 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:35,300 (beam_search:476) INFO:  -1.43 * 1.0 =  -1.43 for ctc
2024-10-27 17:32:35,300 (beam_search:479) INFO: total log probability: -1.43
2024-10-27 17:32:35,300 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:32:35,300 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:35,300 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁BATTERY▁IS▁ABOUT▁IF▁YOU▁DIDN'T▁HAVE▁THE▁BATTERY▁YOU'T▁GET▁THE▁ENERGY▁TO▁UM▁LIGHT▁UP▁THE▁LIGHT▁BULB▁AND▁IT▁WOULDN'T▁WORK

2024-10-27 17:32:35,304 (asr_inference:509) INFO: speech length: 114688
2024-10-27 17:32:39,625 (beam_search:428) INFO: decoder input length: 89
2024-10-27 17:32:39,626 (beam_search:429) INFO: max output length: 89
2024-10-27 17:32:39,626 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:39,865 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:39,865 (beam_search:476) INFO:  -0.75 * 1.0 =  -0.75 for ctc
2024-10-27 17:32:39,865 (beam_search:479) INFO: total log probability: -0.75
2024-10-27 17:32:39,865 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:32:39,865 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:39,865 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁ISING▁ENERGY▁TO▁THE▁LIGHT▁BULB▁SO▁THE▁LIGHT▁BULB▁IS▁LIGHTING▁UP

2024-10-27 17:32:39,867 (asr_inference:509) INFO: speech length: 332176
2024-10-27 17:32:54,935 (beam_search:428) INFO: decoder input length: 259
2024-10-27 17:32:54,935 (beam_search:429) INFO: max output length: 259
2024-10-27 17:32:54,935 (beam_search:430) INFO: min output length: 0
2024-10-27 17:32:57,263 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:32:57,263 (beam_search:476) INFO:  -2.52 * 1.0 =  -2.52 for ctc
2024-10-27 17:32:57,263 (beam_search:479) INFO: total log probability: -2.52
2024-10-27 17:32:57,263 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:32:57,263 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:32:57,263 (beam_search:483) INFO: best hypo: ▁ONE▁PART▁OF▁THE▁WIRE▁IS▁IN▁THE▁ON▁THE▁BOTTOM▁OF▁THE▁LIGHT▁BULB▁AND▁IT'S▁CONNECTED▁TO▁THE▁BATTERY▁AND▁THE▁OTHER▁WIRE▁IS▁CONNECTED▁TO▁THE▁OTHER▁SIDE▁OF▁THE▁BATTERY▁AND▁IT'S▁CONNECTED▁TO▁THE▁LIGHT▁BULB'S▁METAL

2024-10-27 17:32:57,266 (asr_inference:509) INFO: speech length: 127072
2024-10-27 17:33:02,011 (beam_search:428) INFO: decoder input length: 98
2024-10-27 17:33:02,011 (beam_search:429) INFO: max output length: 98
2024-10-27 17:33:02,011 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:02,384 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:02,384 (beam_search:476) INFO:  -1.78 * 1.0 =  -1.78 for ctc
2024-10-27 17:33:02,384 (beam_search:479) INFO: total log probability: -1.78
2024-10-27 17:33:02,384 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:33:02,384 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:02,384 (beam_search:483) INFO: best hypo: ▁IF▁IF▁YOU▁DIDN'T▁PUT▁IT▁IN▁THE▁METAL▁LIKE▁WHERE▁THE▁METAL▁WAS▁THE▁IT▁WOULDN'T▁LIGHT▁UP

2024-10-27 17:33:02,386 (asr_inference:509) INFO: speech length: 103360
2024-10-27 17:33:06,072 (beam_search:428) INFO: decoder input length: 80
2024-10-27 17:33:06,072 (beam_search:429) INFO: max output length: 80
2024-10-27 17:33:06,072 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:06,310 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:06,310 (beam_search:476) INFO:  -4.17 * 1.0 =  -4.17 for ctc
2024-10-27 17:33:06,310 (beam_search:479) INFO: total log probability: -4.17
2024-10-27 17:33:06,310 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:33:06,310 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:06,310 (beam_search:483) INFO: best hypo: ▁IF▁IT▁WASN'T▁THE▁IT▁THE▁ENERGY▁WOULDN'T▁GO▁THROUGH▁SO▁THE▁LIGHT▁BULB

2024-10-27 17:33:06,312 (asr_inference:509) INFO: speech length: 84272
2024-10-27 17:33:09,482 (beam_search:428) INFO: decoder input length: 65
2024-10-27 17:33:09,482 (beam_search:429) INFO: max output length: 65
2024-10-27 17:33:09,482 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:09,570 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:09,570 (beam_search:476) INFO:  -1.17 * 1.0 =  -1.17 for ctc
2024-10-27 17:33:09,570 (beam_search:479) INFO: total log probability: -1.17
2024-10-27 17:33:09,570 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:33:09,570 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:09,570 (beam_search:483) INFO: best hypo: ▁THAT'S▁THE▁THE▁WOULD▁GO▁THROUGH

2024-10-27 17:33:09,573 (asr_inference:509) INFO: speech length: 305520
2024-10-27 17:33:22,612 (beam_search:428) INFO: decoder input length: 238
2024-10-27 17:33:22,613 (beam_search:429) INFO: max output length: 238
2024-10-27 17:33:22,613 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:24,806 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:24,806 (beam_search:476) INFO:  -2.35 * 1.0 =  -2.35 for ctc
2024-10-27 17:33:24,806 (beam_search:479) INFO: total log probability: -2.35
2024-10-27 17:33:24,806 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:33:24,806 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:24,806 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁WELL▁THE▁BATTERY'S▁GIVING▁ENERGY▁TO▁THE▁LIGHT▁BULB▁WHERE▁IT'S▁IN▁THE▁THE▁METAL▁AND▁IT'S▁GOING▁THROUGH▁AND▁DOWN▁THE▁LIGHT▁BULB▁BACK▁TO▁THE▁BATTERY▁AND▁IT'S▁GOING▁BACK▁TO▁THE▁LIGHT▁BULB

2024-10-27 17:33:24,809 (asr_inference:509) INFO: speech length: 126944
2024-10-27 17:33:29,534 (beam_search:428) INFO: decoder input length: 98
2024-10-27 17:33:29,534 (beam_search:429) INFO: max output length: 98
2024-10-27 17:33:29,534 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:29,867 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:29,867 (beam_search:476) INFO:  -2.18 * 1.0 =  -2.18 for ctc
2024-10-27 17:33:29,867 (beam_search:479) INFO: total log probability: -2.18
2024-10-27 17:33:29,867 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:33:29,867 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:29,868 (beam_search:483) INFO: best hypo: ▁IT'S▁GOING▁THE▁SAME▁WAY▁BECAUSE▁WHEN▁IT▁IT'S▁GOING▁AROUND▁AND▁BACK▁TO▁THE▁SAME

2024-10-27 17:33:29,871 (asr_inference:509) INFO: speech length: 275776
2024-10-27 17:33:41,426 (beam_search:428) INFO: decoder input length: 214
2024-10-27 17:33:41,426 (beam_search:429) INFO: max output length: 214
2024-10-27 17:33:41,426 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:43,568 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:43,568 (beam_search:476) INFO:  -3.05 * 1.0 =  -3.05 for ctc
2024-10-27 17:33:43,568 (beam_search:479) INFO: total log probability: -3.05
2024-10-27 17:33:43,568 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:33:43,568 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:43,568 (beam_search:483) INFO: best hypo: ▁WELL▁FIRST▁WHEN▁IT▁IT'S▁GOING▁THE▁SAME▁WAY▁THEN▁IT▁TURNS▁AND▁THEN▁IT▁GOES▁DIFFERENT▁AND▁THEN▁LIKE▁FIRST▁WHEN▁IT▁COMES▁OUT▁OF▁THE▁BATTERY▁IT'S▁GOING▁A▁DIFFERENT▁WAY▁AND▁THEN▁WHEN▁IT'S▁BACK▁IT'S▁GOING▁A▁DIFFERENT▁WAY

2024-10-27 17:33:43,571 (asr_inference:509) INFO: speech length: 196496
2024-10-27 17:33:51,479 (beam_search:428) INFO: decoder input length: 153
2024-10-27 17:33:51,480 (beam_search:429) INFO: max output length: 153
2024-10-27 17:33:51,480 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:52,271 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:52,271 (beam_search:476) INFO:  -3.24 * 1.0 =  -3.24 for ctc
2024-10-27 17:33:52,271 (beam_search:479) INFO: total log probability: -3.24
2024-10-27 17:33:52,271 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:33:52,271 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:52,271 (beam_search:483) INFO: best hypo: ▁ON▁THE▁BATTERY▁AND▁IT▁GOES▁ALL▁AROUND▁THE▁WIRES▁TO▁THE▁LIGHT▁BULB▁AND▁IT▁COMES▁BACK▁AND▁IT▁AND▁IT▁GOES▁THROUGH▁THE▁SAME▁WAY▁AGAIN▁EVERY▁TIME

2024-10-27 17:33:52,274 (asr_inference:509) INFO: speech length: 112736
2024-10-27 17:33:56,356 (beam_search:428) INFO: decoder input length: 87
2024-10-27 17:33:56,356 (beam_search:429) INFO: max output length: 87
2024-10-27 17:33:56,356 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:56,689 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:56,689 (beam_search:476) INFO:  -1.99 * 1.0 =  -1.99 for ctc
2024-10-27 17:33:56,689 (beam_search:479) INFO: total log probability: -1.99
2024-10-27 17:33:56,689 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:33:56,689 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:56,689 (beam_search:483) INFO: best hypo: ▁YES▁BECAUSE▁IF▁YOU▁PUT▁IT▁THE▁WRONG▁WAY▁WHEN▁YOU'RE▁PUTTING▁ON▁A▁BATTERY▁THE▁BATTERY▁WON'T▁WORK

2024-10-27 17:33:56,692 (asr_inference:509) INFO: speech length: 38960
2024-10-27 17:33:58,226 (beam_search:428) INFO: decoder input length: 29
2024-10-27 17:33:58,226 (beam_search:429) INFO: max output length: 29
2024-10-27 17:33:58,227 (beam_search:430) INFO: min output length: 0
2024-10-27 17:33:58,237 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:33:58,238 (beam_search:476) INFO:  -0.74 * 1.0 =  -0.74 for ctc
2024-10-27 17:33:58,238 (beam_search:479) INFO: total log probability: -0.74
2024-10-27 17:33:58,238 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:33:58,238 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:33:58,238 (beam_search:483) INFO: best hypo: ▁THE

2024-10-27 17:33:58,240 (asr_inference:509) INFO: speech length: 131152
2024-10-27 17:34:03,035 (beam_search:428) INFO: decoder input length: 101
2024-10-27 17:34:03,036 (beam_search:429) INFO: max output length: 101
2024-10-27 17:34:03,036 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:03,556 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:03,556 (beam_search:476) INFO:  -2.73 * 1.0 =  -2.73 for ctc
2024-10-27 17:34:03,556 (beam_search:479) INFO: total log probability: -2.73
2024-10-27 17:34:03,556 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:34:03,557 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:03,557 (beam_search:483) INFO: best hypo: ▁WELL▁I▁ON▁THE▁BATTERY▁IT'S▁GOING▁THE▁OPPOSITE▁WAY▁IF▁I▁CLICK▁ON▁THE▁BATTERY▁AGAIN▁IT'S▁GOING▁TO▁GO▁A▁DIFFERENT▁WAY

2024-10-27 17:34:03,559 (asr_inference:509) INFO: speech length: 108624
2024-10-27 17:34:07,801 (beam_search:428) INFO: decoder input length: 84
2024-10-27 17:34:07,801 (beam_search:429) INFO: max output length: 84
2024-10-27 17:34:07,801 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:08,051 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:08,051 (beam_search:476) INFO:  -2.28 * 1.0 =  -2.28 for ctc
2024-10-27 17:34:08,051 (beam_search:479) INFO: total log probability: -2.28
2024-10-27 17:34:08,051 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:34:08,051 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:08,051 (beam_search:483) INFO: best hypo: ▁WELL▁I▁THINK▁IT'S▁ABOUT▁LIKE▁THE▁BATTERYING▁OUT▁ENERGY▁IT'S▁LIKE▁A

2024-10-27 17:34:08,053 (asr_inference:509) INFO: speech length: 10864
2024-10-27 17:34:08,669 (beam_search:428) INFO: decoder input length: 7
2024-10-27 17:34:08,669 (beam_search:429) INFO: max output length: 7
2024-10-27 17:34:08,669 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:08,678 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:08,678 (beam_search:476) INFO:  -1.19 * 1.0 =  -1.19 for ctc
2024-10-27 17:34:08,678 (beam_search:479) INFO: total log probability: -1.19
2024-10-27 17:34:08,678 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:34:08,678 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:08,678 (beam_search:483) INFO: best hypo: ▁I'M▁GOOD

2024-10-27 17:34:08,680 (asr_inference:509) INFO: speech length: 48384
2024-10-27 17:34:10,524 (beam_search:428) INFO: decoder input length: 37
2024-10-27 17:34:10,524 (beam_search:429) INFO: max output length: 37
2024-10-27 17:34:10,524 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:10,559 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:10,559 (beam_search:476) INFO:  -1.14 * 1.0 =  -1.14 for ctc
2024-10-27 17:34:10,559 (beam_search:479) INFO: total log probability: -1.14
2024-10-27 17:34:10,559 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:34:10,559 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:10,559 (beam_search:483) INFO: best hypo: ▁OR▁A▁B▁OR▁D

2024-10-27 17:34:10,561 (asr_inference:509) INFO: speech length: 441824
2024-10-27 17:34:31,430 (beam_search:428) INFO: decoder input length: 344
2024-10-27 17:34:31,430 (beam_search:429) INFO: max output length: 344
2024-10-27 17:34:31,430 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:35,813 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:35,814 (beam_search:476) INFO:  -7.06 * 1.0 =  -7.06 for ctc
2024-10-27 17:34:35,814 (beam_search:479) INFO: total log probability: -7.06
2024-10-27 17:34:35,814 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:34:35,814 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:35,814 (beam_search:483) INFO: best hypo: ▁WELL▁WHAT▁I▁THAT▁IS▁LIKE▁WHEN▁YOU▁IF▁YOU▁CONNECT▁LIKE▁WHEN▁YOU▁CONNECT▁A▁LIGHT▁BULB▁LIKE▁IT'S▁LIKE▁CONNECTING▁A▁MOTOR▁TO▁THE▁BATTERY▁AND▁THE▁OTHER▁WIRE▁THE▁OTHER▁WIRE▁TO▁THE▁OTHER▁OF▁THE▁BATTERY▁AND▁YOU▁CONNECT▁IT▁TO▁BOTH▁OF▁IF▁YOU▁PUT▁BOTH▁OF▁THE▁WIRES▁TOGETHER▁INSTEAD▁OF▁JUST▁IT▁ON▁A▁IT'LL

2024-10-27 17:34:35,816 (asr_inference:509) INFO: speech length: 77152
2024-10-27 17:34:38,730 (beam_search:428) INFO: decoder input length: 59
2024-10-27 17:34:38,730 (beam_search:429) INFO: max output length: 59
2024-10-27 17:34:38,730 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:38,858 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:38,858 (beam_search:476) INFO:  -1.22 * 1.0 =  -1.22 for ctc
2024-10-27 17:34:38,858 (beam_search:479) INFO: total log probability: -1.22
2024-10-27 17:34:38,858 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:34:38,858 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:38,858 (beam_search:483) INFO: best hypo: ▁WE▁HAVE▁A▁BATTERY▁WIRE▁AND▁A▁LIGHT▁BULB▁WITH▁THE▁LIGHT▁BULB

2024-10-27 17:34:38,860 (asr_inference:509) INFO: speech length: 259760
2024-10-27 17:34:49,733 (beam_search:428) INFO: decoder input length: 202
2024-10-27 17:34:49,734 (beam_search:429) INFO: max output length: 202
2024-10-27 17:34:49,734 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:51,218 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:51,218 (beam_search:476) INFO:  -4.21 * 1.0 =  -4.21 for ctc
2024-10-27 17:34:51,218 (beam_search:479) INFO: total log probability: -4.21
2024-10-27 17:34:51,218 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:34:51,218 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:51,218 (beam_search:483) INFO: best hypo: ▁FIRST▁WE▁PUT▁THE▁BATTERY▁IN▁WHERE▁THE▁BATTERY▁THEN▁WE▁CAN▁GET▁THE▁WIRE▁AND▁CONNECT▁IT▁TO▁THE▁AND▁THEN▁WE▁COULD▁PUT▁THE▁LIGHT▁BULB▁WHERE▁THE▁LIGHT▁BULB▁GOES▁AND▁THEN▁WE▁COULD▁CONNECT▁THE▁WIRE

2024-10-27 17:34:51,222 (asr_inference:509) INFO: speech length: 18112
2024-10-27 17:34:52,056 (beam_search:428) INFO: decoder input length: 13
2024-10-27 17:34:52,056 (beam_search:429) INFO: max output length: 13
2024-10-27 17:34:52,056 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:52,070 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:52,070 (beam_search:476) INFO:  -0.72 * 1.0 =  -0.72 for ctc
2024-10-27 17:34:52,070 (beam_search:479) INFO: total log probability: -0.72
2024-10-27 17:34:52,070 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:34:52,070 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:52,070 (beam_search:483) INFO: best hypo: ▁YOU▁HAD▁TO▁IN▁LIKE

2024-10-27 17:34:52,073 (asr_inference:509) INFO: speech length: 156208
2024-10-27 17:34:58,058 (beam_search:428) INFO: decoder input length: 121
2024-10-27 17:34:58,059 (beam_search:429) INFO: max output length: 121
2024-10-27 17:34:58,059 (beam_search:430) INFO: min output length: 0
2024-10-27 17:34:58,529 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:34:58,529 (beam_search:476) INFO:  -8.04 * 1.0 =  -8.04 for ctc
2024-10-27 17:34:58,529 (beam_search:479) INFO: total log probability: -8.04
2024-10-27 17:34:58,529 (beam_search:480) INFO: normalized log probability: -0.32
2024-10-27 17:34:58,529 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:34:58,529 (beam_search:483) INFO: best hypo: ▁AND▁A▁AND▁B▁AND▁AFTER▁YOU▁WERE▁DONE▁WITH▁THAT▁WE▁HA▁WE▁HAD▁TO▁DO▁TWO▁WE▁GOT▁TO▁WITH▁PEOPLE

2024-10-27 17:34:58,531 (asr_inference:509) INFO: speech length: 90352
2024-10-27 17:35:01,768 (beam_search:428) INFO: decoder input length: 70
2024-10-27 17:35:01,768 (beam_search:429) INFO: max output length: 70
2024-10-27 17:35:01,768 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:01,862 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:01,862 (beam_search:476) INFO:  -1.17 * 1.0 =  -1.17 for ctc
2024-10-27 17:35:01,862 (beam_search:479) INFO: total log probability: -1.17
2024-10-27 17:35:01,862 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:35:01,862 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:01,862 (beam_search:483) INFO: best hypo: ▁WE'VE▁BEEN▁DOING▁ELECTRICITY▁AND▁MAGNETISM

2024-10-27 17:35:01,864 (asr_inference:509) INFO: speech length: 22416
2024-10-27 17:35:02,850 (beam_search:428) INFO: decoder input length: 17
2024-10-27 17:35:02,851 (beam_search:429) INFO: max output length: 17
2024-10-27 17:35:02,851 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:02,858 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:02,859 (beam_search:476) INFO:  -0.11 * 1.0 =  -0.11 for ctc
2024-10-27 17:35:02,859 (beam_search:479) INFO: total log probability: -0.11
2024-10-27 17:35:02,859 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:35:02,859 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:02,859 (beam_search:483) INFO: best hypo: ▁WE

2024-10-27 17:35:02,861 (asr_inference:509) INFO: speech length: 116048
2024-10-27 17:35:06,979 (beam_search:428) INFO: decoder input length: 90
2024-10-27 17:35:06,979 (beam_search:429) INFO: max output length: 90
2024-10-27 17:35:06,979 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:07,223 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:07,223 (beam_search:476) INFO:  -1.67 * 1.0 =  -1.67 for ctc
2024-10-27 17:35:07,223 (beam_search:479) INFO: total log probability: -1.67
2024-10-27 17:35:07,223 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:35:07,223 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:07,224 (beam_search:483) INFO: best hypo: ▁WE'VE▁BEEN▁THE▁LIGHT▁BULB▁LIGHT▁OR▁THE▁THE▁TIME▁WE▁HAD▁WE

2024-10-27 17:35:07,226 (asr_inference:509) INFO: speech length: 53104
2024-10-27 17:35:09,262 (beam_search:428) INFO: decoder input length: 40
2024-10-27 17:35:09,262 (beam_search:429) INFO: max output length: 40
2024-10-27 17:35:09,262 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:09,304 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:09,305 (beam_search:476) INFO:  -1.67 * 1.0 =  -1.67 for ctc
2024-10-27 17:35:09,305 (beam_search:479) INFO: total log probability: -1.67
2024-10-27 17:35:09,305 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:35:09,305 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:09,305 (beam_search:483) INFO: best hypo: ▁WE▁MADE▁THE▁UM▁A▁MOTOR

2024-10-27 17:35:09,307 (asr_inference:509) INFO: speech length: 214720
2024-10-27 17:35:18,079 (beam_search:428) INFO: decoder input length: 167
2024-10-27 17:35:18,079 (beam_search:429) INFO: max output length: 167
2024-10-27 17:35:18,079 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:19,116 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:19,116 (beam_search:476) INFO:  -3.52 * 1.0 =  -3.52 for ctc
2024-10-27 17:35:19,116 (beam_search:479) INFO: total log probability: -3.52
2024-10-27 17:35:19,116 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:35:19,116 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:19,116 (beam_search:483) INFO: best hypo: ▁THE▁D▁CELL▁WE▁HOOKED▁TWO▁WIRES▁UP▁TO▁THE▁D▁CELL▁THEN▁WE▁UM▁PUT▁EM▁UP▁TO▁THE▁LIGHT▁BULB▁SO▁THE▁ELECTRICITY▁FLOW▁THROUGH▁THE▁WIRES▁TO▁THE▁LIGHT▁BULB

2024-10-27 17:35:19,118 (asr_inference:509) INFO: speech length: 57808
2024-10-27 17:35:21,220 (beam_search:428) INFO: decoder input length: 44
2024-10-27 17:35:21,220 (beam_search:429) INFO: max output length: 44
2024-10-27 17:35:21,220 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:21,292 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:21,292 (beam_search:476) INFO:  -1.25 * 1.0 =  -1.25 for ctc
2024-10-27 17:35:21,292 (beam_search:479) INFO: total log probability: -1.25
2024-10-27 17:35:21,292 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:35:21,292 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:21,292 (beam_search:483) INFO: best hypo: ▁A▁LITTLE▁BIT▁BUT▁WE▁ONLY▁HAD▁ONE▁LIGHT▁BULB

2024-10-27 17:35:21,295 (asr_inference:509) INFO: speech length: 341728
2024-10-27 17:35:36,228 (beam_search:428) INFO: decoder input length: 266
2024-10-27 17:35:36,229 (beam_search:429) INFO: max output length: 266
2024-10-27 17:35:36,229 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:38,465 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:38,466 (beam_search:476) INFO:  -4.58 * 1.0 =  -4.58 for ctc
2024-10-27 17:35:38,466 (beam_search:479) INFO: total log probability: -4.58
2024-10-27 17:35:38,466 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:35:38,466 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:38,466 (beam_search:483) INFO: best hypo: ▁WELL▁I▁SEE▁THE▁THE▁UM▁THE▁FLOW▁UM▁GOING▁THROUGH▁THE▁LIGHT▁BULB▁THEN▁GOING▁THROUGH▁THE▁METAL▁THE▁THE▁BASE▁OF▁THE▁LIGHT▁BULB▁AND▁THEN▁FLOWING▁THROUGH▁INTO▁LIGHT▁BULB▁AND▁THEN▁THE▁WIRES▁THE▁NEGATIVE▁SIDE▁OF▁THE▁BATTERY

2024-10-27 17:35:38,469 (asr_inference:509) INFO: speech length: 92288
2024-10-27 17:35:41,910 (beam_search:428) INFO: decoder input length: 71
2024-10-27 17:35:41,910 (beam_search:429) INFO: max output length: 71
2024-10-27 17:35:41,910 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:42,084 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:42,084 (beam_search:476) INFO:  -1.44 * 1.0 =  -1.44 for ctc
2024-10-27 17:35:42,084 (beam_search:479) INFO: total log probability: -1.44
2024-10-27 17:35:42,084 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:35:42,084 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:42,084 (beam_search:483) INFO: best hypo: ▁IT▁IS▁DIFFERENT▁BECAUSE▁THEY▁HA▁UM▁YOU▁HAVE▁TWO▁D▁CELLS▁INSTEAD▁OF▁ONE

2024-10-27 17:35:42,087 (asr_inference:509) INFO: speech length: 129168
2024-10-27 17:35:46,855 (beam_search:428) INFO: decoder input length: 100
2024-10-27 17:35:46,855 (beam_search:429) INFO: max output length: 100
2024-10-27 17:35:46,855 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:47,080 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:47,080 (beam_search:476) INFO:  -1.55 * 1.0 =  -1.55 for ctc
2024-10-27 17:35:47,080 (beam_search:479) INFO: total log probability: -1.55
2024-10-27 17:35:47,080 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:35:47,080 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:47,080 (beam_search:483) INFO: best hypo: ▁THE▁UM▁THE▁LIGHT▁BULBS▁ARE▁UP▁MORE▁THAN▁THE▁ONE▁D▁CELL

2024-10-27 17:35:47,082 (asr_inference:509) INFO: speech length: 76256
2024-10-27 17:35:49,908 (beam_search:428) INFO: decoder input length: 59
2024-10-27 17:35:49,908 (beam_search:429) INFO: max output length: 59
2024-10-27 17:35:49,908 (beam_search:430) INFO: min output length: 0
2024-10-27 17:35:50,008 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:35:50,009 (beam_search:476) INFO:  -1.87 * 1.0 =  -1.87 for ctc
2024-10-27 17:35:50,009 (beam_search:479) INFO: total log probability: -1.87
2024-10-27 17:35:50,009 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:35:50,009 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:35:50,009 (beam_search:483) INFO: best hypo: ▁BECAUSE▁YOU▁HAVE▁THE▁AMOUNT▁OF▁POWER▁FROM▁THE▁D

2024-10-27 17:35:50,012 (asr_inference:509) INFO: speech length: 411024
2024-10-27 17:36:09,651 (beam_search:428) INFO: decoder input length: 320
2024-10-27 17:36:09,652 (beam_search:429) INFO: max output length: 320
2024-10-27 17:36:09,652 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:12,719 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:12,719 (beam_search:476) INFO:  -3.08 * 1.0 =  -3.08 for ctc
2024-10-27 17:36:12,719 (beam_search:479) INFO: total log probability: -3.08
2024-10-27 17:36:12,719 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:36:12,719 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:12,719 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁D▁CELL▁BATTERY▁IS▁UM▁IS▁TO▁A▁METAL▁WHICH▁IS▁TO▁A▁WIRE▁THEN▁GOES▁TO▁A▁LIGHT▁BULB▁AND▁THE▁THE▁ONE▁LIGHT▁BULB▁ANOTHER▁LIGHT▁BULB▁AND▁THEN▁IT▁AND▁THEN▁THE▁AND▁THEN▁ANOTHER▁WIRE▁GOES▁INTO▁THE▁NEGATIVE▁SIDE▁OF▁THE▁BATTERY

2024-10-27 17:36:12,742 (asr_inference:509) INFO: speech length: 164272
2024-10-27 17:36:19,049 (beam_search:428) INFO: decoder input length: 127
2024-10-27 17:36:19,049 (beam_search:429) INFO: max output length: 127
2024-10-27 17:36:19,049 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:19,461 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:19,461 (beam_search:476) INFO:  -2.52 * 1.0 =  -2.52 for ctc
2024-10-27 17:36:19,461 (beam_search:479) INFO: total log probability: -2.52
2024-10-27 17:36:19,461 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:36:19,461 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:19,461 (beam_search:483) INFO: best hypo: ▁THERE▁IS▁TWO▁D▁CELLS▁AND▁THE▁LIGHT▁BULBS▁ARE▁UP▁MORE▁BECAUSE▁IT▁HAS▁MORE▁ELECTRICITY▁POWER

2024-10-27 17:36:19,463 (asr_inference:509) INFO: speech length: 29088
2024-10-27 17:36:20,743 (beam_search:428) INFO: decoder input length: 22
2024-10-27 17:36:20,744 (beam_search:429) INFO: max output length: 22
2024-10-27 17:36:20,744 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:20,760 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:20,760 (beam_search:476) INFO:  -0.90 * 1.0 =  -0.90 for ctc
2024-10-27 17:36:20,760 (beam_search:479) INFO: total log probability: -0.90
2024-10-27 17:36:20,760 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:36:20,760 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:20,760 (beam_search:483) INFO: best hypo: ▁YES▁A▁LITTLE

2024-10-27 17:36:20,763 (asr_inference:509) INFO: speech length: 103664
2024-10-27 17:36:24,516 (beam_search:428) INFO: decoder input length: 80
2024-10-27 17:36:24,516 (beam_search:429) INFO: max output length: 80
2024-10-27 17:36:24,516 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:24,651 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:24,651 (beam_search:476) INFO:  -1.59 * 1.0 =  -1.59 for ctc
2024-10-27 17:36:24,651 (beam_search:479) INFO: total log probability: -1.59
2024-10-27 17:36:24,651 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:36:24,651 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:24,651 (beam_search:483) INFO: best hypo: ▁WE▁HAVE▁MADE▁OF▁WHAT▁WE▁HAVE▁UM▁IN▁CLASS

2024-10-27 17:36:24,654 (asr_inference:509) INFO: speech length: 182336
2024-10-27 17:36:31,795 (beam_search:428) INFO: decoder input length: 141
2024-10-27 17:36:31,795 (beam_search:429) INFO: max output length: 141
2024-10-27 17:36:31,795 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:32,539 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:32,539 (beam_search:476) INFO:  -3.50 * 1.0 =  -3.50 for ctc
2024-10-27 17:36:32,539 (beam_search:479) INFO: total log probability: -3.50
2024-10-27 17:36:32,539 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:36:32,539 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:32,539 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁TWO▁D▁THE▁BOTTOM▁ARE▁HOOKED▁UP▁TO▁EACH▁OTHER▁AND▁THEN▁THEY▁TO▁TWO▁LIGHT▁BULBS▁AND▁THEN▁THERE'S▁A▁WIRE▁IN▁THE

2024-10-27 17:36:32,542 (asr_inference:509) INFO: speech length: 63040
2024-10-27 17:36:34,874 (beam_search:428) INFO: decoder input length: 48
2024-10-27 17:36:34,874 (beam_search:429) INFO: max output length: 48
2024-10-27 17:36:34,874 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:34,934 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:34,934 (beam_search:476) INFO:  -1.26 * 1.0 =  -1.26 for ctc
2024-10-27 17:36:34,934 (beam_search:479) INFO: total log probability: -1.26
2024-10-27 17:36:34,934 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:36:34,934 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:34,934 (beam_search:483) INFO: best hypo: ▁THAT▁THE▁CURRENT▁A▁PLACE▁TO▁GO

2024-10-27 17:36:34,936 (asr_inference:509) INFO: speech length: 81056
2024-10-27 17:36:37,810 (beam_search:428) INFO: decoder input length: 62
2024-10-27 17:36:37,810 (beam_search:429) INFO: max output length: 62
2024-10-27 17:36:37,811 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:37,910 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:37,910 (beam_search:476) INFO:  -2.03 * 1.0 =  -2.03 for ctc
2024-10-27 17:36:37,910 (beam_search:479) INFO: total log probability: -2.03
2024-10-27 17:36:37,910 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:36:37,910 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:37,910 (beam_search:483) INFO: best hypo: ▁IS▁THAT▁ALL▁THE▁THE▁THE▁WIRES▁AND▁THE

2024-10-27 17:36:37,912 (asr_inference:509) INFO: speech length: 121584
2024-10-27 17:36:42,388 (beam_search:428) INFO: decoder input length: 94
2024-10-27 17:36:42,388 (beam_search:429) INFO: max output length: 94
2024-10-27 17:36:42,388 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:42,637 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:42,638 (beam_search:476) INFO:  -1.18 * 1.0 =  -1.18 for ctc
2024-10-27 17:36:42,638 (beam_search:479) INFO: total log probability: -1.18
2024-10-27 17:36:42,638 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:36:42,638 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:42,638 (beam_search:483) INFO: best hypo: ▁HAVE▁TO▁BE▁TOUCHING▁OR▁ELSE▁THE▁OR▁THE▁LIGHT▁BULB▁WILL▁NOT▁LIGHT▁UP

2024-10-27 17:36:42,640 (asr_inference:509) INFO: speech length: 97232
2024-10-27 17:36:46,204 (beam_search:428) INFO: decoder input length: 75
2024-10-27 17:36:46,204 (beam_search:429) INFO: max output length: 75
2024-10-27 17:36:46,204 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:46,358 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:46,358 (beam_search:476) INFO:  -1.70 * 1.0 =  -1.70 for ctc
2024-10-27 17:36:46,358 (beam_search:479) INFO: total log probability: -1.70
2024-10-27 17:36:46,358 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:36:46,358 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:46,358 (beam_search:483) INFO: best hypo: ▁BECAUSE▁THE▁WILL▁NOT▁BE▁ABLE▁TO▁GET▁UM▁THROUGH▁TO▁THEM

2024-10-27 17:36:46,360 (asr_inference:509) INFO: speech length: 48544
2024-10-27 17:36:48,194 (beam_search:428) INFO: decoder input length: 37
2024-10-27 17:36:48,194 (beam_search:429) INFO: max output length: 37
2024-10-27 17:36:48,194 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:48,229 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:48,229 (beam_search:476) INFO:  -0.54 * 1.0 =  -0.54 for ctc
2024-10-27 17:36:48,229 (beam_search:479) INFO: total log probability: -0.54
2024-10-27 17:36:48,229 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:36:48,229 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:48,229 (beam_search:483) INFO: best hypo: ▁THE▁IS▁FLOWING▁TO▁THE

2024-10-27 17:36:48,231 (asr_inference:509) INFO: speech length: 106144
2024-10-27 17:36:52,047 (beam_search:428) INFO: decoder input length: 82
2024-10-27 17:36:52,047 (beam_search:429) INFO: max output length: 82
2024-10-27 17:36:52,047 (beam_search:430) INFO: min output length: 0
2024-10-27 17:36:52,279 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:36:52,279 (beam_search:476) INFO:  -2.00 * 1.0 =  -2.00 for ctc
2024-10-27 17:36:52,279 (beam_search:479) INFO: total log probability: -2.00
2024-10-27 17:36:52,279 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:36:52,279 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:36:52,279 (beam_search:483) INFO: best hypo: ▁THE▁WAY▁THAT▁IS▁GOING▁OUT▁OF▁IS▁THE▁NEGATIVE▁SIDE▁AND▁IS▁GOING▁INTO▁THE▁POSITIVE▁SIDE

2024-10-27 17:36:52,283 (asr_inference:509) INFO: speech length: 224880
2024-10-27 17:37:01,646 (beam_search:428) INFO: decoder input length: 175
2024-10-27 17:37:01,647 (beam_search:429) INFO: max output length: 175
2024-10-27 17:37:01,647 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:02,362 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:02,362 (beam_search:476) INFO:  -2.11 * 1.0 =  -2.11 for ctc
2024-10-27 17:37:02,362 (beam_search:479) INFO: total log probability: -2.11
2024-10-27 17:37:02,362 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:37:02,362 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:02,363 (beam_search:483) INFO: best hypo: ▁WELL▁IF▁YOU▁ONE▁IT▁WOULDN'T▁BECAUSE▁NEGATIVE▁AND▁NEGATIVE▁AND▁THEN▁THE▁TWO▁POSITIVES▁WOULD▁BE▁ON▁THE

2024-10-27 17:37:02,365 (asr_inference:509) INFO: speech length: 83328
2024-10-27 17:37:05,347 (beam_search:428) INFO: decoder input length: 64
2024-10-27 17:37:05,347 (beam_search:429) INFO: max output length: 64
2024-10-27 17:37:05,347 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:05,417 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:05,417 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 17:37:05,417 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 17:37:05,417 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:37:05,417 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:05,417 (beam_search:483) INFO: best hypo: ▁WOULD▁GO▁IN▁IN▁THE▁OPPOSITE

2024-10-27 17:37:05,419 (asr_inference:509) INFO: speech length: 109808
2024-10-27 17:37:09,420 (beam_search:428) INFO: decoder input length: 85
2024-10-27 17:37:09,420 (beam_search:429) INFO: max output length: 85
2024-10-27 17:37:09,420 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:09,543 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:09,543 (beam_search:476) INFO:  -2.23 * 1.0 =  -2.23 for ctc
2024-10-27 17:37:09,543 (beam_search:479) INFO: total log probability: -2.23
2024-10-27 17:37:09,543 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:37:09,543 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:09,543 (beam_search:483) INFO: best hypo: ▁THAT▁THE▁WOULD▁THROUGH▁THE▁NO▁TO▁THE

2024-10-27 17:37:09,545 (asr_inference:509) INFO: speech length: 12112
2024-10-27 17:37:10,321 (beam_search:428) INFO: decoder input length: 8
2024-10-27 17:37:10,321 (beam_search:429) INFO: max output length: 8
2024-10-27 17:37:10,321 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:10,328 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:10,329 (beam_search:476) INFO:  -0.87 * 1.0 =  -0.87 for ctc
2024-10-27 17:37:10,329 (beam_search:479) INFO: total log probability: -0.87
2024-10-27 17:37:10,329 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 17:37:10,329 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:10,329 (beam_search:483) INFO: best hypo: RE

2024-10-27 17:37:10,331 (asr_inference:509) INFO: speech length: 25728
2024-10-27 17:37:11,413 (beam_search:428) INFO: decoder input length: 19
2024-10-27 17:37:11,414 (beam_search:429) INFO: max output length: 19
2024-10-27 17:37:11,414 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:11,430 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:11,430 (beam_search:476) INFO:  -0.93 * 1.0 =  -0.93 for ctc
2024-10-27 17:37:11,430 (beam_search:479) INFO: total log probability: -0.93
2024-10-27 17:37:11,430 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:37:11,430 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:11,430 (beam_search:483) INFO: best hypo: ▁I'LL▁TO

2024-10-27 17:37:11,433 (asr_inference:509) INFO: speech length: 114528
2024-10-27 17:37:15,543 (beam_search:428) INFO: decoder input length: 88
2024-10-27 17:37:15,543 (beam_search:429) INFO: max output length: 88
2024-10-27 17:37:15,543 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:15,844 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:15,844 (beam_search:476) INFO:  -3.30 * 1.0 =  -3.30 for ctc
2024-10-27 17:37:15,844 (beam_search:479) INFO: total log probability: -3.30
2024-10-27 17:37:15,844 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:37:15,844 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:15,844 (beam_search:483) INFO: best hypo: ▁WE▁PUT▁EM▁IN▁THE▁CUP▁BY▁THEM▁OR▁WE▁THEM▁ON▁THE▁OF▁THE▁CUP▁OR▁WE▁JUST▁IN

2024-10-27 17:37:15,847 (asr_inference:509) INFO: speech length: 270128
2024-10-27 17:37:27,097 (beam_search:428) INFO: decoder input length: 210
2024-10-27 17:37:27,097 (beam_search:429) INFO: max output length: 210
2024-10-27 17:37:27,097 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:28,143 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:28,143 (beam_search:476) INFO:  -3.25 * 1.0 =  -3.25 for ctc
2024-10-27 17:37:28,143 (beam_search:479) INFO: total log probability: -3.25
2024-10-27 17:37:28,144 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:37:28,144 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:28,144 (beam_search:483) INFO: best hypo: ▁UM▁WE▁WE▁WE▁TO▁PUT▁THE▁METAL▁WASHERS▁IN▁THE▁UM▁CUP▁THAT▁DIDN'T▁THE▁MAGNET▁IN▁IT▁AND▁UNTIL▁THE▁UM▁MAGNETIC

2024-10-27 17:37:28,146 (asr_inference:509) INFO: speech length: 18064
2024-10-27 17:37:29,043 (beam_search:428) INFO: decoder input length: 13
2024-10-27 17:37:29,044 (beam_search:429) INFO: max output length: 13
2024-10-27 17:37:29,044 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:29,056 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:29,056 (beam_search:476) INFO:  -0.91 * 1.0 =  -0.91 for ctc
2024-10-27 17:37:29,056 (beam_search:479) INFO: total log probability: -0.91
2024-10-27 17:37:29,056 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:37:29,056 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:29,056 (beam_search:483) INFO: best hypo: ▁YES▁WE▁DID

2024-10-27 17:37:29,058 (asr_inference:509) INFO: speech length: 272112
2024-10-27 17:37:40,225 (beam_search:428) INFO: decoder input length: 212
2024-10-27 17:37:40,225 (beam_search:429) INFO: max output length: 212
2024-10-27 17:37:40,225 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:41,529 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:41,529 (beam_search:476) INFO:  -2.77 * 1.0 =  -2.77 for ctc
2024-10-27 17:37:41,529 (beam_search:479) INFO: total log probability: -2.77
2024-10-27 17:37:41,529 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:37:41,529 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:41,529 (beam_search:483) INFO: best hypo: ▁YEAH▁UM▁WE▁ACTUALLY▁DID▁THE▁IS▁TO▁PUT▁THE▁WASHERS▁ON▁ON▁THE▁SIDE▁OF▁THE▁CUP▁UM▁ALL▁UP▁TOGETHER▁AND▁THEN▁YOU▁CAN▁GET▁MORE▁BEFORE▁THE▁THE▁UM▁MAGNETIC

2024-10-27 17:37:41,532 (asr_inference:509) INFO: speech length: 192416
2024-10-27 17:37:49,148 (beam_search:428) INFO: decoder input length: 149
2024-10-27 17:37:49,148 (beam_search:429) INFO: max output length: 149
2024-10-27 17:37:49,148 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:49,491 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:49,491 (beam_search:476) INFO:  -5.14 * 1.0 =  -5.14 for ctc
2024-10-27 17:37:49,491 (beam_search:479) INFO: total log probability: -5.14
2024-10-27 17:37:49,491 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 17:37:49,491 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:49,492 (beam_search:483) INFO: best hypo: ▁UM▁ITED▁THE▁WAY▁THAT▁IS▁THAT▁THE▁MAGNETIC▁IS▁U▁U

2024-10-27 17:37:49,494 (asr_inference:509) INFO: speech length: 139760
2024-10-27 17:37:54,821 (beam_search:428) INFO: decoder input length: 108
2024-10-27 17:37:54,822 (beam_search:429) INFO: max output length: 108
2024-10-27 17:37:54,822 (beam_search:430) INFO: min output length: 0
2024-10-27 17:37:55,029 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:37:55,029 (beam_search:476) INFO:  -2.20 * 1.0 =  -2.20 for ctc
2024-10-27 17:37:55,029 (beam_search:479) INFO: total log probability: -2.20
2024-10-27 17:37:55,029 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:37:55,029 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:37:55,029 (beam_search:483) INFO: best hypo: ▁THE▁MAGNETIC▁AND▁WE▁OUT▁THAT▁MORE▁PLASTIC▁ARE▁THEY▁CALLED

2024-10-27 17:37:55,031 (asr_inference:509) INFO: speech length: 390048
2024-10-27 17:38:13,017 (beam_search:428) INFO: decoder input length: 304
2024-10-27 17:38:13,017 (beam_search:429) INFO: max output length: 304
2024-10-27 17:38:13,017 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:15,314 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:15,314 (beam_search:476) INFO:  -7.12 * 1.0 =  -7.12 for ctc
2024-10-27 17:38:15,314 (beam_search:479) INFO: total log probability: -7.12
2024-10-27 17:38:15,314 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:38:15,314 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:15,315 (beam_search:483) INFO: best hypo: ▁PLASTIC▁YELLOW▁UM▁WE▁COULD▁PUT▁IN▁AND▁THEN▁WE▁WED▁THE▁WED▁LIKE▁ONE▁WITH▁WE▁HAD▁SIXTEEN▁WITH▁ONE▁UM▁PLASTIC▁YELLOW▁IN▁THE▁MIDDLE▁WE▁HAD▁SEVEN▁AND▁THEN▁AND▁THEN▁FOUR▁FOUR▁AND▁FOUR

2024-10-27 17:38:15,318 (asr_inference:509) INFO: speech length: 101184
2024-10-27 17:38:19,008 (beam_search:428) INFO: decoder input length: 78
2024-10-27 17:38:19,008 (beam_search:429) INFO: max output length: 78
2024-10-27 17:38:19,008 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:19,201 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:19,201 (beam_search:476) INFO:  -2.22 * 1.0 =  -2.22 for ctc
2024-10-27 17:38:19,201 (beam_search:479) INFO: total log probability: -2.22
2024-10-27 17:38:19,201 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:38:19,201 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:19,201 (beam_search:483) INFO: best hypo: ▁THE▁MAGNETIC▁IS▁BECAUSE▁DOESN'T▁HAVE▁AS▁MANY▁PLASTIC▁YELLOW▁IN

2024-10-27 17:38:19,204 (asr_inference:509) INFO: speech length: 22240
2024-10-27 17:38:20,176 (beam_search:428) INFO: decoder input length: 16
2024-10-27 17:38:20,176 (beam_search:429) INFO: max output length: 16
2024-10-27 17:38:20,176 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:20,181 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:20,181 (beam_search:476) INFO:  -0.42 * 1.0 =  -0.42 for ctc
2024-10-27 17:38:20,181 (beam_search:479) INFO: total log probability: -0.42
2024-10-27 17:38:20,181 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:38:20,181 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:20,181 (beam_search:483) INFO: best hypo: 

2024-10-27 17:38:20,184 (asr_inference:509) INFO: speech length: 11440
2024-10-27 17:38:20,910 (beam_search:428) INFO: decoder input length: 8
2024-10-27 17:38:20,910 (beam_search:429) INFO: max output length: 8
2024-10-27 17:38:20,910 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:20,915 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:20,916 (beam_search:476) INFO:  -0.38 * 1.0 =  -0.38 for ctc
2024-10-27 17:38:20,916 (beam_search:479) INFO: total log probability: -0.38
2024-10-27 17:38:20,916 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:38:20,916 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:20,916 (beam_search:483) INFO: best hypo: ▁IT

2024-10-27 17:38:20,918 (asr_inference:509) INFO: speech length: 54720
2024-10-27 17:38:22,976 (beam_search:428) INFO: decoder input length: 42
2024-10-27 17:38:22,976 (beam_search:429) INFO: max output length: 42
2024-10-27 17:38:22,976 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:23,018 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:23,018 (beam_search:476) INFO:  -0.94 * 1.0 =  -0.94 for ctc
2024-10-27 17:38:23,018 (beam_search:479) INFO: total log probability: -0.94
2024-10-27 17:38:23,018 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:38:23,018 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:23,018 (beam_search:483) INFO: best hypo: ▁WE▁HAVE▁BEEN▁WITH▁MAGNETS▁AND

2024-10-27 17:38:23,021 (asr_inference:509) INFO: speech length: 215968
2024-10-27 17:38:31,792 (beam_search:428) INFO: decoder input length: 168
2024-10-27 17:38:31,792 (beam_search:429) INFO: max output length: 168
2024-10-27 17:38:31,792 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:32,403 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:32,404 (beam_search:476) INFO:  -4.81 * 1.0 =  -4.81 for ctc
2024-10-27 17:38:32,404 (beam_search:479) INFO: total log probability: -4.81
2024-10-27 17:38:32,404 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:38:32,404 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:32,404 (beam_search:483) INFO: best hypo: ▁WELL▁TODAY▁WE▁HAD▁WE▁DID▁SO▁IT'S▁KINDA▁A▁TOGETHER▁BUT▁ALL▁THE▁OTHER▁WE▁USE▁JUST▁OR

2024-10-27 17:38:32,406 (asr_inference:509) INFO: speech length: 169120
2024-10-27 17:38:38,902 (beam_search:428) INFO: decoder input length: 131
2024-10-27 17:38:38,902 (beam_search:429) INFO: max output length: 131
2024-10-27 17:38:38,902 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:39,374 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:39,374 (beam_search:476) INFO:  -2.79 * 1.0 =  -2.79 for ctc
2024-10-27 17:38:39,374 (beam_search:479) INFO: total log probability: -2.79
2024-10-27 17:38:39,374 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:38:39,374 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:39,374 (beam_search:483) INFO: best hypo: ▁MAGNET▁IS▁A▁MAGNET▁BUT▁YOU▁CAN▁TURN▁IT▁ON▁AND▁OFF▁AND▁IT'S▁MORE▁A▁REGULAR▁MAGNET

2024-10-27 17:38:39,377 (asr_inference:509) INFO: speech length: 241680
2024-10-27 17:38:49,434 (beam_search:428) INFO: decoder input length: 188
2024-10-27 17:38:49,434 (beam_search:429) INFO: max output length: 188
2024-10-27 17:38:49,434 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:50,291 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:50,291 (beam_search:476) INFO:  -4.99 * 1.0 =  -4.99 for ctc
2024-10-27 17:38:50,292 (beam_search:479) INFO: total log probability: -4.99
2024-10-27 17:38:50,292 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:38:50,292 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:50,292 (beam_search:483) INFO: best hypo: ▁I▁SEE▁A▁WIRES▁A▁AND▁A▁RIVET▁AND▁WASHERS▁AND▁A▁D▁CELL▁BATTERY▁THOSE▁ARE▁THE▁THAT▁YOU▁NEED▁TO▁MAKE▁AN▁ELECTROMAGNET

2024-10-27 17:38:50,294 (asr_inference:509) INFO: speech length: 172048
2024-10-27 17:38:56,892 (beam_search:428) INFO: decoder input length: 133
2024-10-27 17:38:56,893 (beam_search:429) INFO: max output length: 133
2024-10-27 17:38:56,893 (beam_search:430) INFO: min output length: 0
2024-10-27 17:38:57,314 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:38:57,315 (beam_search:476) INFO:  -2.74 * 1.0 =  -2.74 for ctc
2024-10-27 17:38:57,315 (beam_search:479) INFO: total log probability: -2.74
2024-10-27 17:38:57,315 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:38:57,315 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:38:57,315 (beam_search:483) INFO: best hypo: ▁OF▁THAT▁IS▁THAT▁THE▁THE▁ELECTRICITY▁THROUGH▁THE▁PIECE▁OF▁METAL▁WHICH▁MAKES▁IT▁UH▁MAGNET▁ELECTROMAGNET

2024-10-27 17:38:57,317 (asr_inference:509) INFO: speech length: 324304
2024-10-27 17:39:11,723 (beam_search:428) INFO: decoder input length: 252
2024-10-27 17:39:11,724 (beam_search:429) INFO: max output length: 252
2024-10-27 17:39:11,724 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:13,871 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:13,871 (beam_search:476) INFO:  -6.28 * 1.0 =  -6.28 for ctc
2024-10-27 17:39:13,871 (beam_search:479) INFO: total log probability: -6.28
2024-10-27 17:39:13,871 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:39:13,872 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:13,872 (beam_search:483) INFO: best hypo: ▁THE▁IS▁WHAT▁YOU▁USE▁TO▁THE▁ELECTROMAGNET▁ON▁AND▁OFF▁IF▁THE▁IF▁THE▁IF▁THE▁SWITCH▁IF▁THE▁PIECE▁OF▁IS▁TOUCHING▁THE▁OTHER▁PIECE▁THEN▁THE▁WILL▁WORK▁IF▁IT▁IF▁IT▁IF▁IT▁DOESN'T▁THEN▁THE▁WILL▁BE

2024-10-27 17:39:13,875 (asr_inference:509) INFO: speech length: 49792
2024-10-27 17:39:15,776 (beam_search:428) INFO: decoder input length: 38
2024-10-27 17:39:15,776 (beam_search:429) INFO: max output length: 38
2024-10-27 17:39:15,777 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:15,810 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:15,810 (beam_search:476) INFO:  -1.20 * 1.0 =  -1.20 for ctc
2024-10-27 17:39:15,810 (beam_search:479) INFO: total log probability: -1.20
2024-10-27 17:39:15,810 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:39:15,810 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:15,810 (beam_search:483) INFO: best hypo: ▁THES▁THE▁ELECTROMAGNET▁ON

2024-10-27 17:39:15,813 (asr_inference:509) INFO: speech length: 171104
2024-10-27 17:39:22,336 (beam_search:428) INFO: decoder input length: 133
2024-10-27 17:39:22,336 (beam_search:429) INFO: max output length: 133
2024-10-27 17:39:22,336 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:22,730 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:22,730 (beam_search:476) INFO:  -3.13 * 1.0 =  -3.13 for ctc
2024-10-27 17:39:22,730 (beam_search:479) INFO: total log probability: -3.13
2024-10-27 17:39:22,730 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:39:22,730 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:22,730 (beam_search:483) INFO: best hypo: ▁YOU▁COULD▁PUT▁IN▁MAYBE▁A▁THAT▁CAN▁THAT▁CAN▁BEED▁UP▁BY▁A▁MAGNET

2024-10-27 17:39:22,734 (asr_inference:509) INFO: speech length: 161024
2024-10-27 17:39:28,693 (beam_search:428) INFO: decoder input length: 125
2024-10-27 17:39:28,694 (beam_search:429) INFO: max output length: 125
2024-10-27 17:39:28,694 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:28,940 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:28,941 (beam_search:476) INFO:  -1.47 * 1.0 =  -1.47 for ctc
2024-10-27 17:39:28,941 (beam_search:479) INFO: total log probability: -1.47
2024-10-27 17:39:28,941 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:39:28,941 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:28,941 (beam_search:483) INFO: best hypo: ▁METAL▁CAN▁IT'S▁STEEL▁UM▁STEEL▁OR▁I▁THINK

2024-10-27 17:39:28,943 (asr_inference:509) INFO: speech length: 434672
2024-10-27 17:39:51,084 (beam_search:428) INFO: decoder input length: 339
2024-10-27 17:39:51,084 (beam_search:429) INFO: max output length: 339
2024-10-27 17:39:51,084 (beam_search:430) INFO: min output length: 0
2024-10-27 17:39:54,566 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:39:54,566 (beam_search:476) INFO:  -8.54 * 1.0 =  -8.54 for ctc
2024-10-27 17:39:54,566 (beam_search:479) INFO: total log probability: -8.54
2024-10-27 17:39:54,566 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:39:54,566 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:39:54,566 (beam_search:483) INFO: best hypo: ▁THE▁ELECTRICITY'S▁FLOWING▁FROM▁OF▁THE▁BATTERY▁THEY▁ONE▁IS▁THE▁POSITIVE▁AROUND▁THE▁STEEL▁WHICH▁THEN▁IS▁TO▁THE▁SWITCH▁AND▁THE▁SWITCH▁IS▁TO▁THE▁NEGATIVE▁SIDE▁THE▁THATS▁THE▁SWITCH▁AND▁IT▁LOOKS▁LIKE▁IT'S▁MIGHT▁BE▁IT▁ABOUT▁TO▁PICK▁UP▁THE▁THE▁WASHERS

2024-10-27 17:39:54,569 (asr_inference:509) INFO: speech length: 285376
2024-10-27 17:40:06,451 (beam_search:428) INFO: decoder input length: 222
2024-10-27 17:40:06,451 (beam_search:429) INFO: max output length: 222
2024-10-27 17:40:06,451 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:07,408 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:07,409 (beam_search:476) INFO:  -5.49 * 1.0 =  -5.49 for ctc
2024-10-27 17:40:07,409 (beam_search:479) INFO: total log probability: -5.49
2024-10-27 17:40:07,409 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:40:07,409 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:07,409 (beam_search:483) INFO: best hypo: ▁WASHERS▁BECAUSE▁THE▁UM▁THE▁IS▁ON▁ARE▁ATTRACTED▁TO▁THE▁BUT▁IT▁TURNS▁OFF▁THE▁ELECTROMAGNET▁DOES▁NOT▁WORK▁AND▁THE▁WASHERS

2024-10-27 17:40:07,411 (asr_inference:509) INFO: speech length: 385520
2024-10-27 17:40:25,058 (beam_search:428) INFO: decoder input length: 300
2024-10-27 17:40:25,059 (beam_search:429) INFO: max output length: 300
2024-10-27 17:40:25,059 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:28,073 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:28,073 (beam_search:476) INFO:  -7.53 * 1.0 =  -7.53 for ctc
2024-10-27 17:40:28,073 (beam_search:479) INFO: total log probability: -7.53
2024-10-27 17:40:28,073 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:40:28,073 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:28,073 (beam_search:483) INFO: best hypo: ▁THE▁SWITCH▁IS▁THE▁FLOW▁OF▁ELECTRICITY▁THE▁NEGATIVE▁AND▁THE▁POSITIVE▁END▁OF▁THE▁BATTERY▁THEN▁CAUSE▁YOU▁CAN'T▁ATTACH▁WELL▁YOU▁COULD▁DO▁IT▁ONE▁WAY▁YOU▁PUT▁THE▁A▁WIRE▁ON▁THE▁POSITIVE▁AND▁THE▁AND▁YOU▁CAN▁NOT▁TURN▁IT▁OFF▁WITHOUT▁PULLING▁THE▁STUFF▁OFF

2024-10-27 17:40:28,075 (asr_inference:509) INFO: speech length: 124064
2024-10-27 17:40:32,724 (beam_search:428) INFO: decoder input length: 96
2024-10-27 17:40:32,724 (beam_search:429) INFO: max output length: 96
2024-10-27 17:40:32,724 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:33,015 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:33,016 (beam_search:476) INFO:  -1.58 * 1.0 =  -1.58 for ctc
2024-10-27 17:40:33,016 (beam_search:479) INFO: total log probability: -1.58
2024-10-27 17:40:33,016 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:40:33,016 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:33,016 (beam_search:483) INFO: best hypo: ▁UM▁I▁WOULD▁THAT▁IF▁IT▁HAD▁TWO▁BATTERIES▁MAYBE▁THAT▁WOULD▁MAKE▁THE▁MAGNET▁MORE▁POWERFUL

2024-10-27 17:40:33,018 (asr_inference:509) INFO: speech length: 220256
2024-10-27 17:40:41,886 (beam_search:428) INFO: decoder input length: 171
2024-10-27 17:40:41,887 (beam_search:429) INFO: max output length: 171
2024-10-27 17:40:41,887 (beam_search:430) INFO: min output length: 0
2024-10-27 17:40:42,549 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:40:42,549 (beam_search:476) INFO:  -3.13 * 1.0 =  -3.13 for ctc
2024-10-27 17:40:42,550 (beam_search:479) INFO: total log probability: -3.13
2024-10-27 17:40:42,550 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:40:42,550 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:40:42,550 (beam_search:483) INFO: best hypo: ▁BECAUSE▁IF▁YOU▁HAVE▁TWO▁D▁CELL▁IT▁HAS▁TWO▁D▁CELL▁BATTERIES▁IT▁HAS▁MORE▁ELECTRICITY▁INSIDE▁THEM▁THAN▁ONE

2024-10-27 17:40:42,552 (asr_inference:509) INFO: speech length: 380416
2024-10-27 17:41:00,116 (beam_search:428) INFO: decoder input length: 296
2024-10-27 17:41:00,116 (beam_search:429) INFO: max output length: 296
2024-10-27 17:41:00,116 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:02,893 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:02,893 (beam_search:476) INFO:  -6.07 * 1.0 =  -6.07 for ctc
2024-10-27 17:41:02,893 (beam_search:479) INFO: total log probability: -6.07
2024-10-27 17:41:02,893 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:41:02,893 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:02,893 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁MORE▁WINDS▁THAT▁WE▁HAVE▁HAD▁THE▁MORE▁WASHERS▁THAT▁WE▁WOULD▁GET▁BECAUSE▁IF▁WE▁PUT▁IF▁WE▁PUT▁UM▁UM▁TEN▁WASHERS▁I▁I▁TEN▁WINDS▁THEN▁WE▁WOULD▁GET▁THREE▁WASHERS▁SO▁EVERY▁THAT▁YOU▁IT▁THE▁NUMBER▁OF▁WASHERS▁THAT▁YOU▁GOING▁UP

2024-10-27 17:41:02,896 (asr_inference:509) INFO: speech length: 159616
2024-10-27 17:41:08,756 (beam_search:428) INFO: decoder input length: 124
2024-10-27 17:41:08,756 (beam_search:429) INFO: max output length: 124
2024-10-27 17:41:08,756 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:09,043 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:09,043 (beam_search:476) INFO:  -2.08 * 1.0 =  -2.08 for ctc
2024-10-27 17:41:09,043 (beam_search:479) INFO: total log probability: -2.08
2024-10-27 17:41:09,043 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:41:09,043 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:09,043 (beam_search:483) INFO: best hypo: ▁THE▁IT▁UM▁THE▁MORE▁THE▁MORE▁THE▁ELECTROMAGNET▁THE▁MORE▁ELECTROMAGNET▁IS

2024-10-27 17:41:09,046 (asr_inference:509) INFO: speech length: 404400
2024-10-27 17:41:28,175 (beam_search:428) INFO: decoder input length: 315
2024-10-27 17:41:28,175 (beam_search:429) INFO: max output length: 315
2024-10-27 17:41:28,175 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:31,073 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:31,074 (beam_search:476) INFO:  -5.53 * 1.0 =  -5.53 for ctc
2024-10-27 17:41:31,074 (beam_search:479) INFO: total log probability: -5.53
2024-10-27 17:41:31,074 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:41:31,074 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:31,074 (beam_search:483) INFO: best hypo: ▁BECAUSE▁THE▁ELECTRICITY▁IS▁THROUGH▁THE▁WIRES▁SO▁IF▁IT▁GOES▁AROUND▁THE▁THE▁STEEL▁RIVET▁UM▁THAT▁A▁HUNDRED▁TIMES▁THEN▁IT'S▁MORE▁LIKE▁TO▁HAVE▁UM▁IT'S▁IT▁HAS▁FIFTEEN▁WASHERS▁BUT▁IF▁I▁GO▁TO▁TO▁TEN▁ONLY▁GET▁THREE▁WASHERS

2024-10-27 17:41:31,077 (asr_inference:509) INFO: speech length: 102992
2024-10-27 17:41:34,732 (beam_search:428) INFO: decoder input length: 79
2024-10-27 17:41:34,732 (beam_search:429) INFO: max output length: 79
2024-10-27 17:41:34,732 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:34,962 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:34,963 (beam_search:476) INFO:  -3.24 * 1.0 =  -3.24 for ctc
2024-10-27 17:41:34,963 (beam_search:479) INFO: total log probability: -3.24
2024-10-27 17:41:34,963 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:41:34,963 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:34,963 (beam_search:483) INFO: best hypo: ▁BECAUSE▁IT▁HAS▁MORE▁ELECTRICITY▁IN▁THE▁WIRES▁SO▁IT▁CAN▁MAKE▁IT▁THE▁ELECTROMAGNETS▁STRONGER

2024-10-27 17:41:34,965 (asr_inference:509) INFO: speech length: 148864
2024-10-27 17:41:40,559 (beam_search:428) INFO: decoder input length: 115
2024-10-27 17:41:40,560 (beam_search:429) INFO: max output length: 115
2024-10-27 17:41:40,560 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:40,713 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:40,713 (beam_search:476) INFO:  -1.32 * 1.0 =  -1.32 for ctc
2024-10-27 17:41:40,713 (beam_search:479) INFO: total log probability: -1.32
2024-10-27 17:41:40,713 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:41:40,713 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:40,713 (beam_search:483) INFO: best hypo: ▁UM▁IT▁IT▁AND▁OF▁IT▁IN

2024-10-27 17:41:40,716 (asr_inference:509) INFO: speech length: 126144
2024-10-27 17:41:45,299 (beam_search:428) INFO: decoder input length: 98
2024-10-27 17:41:45,299 (beam_search:429) INFO: max output length: 98
2024-10-27 17:41:45,299 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:45,684 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:45,684 (beam_search:476) INFO:  -2.77 * 1.0 =  -2.77 for ctc
2024-10-27 17:41:45,684 (beam_search:479) INFO: total log probability: -2.77
2024-10-27 17:41:45,684 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:41:45,684 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:45,684 (beam_search:483) INFO: best hypo: ▁UM▁WHEN▁YOU▁A▁CIRCUIT▁THEN▁IT▁CAN▁TURN▁OFF▁A▁LIGHT▁AND▁WHEN▁YOU▁OPEN▁A▁CIRCUIT▁IT▁CAN▁TURN▁ON▁A▁LIGHT

2024-10-27 17:41:45,687 (asr_inference:509) INFO: speech length: 31088
2024-10-27 17:41:46,945 (beam_search:428) INFO: decoder input length: 23
2024-10-27 17:41:46,945 (beam_search:429) INFO: max output length: 23
2024-10-27 17:41:46,945 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:46,956 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:46,956 (beam_search:476) INFO:  -0.19 * 1.0 =  -0.19 for ctc
2024-10-27 17:41:46,956 (beam_search:479) INFO: total log probability: -0.19
2024-10-27 17:41:46,956 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:41:46,956 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:46,956 (beam_search:483) INFO: best hypo: ▁IT

2024-10-27 17:41:46,960 (asr_inference:509) INFO: speech length: 231408
2024-10-27 17:41:56,413 (beam_search:428) INFO: decoder input length: 180
2024-10-27 17:41:56,413 (beam_search:429) INFO: max output length: 180
2024-10-27 17:41:56,413 (beam_search:430) INFO: min output length: 0
2024-10-27 17:41:57,585 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:41:57,585 (beam_search:476) INFO:  -3.96 * 1.0 =  -3.96 for ctc
2024-10-27 17:41:57,585 (beam_search:479) INFO: total log probability: -3.96
2024-10-27 17:41:57,585 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:41:57,585 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:41:57,585 (beam_search:483) INFO: best hypo: ▁IF▁THE'S▁WIRE'S▁CONNECTED▁TO▁THE▁END▁OF▁THE▁BATTERY▁THE▁MOTOR'T▁RUN▁AND▁THEN▁THE▁SWITCH▁HAS▁TO▁THE▁WAY▁THE▁BATTERY▁FOR▁IT▁TO▁MAKE▁THE▁MOTOR

2024-10-27 17:41:57,589 (asr_inference:509) INFO: speech length: 63216
2024-10-27 17:41:59,945 (beam_search:428) INFO: decoder input length: 48
2024-10-27 17:41:59,945 (beam_search:429) INFO: max output length: 48
2024-10-27 17:41:59,945 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:00,029 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:00,029 (beam_search:476) INFO:  -1.62 * 1.0 =  -1.62 for ctc
2024-10-27 17:42:00,029 (beam_search:479) INFO: total log probability: -1.62
2024-10-27 17:42:00,029 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:42:00,029 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:00,029 (beam_search:483) INFO: best hypo: ▁A▁BATTERY'S▁CONNECTED▁TO▁A▁TO▁MAKE▁THE▁RUN

2024-10-27 17:42:00,032 (asr_inference:509) INFO: speech length: 75936
2024-10-27 17:42:02,830 (beam_search:428) INFO: decoder input length: 58
2024-10-27 17:42:02,831 (beam_search:429) INFO: max output length: 58
2024-10-27 17:42:02,831 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:02,956 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:02,956 (beam_search:476) INFO:  -0.63 * 1.0 =  -0.63 for ctc
2024-10-27 17:42:02,956 (beam_search:479) INFO: total log probability: -0.63
2024-10-27 17:42:02,956 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:42:02,956 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:02,956 (beam_search:483) INFO: best hypo: ▁THERE▁IS▁A▁SWITCH▁ON▁IT▁TO▁MAKE▁THE▁MOTOR▁ON▁AND▁OFF

2024-10-27 17:42:02,958 (asr_inference:509) INFO: speech length: 317856
2024-10-27 17:42:17,103 (beam_search:428) INFO: decoder input length: 247
2024-10-27 17:42:17,103 (beam_search:429) INFO: max output length: 247
2024-10-27 17:42:17,103 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:18,973 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:18,973 (beam_search:476) INFO:  -4.59 * 1.0 =  -4.59 for ctc
2024-10-27 17:42:18,973 (beam_search:479) INFO: total log probability: -4.59
2024-10-27 17:42:18,973 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:42:18,973 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:18,973 (beam_search:483) INFO: best hypo: ▁WHEN▁THE▁SWITCH▁IS▁IS▁OPEN▁THEN▁THE▁MOTOR▁CAN'T▁RUN▁CAUSE▁THE▁SWITCH▁IS▁IT▁CONNECTS▁THE▁SO▁THE▁MOTOR▁CAN▁AND▁AND▁IF▁THE▁SWITCH▁THEN▁THE▁CAN▁CAUSE▁IT'S▁CONNECTING▁ALL▁THE▁WIRES

2024-10-27 17:42:18,976 (asr_inference:509) INFO: speech length: 347104
2024-10-27 17:42:34,293 (beam_search:428) INFO: decoder input length: 270
2024-10-27 17:42:34,293 (beam_search:429) INFO: max output length: 270
2024-10-27 17:42:34,293 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:35,566 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:35,567 (beam_search:476) INFO:  -4.38 * 1.0 =  -4.38 for ctc
2024-10-27 17:42:35,567 (beam_search:479) INFO: total log probability: -4.38
2024-10-27 17:42:35,567 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:42:35,567 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:35,567 (beam_search:483) INFO: best hypo: ▁UM▁A▁IT▁IS▁IS▁COULD▁IS▁WHEN▁YOU▁HAVE▁A▁OF▁WIRES▁CONNECTED▁TO▁EACH▁OTHER▁WITH▁A▁SWITCH▁IF▁THE▁SWITCH▁OPEN

2024-10-27 17:42:35,570 (asr_inference:509) INFO: speech length: 164960
2024-10-27 17:42:41,853 (beam_search:428) INFO: decoder input length: 128
2024-10-27 17:42:41,853 (beam_search:429) INFO: max output length: 128
2024-10-27 17:42:41,853 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:42,368 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:42,368 (beam_search:476) INFO:  -1.81 * 1.0 =  -1.81 for ctc
2024-10-27 17:42:42,368 (beam_search:479) INFO: total log probability: -1.81
2024-10-27 17:42:42,368 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:42:42,368 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:42,368 (beam_search:483) INFO: best hypo: ▁AND▁IT▁WILL▁BE▁THEN▁WHAT▁IS▁TRYING▁TO▁ON▁WILL▁BE▁OFF▁THEN▁IF▁YOU▁THE▁SWITCH▁IT▁IT▁WILL▁TURN▁ON

2024-10-27 17:42:42,371 (asr_inference:509) INFO: speech length: 116896
2024-10-27 17:42:46,542 (beam_search:428) INFO: decoder input length: 90
2024-10-27 17:42:46,542 (beam_search:429) INFO: max output length: 90
2024-10-27 17:42:46,542 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:46,722 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:46,723 (beam_search:476) INFO:  -2.50 * 1.0 =  -2.50 for ctc
2024-10-27 17:42:46,723 (beam_search:479) INFO: total log probability: -2.50
2024-10-27 17:42:46,723 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:42:46,723 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:46,723 (beam_search:483) INFO: best hypo: ▁THE▁YOU▁CAN▁MAKE▁THEM▁MAKE▁A▁MOTOR▁OR▁LIGHT▁RUN

2024-10-27 17:42:46,725 (asr_inference:509) INFO: speech length: 94448
2024-10-27 17:42:50,231 (beam_search:428) INFO: decoder input length: 73
2024-10-27 17:42:50,231 (beam_search:429) INFO: max output length: 73
2024-10-27 17:42:50,231 (beam_search:430) INFO: min output length: 0
2024-10-27 17:42:50,386 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:42:50,387 (beam_search:476) INFO:  -0.66 * 1.0 =  -0.66 for ctc
2024-10-27 17:42:50,387 (beam_search:479) INFO: total log probability: -0.66
2024-10-27 17:42:50,387 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:42:50,387 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:42:50,387 (beam_search:483) INFO: best hypo: ▁A▁BATTERY▁IS▁CONNECTED▁TO▁WIRES▁TO▁MAKE▁TWO▁LIGHTS▁ON

2024-10-27 17:42:50,390 (asr_inference:509) INFO: speech length: 329664
2024-10-27 17:43:04,530 (beam_search:428) INFO: decoder input length: 257
2024-10-27 17:43:04,530 (beam_search:429) INFO: max output length: 257
2024-10-27 17:43:04,530 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:06,149 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:06,149 (beam_search:476) INFO:  -5.54 * 1.0 =  -5.54 for ctc
2024-10-27 17:43:06,149 (beam_search:479) INFO: total log probability: -5.54
2024-10-27 17:43:06,149 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:43:06,149 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:06,149 (beam_search:483) INFO: best hypo: ▁UM▁THE▁THE▁ELECTRICITY▁FROM▁THE▁BATTERY▁GOES▁TO▁ONE▁TO▁BOTHS▁AND▁THEN▁THEY▁BOTH▁LIGHTS▁ARE▁CONNECTED▁SO▁IT▁IF▁ONE▁ISN'T▁FLOWING▁THEN▁IT▁CAN▁GET▁THE

2024-10-27 17:43:06,152 (asr_inference:509) INFO: speech length: 260336
2024-10-27 17:43:17,081 (beam_search:428) INFO: decoder input length: 202
2024-10-27 17:43:17,081 (beam_search:429) INFO: max output length: 202
2024-10-27 17:43:17,081 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:18,089 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:18,089 (beam_search:476) INFO:  -4.24 * 1.0 =  -4.24 for ctc
2024-10-27 17:43:18,089 (beam_search:479) INFO: total log probability: -4.24
2024-10-27 17:43:18,089 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:43:18,089 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:18,090 (beam_search:483) INFO: best hypo: ▁ONE▁IN▁THE▁ON▁EACH▁SIDE▁OF▁THE▁BATTERY▁IT▁TO▁SO▁THERE▁ELECTRICITY▁TO▁THE▁LIGHT▁BULBS▁AND▁THEN▁ONE▁TO▁THE▁ELECTRICITY▁IN▁THE▁LIGHT

2024-10-27 17:43:18,092 (asr_inference:509) INFO: speech length: 134864
2024-10-27 17:43:23,127 (beam_search:428) INFO: decoder input length: 104
2024-10-27 17:43:23,127 (beam_search:429) INFO: max output length: 104
2024-10-27 17:43:23,127 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:23,446 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:23,446 (beam_search:476) INFO:  -1.64 * 1.0 =  -1.64 for ctc
2024-10-27 17:43:23,446 (beam_search:479) INFO: total log probability: -1.64
2024-10-27 17:43:23,446 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:43:23,446 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:23,446 (beam_search:483) INFO: best hypo: ▁IT▁FLOWS▁FROM▁THE▁BATTERY▁UP▁THE▁WIRE▁AND▁AND▁GOES▁INTO▁AND▁THEN▁GOES▁INTO▁THE▁BULB

2024-10-27 17:43:23,449 (asr_inference:509) INFO: speech length: 150704
2024-10-27 17:43:29,107 (beam_search:428) INFO: decoder input length: 117
2024-10-27 17:43:29,107 (beam_search:429) INFO: max output length: 117
2024-10-27 17:43:29,107 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:29,529 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:29,530 (beam_search:476) INFO:  -0.75 * 1.0 =  -0.75 for ctc
2024-10-27 17:43:29,530 (beam_search:479) INFO: total log probability: -0.75
2024-10-27 17:43:29,530 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:43:29,530 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:29,530 (beam_search:483) INFO: best hypo: ▁UM▁ONE▁TWO▁WIRES▁ARE▁CONNECTED▁TO▁THE▁BATTERY▁AND▁ONE▁WIRE'S▁CONNECTED▁TO▁THE▁LIGHT▁BULBS

2024-10-27 17:43:29,533 (asr_inference:509) INFO: speech length: 114816
2024-10-27 17:43:33,864 (beam_search:428) INFO: decoder input length: 89
2024-10-27 17:43:33,865 (beam_search:429) INFO: max output length: 89
2024-10-27 17:43:33,865 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:34,032 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:34,032 (beam_search:476) INFO:  -2.70 * 1.0 =  -2.70 for ctc
2024-10-27 17:43:34,032 (beam_search:479) INFO: total log probability: -2.70
2024-10-27 17:43:34,032 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:43:34,032 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:34,032 (beam_search:483) INFO: best hypo: ▁UM▁IT▁MAKES▁IT▁SO▁THAT▁THERE▁IS▁ENERGY▁FLOWING▁THE

2024-10-27 17:43:34,035 (asr_inference:509) INFO: speech length: 256640
2024-10-27 17:43:44,541 (beam_search:428) INFO: decoder input length: 199
2024-10-27 17:43:44,541 (beam_search:429) INFO: max output length: 199
2024-10-27 17:43:44,541 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:45,230 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:45,230 (beam_search:476) INFO:  -3.22 * 1.0 =  -3.22 for ctc
2024-10-27 17:43:45,230 (beam_search:479) INFO: total log probability: -3.22
2024-10-27 17:43:45,230 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:43:45,230 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:45,230 (beam_search:483) INFO: best hypo: ▁THE▁THE▁ELECTRICITY▁IS▁FROM▁THE▁BATTERY▁TO▁AND▁THEN▁IT▁AND▁THEN▁IT▁GOES▁INTO▁THE▁LIGHT▁BULB

2024-10-27 17:43:45,232 (asr_inference:509) INFO: speech length: 82256
2024-10-27 17:43:48,204 (beam_search:428) INFO: decoder input length: 63
2024-10-27 17:43:48,205 (beam_search:429) INFO: max output length: 63
2024-10-27 17:43:48,205 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:48,285 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:48,286 (beam_search:476) INFO:  -1.67 * 1.0 =  -1.67 for ctc
2024-10-27 17:43:48,286 (beam_search:479) INFO: total log probability: -1.67
2024-10-27 17:43:48,286 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:43:48,286 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:48,286 (beam_search:483) INFO: best hypo: ▁ELECTRICITY▁IS▁FLOWING▁IN▁A▁IN▁A

2024-10-27 17:43:48,288 (asr_inference:509) INFO: speech length: 197072
2024-10-27 17:43:55,941 (beam_search:428) INFO: decoder input length: 153
2024-10-27 17:43:55,941 (beam_search:429) INFO: max output length: 153
2024-10-27 17:43:55,942 (beam_search:430) INFO: min output length: 0
2024-10-27 17:43:56,315 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:43:56,316 (beam_search:476) INFO:  -3.84 * 1.0 =  -3.84 for ctc
2024-10-27 17:43:56,316 (beam_search:479) INFO: total log probability: -3.84
2024-10-27 17:43:56,316 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 17:43:56,316 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:43:56,316 (beam_search:483) INFO: best hypo: ▁UM▁UM▁THAT▁YOU▁CAN▁SOME▁THINGS▁BYING▁THEM▁ON▁A▁MAGNET

2024-10-27 17:43:56,318 (asr_inference:509) INFO: speech length: 151744
2024-10-27 17:44:01,859 (beam_search:428) INFO: decoder input length: 118
2024-10-27 17:44:01,859 (beam_search:429) INFO: max output length: 118
2024-10-27 17:44:01,859 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:02,254 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:02,254 (beam_search:476) INFO:  -4.59 * 1.0 =  -4.59 for ctc
2024-10-27 17:44:02,254 (beam_search:479) INFO: total log probability: -4.59
2024-10-27 17:44:02,254 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:44:02,254 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:02,254 (beam_search:483) INFO: best hypo: ▁UM▁WEED▁A▁ON▁A▁MAGNET▁AND▁THEN▁TRIED▁TO▁CONNECT▁IT▁TO▁ANOTHER▁PAPER▁CLIP▁IT▁TOGETHER

2024-10-27 17:44:02,256 (asr_inference:509) INFO: speech length: 207648
2024-10-27 17:44:10,525 (beam_search:428) INFO: decoder input length: 161
2024-10-27 17:44:10,525 (beam_search:429) INFO: max output length: 161
2024-10-27 17:44:10,525 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:11,054 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:11,055 (beam_search:476) INFO:  -2.91 * 1.0 =  -2.91 for ctc
2024-10-27 17:44:11,055 (beam_search:479) INFO: total log probability: -2.91
2024-10-27 17:44:11,055 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:44:11,055 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:11,055 (beam_search:483) INFO: best hypo: ▁THAT▁THAT▁YOU▁HAVE▁THAT▁YOU▁HAVE▁TO▁CONNECT▁A▁SIDE▁OF▁THE▁MAGNET▁TO▁GET▁IT▁TO

2024-10-27 17:44:11,057 (asr_inference:509) INFO: speech length: 228912
2024-10-27 17:44:20,265 (beam_search:428) INFO: decoder input length: 178
2024-10-27 17:44:20,266 (beam_search:429) INFO: max output length: 178
2024-10-27 17:44:20,266 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:21,134 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:21,134 (beam_search:476) INFO:  -4.08 * 1.0 =  -4.08 for ctc
2024-10-27 17:44:21,134 (beam_search:479) INFO: total log probability: -4.08
2024-10-27 17:44:21,134 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:44:21,134 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:21,134 (beam_search:483) INFO: best hypo: ▁YOU▁NEED▁A▁YOU▁NEED▁TO▁A▁SIDE▁TO▁THE▁MAGNET▁AND▁THEN▁AND▁THEN▁TOUCH▁THAT▁SIDE▁TO▁A▁OF▁THE▁YOU'RE▁TO

2024-10-27 17:44:21,137 (asr_inference:509) INFO: speech length: 87760
2024-10-27 17:44:24,296 (beam_search:428) INFO: decoder input length: 68
2024-10-27 17:44:24,297 (beam_search:429) INFO: max output length: 68
2024-10-27 17:44:24,297 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:24,447 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:24,447 (beam_search:476) INFO:  -2.57 * 1.0 =  -2.57 for ctc
2024-10-27 17:44:24,447 (beam_search:479) INFO: total log probability: -2.57
2024-10-27 17:44:24,447 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:44:24,447 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:24,447 (beam_search:483) INFO: best hypo: ▁THE'S▁THE▁NAIL▁SO▁IT▁CAN▁PICK▁UP▁THE▁PAPER▁CLIP

2024-10-27 17:44:24,450 (asr_inference:509) INFO: speech length: 271536
2024-10-27 17:44:35,432 (beam_search:428) INFO: decoder input length: 211
2024-10-27 17:44:35,432 (beam_search:429) INFO: max output length: 211
2024-10-27 17:44:35,432 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:36,592 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:36,592 (beam_search:476) INFO:  -4.57 * 1.0 =  -4.57 for ctc
2024-10-27 17:44:36,592 (beam_search:479) INFO: total log probability: -4.57
2024-10-27 17:44:36,592 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:44:36,592 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:36,592 (beam_search:483) INFO: best hypo: ▁THE▁MAGNET▁HAS▁STEEL▁THE▁YOU▁HAVE▁TO▁BE▁AND▁THE▁THAT▁YOU'RE▁TO▁PICK▁UP▁WITH▁WHAT▁YOU'RE▁ALSO▁HAS▁TO▁BE▁STEEL

2024-10-27 17:44:36,595 (asr_inference:509) INFO: speech length: 307312
2024-10-27 17:44:49,766 (beam_search:428) INFO: decoder input length: 239
2024-10-27 17:44:49,766 (beam_search:429) INFO: max output length: 239
2024-10-27 17:44:49,766 (beam_search:430) INFO: min output length: 0
2024-10-27 17:44:50,565 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:44:50,565 (beam_search:476) INFO:  -4.21 * 1.0 =  -4.21 for ctc
2024-10-27 17:44:50,567 (beam_search:479) INFO: total log probability: -4.21
2024-10-27 17:44:50,567 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:44:50,567 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:44:50,567 (beam_search:483) INFO: best hypo: ▁WE'VE▁WEVE▁HOW▁YOU▁CAN▁HOW▁MUCH▁IT▁IT▁TO▁BREAK▁THE▁MAGNETIC▁IN▁BETWEEN▁TWO

2024-10-27 17:44:50,569 (asr_inference:509) INFO: speech length: 386000
2024-10-27 17:45:08,015 (beam_search:428) INFO: decoder input length: 301
2024-10-27 17:45:08,015 (beam_search:429) INFO: max output length: 301
2024-10-27 17:45:08,015 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:11,116 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:11,116 (beam_search:476) INFO:  -5.19 * 1.0 =  -5.19 for ctc
2024-10-27 17:45:11,116 (beam_search:479) INFO: total log probability: -5.19
2024-10-27 17:45:11,116 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:45:11,116 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:11,116 (beam_search:483) INFO: best hypo: ▁WE▁TOOK▁A▁AND▁PUT▁A▁STICK▁THAT▁A▁MAGNET▁ON▁IT▁IN▁A▁THE▁HAD▁AND▁THEN▁A▁MAGNET▁ON▁THE▁OTHER▁SIDE▁SO▁THEY'D▁THEN▁WE▁USED▁AND▁THEN▁WE▁USED▁THEN▁WE▁PUT▁IN▁WASHERS▁TO▁SEE▁HOW▁HOW▁MANY▁WASHERS▁IT▁TOOK▁TO▁MAKE▁THE▁MAGNETS▁GO

2024-10-27 17:45:11,119 (asr_inference:509) INFO: speech length: 410304
2024-10-27 17:45:30,191 (beam_search:428) INFO: decoder input length: 320
2024-10-27 17:45:30,191 (beam_search:429) INFO: max output length: 320
2024-10-27 17:45:30,191 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:32,636 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:32,636 (beam_search:476) INFO:  -7.15 * 1.0 =  -7.15 for ctc
2024-10-27 17:45:32,636 (beam_search:479) INFO: total log probability: -7.15
2024-10-27 17:45:32,636 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:45:32,636 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:32,636 (beam_search:483) INFO: best hypo: ▁I▁OUT▁THAT▁MAGNETS▁IT'S▁IT'S▁TO▁IT▁GET▁THEM▁A▁WHEN▁THEY▁JUST▁TOGETHER▁WITH▁IN▁THEM▁BUT▁IT'S▁TO▁GET▁THEM▁WHEN▁THERE'S▁A▁THERE'S▁A▁LITTLE

2024-10-27 17:45:32,639 (asr_inference:509) INFO: speech length: 302768
2024-10-27 17:45:45,325 (beam_search:428) INFO: decoder input length: 236
2024-10-27 17:45:45,325 (beam_search:429) INFO: max output length: 236
2024-10-27 17:45:45,325 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:46,547 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:46,547 (beam_search:476) INFO:  -3.29 * 1.0 =  -3.29 for ctc
2024-10-27 17:45:46,549 (beam_search:479) INFO: total log probability: -3.29
2024-10-27 17:45:46,549 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:45:46,549 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:46,549 (beam_search:483) INFO: best hypo: ▁WE▁CONNECTED▁A▁BATTERY▁TO▁A▁SWITCH▁THEN▁WE▁CONNECTED▁THE▁BATTERY▁SWITCH▁TO▁A▁BIG▁METAL▁BY▁THE▁A▁WIRE▁AROUND▁IT▁THEN▁WEED▁UP▁WASHERS

2024-10-27 17:45:46,552 (asr_inference:509) INFO: speech length: 84256
2024-10-27 17:45:49,755 (beam_search:428) INFO: decoder input length: 65
2024-10-27 17:45:49,755 (beam_search:429) INFO: max output length: 65
2024-10-27 17:45:49,755 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:49,839 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:49,840 (beam_search:476) INFO:  -0.96 * 1.0 =  -0.96 for ctc
2024-10-27 17:45:49,840 (beam_search:479) INFO: total log probability: -0.96
2024-10-27 17:45:49,840 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:45:49,840 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:49,840 (beam_search:483) INFO: best hypo: ▁IT▁WAS▁KIND▁OF▁LIKE▁A▁CIRCUIT

2024-10-27 17:45:49,843 (asr_inference:509) INFO: speech length: 212720
2024-10-27 17:45:58,071 (beam_search:428) INFO: decoder input length: 165
2024-10-27 17:45:58,071 (beam_search:429) INFO: max output length: 165
2024-10-27 17:45:58,071 (beam_search:430) INFO: min output length: 0
2024-10-27 17:45:58,652 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:45:58,652 (beam_search:476) INFO:  -3.27 * 1.0 =  -3.27 for ctc
2024-10-27 17:45:58,652 (beam_search:479) INFO: total log probability: -3.27
2024-10-27 17:45:58,652 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:45:58,652 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:45:58,652 (beam_search:483) INFO: best hypo: ▁UM▁THE▁SWITCHS▁THE▁MAGNET▁ON▁AND▁OFF▁CAUSE▁IT'S▁KIND▁OF▁LIKEING▁A

2024-10-27 17:45:58,656 (asr_inference:509) INFO: speech length: 212368
2024-10-27 17:46:07,303 (beam_search:428) INFO: decoder input length: 165
2024-10-27 17:46:07,304 (beam_search:429) INFO: max output length: 165
2024-10-27 17:46:07,304 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:08,350 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:08,350 (beam_search:476) INFO:  -4.26 * 1.0 =  -4.26 for ctc
2024-10-27 17:46:08,352 (beam_search:479) INFO: total log probability: -4.26
2024-10-27 17:46:08,352 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:46:08,352 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:08,352 (beam_search:483) INFO: best hypo: ▁WHEN▁IF▁THE▁SWITCH▁ISN'T▁THEN▁IT▁CAN'T▁MAKE▁THE▁NAIL▁INTO▁A▁MAGNET▁CAUSE▁IT▁ALL▁HAS▁TO▁BE▁CONNECTED▁FOR▁THE▁NAIL▁TO▁BE▁A▁MAGNET

2024-10-27 17:46:08,355 (asr_inference:509) INFO: speech length: 194784
2024-10-27 17:46:16,210 (beam_search:428) INFO: decoder input length: 151
2024-10-27 17:46:16,210 (beam_search:429) INFO: max output length: 151
2024-10-27 17:46:16,210 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:16,395 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:16,395 (beam_search:476) INFO:  -2.01 * 1.0 =  -2.01 for ctc
2024-10-27 17:46:16,395 (beam_search:479) INFO: total log probability: -2.01
2024-10-27 17:46:16,396 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 17:46:16,396 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:16,396 (beam_search:483) INFO: best hypo: ▁UM▁THE▁DID▁AN▁EXAMPLE▁THE

2024-10-27 17:46:16,399 (asr_inference:509) INFO: speech length: 371792
2024-10-27 17:46:33,471 (beam_search:428) INFO: decoder input length: 289
2024-10-27 17:46:33,471 (beam_search:429) INFO: max output length: 289
2024-10-27 17:46:33,471 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:35,700 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:35,701 (beam_search:476) INFO:  -5.89 * 1.0 =  -5.89 for ctc
2024-10-27 17:46:35,701 (beam_search:479) INFO: total log probability: -5.89
2024-10-27 17:46:35,701 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:46:35,701 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:35,701 (beam_search:483) INFO: best hypo: ▁UM▁HE▁HE▁CONNECTED▁THE▁BATTERY▁AND▁SWITCH▁THEND▁IT▁AROUND▁AND▁THEN▁UH▁THEND▁THE▁WIRE▁AROUND▁THE▁NAIL▁AND▁HAD▁ONE▁OF▁COME▁UP▁TO▁THE▁SWITCH▁AND▁SEE▁MANY▁WASHERS▁THEY▁COULD▁PICK▁UP

2024-10-27 17:46:35,705 (asr_inference:509) INFO: speech length: 330192
2024-10-27 17:46:50,206 (beam_search:428) INFO: decoder input length: 257
2024-10-27 17:46:50,206 (beam_search:429) INFO: max output length: 257
2024-10-27 17:46:50,206 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:51,268 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:51,268 (beam_search:476) INFO:  -3.95 * 1.0 =  -3.95 for ctc
2024-10-27 17:46:51,268 (beam_search:479) INFO: total log probability: -3.95
2024-10-27 17:46:51,268 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:46:51,268 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:51,268 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁FROM▁THE▁BATTERY▁GOES▁THROUGH▁THE▁SWITCH▁AND▁THE▁NAIL▁AND▁A▁IT▁IT▁SO▁IT'S▁AN▁ELECTROMAGNET

2024-10-27 17:46:51,270 (asr_inference:509) INFO: speech length: 131792
2024-10-27 17:46:56,037 (beam_search:428) INFO: decoder input length: 102
2024-10-27 17:46:56,037 (beam_search:429) INFO: max output length: 102
2024-10-27 17:46:56,037 (beam_search:430) INFO: min output length: 0
2024-10-27 17:46:56,339 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:46:56,340 (beam_search:476) INFO:  -2.34 * 1.0 =  -2.34 for ctc
2024-10-27 17:46:56,340 (beam_search:479) INFO: total log probability: -2.34
2024-10-27 17:46:56,340 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:46:56,340 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:46:56,340 (beam_search:483) INFO: best hypo: ▁IF▁YOU▁CANING▁UP▁THE▁WASHERS▁WITH▁IT▁THEN▁YOU▁CAN▁IT'S▁MAGNETIC

2024-10-27 17:46:56,342 (asr_inference:509) INFO: speech length: 164224
2024-10-27 17:47:02,527 (beam_search:428) INFO: decoder input length: 127
2024-10-27 17:47:02,527 (beam_search:429) INFO: max output length: 127
2024-10-27 17:47:02,527 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:02,973 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:02,973 (beam_search:476) INFO:  -4.40 * 1.0 =  -4.40 for ctc
2024-10-27 17:47:02,973 (beam_search:479) INFO: total log probability: -4.40
2024-10-27 17:47:02,973 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:47:02,973 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:02,973 (beam_search:483) INFO: best hypo: ▁IF▁IT▁ISN'T▁TO▁PICK▁UP▁ANY▁IT▁WASHERS▁AND▁THE▁SWITCH▁IT'S▁OPEN

2024-10-27 17:47:02,975 (asr_inference:509) INFO: speech length: 159536
2024-10-27 17:47:08,781 (beam_search:428) INFO: decoder input length: 124
2024-10-27 17:47:08,781 (beam_search:429) INFO: max output length: 124
2024-10-27 17:47:08,781 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:09,069 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:09,069 (beam_search:476) INFO:  -2.10 * 1.0 =  -2.10 for ctc
2024-10-27 17:47:09,069 (beam_search:479) INFO: total log probability: -2.10
2024-10-27 17:47:09,069 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:47:09,069 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:09,069 (beam_search:483) INFO: best hypo: ▁UM▁IF▁YOU▁NEED▁TO▁MOVE▁SOMETHING▁AND▁METAL▁TO▁A▁DIFFERENT▁PLACE

2024-10-27 17:47:09,071 (asr_inference:509) INFO: speech length: 154496
2024-10-27 17:47:14,869 (beam_search:428) INFO: decoder input length: 120
2024-10-27 17:47:14,869 (beam_search:429) INFO: max output length: 120
2024-10-27 17:47:14,869 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:15,207 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:15,208 (beam_search:476) INFO:  -4.94 * 1.0 =  -4.94 for ctc
2024-10-27 17:47:15,208 (beam_search:479) INFO: total log probability: -4.94
2024-10-27 17:47:15,208 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 17:47:15,208 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:15,208 (beam_search:483) INFO: best hypo: ▁WITH▁A▁MAGNET▁YOU▁CAN'T▁IF▁YOU▁WANT▁TO▁YOU▁HAVE▁TO▁IT▁OFF

2024-10-27 17:47:15,211 (asr_inference:509) INFO: speech length: 375296
2024-10-27 17:47:31,838 (beam_search:428) INFO: decoder input length: 292
2024-10-27 17:47:31,838 (beam_search:429) INFO: max output length: 292
2024-10-27 17:47:31,838 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:35,513 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:35,514 (beam_search:476) INFO:  -8.54 * 1.0 =  -8.54 for ctc
2024-10-27 17:47:35,514 (beam_search:479) INFO: total log probability: -8.54
2024-10-27 17:47:35,514 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:47:35,514 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:35,514 (beam_search:483) INFO: best hypo: ▁WELL▁TODAY▁WE▁DID▁SCIENCE▁AND▁WE▁USED▁A▁BOARD▁AND▁WED▁A▁LIGHT▁BULB▁AND▁A▁LIGHT▁BULB▁AND▁A▁BATTERY▁IN▁A▁AND▁THE▁BATTERY▁A▁BATTERY▁AND▁WE▁MADE▁A▁SO▁THE▁AND▁WE▁ALSO▁HAD▁A▁AND▁WE▁HAD▁TO▁MAKE▁THE▁AND▁THE▁LIGHT▁THE▁LIGHT▁HAD▁TO▁GO▁ON▁AND▁THE▁HAD▁A▁LITTLE▁PIECE▁OF▁THAT▁HAD▁TO

2024-10-27 17:47:35,517 (asr_inference:509) INFO: speech length: 338640
2024-10-27 17:47:50,486 (beam_search:428) INFO: decoder input length: 264
2024-10-27 17:47:50,486 (beam_search:429) INFO: max output length: 264
2024-10-27 17:47:50,486 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:53,141 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:53,142 (beam_search:476) INFO:  -5.31 * 1.0 =  -5.31 for ctc
2024-10-27 17:47:53,142 (beam_search:479) INFO: total log probability: -5.31
2024-10-27 17:47:53,142 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:47:53,142 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:53,142 (beam_search:483) INFO: best hypo: ▁WELL▁WE▁UM▁WE▁SORT▁OF▁HAD▁WIRES▁AND▁WE▁PUT▁THE▁WIRES▁ON▁THE▁BATTERY▁ENDS▁FROM▁THE▁POSITIVE▁TO▁THE▁NEGATIVE▁NO▁NEGATIVE▁TO▁THE▁POSITIVE▁AND▁SO▁THEN▁WEED▁THAT▁UP▁TO▁THE▁MOTOR▁AND▁THEN▁WE▁HOOKED▁ONE▁OF▁THES▁WIRES▁TO▁THE▁SWITCH▁THAT▁WE▁HAD

2024-10-27 17:47:53,145 (asr_inference:509) INFO: speech length: 79072
2024-10-27 17:47:56,049 (beam_search:428) INFO: decoder input length: 61
2024-10-27 17:47:56,049 (beam_search:429) INFO: max output length: 61
2024-10-27 17:47:56,049 (beam_search:430) INFO: min output length: 0
2024-10-27 17:47:56,200 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:47:56,200 (beam_search:476) INFO:  -3.68 * 1.0 =  -3.68 for ctc
2024-10-27 17:47:56,200 (beam_search:479) INFO: total log probability: -3.68
2024-10-27 17:47:56,200 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:47:56,200 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:47:56,200 (beam_search:483) INFO: best hypo: ▁I▁DON'T▁REALLY▁KNOW▁I▁DON'T▁THINK▁MY▁US▁BUT▁UM▁YEAH

2024-10-27 17:47:56,202 (asr_inference:509) INFO: speech length: 158304
2024-10-27 17:48:02,284 (beam_search:428) INFO: decoder input length: 123
2024-10-27 17:48:02,284 (beam_search:429) INFO: max output length: 123
2024-10-27 17:48:02,284 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:02,972 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:02,972 (beam_search:476) INFO:  -0.92 * 1.0 =  -0.92 for ctc
2024-10-27 17:48:02,972 (beam_search:479) INFO: total log probability: -0.92
2024-10-27 17:48:02,972 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 17:48:02,972 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:02,972 (beam_search:483) INFO: best hypo: ▁WELL▁THERE'S▁TWO▁WIRES▁CONNECTED▁TO▁THE▁BATTERY▁AND▁TO▁THE▁LIGHT▁BULB▁AND▁THE▁ENERGY▁THE▁BATTERY▁HAS▁IT▁TO▁THE▁LIGHT▁BULB▁AND▁IT▁MAKES▁IT▁LIGHT▁UP

2024-10-27 17:48:02,974 (asr_inference:509) INFO: speech length: 213216
2024-10-27 17:48:11,398 (beam_search:428) INFO: decoder input length: 166
2024-10-27 17:48:11,398 (beam_search:429) INFO: max output length: 166
2024-10-27 17:48:11,398 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:12,272 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:12,272 (beam_search:476) INFO:  -2.33 * 1.0 =  -2.33 for ctc
2024-10-27 17:48:12,272 (beam_search:479) INFO: total log probability: -2.33
2024-10-27 17:48:12,272 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:48:12,272 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:12,272 (beam_search:483) INFO: best hypo: ▁I▁CAN▁SEE▁THAT▁IT'S▁GOING▁FROM▁THE▁TO▁THE▁RIGHT▁S▁AND▁IT▁GOES▁THROUGH▁THE▁LIGHT▁BULB▁LIKE▁IF▁IT▁WAS▁STILL▁SORT▁OF▁UM

2024-10-27 17:48:12,275 (asr_inference:509) INFO: speech length: 53072
2024-10-27 17:48:14,272 (beam_search:428) INFO: decoder input length: 40
2024-10-27 17:48:14,272 (beam_search:429) INFO: max output length: 40
2024-10-27 17:48:14,272 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:14,346 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:14,346 (beam_search:476) INFO:  -0.59 * 1.0 =  -0.59 for ctc
2024-10-27 17:48:14,346 (beam_search:479) INFO: total log probability: -0.59
2024-10-27 17:48:14,346 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:48:14,346 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:14,346 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁IT▁IS▁GOING▁FROM▁THE▁NEGATIVE▁TO▁THE▁POSITIVE

2024-10-27 17:48:14,348 (asr_inference:509) INFO: speech length: 183376
2024-10-27 17:48:21,488 (beam_search:428) INFO: decoder input length: 142
2024-10-27 17:48:21,488 (beam_search:429) INFO: max output length: 142
2024-10-27 17:48:21,488 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:22,047 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:22,047 (beam_search:476) INFO:  -2.02 * 1.0 =  -2.02 for ctc
2024-10-27 17:48:22,049 (beam_search:479) INFO: total log probability: -2.02
2024-10-27 17:48:22,049 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:48:22,049 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:22,049 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁BATTERY▁HAS▁TWO▁WIRES▁CONNECTED▁TO▁IT▁AND▁THEN▁IT'S▁CONNECTED▁TO▁THE▁MOTOR▁WHICH▁MAKES▁THE▁LITTLE

2024-10-27 17:48:22,051 (asr_inference:509) INFO: speech length: 120464
2024-10-27 17:48:26,425 (beam_search:428) INFO: decoder input length: 93
2024-10-27 17:48:26,425 (beam_search:429) INFO: max output length: 93
2024-10-27 17:48:26,425 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:26,786 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:26,787 (beam_search:476) INFO:  -2.36 * 1.0 =  -2.36 for ctc
2024-10-27 17:48:26,787 (beam_search:479) INFO: total log probability: -2.36
2024-10-27 17:48:26,787 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:48:26,787 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:26,787 (beam_search:483) INFO: best hypo: ▁UM▁I▁THINK▁IT▁WON'T▁WORK▁AS▁I▁I▁JUST▁DON'T▁THINK▁IT▁I▁DON'T▁REALLY▁IT

2024-10-27 17:48:26,789 (asr_inference:509) INFO: speech length: 28128
2024-10-27 17:48:27,969 (beam_search:428) INFO: decoder input length: 21
2024-10-27 17:48:27,969 (beam_search:429) INFO: max output length: 21
2024-10-27 17:48:27,969 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:27,996 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:27,996 (beam_search:476) INFO:  -0.17 * 1.0 =  -0.17 for ctc
2024-10-27 17:48:27,996 (beam_search:479) INFO: total log probability: -0.17
2024-10-27 17:48:27,996 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 17:48:27,996 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:27,997 (beam_search:483) INFO: best hypo: ▁NO▁I'M▁NOT▁REALLY▁SURE

2024-10-27 17:48:28,000 (asr_inference:509) INFO: speech length: 159984
2024-10-27 17:48:34,101 (beam_search:428) INFO: decoder input length: 124
2024-10-27 17:48:34,101 (beam_search:429) INFO: max output length: 124
2024-10-27 17:48:34,101 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:34,669 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:34,669 (beam_search:476) INFO:  -2.85 * 1.0 =  -2.85 for ctc
2024-10-27 17:48:34,669 (beam_search:479) INFO: total log probability: -2.85
2024-10-27 17:48:34,669 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:48:34,669 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:34,669 (beam_search:483) INFO: best hypo: ▁WELL▁AS▁I▁YOU▁BEFORE▁THE▁WIRES▁ARE▁CONNECTED▁TO▁THE▁MOTOR▁AND▁THE▁ELECTRICITY▁FROM▁THE▁BATTERY▁GOES▁THROUGH▁THE▁MOTOR▁AND▁MAKES▁THE

2024-10-27 17:48:34,672 (asr_inference:509) INFO: speech length: 62016
2024-10-27 17:48:36,994 (beam_search:428) INFO: decoder input length: 47
2024-10-27 17:48:36,994 (beam_search:429) INFO: max output length: 47
2024-10-27 17:48:36,994 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:37,112 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:37,112 (beam_search:476) INFO:  -2.11 * 1.0 =  -2.11 for ctc
2024-10-27 17:48:37,112 (beam_search:479) INFO: total log probability: -2.11
2024-10-27 17:48:37,112 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:48:37,112 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:37,112 (beam_search:483) INFO: best hypo: ▁IT▁LOOKS▁LIKE▁IT'S▁STILL▁BUT▁IT▁DOESN'T▁LOOK▁AS▁TO▁ME

2024-10-27 17:48:37,114 (asr_inference:509) INFO: speech length: 15344
2024-10-27 17:48:37,885 (beam_search:428) INFO: decoder input length: 11
2024-10-27 17:48:37,885 (beam_search:429) INFO: max output length: 11
2024-10-27 17:48:37,885 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:37,894 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:37,894 (beam_search:476) INFO:  -2.04 * 1.0 =  -2.04 for ctc
2024-10-27 17:48:37,894 (beam_search:479) INFO: total log probability: -2.04
2024-10-27 17:48:37,894 (beam_search:480) INFO: normalized log probability: -0.41
2024-10-27 17:48:37,894 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:37,894 (beam_search:483) INFO: best hypo: 'D▁BE

2024-10-27 17:48:37,897 (asr_inference:509) INFO: speech length: 109200
2024-10-27 17:48:41,984 (beam_search:428) INFO: decoder input length: 84
2024-10-27 17:48:41,984 (beam_search:429) INFO: max output length: 84
2024-10-27 17:48:41,984 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:42,199 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:42,199 (beam_search:476) INFO:  -1.59 * 1.0 =  -1.59 for ctc
2024-10-27 17:48:42,199 (beam_search:479) INFO: total log probability: -1.59
2024-10-27 17:48:42,199 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:48:42,199 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:42,199 (beam_search:483) INFO: best hypo: ▁YEAH▁IT'S▁THE▁THE▁THE▁THAT▁IT▁WAS▁WHEN▁THE▁BATTERY▁WAS▁THE

2024-10-27 17:48:42,202 (asr_inference:509) INFO: speech length: 30048
2024-10-27 17:48:43,442 (beam_search:428) INFO: decoder input length: 22
2024-10-27 17:48:43,442 (beam_search:429) INFO: max output length: 22
2024-10-27 17:48:43,442 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:43,466 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:43,466 (beam_search:476) INFO:  -0.49 * 1.0 =  -0.49 for ctc
2024-10-27 17:48:43,466 (beam_search:479) INFO: total log probability: -0.49
2024-10-27 17:48:43,466 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:48:43,466 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:43,466 (beam_search:483) INFO: best hypo: ▁I▁DON'T▁REALLY▁KNOW

2024-10-27 17:48:43,469 (asr_inference:509) INFO: speech length: 21296
2024-10-27 17:48:44,385 (beam_search:428) INFO: decoder input length: 16
2024-10-27 17:48:44,385 (beam_search:429) INFO: max output length: 16
2024-10-27 17:48:44,385 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:44,398 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:44,398 (beam_search:476) INFO:  -0.35 * 1.0 =  -0.35 for ctc
2024-10-27 17:48:44,398 (beam_search:479) INFO: total log probability: -0.35
2024-10-27 17:48:44,398 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:48:44,398 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:44,398 (beam_search:483) INFO: best hypo: ▁UM▁I▁IT

2024-10-27 17:48:44,400 (asr_inference:509) INFO: speech length: 206144
2024-10-27 17:48:52,695 (beam_search:428) INFO: decoder input length: 160
2024-10-27 17:48:52,695 (beam_search:429) INFO: max output length: 160
2024-10-27 17:48:52,695 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:53,626 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:53,626 (beam_search:476) INFO:  -2.91 * 1.0 =  -2.91 for ctc
2024-10-27 17:48:53,626 (beam_search:479) INFO: total log probability: -2.91
2024-10-27 17:48:53,626 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:48:53,626 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:53,626 (beam_search:483) INFO: best hypo: ▁WELL▁IT'S▁IF▁YOU▁THE▁BATTERY▁AROUND▁LIKE▁YOU▁TOLD▁ME▁TO▁DO▁IT▁WILL▁GO▁THE▁OPPOSITE▁THAT▁IT▁DOES▁WHEN▁IT'S▁THE▁FROM▁NEGATIVE▁TO▁POSITIVE

2024-10-27 17:48:53,629 (asr_inference:509) INFO: speech length: 28112
2024-10-27 17:48:54,809 (beam_search:428) INFO: decoder input length: 21
2024-10-27 17:48:54,809 (beam_search:429) INFO: max output length: 21
2024-10-27 17:48:54,809 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:54,834 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:54,834 (beam_search:476) INFO:  -0.89 * 1.0 =  -0.89 for ctc
2024-10-27 17:48:54,834 (beam_search:479) INFO: total log probability: -0.89
2024-10-27 17:48:54,834 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:48:54,834 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:54,834 (beam_search:483) INFO: best hypo: ▁I▁DON'T▁THINK▁IT

2024-10-27 17:48:54,837 (asr_inference:509) INFO: speech length: 103872
2024-10-27 17:48:58,709 (beam_search:428) INFO: decoder input length: 80
2024-10-27 17:48:58,709 (beam_search:429) INFO: max output length: 80
2024-10-27 17:48:58,709 (beam_search:430) INFO: min output length: 0
2024-10-27 17:48:58,951 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:48:58,951 (beam_search:476) INFO:  -1.09 * 1.0 =  -1.09 for ctc
2024-10-27 17:48:58,951 (beam_search:479) INFO: total log probability: -1.09
2024-10-27 17:48:58,951 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:48:58,951 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:48:58,951 (beam_search:483) INFO: best hypo: ▁WELL▁I▁DON'T▁THINK▁IT▁IF▁IT'S▁FROM▁THE▁POSITIVE▁TO▁AS▁AS▁THE▁ENERGY

2024-10-27 17:48:58,954 (asr_inference:509) INFO: speech length: 122304
2024-10-27 17:49:03,497 (beam_search:428) INFO: decoder input length: 95
2024-10-27 17:49:03,497 (beam_search:429) INFO: max output length: 95
2024-10-27 17:49:03,497 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:03,715 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:03,715 (beam_search:476) INFO:  -2.23 * 1.0 =  -2.23 for ctc
2024-10-27 17:49:03,715 (beam_search:479) INFO: total log probability: -2.23
2024-10-27 17:49:03,715 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:49:03,715 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:03,715 (beam_search:483) INFO: best hypo: ▁BECAUSE▁IT'S▁FLOWING▁THE▁SAME▁BUT▁IT'S▁JUST▁THE

2024-10-27 17:49:03,717 (asr_inference:509) INFO: speech length: 64496
2024-10-27 17:49:06,063 (beam_search:428) INFO: decoder input length: 49
2024-10-27 17:49:06,063 (beam_search:429) INFO: max output length: 49
2024-10-27 17:49:06,063 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:06,102 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:06,102 (beam_search:476) INFO:  -0.87 * 1.0 =  -0.87 for ctc
2024-10-27 17:49:06,102 (beam_search:479) INFO: total log probability: -0.87
2024-10-27 17:49:06,102 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:49:06,103 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:06,103 (beam_search:483) INFO: best hypo: ▁UM▁WHAT▁DO▁EXACTLY

2024-10-27 17:49:06,105 (asr_inference:509) INFO: speech length: 218528
2024-10-27 17:49:14,834 (beam_search:428) INFO: decoder input length: 170
2024-10-27 17:49:14,834 (beam_search:429) INFO: max output length: 170
2024-10-27 17:49:14,834 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:15,703 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:15,704 (beam_search:476) INFO:  -4.16 * 1.0 =  -4.16 for ctc
2024-10-27 17:49:15,704 (beam_search:479) INFO: total log probability: -4.16
2024-10-27 17:49:15,704 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:49:15,704 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:15,704 (beam_search:483) INFO: best hypo: ▁UM▁I▁DON'T▁THINK▁MUCH▁BECAUSE▁IT▁JUST▁SORT▁OF▁SINCE▁YOU▁THE▁BATTERY'S▁SORT▁OF▁LIKEING▁THE▁WAY▁OF▁THE

2024-10-27 17:49:15,707 (asr_inference:509) INFO: speech length: 121760
2024-10-27 17:49:20,369 (beam_search:428) INFO: decoder input length: 94
2024-10-27 17:49:20,369 (beam_search:429) INFO: max output length: 94
2024-10-27 17:49:20,369 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:20,535 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:20,536 (beam_search:476) INFO:  -0.81 * 1.0 =  -0.81 for ctc
2024-10-27 17:49:20,536 (beam_search:479) INFO: total log probability: -0.81
2024-10-27 17:49:20,536 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:49:20,536 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:20,536 (beam_search:483) INFO: best hypo: ▁UM▁LIKE▁I▁SAID▁IT▁JUST▁THE▁WAY▁THE▁THE

2024-10-27 17:49:20,539 (asr_inference:509) INFO: speech length: 71712
2024-10-27 17:49:23,147 (beam_search:428) INFO: decoder input length: 55
2024-10-27 17:49:23,147 (beam_search:429) INFO: max output length: 55
2024-10-27 17:49:23,147 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:23,255 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:23,255 (beam_search:476) INFO:  -2.27 * 1.0 =  -2.27 for ctc
2024-10-27 17:49:23,255 (beam_search:479) INFO: total log probability: -2.27
2024-10-27 17:49:23,255 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:49:23,255 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:23,255 (beam_search:483) INFO: best hypo: ▁OF▁GOING▁FROM▁LEFT▁TO▁RIGHT▁IT▁GO▁FROM▁RIGHT▁TO

2024-10-27 17:49:23,257 (asr_inference:509) INFO: speech length: 188880
2024-10-27 17:49:30,656 (beam_search:428) INFO: decoder input length: 147
2024-10-27 17:49:30,656 (beam_search:429) INFO: max output length: 147
2024-10-27 17:49:30,656 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:31,297 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:31,297 (beam_search:476) INFO:  -3.23 * 1.0 =  -3.23 for ctc
2024-10-27 17:49:31,297 (beam_search:479) INFO: total log probability: -3.23
2024-10-27 17:49:31,297 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:49:31,297 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:31,297 (beam_search:483) INFO: best hypo: ▁WELL▁MAYBE▁BECAUSE▁IT▁THE▁BATTERY▁THE'S▁WAY▁OF▁IT'S▁LIKE▁THE▁FROM▁NEGATIVE▁TO▁POSITIVE▁TO▁POSITIVE▁TO▁NEGATIVE

2024-10-27 17:49:31,299 (asr_inference:509) INFO: speech length: 123664
2024-10-27 17:49:36,061 (beam_search:428) INFO: decoder input length: 96
2024-10-27 17:49:36,061 (beam_search:429) INFO: max output length: 96
2024-10-27 17:49:36,061 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:36,334 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:36,334 (beam_search:476) INFO:  -1.78 * 1.0 =  -1.78 for ctc
2024-10-27 17:49:36,334 (beam_search:479) INFO: total log probability: -1.78
2024-10-27 17:49:36,334 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:49:36,334 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:36,334 (beam_search:483) INFO: best hypo: ▁BECAUSE▁IT'S▁JUST▁LIKEING▁THE▁WAY▁THE▁ELECTRICITY▁LIKE▁YOU▁THE▁AND▁THE▁MOTOR

2024-10-27 17:49:36,337 (asr_inference:509) INFO: speech length: 10960
2024-10-27 17:49:37,005 (beam_search:428) INFO: decoder input length: 8
2024-10-27 17:49:37,005 (beam_search:429) INFO: max output length: 8
2024-10-27 17:49:37,005 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:37,013 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:37,013 (beam_search:476) INFO:  -0.08 * 1.0 =  -0.08 for ctc
2024-10-27 17:49:37,013 (beam_search:479) INFO: total log probability: -0.08
2024-10-27 17:49:37,013 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 17:49:37,013 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:37,013 (beam_search:483) INFO: best hypo: ▁YOU'RE

2024-10-27 17:49:37,016 (asr_inference:509) INFO: speech length: 330992
2024-10-27 17:49:51,292 (beam_search:428) INFO: decoder input length: 258
2024-10-27 17:49:51,292 (beam_search:429) INFO: max output length: 258
2024-10-27 17:49:51,292 (beam_search:430) INFO: min output length: 0
2024-10-27 17:49:52,691 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:49:52,691 (beam_search:476) INFO:  -3.91 * 1.0 =  -3.91 for ctc
2024-10-27 17:49:52,691 (beam_search:479) INFO: total log probability: -3.91
2024-10-27 17:49:52,691 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:49:52,691 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:49:52,691 (beam_search:483) INFO: best hypo: ▁WELL▁WE▁HAVE▁UM▁THEY▁WASHERS▁WE▁HAVE▁TWO▁AND▁WE▁HAVE▁TWO▁MAGNETS▁AND▁WE▁USE▁THEM▁TO▁SEE▁HOW▁THE▁IS▁UM▁HOW▁IT▁WORKS

2024-10-27 17:49:52,693 (asr_inference:509) INFO: speech length: 224128
2024-10-27 17:50:01,633 (beam_search:428) INFO: decoder input length: 174
2024-10-27 17:50:01,633 (beam_search:429) INFO: max output length: 174
2024-10-27 17:50:01,633 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:02,744 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:02,744 (beam_search:476) INFO:  -4.31 * 1.0 =  -4.31 for ctc
2024-10-27 17:50:02,744 (beam_search:479) INFO: total log probability: -4.31
2024-10-27 17:50:02,744 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:50:02,744 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:02,744 (beam_search:483) INFO: best hypo: ▁UM▁WELL▁WE▁TO▁PUT▁IT▁TOGETHER▁AND▁WE▁DID▁IT▁WITH▁A▁IN▁THE▁MIDDLE▁AND▁IT▁WE▁HAD▁TO▁SEE▁HOW▁MANY▁WASHERS▁IT▁TOOK▁SO▁THE▁IT▁WOULD▁TAKE▁THE▁MAGNETS▁APART

2024-10-27 17:50:02,746 (asr_inference:509) INFO: speech length: 120080
2024-10-27 17:50:07,147 (beam_search:428) INFO: decoder input length: 93
2024-10-27 17:50:07,147 (beam_search:429) INFO: max output length: 93
2024-10-27 17:50:07,147 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:07,335 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:07,335 (beam_search:476) INFO:  -0.78 * 1.0 =  -0.78 for ctc
2024-10-27 17:50:07,335 (beam_search:479) INFO: total log probability: -0.78
2024-10-27 17:50:07,335 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:50:07,335 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:07,335 (beam_search:483) INFO: best hypo: ▁WE▁JUST▁HAD▁TO▁USE▁THEM▁AS▁LIKE▁A▁TO▁THE▁MAGNETS

2024-10-27 17:50:07,338 (asr_inference:509) INFO: speech length: 217248
2024-10-27 17:50:16,027 (beam_search:428) INFO: decoder input length: 169
2024-10-27 17:50:16,027 (beam_search:429) INFO: max output length: 169
2024-10-27 17:50:16,027 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:16,568 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:16,568 (beam_search:476) INFO:  -2.13 * 1.0 =  -2.13 for ctc
2024-10-27 17:50:16,568 (beam_search:479) INFO: total log probability: -2.13
2024-10-27 17:50:16,568 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:50:16,568 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:16,568 (beam_search:483) INFO: best hypo: ▁WELL▁USING▁THE▁WASHERS▁WE▁HAD▁TO▁MAKE▁THE▁UM▁TO▁MAKE▁THE▁MAGNETS▁UM▁NOT▁TO▁BE

2024-10-27 17:50:16,571 (asr_inference:509) INFO: speech length: 9136
2024-10-27 17:50:17,201 (beam_search:428) INFO: decoder input length: 6
2024-10-27 17:50:17,202 (beam_search:429) INFO: max output length: 6
2024-10-27 17:50:17,202 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:17,205 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:17,205 (beam_search:476) INFO:  -0.08 * 1.0 =  -0.08 for ctc
2024-10-27 17:50:17,205 (beam_search:479) INFO: total log probability: -0.08
2024-10-27 17:50:17,205 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:50:17,205 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:17,205 (beam_search:483) INFO: best hypo: 

2024-10-27 17:50:17,207 (asr_inference:509) INFO: speech length: 36944
2024-10-27 17:50:18,705 (beam_search:428) INFO: decoder input length: 28
2024-10-27 17:50:18,705 (beam_search:429) INFO: max output length: 28
2024-10-27 17:50:18,705 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:18,725 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:18,725 (beam_search:476) INFO:  -1.39 * 1.0 =  -1.39 for ctc
2024-10-27 17:50:18,725 (beam_search:479) INFO: total log probability: -1.39
2024-10-27 17:50:18,725 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 17:50:18,725 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:18,725 (beam_search:483) INFO: best hypo: ▁YOU▁DO▁IT

2024-10-27 17:50:18,727 (asr_inference:509) INFO: speech length: 93504
2024-10-27 17:50:22,113 (beam_search:428) INFO: decoder input length: 72
2024-10-27 17:50:22,113 (beam_search:429) INFO: max output length: 72
2024-10-27 17:50:22,113 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:22,189 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:22,189 (beam_search:476) INFO:  -1.76 * 1.0 =  -1.76 for ctc
2024-10-27 17:50:22,189 (beam_search:479) INFO: total log probability: -1.76
2024-10-27 17:50:22,189 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 17:50:22,189 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:22,189 (beam_search:483) INFO: best hypo: ▁WELL▁WE▁WERE▁ABOUT▁AND▁CIRCUITS

2024-10-27 17:50:22,191 (asr_inference:509) INFO: speech length: 86896
2024-10-27 17:50:25,279 (beam_search:428) INFO: decoder input length: 67
2024-10-27 17:50:25,279 (beam_search:429) INFO: max output length: 67
2024-10-27 17:50:25,279 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:25,433 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:25,434 (beam_search:476) INFO:  -0.75 * 1.0 =  -0.75 for ctc
2024-10-27 17:50:25,434 (beam_search:479) INFO: total log probability: -0.75
2024-10-27 17:50:25,434 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:50:25,434 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:25,434 (beam_search:483) INFO: best hypo: ▁WELL▁IT'S▁LIKE▁A▁CIRCUIT▁THAT▁IT'S▁YOU▁CAN▁TWO▁PATHWAYS

2024-10-27 17:50:25,437 (asr_inference:509) INFO: speech length: 178416
2024-10-27 17:50:32,518 (beam_search:428) INFO: decoder input length: 138
2024-10-27 17:50:32,518 (beam_search:429) INFO: max output length: 138
2024-10-27 17:50:32,518 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:33,096 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:33,096 (beam_search:476) INFO:  -4.06 * 1.0 =  -4.06 for ctc
2024-10-27 17:50:33,096 (beam_search:479) INFO: total log probability: -4.06
2024-10-27 17:50:33,096 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:50:33,096 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:33,096 (beam_search:483) INFO: best hypo: ▁WELL▁IT'S▁IF▁YOU▁HAVE▁A▁A▁YOU▁CAN▁ONLY▁ONE▁WITH▁YOUR▁FINGER▁WHICH▁IS▁A▁A▁WHICH▁IS▁CIRCUIT▁FROM

2024-10-27 17:50:33,098 (asr_inference:509) INFO: speech length: 27280
2024-10-27 17:50:34,189 (beam_search:428) INFO: decoder input length: 20
2024-10-27 17:50:34,189 (beam_search:429) INFO: max output length: 20
2024-10-27 17:50:34,189 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:34,215 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:34,215 (beam_search:476) INFO:  -0.09 * 1.0 =  -0.09 for ctc
2024-10-27 17:50:34,215 (beam_search:479) INFO: total log probability: -0.09
2024-10-27 17:50:34,215 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 17:50:34,215 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:34,215 (beam_search:483) INFO: best hypo: ▁UM▁I▁DON'T▁REALLY▁KNOW

2024-10-27 17:50:34,218 (asr_inference:509) INFO: speech length: 199264
2024-10-27 17:50:42,236 (beam_search:428) INFO: decoder input length: 155
2024-10-27 17:50:42,236 (beam_search:429) INFO: max output length: 155
2024-10-27 17:50:42,236 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:43,113 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:43,113 (beam_search:476) INFO:  -5.05 * 1.0 =  -5.05 for ctc
2024-10-27 17:50:43,113 (beam_search:479) INFO: total log probability: -5.05
2024-10-27 17:50:43,113 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:50:43,113 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:43,113 (beam_search:483) INFO: best hypo: ▁WELL▁THERE'▁TWO▁LIGHT▁AND▁A▁ONE▁BATTERY▁SO▁I'M▁GUESSING▁CAUSE▁WE▁DID▁THIS▁IN▁CLASS▁IT'S▁NOT▁IT'S▁PROBABLY▁GONNA▁BE

2024-10-27 17:50:43,115 (asr_inference:509) INFO: speech length: 20736
2024-10-27 17:50:44,073 (beam_search:428) INFO: decoder input length: 15
2024-10-27 17:50:44,073 (beam_search:429) INFO: max output length: 15
2024-10-27 17:50:44,073 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:44,083 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:44,083 (beam_search:476) INFO:  -0.50 * 1.0 =  -0.50 for ctc
2024-10-27 17:50:44,083 (beam_search:479) INFO: total log probability: -0.50
2024-10-27 17:50:44,083 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:50:44,083 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:44,083 (beam_search:483) INFO: best hypo: ▁ITS

2024-10-27 17:50:44,085 (asr_inference:509) INFO: speech length: 102304
2024-10-27 17:50:47,785 (beam_search:428) INFO: decoder input length: 79
2024-10-27 17:50:47,785 (beam_search:429) INFO: max output length: 79
2024-10-27 17:50:47,786 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:47,900 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:47,900 (beam_search:476) INFO:  -2.67 * 1.0 =  -2.67 for ctc
2024-10-27 17:50:47,900 (beam_search:479) INFO: total log probability: -2.67
2024-10-27 17:50:47,901 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 17:50:47,901 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:47,901 (beam_search:483) INFO: best hypo: ▁WELL▁THIS▁IS▁AS▁IT▁CALLED▁A

2024-10-27 17:50:47,904 (asr_inference:509) INFO: speech length: 78640
2024-10-27 17:50:50,736 (beam_search:428) INFO: decoder input length: 60
2024-10-27 17:50:50,736 (beam_search:429) INFO: max output length: 60
2024-10-27 17:50:50,736 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:50,774 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:50,775 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 17:50:50,775 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 17:50:50,775 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 17:50:50,775 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:50,775 (beam_search:483) INFO: best hypo: ▁WHAT'S

2024-10-27 17:50:50,777 (asr_inference:509) INFO: speech length: 180752
2024-10-27 17:50:57,741 (beam_search:428) INFO: decoder input length: 140
2024-10-27 17:50:57,741 (beam_search:429) INFO: max output length: 140
2024-10-27 17:50:57,742 (beam_search:430) INFO: min output length: 0
2024-10-27 17:50:58,211 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:50:58,212 (beam_search:476) INFO:  -1.18 * 1.0 =  -1.18 for ctc
2024-10-27 17:50:58,212 (beam_search:479) INFO: total log probability: -1.18
2024-10-27 17:50:58,212 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:50:58,212 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:50:58,212 (beam_search:483) INFO: best hypo: ▁WELL▁YOU▁CAN▁TWO▁PATHWAYS▁UM▁WITH▁ONE▁LIGHT▁BULB▁AND▁ANOTHER▁LIGHT▁BULB▁YOU▁COULD▁ANOTHER▁ONE

2024-10-27 17:50:58,214 (asr_inference:509) INFO: speech length: 130480
2024-10-27 17:51:03,327 (beam_search:428) INFO: decoder input length: 101
2024-10-27 17:51:03,327 (beam_search:429) INFO: max output length: 101
2024-10-27 17:51:03,327 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:03,612 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:03,612 (beam_search:476) INFO:  -1.83 * 1.0 =  -1.83 for ctc
2024-10-27 17:51:03,612 (beam_search:479) INFO: total log probability: -1.83
2024-10-27 17:51:03,612 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:51:03,612 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:03,613 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁BATTERY▁ENERGY▁TO▁THE▁THE▁WIRES▁TO▁THE▁LIGHT▁BULB▁MAKES▁IT▁LIGHT▁UP

2024-10-27 17:51:03,615 (asr_inference:509) INFO: speech length: 182624
2024-10-27 17:51:10,521 (beam_search:428) INFO: decoder input length: 142
2024-10-27 17:51:10,522 (beam_search:429) INFO: max output length: 142
2024-10-27 17:51:10,522 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:11,476 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:11,476 (beam_search:476) INFO:  -1.96 * 1.0 =  -1.96 for ctc
2024-10-27 17:51:11,476 (beam_search:479) INFO: total log probability: -1.96
2024-10-27 17:51:11,476 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:51:11,476 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:11,476 (beam_search:483) INFO: best hypo: ▁WELL▁ONE▁IS▁FROM▁THE▁BATTERY▁TO▁THE▁LIGHT▁BULB▁AND▁THEN▁ONE▁YOU▁CAN▁I'M▁NOT▁SURE▁IF▁IT▁IS▁BUT▁UM▁FROM▁ONE▁LIGHT▁BULB▁TO▁ANOTHER▁THAT▁LOOKS▁LIKE▁ANOTHER▁ONE▁TO▁ME

2024-10-27 17:51:11,479 (asr_inference:509) INFO: speech length: 82160
2024-10-27 17:51:14,520 (beam_search:428) INFO: decoder input length: 63
2024-10-27 17:51:14,520 (beam_search:429) INFO: max output length: 63
2024-10-27 17:51:14,520 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:14,624 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:14,624 (beam_search:476) INFO:  -0.75 * 1.0 =  -0.75 for ctc
2024-10-27 17:51:14,624 (beam_search:479) INFO: total log probability: -0.75
2024-10-27 17:51:14,624 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:51:14,624 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:14,624 (beam_search:483) INFO: best hypo: ▁UM▁DO▁YOU▁MEAN▁WITH▁THE▁TWO▁LIGHT▁BULBS

2024-10-27 17:51:14,627 (asr_inference:509) INFO: speech length: 169120
2024-10-27 17:51:21,098 (beam_search:428) INFO: decoder input length: 131
2024-10-27 17:51:21,099 (beam_search:429) INFO: max output length: 131
2024-10-27 17:51:21,099 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:21,858 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:21,859 (beam_search:476) INFO:  -1.96 * 1.0 =  -1.96 for ctc
2024-10-27 17:51:21,859 (beam_search:479) INFO: total log probability: -1.96
2024-10-27 17:51:21,859 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:51:21,859 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:21,859 (beam_search:483) INFO: best hypo: ▁YOU▁COULD▁DO▁IT▁LIKE▁WITH▁A▁LIGHT▁BULB▁AND▁THEN▁YOU▁COULD▁GET▁A▁WIRE▁TO▁IT▁AND▁THEN▁THE▁WIRE▁TO▁THE▁LIGHT▁BULB▁WHICH▁A▁ONE▁WIRE▁TO▁THE▁BATTERY

2024-10-27 17:51:21,862 (asr_inference:509) INFO: speech length: 41888
2024-10-27 17:51:23,502 (beam_search:428) INFO: decoder input length: 32
2024-10-27 17:51:23,502 (beam_search:429) INFO: max output length: 32
2024-10-27 17:51:23,502 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:23,541 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:23,541 (beam_search:476) INFO:  -1.30 * 1.0 =  -1.30 for ctc
2024-10-27 17:51:23,542 (beam_search:479) INFO: total log probability: -1.30
2024-10-27 17:51:23,542 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:51:23,542 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:23,542 (beam_search:483) INFO: best hypo: ▁UM▁I▁THINK▁IT▁A▁LIKE▁THIS

2024-10-27 17:51:23,544 (asr_inference:509) INFO: speech length: 83392
2024-10-27 17:51:26,431 (beam_search:428) INFO: decoder input length: 64
2024-10-27 17:51:26,432 (beam_search:429) INFO: max output length: 64
2024-10-27 17:51:26,432 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:26,481 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:26,481 (beam_search:476) INFO:  -1.05 * 1.0 =  -1.05 for ctc
2024-10-27 17:51:26,481 (beam_search:479) INFO: total log probability: -1.05
2024-10-27 17:51:26,481 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:51:26,481 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:26,481 (beam_search:483) INFO: best hypo: ▁THIS▁IS▁A▁CIRCUIT

2024-10-27 17:51:26,484 (asr_inference:509) INFO: speech length: 293184
2024-10-27 17:51:39,363 (beam_search:428) INFO: decoder input length: 228
2024-10-27 17:51:39,364 (beam_search:429) INFO: max output length: 228
2024-10-27 17:51:39,364 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:41,634 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:41,634 (beam_search:476) INFO:  -5.72 * 1.0 =  -5.72 for ctc
2024-10-27 17:51:41,634 (beam_search:479) INFO: total log probability: -5.72
2024-10-27 17:51:41,634 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:51:41,634 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:41,634 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁BATTERY▁HAS▁A▁WIRE▁TO▁THEY▁LITTLE▁THAT'S▁TO▁THE▁LIGHT▁BULB▁WHICH▁THAT▁LIGHT▁UP▁BUT▁THEN▁YOU▁NEED▁THE▁WIRE▁TO▁THE▁THE▁OTHER▁SIDE▁OF▁THE▁BATTERY▁WHICH▁HAS▁ANOTHER▁WIRE▁THAT▁TO▁THE▁LIGHT▁BULB▁AND▁THEN▁THAT▁HAS▁ANOTHER▁WIRE▁THAT▁GOES▁TO▁THE▁BATTERY

2024-10-27 17:51:41,638 (asr_inference:509) INFO: speech length: 185200
2024-10-27 17:51:48,709 (beam_search:428) INFO: decoder input length: 144
2024-10-27 17:51:48,709 (beam_search:429) INFO: max output length: 144
2024-10-27 17:51:48,709 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:49,591 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:49,591 (beam_search:476) INFO:  -3.48 * 1.0 =  -3.48 for ctc
2024-10-27 17:51:49,591 (beam_search:479) INFO: total log probability: -3.48
2024-10-27 17:51:49,591 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:51:49,591 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:49,591 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁TWO▁CIRCUITS▁IS▁THAT▁YOU▁CAN▁LIKE▁TWO▁PATHWAYS▁LIKE▁YOU▁CAN▁IN▁THIS▁ONE▁BECAUSE▁YOU▁CAN▁ONE▁FROM▁THE▁BATTERY▁TO▁LIGHT▁BULB▁AND▁THE▁BATTERY▁TO▁ANOTHER▁LIGHT▁BULB

2024-10-27 17:51:49,594 (asr_inference:509) INFO: speech length: 82144
2024-10-27 17:51:52,490 (beam_search:428) INFO: decoder input length: 63
2024-10-27 17:51:52,491 (beam_search:429) INFO: max output length: 63
2024-10-27 17:51:52,491 (beam_search:430) INFO: min output length: 0
2024-10-27 17:51:52,629 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:51:52,629 (beam_search:476) INFO:  -2.32 * 1.0 =  -2.32 for ctc
2024-10-27 17:51:52,629 (beam_search:479) INFO: total log probability: -2.32
2024-10-27 17:51:52,629 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:51:52,629 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:51:52,629 (beam_search:483) INFO: best hypo: ▁THE▁OTHER▁LIGHT▁BULB▁WILL▁STILL▁KEEP▁WORKING▁BECAUSE▁IT'S▁PARALLEL

2024-10-27 17:51:52,631 (asr_inference:509) INFO: speech length: 326480
2024-10-27 17:52:06,992 (beam_search:428) INFO: decoder input length: 254
2024-10-27 17:52:06,992 (beam_search:429) INFO: max output length: 254
2024-10-27 17:52:06,992 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:08,900 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:08,900 (beam_search:476) INFO:  -9.41 * 1.0 =  -9.41 for ctc
2024-10-27 17:52:08,900 (beam_search:479) INFO: total log probability: -9.41
2024-10-27 17:52:08,900 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 17:52:08,900 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:08,901 (beam_search:483) INFO: best hypo: ▁BECAUSE▁WE▁DID▁THIS▁EXPERIMENT▁THEY▁IT▁TOLD▁US▁IN▁THE▁SCIENCE▁THAT▁UM▁PEOPLE▁WERE▁THESE▁BIG▁OFS▁AND▁THEY▁WERE▁EM▁AND▁ONE▁OUT▁THE▁THE▁OUT▁AND▁SO▁WE▁HAD▁TO▁TO▁TWO▁AND▁OUR

2024-10-27 17:52:08,903 (asr_inference:509) INFO: speech length: 70640
2024-10-27 17:52:11,630 (beam_search:428) INFO: decoder input length: 54
2024-10-27 17:52:11,630 (beam_search:429) INFO: max output length: 54
2024-10-27 17:52:11,630 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:11,714 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:11,715 (beam_search:476) INFO:  -2.26 * 1.0 =  -2.26 for ctc
2024-10-27 17:52:11,715 (beam_search:479) INFO: total log probability: -2.26
2024-10-27 17:52:11,715 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:52:11,715 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:11,715 (beam_search:483) INFO: best hypo: ▁ALSO▁A▁PARALLEL▁BECAUSE▁YOU▁CAN▁TWO▁PATHWAYS▁AGAIN

2024-10-27 17:52:11,718 (asr_inference:509) INFO: speech length: 243632
2024-10-27 17:52:21,892 (beam_search:428) INFO: decoder input length: 189
2024-10-27 17:52:21,892 (beam_search:429) INFO: max output length: 189
2024-10-27 17:52:21,892 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:23,022 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:23,023 (beam_search:476) INFO:  -3.19 * 1.0 =  -3.19 for ctc
2024-10-27 17:52:23,023 (beam_search:479) INFO: total log probability: -3.19
2024-10-27 17:52:23,023 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:52:23,023 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:23,023 (beam_search:483) INFO: best hypo: ▁UM▁WELL▁I▁THINK▁THIS▁WOULD▁BECAUSE▁IT▁HAS▁TWO▁TOUCHING▁BUT▁I▁THINK▁THE▁IS▁THAT▁IT'S▁NOT▁I▁THINK▁THE▁MINUS▁SIDE▁SHOULD▁BE▁WHERE▁THE▁PLUS▁IS

2024-10-27 17:52:23,025 (asr_inference:509) INFO: speech length: 90240
2024-10-27 17:52:26,304 (beam_search:428) INFO: decoder input length: 69
2024-10-27 17:52:26,304 (beam_search:429) INFO: max output length: 69
2024-10-27 17:52:26,304 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:26,398 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:26,398 (beam_search:476) INFO:  -1.00 * 1.0 =  -1.00 for ctc
2024-10-27 17:52:26,399 (beam_search:479) INFO: total log probability: -1.00
2024-10-27 17:52:26,399 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:52:26,399 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:26,399 (beam_search:483) INFO: best hypo: ▁UM▁THERE'S▁A▁THAT▁SAYS

2024-10-27 17:52:26,401 (asr_inference:509) INFO: speech length: 86784
2024-10-27 17:52:29,532 (beam_search:428) INFO: decoder input length: 67
2024-10-27 17:52:29,533 (beam_search:429) INFO: max output length: 67
2024-10-27 17:52:29,533 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:29,612 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:29,613 (beam_search:476) INFO:  -1.02 * 1.0 =  -1.02 for ctc
2024-10-27 17:52:29,613 (beam_search:479) INFO: total log probability: -1.02
2024-10-27 17:52:29,613 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:52:29,613 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:29,613 (beam_search:483) INFO: best hypo: ▁WELL▁I▁THINK▁THEY▁THEY▁UP▁MORE

2024-10-27 17:52:29,615 (asr_inference:509) INFO: speech length: 67456
2024-10-27 17:52:32,098 (beam_search:428) INFO: decoder input length: 52
2024-10-27 17:52:32,098 (beam_search:429) INFO: max output length: 52
2024-10-27 17:52:32,098 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:32,164 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:32,164 (beam_search:476) INFO:  -1.59 * 1.0 =  -1.59 for ctc
2024-10-27 17:52:32,164 (beam_search:479) INFO: total log probability: -1.59
2024-10-27 17:52:32,164 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:52:32,164 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:32,164 (beam_search:483) INFO: best hypo: ▁I▁THE▁I▁NOTICED▁THAT▁THEY▁MORE

2024-10-27 17:52:32,167 (asr_inference:509) INFO: speech length: 134960
2024-10-27 17:52:37,490 (beam_search:428) INFO: decoder input length: 104
2024-10-27 17:52:37,490 (beam_search:429) INFO: max output length: 104
2024-10-27 17:52:37,490 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:37,795 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:37,795 (beam_search:476) INFO:  -2.69 * 1.0 =  -2.69 for ctc
2024-10-27 17:52:37,795 (beam_search:479) INFO: total log probability: -2.69
2024-10-27 17:52:37,795 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:52:37,795 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:37,795 (beam_search:483) INFO: best hypo: ▁THAT▁WHEN▁YOU▁THE▁THEY▁YOU▁CAN▁SEE▁THE▁THAT▁IT▁WAS▁THEN▁IT▁LIGHTS▁UP

2024-10-27 17:52:37,797 (asr_inference:509) INFO: speech length: 255744
2024-10-27 17:52:48,241 (beam_search:428) INFO: decoder input length: 199
2024-10-27 17:52:48,241 (beam_search:429) INFO: max output length: 199
2024-10-27 17:52:48,241 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:49,703 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:49,703 (beam_search:476) INFO:  -5.00 * 1.0 =  -5.00 for ctc
2024-10-27 17:52:49,703 (beam_search:479) INFO: total log probability: -5.00
2024-10-27 17:52:49,703 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:52:49,703 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:49,703 (beam_search:483) INFO: best hypo: ▁WELL▁LIKE▁I▁WE▁DID▁THIS▁ONE▁IN▁CLASS▁AND▁MANY▁TIMES▁AND▁IT▁DOESN'T▁VERY▁WELL▁SO▁I▁WOULD▁ANOTHER▁BATTERY▁AND'D▁MAKE▁IT▁UM▁TO▁I▁MEAN▁IT▁IT▁WOULD▁MAKE▁EM▁BRIGHTER

2024-10-27 17:52:49,705 (asr_inference:509) INFO: speech length: 33696
2024-10-27 17:52:51,041 (beam_search:428) INFO: decoder input length: 25
2024-10-27 17:52:51,041 (beam_search:429) INFO: max output length: 25
2024-10-27 17:52:51,041 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:51,062 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:51,062 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 17:52:51,062 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 17:52:51,062 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:52:51,062 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:51,062 (beam_search:483) INFO: best hypo: ▁YOU▁CAN'T

2024-10-27 17:52:51,065 (asr_inference:509) INFO: speech length: 88400
2024-10-27 17:52:54,314 (beam_search:428) INFO: decoder input length: 68
2024-10-27 17:52:54,314 (beam_search:429) INFO: max output length: 68
2024-10-27 17:52:54,314 (beam_search:430) INFO: min output length: 0
2024-10-27 17:52:54,452 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:52:54,452 (beam_search:476) INFO:  -2.02 * 1.0 =  -2.02 for ctc
2024-10-27 17:52:54,452 (beam_search:479) INFO: total log probability: -2.02
2024-10-27 17:52:54,452 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:52:54,452 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:52:54,452 (beam_search:483) INFO: best hypo: ▁UM▁WHAT▁DO▁YOU▁MEAN▁BY▁IF▁WE▁MADE▁IT▁MAKE▁IT▁MORE

2024-10-27 17:52:54,454 (asr_inference:509) INFO: speech length: 141696
2024-10-27 17:52:59,807 (beam_search:428) INFO: decoder input length: 110
2024-10-27 17:52:59,807 (beam_search:429) INFO: max output length: 110
2024-10-27 17:52:59,807 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:00,250 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:00,251 (beam_search:476) INFO:  -2.21 * 1.0 =  -2.21 for ctc
2024-10-27 17:53:00,251 (beam_search:479) INFO: total log probability: -2.21
2024-10-27 17:53:00,251 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:53:00,251 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:00,251 (beam_search:483) INFO: best hypo: ▁UM▁I▁DON'T▁KNOW▁I▁THINK▁IT▁PROBABLY▁BUT▁THE▁GOOD▁ABOUT▁CIRCUITS▁IS▁THAT▁WHEN▁ONE▁OUT▁THE▁DON'T

2024-10-27 17:53:00,253 (asr_inference:509) INFO: speech length: 141552
2024-10-27 17:53:05,721 (beam_search:428) INFO: decoder input length: 110
2024-10-27 17:53:05,721 (beam_search:429) INFO: max output length: 110
2024-10-27 17:53:05,721 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:06,152 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:06,152 (beam_search:476) INFO:  -2.98 * 1.0 =  -2.98 for ctc
2024-10-27 17:53:06,152 (beam_search:479) INFO: total log probability: -2.98
2024-10-27 17:53:06,152 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:53:06,152 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:06,152 (beam_search:483) INFO: best hypo: ▁WELL▁I▁CANT▁SEE▁TWO▁BUT▁THEY'RE▁SEPARATE▁AND▁ONE▁I▁THINK▁IS▁A▁PARALLEL▁CIRCUIT▁AND▁ONE▁IS▁A

2024-10-27 17:53:06,154 (asr_inference:509) INFO: speech length: 214336
2024-10-27 17:53:14,508 (beam_search:428) INFO: decoder input length: 166
2024-10-27 17:53:14,508 (beam_search:429) INFO: max output length: 166
2024-10-27 17:53:14,508 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:15,058 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:15,058 (beam_search:476) INFO:  -4.04 * 1.0 =  -4.04 for ctc
2024-10-27 17:53:15,058 (beam_search:479) INFO: total log probability: -4.04
2024-10-27 17:53:15,058 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 17:53:15,058 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:15,058 (beam_search:483) INFO: best hypo: ▁WELL▁A▁CIRCUIT▁UM▁THE▁CIRCUIT▁FROM▁SO▁YOU▁WOULD▁BE▁USUALLY▁BE▁TO▁WITH▁YOUR▁A▁OR▁NOT

2024-10-27 17:53:15,061 (asr_inference:509) INFO: speech length: 374960
2024-10-27 17:53:32,144 (beam_search:428) INFO: decoder input length: 292
2024-10-27 17:53:32,144 (beam_search:429) INFO: max output length: 292
2024-10-27 17:53:32,144 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:35,730 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:35,730 (beam_search:476) INFO:  -5.94 * 1.0 =  -5.94 for ctc
2024-10-27 17:53:35,730 (beam_search:479) INFO: total log probability: -5.94
2024-10-27 17:53:35,730 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:53:35,730 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:35,730 (beam_search:483) INFO: best hypo: ▁I▁SEE▁IS▁TWO▁AND▁ONE▁THE▁ONE▁ON▁THE▁LEFT▁HAS▁A▁BATTERY▁THEN▁TWO▁LIGHT▁BULBS▁AND▁THE▁ONE▁ON▁THE▁RIGHT▁HAS▁A▁BATTERY▁AND▁THEN▁HAS▁A▁LIGHT▁BULB▁AND▁THEN▁IT▁HAS▁ANOTHER▁LIGHT▁BULB▁BUT▁IT'S▁SORT▁OF▁MORE▁LIKE▁UM▁IT▁I▁THINK▁THAT'S▁PARALLEL▁THE▁ONE▁ON▁THE▁IS▁A▁CIRCUIT

2024-10-27 17:53:35,733 (asr_inference:509) INFO: speech length: 51328
2024-10-27 17:53:37,683 (beam_search:428) INFO: decoder input length: 39
2024-10-27 17:53:37,684 (beam_search:429) INFO: max output length: 39
2024-10-27 17:53:37,684 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:37,713 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:37,713 (beam_search:476) INFO:  -0.80 * 1.0 =  -0.80 for ctc
2024-10-27 17:53:37,713 (beam_search:479) INFO: total log probability: -0.80
2024-10-27 17:53:37,713 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:53:37,713 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:37,713 (beam_search:483) INFO: best hypo: ▁ALL▁THE▁AND

2024-10-27 17:53:37,716 (asr_inference:509) INFO: speech length: 14464
2024-10-27 17:53:38,501 (beam_search:428) INFO: decoder input length: 10
2024-10-27 17:53:38,501 (beam_search:429) INFO: max output length: 10
2024-10-27 17:53:38,501 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:38,510 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:38,510 (beam_search:476) INFO:  -0.07 * 1.0 =  -0.07 for ctc
2024-10-27 17:53:38,510 (beam_search:479) INFO: total log probability: -0.07
2024-10-27 17:53:38,510 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 17:53:38,510 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:38,510 (beam_search:483) INFO: best hypo: ▁IT'S

2024-10-27 17:53:38,513 (asr_inference:509) INFO: speech length: 72416
2024-10-27 17:53:41,115 (beam_search:428) INFO: decoder input length: 56
2024-10-27 17:53:41,115 (beam_search:429) INFO: max output length: 56
2024-10-27 17:53:41,115 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:41,250 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:41,250 (beam_search:476) INFO:  -1.17 * 1.0 =  -1.17 for ctc
2024-10-27 17:53:41,250 (beam_search:479) INFO: total log probability: -1.17
2024-10-27 17:53:41,250 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:53:41,250 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:41,250 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THE▁OTHER▁ONE▁WOULD▁OUT▁AS▁CAUSE▁IT'S▁NOT▁A▁PARALLEL▁CIRCUIT

2024-10-27 17:53:41,253 (asr_inference:509) INFO: speech length: 312096
2024-10-27 17:53:54,949 (beam_search:428) INFO: decoder input length: 243
2024-10-27 17:53:54,950 (beam_search:429) INFO: max output length: 243
2024-10-27 17:53:54,950 (beam_search:430) INFO: min output length: 0
2024-10-27 17:53:57,237 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:53:57,238 (beam_search:476) INFO:  -5.52 * 1.0 =  -5.52 for ctc
2024-10-27 17:53:57,238 (beam_search:479) INFO: total log probability: -5.52
2024-10-27 17:53:57,238 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:53:57,238 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:53:57,238 (beam_search:483) INFO: best hypo: ▁UM▁WELL▁YEAH▁WHEN▁WE▁WERE▁THE▁EXPERIMENT▁OF▁THE▁PEOPLE▁W▁THE▁OF▁LIGHTS▁WE▁HAD▁IT▁AND▁THEN▁WE▁PUT▁LIKE▁A▁LITTLE▁PIECE▁OF▁PAPER▁IT▁TO▁SEE▁IF▁IT▁WOULD▁ELECTRICITY▁AND▁IT▁DID▁AND▁AS▁AS▁THAT▁THE▁OTHER▁ONE▁UM▁OUT▁AS▁WELL

2024-10-27 17:53:57,241 (asr_inference:509) INFO: speech length: 134960
2024-10-27 17:54:02,274 (beam_search:428) INFO: decoder input length: 104
2024-10-27 17:54:02,275 (beam_search:429) INFO: max output length: 104
2024-10-27 17:54:02,275 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:02,586 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:02,586 (beam_search:476) INFO:  -3.37 * 1.0 =  -3.37 for ctc
2024-10-27 17:54:02,586 (beam_search:479) INFO: total log probability: -3.37
2024-10-27 17:54:02,586 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:54:02,586 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:02,586 (beam_search:483) INFO: best hypo: ▁WELL▁SINCE▁UM▁ELECTRICITY▁CAN▁NOT▁GO▁THROUGH▁PAPER▁IT▁WOULD▁THE▁LIGHT▁BULBS▁TO▁OUT

2024-10-27 17:54:02,588 (asr_inference:509) INFO: speech length: 30320
2024-10-27 17:54:03,903 (beam_search:428) INFO: decoder input length: 23
2024-10-27 17:54:03,903 (beam_search:429) INFO: max output length: 23
2024-10-27 17:54:03,903 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:03,925 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:03,925 (beam_search:476) INFO:  -0.86 * 1.0 =  -0.86 for ctc
2024-10-27 17:54:03,925 (beam_search:479) INFO: total log probability: -0.86
2024-10-27 17:54:03,925 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:54:03,925 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:03,925 (beam_search:483) INFO: best hypo: ▁IT▁CAN'T▁GO

2024-10-27 17:54:03,927 (asr_inference:509) INFO: speech length: 193200
2024-10-27 17:54:11,230 (beam_search:428) INFO: decoder input length: 150
2024-10-27 17:54:11,230 (beam_search:429) INFO: max output length: 150
2024-10-27 17:54:11,230 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:12,172 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:12,173 (beam_search:476) INFO:  -3.37 * 1.0 =  -3.37 for ctc
2024-10-27 17:54:12,173 (beam_search:479) INFO: total log probability: -3.37
2024-10-27 17:54:12,173 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:54:12,173 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:12,173 (beam_search:483) INFO: best hypo: ▁I▁SEE▁A▁BATTERY▁AND▁A▁AND▁I▁DON'T▁KNOW▁WHAT▁THE▁IS▁I▁THINK▁IT▁BE▁A▁LIGHT▁BULB▁OR▁SOMETHING▁BUT▁I▁IT▁LOOKS▁LIKE▁TO▁ME▁A▁PARALLEL▁CIRCUIT

2024-10-27 17:54:12,176 (asr_inference:509) INFO: speech length: 83504
2024-10-27 17:54:15,242 (beam_search:428) INFO: decoder input length: 64
2024-10-27 17:54:15,242 (beam_search:429) INFO: max output length: 64
2024-10-27 17:54:15,242 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:15,350 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:15,350 (beam_search:476) INFO:  -1.09 * 1.0 =  -1.09 for ctc
2024-10-27 17:54:15,350 (beam_search:479) INFO: total log probability: -1.09
2024-10-27 17:54:15,350 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:54:15,350 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:15,350 (beam_search:483) INFO: best hypo: ▁BECAUSE▁YOU▁CAN▁THE▁PATHWAY▁FROM▁ONE▁TO▁THE▁OTHER

2024-10-27 17:54:15,352 (asr_inference:509) INFO: speech length: 46560
2024-10-27 17:54:17,136 (beam_search:428) INFO: decoder input length: 35
2024-10-27 17:54:17,136 (beam_search:429) INFO: max output length: 35
2024-10-27 17:54:17,136 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:17,178 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:17,178 (beam_search:476) INFO:  -0.54 * 1.0 =  -0.54 for ctc
2024-10-27 17:54:17,178 (beam_search:479) INFO: total log probability: -0.54
2024-10-27 17:54:17,178 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 17:54:17,178 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:17,178 (beam_search:483) INFO: best hypo: ▁I▁THINK▁IT'S▁A▁CIRCUIT

2024-10-27 17:54:17,181 (asr_inference:509) INFO: speech length: 9488
2024-10-27 17:54:17,779 (beam_search:428) INFO: decoder input length: 6
2024-10-27 17:54:17,780 (beam_search:429) INFO: max output length: 6
2024-10-27 17:54:17,780 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:17,783 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:17,783 (beam_search:476) INFO:  -0.14 * 1.0 =  -0.14 for ctc
2024-10-27 17:54:17,783 (beam_search:479) INFO: total log probability: -0.14
2024-10-27 17:54:17,783 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:54:17,783 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:17,783 (beam_search:483) INFO: best hypo: 

2024-10-27 17:54:17,785 (asr_inference:509) INFO: speech length: 28816
2024-10-27 17:54:18,958 (beam_search:428) INFO: decoder input length: 22
2024-10-27 17:54:18,958 (beam_search:429) INFO: max output length: 22
2024-10-27 17:54:18,958 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:18,964 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:18,964 (beam_search:476) INFO:  -0.00 * 1.0 =  -0.00 for ctc
2024-10-27 17:54:18,964 (beam_search:479) INFO: total log probability: -0.00
2024-10-27 17:54:18,964 (beam_search:480) INFO: normalized log probability: -0.00
2024-10-27 17:54:18,964 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:18,964 (beam_search:483) INFO: best hypo: 

2024-10-27 17:54:18,966 (asr_inference:509) INFO: speech length: 53040
2024-10-27 17:54:20,872 (beam_search:428) INFO: decoder input length: 40
2024-10-27 17:54:20,873 (beam_search:429) INFO: max output length: 40
2024-10-27 17:54:20,873 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:20,923 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:20,923 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 17:54:20,923 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 17:54:20,923 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:54:20,923 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:20,924 (beam_search:483) INFO: best hypo: ▁WE'VE▁BEEN▁DOING▁THINGS▁WITH▁MAGNETS

2024-10-27 17:54:20,926 (asr_inference:509) INFO: speech length: 121472
2024-10-27 17:54:25,561 (beam_search:428) INFO: decoder input length: 94
2024-10-27 17:54:25,561 (beam_search:429) INFO: max output length: 94
2024-10-27 17:54:25,563 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:25,831 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:25,831 (beam_search:476) INFO:  -2.73 * 1.0 =  -2.73 for ctc
2024-10-27 17:54:25,831 (beam_search:479) INFO: total log probability: -2.73
2024-10-27 17:54:25,831 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:54:25,831 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:25,831 (beam_search:483) INFO: best hypo: ▁WE▁PUT▁MAGNETS▁IN▁A▁BOX▁AND▁IT▁THEN▁USED▁DIFFERENT▁KINDS▁OF▁TO▁IT

2024-10-27 17:54:25,834 (asr_inference:509) INFO: speech length: 50016
2024-10-27 17:54:27,899 (beam_search:428) INFO: decoder input length: 38
2024-10-27 17:54:27,900 (beam_search:429) INFO: max output length: 38
2024-10-27 17:54:27,900 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:27,914 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:27,914 (beam_search:476) INFO:  -0.28 * 1.0 =  -0.28 for ctc
2024-10-27 17:54:27,914 (beam_search:479) INFO: total log probability: -0.28
2024-10-27 17:54:27,914 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:54:27,914 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:27,914 (beam_search:483) INFO: best hypo: ▁IS

2024-10-27 17:54:27,916 (asr_inference:509) INFO: speech length: 111936
2024-10-27 17:54:32,017 (beam_search:428) INFO: decoder input length: 86
2024-10-27 17:54:32,017 (beam_search:429) INFO: max output length: 86
2024-10-27 17:54:32,017 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:32,065 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:32,065 (beam_search:476) INFO:  -0.38 * 1.0 =  -0.38 for ctc
2024-10-27 17:54:32,065 (beam_search:479) INFO: total log probability: -0.38
2024-10-27 17:54:32,065 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:54:32,065 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:32,065 (beam_search:483) INFO: best hypo: ▁AND▁UM

2024-10-27 17:54:32,067 (asr_inference:509) INFO: speech length: 235888
2024-10-27 17:54:41,516 (beam_search:428) INFO: decoder input length: 183
2024-10-27 17:54:41,516 (beam_search:429) INFO: max output length: 183
2024-10-27 17:54:41,516 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:42,098 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:42,099 (beam_search:476) INFO:  -2.36 * 1.0 =  -2.36 for ctc
2024-10-27 17:54:42,099 (beam_search:479) INFO: total log probability: -2.36
2024-10-27 17:54:42,099 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:54:42,099 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:42,099 (beam_search:483) INFO: best hypo: ▁UM▁I▁UM▁MAGNETS▁ARE▁IMPORTANT▁FOR▁LIKE▁UM▁UM▁FOR▁LIKE▁THINGS▁THAT▁USE▁I▁THINK

2024-10-27 17:54:42,102 (asr_inference:509) INFO: speech length: 147504
2024-10-27 17:54:47,863 (beam_search:428) INFO: decoder input length: 114
2024-10-27 17:54:47,863 (beam_search:429) INFO: max output length: 114
2024-10-27 17:54:47,863 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:48,210 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:48,210 (beam_search:476) INFO:  -3.34 * 1.0 =  -3.34 for ctc
2024-10-27 17:54:48,210 (beam_search:479) INFO: total log probability: -3.34
2024-10-27 17:54:48,210 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 17:54:48,210 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:48,210 (beam_search:483) INFO: best hypo: ▁WE▁WERE▁TRYING▁TO▁FIND▁THEM▁CAUSE▁OTHER▁GROUPS▁EM▁THE▁AND▁WE▁TRIED▁TO▁FIND▁IT

2024-10-27 17:54:48,213 (asr_inference:509) INFO: speech length: 236240
2024-10-27 17:54:57,997 (beam_search:428) INFO: decoder input length: 184
2024-10-27 17:54:57,998 (beam_search:429) INFO: max output length: 184
2024-10-27 17:54:57,998 (beam_search:430) INFO: min output length: 0
2024-10-27 17:54:58,988 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:54:58,989 (beam_search:476) INFO:  -3.27 * 1.0 =  -3.27 for ctc
2024-10-27 17:54:58,989 (beam_search:479) INFO: total log probability: -3.27
2024-10-27 17:54:58,989 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:54:58,989 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:54:58,989 (beam_search:483) INFO: best hypo: ▁WE▁UM▁LIKE▁WE▁DIDN'T▁KNOW▁WHERE▁THEY▁WERE▁SO▁WE▁USED▁UM▁MAGNETS▁LIKE▁I▁MEAN▁METAL▁AND▁A▁TO▁IT▁CAUSE▁THEY▁IT▁IN▁THE▁BOX

2024-10-27 17:54:58,992 (asr_inference:509) INFO: speech length: 129632
2024-10-27 17:55:03,745 (beam_search:428) INFO: decoder input length: 100
2024-10-27 17:55:03,745 (beam_search:429) INFO: max output length: 100
2024-10-27 17:55:03,745 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:04,079 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:04,079 (beam_search:476) INFO:  -1.63 * 1.0 =  -1.63 for ctc
2024-10-27 17:55:04,079 (beam_search:479) INFO: total log probability: -1.63
2024-10-27 17:55:04,079 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:55:04,079 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:04,079 (beam_search:483) INFO: best hypo: ▁WHEN▁WE▁USED▁IT▁THE▁KIND▁OFED▁OUT▁WHERE▁IT▁WAS▁IN▁THE▁BOX▁SO▁THEN▁WE▁THAT

2024-10-27 17:55:04,082 (asr_inference:509) INFO: speech length: 80400
2024-10-27 17:55:07,128 (beam_search:428) INFO: decoder input length: 62
2024-10-27 17:55:07,128 (beam_search:429) INFO: max output length: 62
2024-10-27 17:55:07,128 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:07,218 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:07,218 (beam_search:476) INFO:  -1.03 * 1.0 =  -1.03 for ctc
2024-10-27 17:55:07,218 (beam_search:479) INFO: total log probability: -1.03
2024-10-27 17:55:07,218 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:55:07,218 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:07,218 (beam_search:483) INFO: best hypo: ▁THE▁WHERE▁THE▁METAL▁IS▁AND▁LIKE▁YEAH

2024-10-27 17:55:07,220 (asr_inference:509) INFO: speech length: 159952
2024-10-27 17:55:13,413 (beam_search:428) INFO: decoder input length: 124
2024-10-27 17:55:13,413 (beam_search:429) INFO: max output length: 124
2024-10-27 17:55:13,413 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:13,942 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:13,942 (beam_search:476) INFO:  -2.89 * 1.0 =  -2.89 for ctc
2024-10-27 17:55:13,942 (beam_search:479) INFO: total log probability: -2.89
2024-10-27 17:55:13,943 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:55:13,943 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:13,943 (beam_search:483) INFO: best hypo: ▁THERE'S▁METAL▁OR▁A▁THE▁SO▁THEN▁IF▁THERE'S▁A▁METAL▁OR▁UM▁LIKE▁A▁ROCK▁SOMEWHERE▁UM▁IT▁IT▁OUT

2024-10-27 17:55:13,945 (asr_inference:509) INFO: speech length: 80560
2024-10-27 17:55:16,864 (beam_search:428) INFO: decoder input length: 62
2024-10-27 17:55:16,864 (beam_search:429) INFO: max output length: 62
2024-10-27 17:55:16,864 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:16,962 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:16,962 (beam_search:476) INFO:  -0.73 * 1.0 =  -0.73 for ctc
2024-10-27 17:55:16,962 (beam_search:479) INFO: total log probability: -0.73
2024-10-27 17:55:16,962 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:55:16,962 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:16,962 (beam_search:483) INFO: best hypo: ▁TO▁SO▁THEN▁IT▁CAN▁THE▁OTHER▁A▁MAGNET

2024-10-27 17:55:16,964 (asr_inference:509) INFO: speech length: 136512
2024-10-27 17:55:22,065 (beam_search:428) INFO: decoder input length: 106
2024-10-27 17:55:22,065 (beam_search:429) INFO: max output length: 106
2024-10-27 17:55:22,065 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:22,229 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:22,229 (beam_search:476) INFO:  -1.79 * 1.0 =  -1.79 for ctc
2024-10-27 17:55:22,229 (beam_search:479) INFO: total log probability: -1.79
2024-10-27 17:55:22,229 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 17:55:22,229 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:22,229 (beam_search:483) INFO: best hypo: ▁UM▁WELL▁ITS▁THE▁UM▁ITS▁UM

2024-10-27 17:55:22,232 (asr_inference:509) INFO: speech length: 68240
2024-10-27 17:55:24,670 (beam_search:428) INFO: decoder input length: 52
2024-10-27 17:55:24,670 (beam_search:429) INFO: max output length: 52
2024-10-27 17:55:24,670 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:24,764 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:24,765 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 17:55:24,765 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 17:55:24,765 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:55:24,765 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:24,765 (beam_search:483) INFO: best hypo: ▁ITN'T▁METAL▁IT▁JUST▁OUT▁WHERE▁IT▁IS

2024-10-27 17:55:24,767 (asr_inference:509) INFO: speech length: 234528
2024-10-27 17:55:34,195 (beam_search:428) INFO: decoder input length: 182
2024-10-27 17:55:34,195 (beam_search:429) INFO: max output length: 182
2024-10-27 17:55:34,195 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:35,363 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:35,363 (beam_search:476) INFO:  -5.25 * 1.0 =  -5.25 for ctc
2024-10-27 17:55:35,363 (beam_search:479) INFO: total log probability: -5.25
2024-10-27 17:55:35,363 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:55:35,363 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:35,364 (beam_search:483) INFO: best hypo: ▁WHEN▁THERE'S▁THIS▁THERE'S▁AND▁AND▁THEN▁THERE'S▁UM▁A▁LITTLE▁SO▁THAT▁YOU▁PUT▁IT▁IN▁THE▁AND▁THEN▁IT▁WILL▁S▁SPIN▁AROUND▁AND▁POINT▁OUT▁THE

2024-10-27 17:55:35,366 (asr_inference:509) INFO: speech length: 79968
2024-10-27 17:55:38,358 (beam_search:428) INFO: decoder input length: 61
2024-10-27 17:55:38,358 (beam_search:429) INFO: max output length: 61
2024-10-27 17:55:38,358 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:38,440 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:38,440 (beam_search:476) INFO:  -1.26 * 1.0 =  -1.26 for ctc
2024-10-27 17:55:38,440 (beam_search:479) INFO: total log probability: -1.26
2024-10-27 17:55:38,440 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:55:38,440 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:38,440 (beam_search:483) INFO: best hypo: ▁YEAH▁WITH▁OTHER▁UM▁IN▁MY▁GROUP

2024-10-27 17:55:38,443 (asr_inference:509) INFO: speech length: 130848
2024-10-27 17:55:43,324 (beam_search:428) INFO: decoder input length: 101
2024-10-27 17:55:43,324 (beam_search:429) INFO: max output length: 101
2024-10-27 17:55:43,324 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:43,613 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:43,613 (beam_search:476) INFO:  -1.92 * 1.0 =  -1.92 for ctc
2024-10-27 17:55:43,613 (beam_search:479) INFO: total log probability: -1.92
2024-10-27 17:55:43,613 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:55:43,613 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:43,613 (beam_search:483) INFO: best hypo: ▁USING▁LIKE▁A▁NAIL▁OR▁ANOTHER▁MAGNET▁AND▁THEN▁LIKE▁IT▁ON▁THE▁BOX▁AND▁IT▁IT

2024-10-27 17:55:43,615 (asr_inference:509) INFO: speech length: 171808
2024-10-27 17:55:50,252 (beam_search:428) INFO: decoder input length: 133
2024-10-27 17:55:50,252 (beam_search:429) INFO: max output length: 133
2024-10-27 17:55:50,252 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:50,523 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:50,524 (beam_search:476) INFO:  -1.37 * 1.0 =  -1.37 for ctc
2024-10-27 17:55:50,524 (beam_search:479) INFO: total log probability: -1.37
2024-10-27 17:55:50,524 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:55:50,524 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:50,524 (beam_search:483) INFO: best hypo: ▁WELL▁IT▁ONLY▁TO▁LIKE▁A▁KIND▁OF▁AND▁UM▁YEAH

2024-10-27 17:55:50,526 (asr_inference:509) INFO: speech length: 89200
2024-10-27 17:55:53,716 (beam_search:428) INFO: decoder input length: 69
2024-10-27 17:55:53,716 (beam_search:429) INFO: max output length: 69
2024-10-27 17:55:53,716 (beam_search:430) INFO: min output length: 0
2024-10-27 17:55:53,844 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:55:53,844 (beam_search:476) INFO:  -2.67 * 1.0 =  -2.67 for ctc
2024-10-27 17:55:53,844 (beam_search:479) INFO: total log probability: -2.67
2024-10-27 17:55:53,844 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 17:55:53,844 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:55:53,844 (beam_search:483) INFO: best hypo: ▁THERE▁ARE▁DIFFERENT▁OF▁AND▁THEY▁ONLY▁UM▁STICK▁TO▁CERTAIN

2024-10-27 17:55:53,847 (asr_inference:509) INFO: speech length: 200544
2024-10-27 17:56:01,726 (beam_search:428) INFO: decoder input length: 156
2024-10-27 17:56:01,726 (beam_search:429) INFO: max output length: 156
2024-10-27 17:56:01,726 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:02,105 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:02,105 (beam_search:476) INFO:  -2.48 * 1.0 =  -2.48 for ctc
2024-10-27 17:56:02,105 (beam_search:479) INFO: total log probability: -2.48
2024-10-27 17:56:02,105 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:56:02,105 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:02,105 (beam_search:483) INFO: best hypo: ▁AND▁NOT▁STEEL▁AND▁UM▁UM'M▁NOT▁AND▁LIKE▁LIKE▁STUFF

2024-10-27 17:56:02,108 (asr_inference:509) INFO: speech length: 25872
2024-10-27 17:56:03,241 (beam_search:428) INFO: decoder input length: 19
2024-10-27 17:56:03,241 (beam_search:429) INFO: max output length: 19
2024-10-27 17:56:03,241 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:03,253 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:03,253 (beam_search:476) INFO:  -0.16 * 1.0 =  -0.16 for ctc
2024-10-27 17:56:03,253 (beam_search:479) INFO: total log probability: -0.16
2024-10-27 17:56:03,253 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:56:03,253 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:03,253 (beam_search:483) INFO: best hypo: ▁WE▁DID

2024-10-27 17:56:03,256 (asr_inference:509) INFO: speech length: 242656
2024-10-27 17:56:13,213 (beam_search:428) INFO: decoder input length: 189
2024-10-27 17:56:13,213 (beam_search:429) INFO: max output length: 189
2024-10-27 17:56:13,213 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:14,146 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:14,146 (beam_search:476) INFO:  -2.95 * 1.0 =  -2.95 for ctc
2024-10-27 17:56:14,146 (beam_search:479) INFO: total log probability: -2.95
2024-10-27 17:56:14,146 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:56:14,146 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:14,146 (beam_search:483) INFO: best hypo: ▁WELL▁WE▁HAD▁A▁UM▁AND▁THEN▁OUR▁PUT▁SOME▁OF▁IT▁ON▁AND▁WE▁HAD▁A▁MAGNET▁AND▁PUT▁IT▁AND▁SEE▁UM▁HOW▁IT▁AND

2024-10-27 17:56:14,148 (asr_inference:509) INFO: speech length: 102240
2024-10-27 17:56:17,859 (beam_search:428) INFO: decoder input length: 79
2024-10-27 17:56:17,859 (beam_search:429) INFO: max output length: 79
2024-10-27 17:56:17,859 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:18,027 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:18,027 (beam_search:476) INFO:  -1.94 * 1.0 =  -1.94 for ctc
2024-10-27 17:56:18,027 (beam_search:479) INFO: total log probability: -1.94
2024-10-27 17:56:18,027 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:56:18,027 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:18,027 (beam_search:483) INFO: best hypo: ▁IT▁KIND▁OF▁LIKE▁IT▁UP▁AND▁IT▁THE▁MAGNET▁IT▁WAS

2024-10-27 17:56:18,030 (asr_inference:509) INFO: speech length: 257040
2024-10-27 17:56:28,913 (beam_search:428) INFO: decoder input length: 200
2024-10-27 17:56:28,913 (beam_search:429) INFO: max output length: 200
2024-10-27 17:56:28,913 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:30,287 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:30,288 (beam_search:476) INFO:  -6.46 * 1.0 =  -6.46 for ctc
2024-10-27 17:56:30,288 (beam_search:479) INFO: total log probability: -6.46
2024-10-27 17:56:30,288 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 17:56:30,288 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:30,288 (beam_search:483) INFO: best hypo: ▁THE▁IRON▁STUCK▁TO▁THE▁MAGNET▁CAUSE▁UM▁THE▁MAGNET▁HAS▁THIS▁IN▁IT▁THAT▁UM▁LIKE▁MAKES▁IT▁GO▁UP▁SO▁ITS▁IT▁BUT▁IT▁CAN'T▁TO▁IT▁CAUSE'S▁THE

2024-10-27 17:56:30,290 (asr_inference:509) INFO: speech length: 123664
2024-10-27 17:56:34,856 (beam_search:428) INFO: decoder input length: 96
2024-10-27 17:56:34,856 (beam_search:429) INFO: max output length: 96
2024-10-27 17:56:34,856 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:35,062 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:35,062 (beam_search:476) INFO:  -1.47 * 1.0 =  -1.47 for ctc
2024-10-27 17:56:35,062 (beam_search:479) INFO: total log probability: -1.47
2024-10-27 17:56:35,062 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:56:35,062 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:35,062 (beam_search:483) INFO: best hypo: ▁NO▁IT▁DID▁NOT▁IT▁JUST▁UM▁OF▁AROUND▁THE▁MAGNET▁WAS▁THE

2024-10-27 17:56:35,065 (asr_inference:509) INFO: speech length: 41152
2024-10-27 17:56:36,655 (beam_search:428) INFO: decoder input length: 31
2024-10-27 17:56:36,655 (beam_search:429) INFO: max output length: 31
2024-10-27 17:56:36,655 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:36,692 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:36,692 (beam_search:476) INFO:  -0.46 * 1.0 =  -0.46 for ctc
2024-10-27 17:56:36,692 (beam_search:479) INFO: total log probability: -0.46
2024-10-27 17:56:36,692 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 17:56:36,692 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:36,692 (beam_search:483) INFO: best hypo: ▁UM▁I'M▁NOT▁REALLY▁SURE

2024-10-27 17:56:36,694 (asr_inference:509) INFO: speech length: 288880
2024-10-27 17:56:48,836 (beam_search:428) INFO: decoder input length: 225
2024-10-27 17:56:48,836 (beam_search:429) INFO: max output length: 225
2024-10-27 17:56:48,836 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:50,478 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:50,478 (beam_search:476) INFO:  -3.84 * 1.0 =  -3.84 for ctc
2024-10-27 17:56:50,478 (beam_search:479) INFO: total log probability: -3.84
2024-10-27 17:56:50,478 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:56:50,479 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:50,479 (beam_search:483) INFO: best hypo: ▁THE▁MAGNET▁IS▁UM▁INTO▁THE▁UH▁UM▁LIKE▁TO▁THE▁PENCIL▁BUT▁IT'S▁UM▁BECAUSE▁THERE'S▁INSIDE▁THE▁PENCIL▁THAT'S▁WHAT▁WE▁USED▁TO▁AND▁THE▁MAGNET▁UM▁TO▁THE▁UM

2024-10-27 17:56:50,481 (asr_inference:509) INFO: speech length: 97968
2024-10-27 17:56:54,133 (beam_search:428) INFO: decoder input length: 76
2024-10-27 17:56:54,133 (beam_search:429) INFO: max output length: 76
2024-10-27 17:56:54,133 (beam_search:430) INFO: min output length: 0
2024-10-27 17:56:54,267 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:56:54,267 (beam_search:476) INFO:  -1.78 * 1.0 =  -1.78 for ctc
2024-10-27 17:56:54,267 (beam_search:479) INFO: total log probability: -1.78
2024-10-27 17:56:54,267 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:56:54,267 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:56:54,267 (beam_search:483) INFO: best hypo: ▁THE▁THE▁MAGNET▁IS▁TO▁THE▁MAGNET▁WITH▁THE▁THE

2024-10-27 17:56:54,270 (asr_inference:509) INFO: speech length: 164032
2024-10-27 17:57:00,625 (beam_search:428) INFO: decoder input length: 127
2024-10-27 17:57:00,625 (beam_search:429) INFO: max output length: 127
2024-10-27 17:57:00,625 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:01,204 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:01,204 (beam_search:476) INFO:  -2.30 * 1.0 =  -2.30 for ctc
2024-10-27 17:57:01,204 (beam_search:479) INFO: total log probability: -2.30
2024-10-27 17:57:01,204 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:57:01,204 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:01,204 (beam_search:483) INFO: best hypo: ▁THE▁MAGNET▁IS▁ON▁SIDE▁AND▁THE▁OTHER▁SO▁IT'S▁LIKE▁THE▁MAGNET▁POWER'S▁MAKING▁IT▁ON▁TOP▁OF▁THE▁OTHER▁MAGNETS

2024-10-27 17:57:01,207 (asr_inference:509) INFO: speech length: 165088
2024-10-27 17:57:07,555 (beam_search:428) INFO: decoder input length: 128
2024-10-27 17:57:07,555 (beam_search:429) INFO: max output length: 128
2024-10-27 17:57:07,555 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:08,015 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:08,015 (beam_search:476) INFO:  -3.23 * 1.0 =  -3.23 for ctc
2024-10-27 17:57:08,015 (beam_search:479) INFO: total log probability: -3.23
2024-10-27 17:57:08,015 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:57:08,015 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:08,016 (beam_search:483) INFO: best hypo: ▁THE▁POWER▁WITH▁TWO▁GO▁TOGETHER▁UMS▁MORE▁POWER▁SO▁THEN▁WHEN▁MAGNET▁IT▁ITS▁ON▁TOP▁OF▁IT

2024-10-27 17:57:08,018 (asr_inference:509) INFO: speech length: 29072
2024-10-27 17:57:09,194 (beam_search:428) INFO: decoder input length: 22
2024-10-27 17:57:09,194 (beam_search:429) INFO: max output length: 22
2024-10-27 17:57:09,194 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:09,212 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:09,212 (beam_search:476) INFO:  -1.61 * 1.0 =  -1.61 for ctc
2024-10-27 17:57:09,212 (beam_search:479) INFO: total log probability: -1.61
2024-10-27 17:57:09,212 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 17:57:09,212 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:09,212 (beam_search:483) INFO: best hypo: ▁TWO'T▁TOGETHER

2024-10-27 17:57:09,214 (asr_inference:509) INFO: speech length: 32576
2024-10-27 17:57:10,514 (beam_search:428) INFO: decoder input length: 24
2024-10-27 17:57:10,514 (beam_search:429) INFO: max output length: 24
2024-10-27 17:57:10,514 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:10,540 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:10,540 (beam_search:476) INFO:  -0.21 * 1.0 =  -0.21 for ctc
2024-10-27 17:57:10,540 (beam_search:479) INFO: total log probability: -0.21
2024-10-27 17:57:10,540 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 17:57:10,540 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:10,540 (beam_search:483) INFO: best hypo: ▁YEAH▁SO▁WHAT'S▁THE

2024-10-27 17:57:10,542 (asr_inference:509) INFO: speech length: 112752
2024-10-27 17:57:14,633 (beam_search:428) INFO: decoder input length: 87
2024-10-27 17:57:14,633 (beam_search:429) INFO: max output length: 87
2024-10-27 17:57:14,633 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:14,825 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:14,825 (beam_search:476) INFO:  -3.55 * 1.0 =  -3.55 for ctc
2024-10-27 17:57:14,825 (beam_search:479) INFO: total log probability: -3.55
2024-10-27 17:57:14,825 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 17:57:14,825 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:14,825 (beam_search:483) INFO: best hypo: ▁THE▁IS▁UM▁THEY'RE▁AND▁THEY'RE▁NOT▁AS▁STRONG

2024-10-27 17:57:14,828 (asr_inference:509) INFO: speech length: 180496
2024-10-27 17:57:21,876 (beam_search:428) INFO: decoder input length: 140
2024-10-27 17:57:21,876 (beam_search:429) INFO: max output length: 140
2024-10-27 17:57:21,876 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:22,476 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:22,476 (beam_search:476) INFO:  -2.15 * 1.0 =  -2.15 for ctc
2024-10-27 17:57:22,476 (beam_search:479) INFO: total log probability: -2.15
2024-10-27 17:57:22,476 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:57:22,476 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:22,476 (beam_search:483) INFO: best hypo: ▁ARE▁TWO▁DIFFERENT▁KIND▁OF▁MAGNETS▁AND▁UM▁SOME▁ARE▁I'M▁NOT▁SURE▁BUT▁I▁THINK▁SOME▁ARE▁THAN▁THE▁OTHER

2024-10-27 17:57:22,479 (asr_inference:509) INFO: speech length: 144016
2024-10-27 17:57:28,044 (beam_search:428) INFO: decoder input length: 112
2024-10-27 17:57:28,044 (beam_search:429) INFO: max output length: 112
2024-10-27 17:57:28,044 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:28,422 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:28,422 (beam_search:476) INFO:  -2.58 * 1.0 =  -2.58 for ctc
2024-10-27 17:57:28,422 (beam_search:479) INFO: total log probability: -2.58
2024-10-27 17:57:28,422 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:57:28,422 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:28,423 (beam_search:483) INFO: best hypo: ▁I▁SEE▁THE▁TWO▁DIFFERENT▁OF▁OR▁MAGNETS▁ARE▁TOGETHER▁CAUSE▁ONE▁IS▁AND▁THE▁OTHER▁ISN'T

2024-10-27 17:57:28,425 (asr_inference:509) INFO: speech length: 229952
2024-10-27 17:57:37,832 (beam_search:428) INFO: decoder input length: 179
2024-10-27 17:57:37,832 (beam_search:429) INFO: max output length: 179
2024-10-27 17:57:37,832 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:38,926 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:38,926 (beam_search:476) INFO:  -4.72 * 1.0 =  -4.72 for ctc
2024-10-27 17:57:38,926 (beam_search:479) INFO: total log probability: -4.72
2024-10-27 17:57:38,926 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:57:38,926 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:38,926 (beam_search:483) INFO: best hypo: ▁UM▁THE▁OTHERS▁ARE▁THE▁SO▁THEY▁COULD▁BUT▁I'M▁NOT▁THE▁POWERS▁I▁THINK▁DIFFERENT▁SO▁THEN▁THEY▁WON'T▁COME▁TOGETHER▁CAUSE▁THEY'RE▁THE▁SAME

2024-10-27 17:57:38,929 (asr_inference:509) INFO: speech length: 39392
2024-10-27 17:57:40,465 (beam_search:428) INFO: decoder input length: 30
2024-10-27 17:57:40,466 (beam_search:429) INFO: max output length: 30
2024-10-27 17:57:40,466 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:40,503 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:40,503 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 17:57:40,503 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 17:57:40,503 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:57:40,503 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:40,503 (beam_search:483) INFO: best hypo: ▁I'M▁NOT▁QUITE▁SURE

2024-10-27 17:57:40,506 (asr_inference:509) INFO: speech length: 219424
2024-10-27 17:57:49,367 (beam_search:428) INFO: decoder input length: 170
2024-10-27 17:57:49,367 (beam_search:429) INFO: max output length: 170
2024-10-27 17:57:49,367 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:50,518 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:50,518 (beam_search:476) INFO:  -5.17 * 1.0 =  -5.17 for ctc
2024-10-27 17:57:50,518 (beam_search:479) INFO: total log probability: -5.17
2024-10-27 17:57:50,518 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:57:50,518 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:50,518 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁THEY▁LIKE▁THE▁SAME▁BUT▁THE▁POWER'S▁REALLY▁UM▁SO▁THEN▁IT▁WON'T▁LIKE▁AND▁THE▁OTHER'T▁SO▁THEN▁UM▁IT▁THE▁DOESN'T▁WORK

2024-10-27 17:57:50,521 (asr_inference:509) INFO: speech length: 156944
2024-10-27 17:57:56,512 (beam_search:428) INFO: decoder input length: 122
2024-10-27 17:57:56,512 (beam_search:429) INFO: max output length: 122
2024-10-27 17:57:56,512 (beam_search:430) INFO: min output length: 0
2024-10-27 17:57:57,112 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:57:57,113 (beam_search:476) INFO:  -3.42 * 1.0 =  -3.42 for ctc
2024-10-27 17:57:57,113 (beam_search:479) INFO: total log probability: -3.42
2024-10-27 17:57:57,113 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:57:57,113 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:57:57,113 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THE▁MAGNET▁WILL▁UP▁THE▁ROCK▁THE▁PAPER▁CLIP▁UM▁THE▁AND▁I▁THINK▁IT'S▁LIKE▁A▁WASHER▁BUT▁I'M▁NOT▁SURE

2024-10-27 17:57:57,115 (asr_inference:509) INFO: speech length: 120960
2024-10-27 17:58:01,517 (beam_search:428) INFO: decoder input length: 93
2024-10-27 17:58:01,518 (beam_search:429) INFO: max output length: 93
2024-10-27 17:58:01,518 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:01,855 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:01,855 (beam_search:476) INFO:  -2.63 * 1.0 =  -2.63 for ctc
2024-10-27 17:58:01,855 (beam_search:479) INFO: total log probability: -2.63
2024-10-27 17:58:01,855 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:58:01,855 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:01,855 (beam_search:483) INFO: best hypo: ▁BECAUSE▁THE▁OTHERS▁ARE▁NOT▁OF▁METAL▁AND▁IT▁LOOKS▁LIKE▁THE▁KIND▁OF▁METAL▁WOULD▁STICK▁TO▁MA▁A▁MAGNET

2024-10-27 17:58:01,857 (asr_inference:509) INFO: speech length: 67536
2024-10-27 17:58:04,251 (beam_search:428) INFO: decoder input length: 52
2024-10-27 17:58:04,251 (beam_search:429) INFO: max output length: 52
2024-10-27 17:58:04,251 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:04,297 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:04,297 (beam_search:476) INFO:  -1.38 * 1.0 =  -1.38 for ctc
2024-10-27 17:58:04,297 (beam_search:479) INFO: total log probability: -1.38
2024-10-27 17:58:04,297 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:58:04,297 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:04,297 (beam_search:483) INFO: best hypo: ▁THE▁I▁THINK▁IN▁THE

2024-10-27 17:58:04,300 (asr_inference:509) INFO: speech length: 263696
2024-10-27 17:58:15,560 (beam_search:428) INFO: decoder input length: 205
2024-10-27 17:58:15,560 (beam_search:429) INFO: max output length: 205
2024-10-27 17:58:15,560 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:16,860 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:16,860 (beam_search:476) INFO:  -2.39 * 1.0 =  -2.39 for ctc
2024-10-27 17:58:16,862 (beam_search:479) INFO: total log probability: -2.39
2024-10-27 17:58:16,862 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:58:16,862 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:16,862 (beam_search:483) INFO: best hypo: ▁WE▁HAVE▁LIKE▁A▁AND▁UM▁THAT▁CONNECTS▁TO▁WIRES▁SO▁WE▁PUT▁IT▁ON▁A▁BATTERY▁AND▁UM▁PUT▁THE▁UM▁THE▁WIRES▁TO▁THE▁NEGATIVE▁AND▁POSITIVE▁SO▁THEN▁IT▁WILL▁START

2024-10-27 17:58:16,865 (asr_inference:509) INFO: speech length: 291584
2024-10-27 17:58:29,526 (beam_search:428) INFO: decoder input length: 227
2024-10-27 17:58:29,527 (beam_search:429) INFO: max output length: 227
2024-10-27 17:58:29,527 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:30,968 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:30,968 (beam_search:476) INFO:  -4.77 * 1.0 =  -4.77 for ctc
2024-10-27 17:58:30,968 (beam_search:479) INFO: total log probability: -4.77
2024-10-27 17:58:30,968 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 17:58:30,968 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:30,968 (beam_search:483) INFO: best hypo: ▁WE▁UM▁SO▁WE▁HAD▁A▁SWITCH▁WITH▁IT▁TOO▁SO▁WE▁HAD▁ANOTHER▁WIRE▁AND▁PUT▁UM▁ANOTHER▁ON▁SO▁THEN▁IF▁WE▁PUT▁A▁METAL▁THING▁IT▁IT▁WILL▁STOP▁AND▁YEAH

2024-10-27 17:58:30,971 (asr_inference:509) INFO: speech length: 82368
2024-10-27 17:58:33,908 (beam_search:428) INFO: decoder input length: 63
2024-10-27 17:58:33,908 (beam_search:429) INFO: max output length: 63
2024-10-27 17:58:33,908 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:34,044 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:34,044 (beam_search:476) INFO:  -1.23 * 1.0 =  -1.23 for ctc
2024-10-27 17:58:34,044 (beam_search:479) INFO: total log probability: -1.23
2024-10-27 17:58:34,044 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:58:34,044 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:34,044 (beam_search:483) INFO: best hypo: ▁THE▁SWITCH▁IS▁UM▁SO▁IT▁UM▁TURNS▁ON▁ANDS▁OFF

2024-10-27 17:58:34,047 (asr_inference:509) INFO: speech length: 161984
2024-10-27 17:58:40,096 (beam_search:428) INFO: decoder input length: 126
2024-10-27 17:58:40,096 (beam_search:429) INFO: max output length: 126
2024-10-27 17:58:40,096 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:40,688 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:40,688 (beam_search:476) INFO:  -3.15 * 1.0 =  -3.15 for ctc
2024-10-27 17:58:40,688 (beam_search:479) INFO: total log probability: -3.15
2024-10-27 17:58:40,688 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:58:40,688 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:40,688 (beam_search:483) INFO: best hypo: ▁THERE'S▁THESE▁TWO▁LITTLE▁AND▁YOU▁PUT▁THE▁WIRE▁ON▁IT▁AND▁THE▁WIRE▁AND▁THEN▁YOU▁HAVE▁THIS▁METAL▁UM▁THAT▁YOU▁PUT

2024-10-27 17:58:40,691 (asr_inference:509) INFO: speech length: 162480
2024-10-27 17:58:47,043 (beam_search:428) INFO: decoder input length: 126
2024-10-27 17:58:47,043 (beam_search:429) INFO: max output length: 126
2024-10-27 17:58:47,043 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:47,509 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:47,509 (beam_search:476) INFO:  -1.97 * 1.0 =  -1.97 for ctc
2024-10-27 17:58:47,509 (beam_search:479) INFO: total log probability: -1.97
2024-10-27 17:58:47,509 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 17:58:47,509 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:47,509 (beam_search:483) INFO: best hypo: ▁THE▁WIRES▁ARE▁CONNECTED▁TO▁IT▁SO▁THEN▁I▁THINK▁THE▁METAL▁UMS▁ON▁IT▁SO▁I▁THINK▁UM▁IT

2024-10-27 17:58:47,512 (asr_inference:509) INFO: speech length: 79568
2024-10-27 17:58:50,426 (beam_search:428) INFO: decoder input length: 61
2024-10-27 17:58:50,426 (beam_search:429) INFO: max output length: 61
2024-10-27 17:58:50,426 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:50,512 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:50,512 (beam_search:476) INFO:  -0.30 * 1.0 =  -0.30 for ctc
2024-10-27 17:58:50,512 (beam_search:479) INFO: total log probability: -0.30
2024-10-27 17:58:50,513 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 17:58:50,513 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:50,513 (beam_search:483) INFO: best hypo: ▁IT'S▁IT▁THE▁MOTOR▁AND▁THE▁UM

2024-10-27 17:58:50,515 (asr_inference:509) INFO: speech length: 178992
2024-10-27 17:58:57,508 (beam_search:428) INFO: decoder input length: 139
2024-10-27 17:58:57,508 (beam_search:429) INFO: max output length: 139
2024-10-27 17:58:57,508 (beam_search:430) INFO: min output length: 0
2024-10-27 17:58:57,835 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:58:57,835 (beam_search:476) INFO:  -0.66 * 1.0 =  -0.66 for ctc
2024-10-27 17:58:57,835 (beam_search:479) INFO: total log probability: -0.66
2024-10-27 17:58:57,835 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 17:58:57,835 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:58:57,836 (beam_search:483) INFO: best hypo: ▁UH▁THE▁ENERGY▁THE▁UM▁THE▁D▁CELL▁BATTERY▁IT▁THROUGH▁THE▁WIRE

2024-10-27 17:58:57,838 (asr_inference:509) INFO: speech length: 58256
2024-10-27 17:58:59,979 (beam_search:428) INFO: decoder input length: 45
2024-10-27 17:58:59,979 (beam_search:429) INFO: max output length: 45
2024-10-27 17:58:59,979 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:00,041 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:00,041 (beam_search:476) INFO:  -0.67 * 1.0 =  -0.67 for ctc
2024-10-27 17:59:00,041 (beam_search:479) INFO: total log probability: -0.67
2024-10-27 17:59:00,041 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:59:00,041 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:00,041 (beam_search:483) INFO: best hypo: ▁UH▁WE▁ALSO▁UM▁MADE▁A▁LIGHT▁BULB

2024-10-27 17:59:00,044 (asr_inference:509) INFO: speech length: 154432
2024-10-27 17:59:05,776 (beam_search:428) INFO: decoder input length: 120
2024-10-27 17:59:05,776 (beam_search:429) INFO: max output length: 120
2024-10-27 17:59:05,776 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:06,178 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:06,179 (beam_search:476) INFO:  -2.66 * 1.0 =  -2.66 for ctc
2024-10-27 17:59:06,179 (beam_search:479) INFO: total log probability: -2.66
2024-10-27 17:59:06,179 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:59:06,179 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:06,179 (beam_search:483) INFO: best hypo: ▁IT'S▁LIKE▁THE▁FIRST▁BUT▁WE▁HAD▁THE▁UM▁AND▁A▁SMALL▁LIGHT▁BULB▁INSTEAD▁OF▁A▁A

2024-10-27 17:59:06,181 (asr_inference:509) INFO: speech length: 260048
2024-10-27 17:59:16,898 (beam_search:428) INFO: decoder input length: 202
2024-10-27 17:59:16,898 (beam_search:429) INFO: max output length: 202
2024-10-27 17:59:16,898 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:18,161 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:18,161 (beam_search:476) INFO:  -3.61 * 1.0 =  -3.61 for ctc
2024-10-27 17:59:18,161 (beam_search:479) INFO: total log probability: -3.61
2024-10-27 17:59:18,161 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 17:59:18,161 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:18,161 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁ENERGY▁FROM▁THE▁NEGATIVE▁UM▁IT▁GOES▁UM▁OUT▁FROM▁THE▁NEGATIVE▁AND▁THEN▁IT▁WILL▁LIKE▁BACK▁INTO▁THE▁POSITIVE▁SO▁THEN▁THE▁ENERGY▁GOES▁UM▁LIKE▁UM▁IN▁A

2024-10-27 17:59:18,164 (asr_inference:509) INFO: speech length: 60144
2024-10-27 17:59:20,389 (beam_search:428) INFO: decoder input length: 46
2024-10-27 17:59:20,389 (beam_search:429) INFO: max output length: 46
2024-10-27 17:59:20,389 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:20,493 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:20,494 (beam_search:476) INFO:  -1.24 * 1.0 =  -1.24 for ctc
2024-10-27 17:59:20,494 (beam_search:479) INFO: total log probability: -1.24
2024-10-27 17:59:20,494 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:59:20,494 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:20,494 (beam_search:483) INFO: best hypo: ▁UM▁I'M▁NOTE▁SURE▁CAUSE▁WE▁DIDN'T▁REALLY▁DO▁THAT

2024-10-27 17:59:20,497 (asr_inference:509) INFO: speech length: 53712
2024-10-27 17:59:22,547 (beam_search:428) INFO: decoder input length: 41
2024-10-27 17:59:22,547 (beam_search:429) INFO: max output length: 41
2024-10-27 17:59:22,547 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:22,580 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:22,581 (beam_search:476) INFO:  -1.97 * 1.0 =  -1.97 for ctc
2024-10-27 17:59:22,581 (beam_search:479) INFO: total log probability: -1.97
2024-10-27 17:59:22,581 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 17:59:22,581 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:22,581 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁THE▁OTHER

2024-10-27 17:59:22,583 (asr_inference:509) INFO: speech length: 116256
2024-10-27 17:59:26,909 (beam_search:428) INFO: decoder input length: 90
2024-10-27 17:59:26,909 (beam_search:429) INFO: max output length: 90
2024-10-27 17:59:26,909 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:27,077 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:27,077 (beam_search:476) INFO:  -2.62 * 1.0 =  -2.62 for ctc
2024-10-27 17:59:27,077 (beam_search:479) INFO: total log probability: -2.62
2024-10-27 17:59:27,077 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 17:59:27,077 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:27,077 (beam_search:483) INFO: best hypo: ▁A▁CIRCUIT▁IS▁LIKE▁UM▁WHEN▁ENERGY▁GOES▁AND▁COMES▁BACK

2024-10-27 17:59:27,079 (asr_inference:509) INFO: speech length: 190000
2024-10-27 17:59:34,392 (beam_search:428) INFO: decoder input length: 147
2024-10-27 17:59:34,392 (beam_search:429) INFO: max output length: 147
2024-10-27 17:59:34,392 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:35,171 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:35,171 (beam_search:476) INFO:  -3.27 * 1.0 =  -3.27 for ctc
2024-10-27 17:59:35,171 (beam_search:479) INFO: total log probability: -3.27
2024-10-27 17:59:35,171 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 17:59:35,171 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:35,171 (beam_search:483) INFO: best hypo: ▁ON▁THE▁BATTERY▁IT▁HAS▁TO▁BE▁ON▁THE▁UM▁FOR▁THE▁AND▁POSITIVE▁AND▁THE▁OTHER▁SIDE▁OF▁THE▁WIRE▁HAS▁TO▁UM▁TO▁ON▁THE▁LIGHT▁BULB

2024-10-27 17:59:35,174 (asr_inference:509) INFO: speech length: 47728
2024-10-27 17:59:36,872 (beam_search:428) INFO: decoder input length: 36
2024-10-27 17:59:36,872 (beam_search:429) INFO: max output length: 36
2024-10-27 17:59:36,872 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:36,896 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:36,896 (beam_search:476) INFO:  -0.71 * 1.0 =  -0.71 for ctc
2024-10-27 17:59:36,896 (beam_search:479) INFO: total log probability: -0.71
2024-10-27 17:59:36,896 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 17:59:36,896 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:36,896 (beam_search:483) INFO: best hypo: ▁WE▁PUT▁IT

2024-10-27 17:59:36,899 (asr_inference:509) INFO: speech length: 68608
2024-10-27 17:59:39,468 (beam_search:428) INFO: decoder input length: 53
2024-10-27 17:59:39,468 (beam_search:429) INFO: max output length: 53
2024-10-27 17:59:39,468 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:39,549 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:39,549 (beam_search:476) INFO:  -0.79 * 1.0 =  -0.79 for ctc
2024-10-27 17:59:39,549 (beam_search:479) INFO: total log probability: -0.79
2024-10-27 17:59:39,549 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 17:59:39,549 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:39,549 (beam_search:483) INFO: best hypo: ▁WELL▁IN▁I▁I▁I▁DON'T▁SO

2024-10-27 17:59:39,551 (asr_inference:509) INFO: speech length: 135088
2024-10-27 17:59:44,697 (beam_search:428) INFO: decoder input length: 105
2024-10-27 17:59:44,697 (beam_search:429) INFO: max output length: 105
2024-10-27 17:59:44,697 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:44,964 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:44,964 (beam_search:476) INFO:  -0.42 * 1.0 =  -0.42 for ctc
2024-10-27 17:59:44,964 (beam_search:479) INFO: total log probability: -0.42
2024-10-27 17:59:44,964 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 17:59:44,964 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:44,964 (beam_search:483) INFO: best hypo: ▁BECAUSE▁THE▁UM▁IT▁HAS▁TO▁GO▁THROUGH▁LIKE▁UM▁THIS▁LITTLE▁OF▁IT

2024-10-27 17:59:44,967 (asr_inference:509) INFO: speech length: 25296
2024-10-27 17:59:46,015 (beam_search:428) INFO: decoder input length: 19
2024-10-27 17:59:46,016 (beam_search:429) INFO: max output length: 19
2024-10-27 17:59:46,016 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:46,021 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:46,021 (beam_search:476) INFO:  -0.03 * 1.0 =  -0.03 for ctc
2024-10-27 17:59:46,021 (beam_search:479) INFO: total log probability: -0.03
2024-10-27 17:59:46,021 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 17:59:46,021 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:46,021 (beam_search:483) INFO: best hypo: 

2024-10-27 17:59:46,024 (asr_inference:509) INFO: speech length: 151296
2024-10-27 17:59:51,643 (beam_search:428) INFO: decoder input length: 117
2024-10-27 17:59:51,643 (beam_search:429) INFO: max output length: 117
2024-10-27 17:59:51,643 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:51,977 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:51,977 (beam_search:476) INFO:  -2.79 * 1.0 =  -2.79 for ctc
2024-10-27 17:59:51,977 (beam_search:479) INFO: total log probability: -2.79
2024-10-27 17:59:51,977 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 17:59:51,977 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:51,977 (beam_search:483) INFO: best hypo: ▁UH▁WE▁HAD▁THESE▁LITTLES▁AND▁WE▁UM▁IN▁A▁AND▁JUST▁UM▁SAW▁THE▁THINGS

2024-10-27 17:59:51,979 (asr_inference:509) INFO: speech length: 119616
2024-10-27 17:59:56,337 (beam_search:428) INFO: decoder input length: 92
2024-10-27 17:59:56,338 (beam_search:429) INFO: max output length: 92
2024-10-27 17:59:56,338 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:56,533 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:56,533 (beam_search:476) INFO:  -1.65 * 1.0 =  -1.65 for ctc
2024-10-27 17:59:56,533 (beam_search:479) INFO: total log probability: -1.65
2024-10-27 17:59:56,533 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 17:59:56,533 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:56,534 (beam_search:483) INFO: best hypo: ▁UM▁WHAT▁THE▁WAS▁AND▁LIGHT▁BULB▁AND▁AND▁LIKE▁THE▁WIRE

2024-10-27 17:59:56,536 (asr_inference:509) INFO: speech length: 53088
2024-10-27 17:59:58,567 (beam_search:428) INFO: decoder input length: 40
2024-10-27 17:59:58,567 (beam_search:429) INFO: max output length: 40
2024-10-27 17:59:58,567 (beam_search:430) INFO: min output length: 0
2024-10-27 17:59:58,620 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 17:59:58,621 (beam_search:476) INFO:  -0.77 * 1.0 =  -0.77 for ctc
2024-10-27 17:59:58,621 (beam_search:479) INFO: total log probability: -0.77
2024-10-27 17:59:58,621 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 17:59:58,621 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 17:59:58,621 (beam_search:483) INFO: best hypo: ▁I▁SEE▁THE▁LIGHT▁BULB▁THE▁AND▁THE

2024-10-27 17:59:58,623 (asr_inference:509) INFO: speech length: 48736
2024-10-27 18:00:00,418 (beam_search:428) INFO: decoder input length: 37
2024-10-27 18:00:00,418 (beam_search:429) INFO: max output length: 37
2024-10-27 18:00:00,418 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:00,469 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:00,469 (beam_search:476) INFO:  -1.66 * 1.0 =  -1.66 for ctc
2024-10-27 18:00:00,469 (beam_search:479) INFO: total log probability: -1.66
2024-10-27 18:00:00,469 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:00:00,469 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:00,469 (beam_search:483) INFO: best hypo: ▁UM▁I▁DON'T▁I▁IT▁BUT

2024-10-27 18:00:00,471 (asr_inference:509) INFO: speech length: 40240
2024-10-27 18:00:02,003 (beam_search:428) INFO: decoder input length: 30
2024-10-27 18:00:02,003 (beam_search:429) INFO: max output length: 30
2024-10-27 18:00:02,003 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:02,038 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:02,038 (beam_search:476) INFO:  -0.41 * 1.0 =  -0.41 for ctc
2024-10-27 18:00:02,038 (beam_search:479) INFO: total log probability: -0.41
2024-10-27 18:00:02,038 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:00:02,038 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:02,038 (beam_search:483) INFO: best hypo: ▁UM▁I'M▁NOTE▁SURE

2024-10-27 18:00:02,041 (asr_inference:509) INFO: speech length: 93856
2024-10-27 18:00:05,440 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:00:05,440 (beam_search:429) INFO: max output length: 72
2024-10-27 18:00:05,440 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:05,599 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:05,599 (beam_search:476) INFO:  -1.11 * 1.0 =  -1.11 for ctc
2024-10-27 18:00:05,599 (beam_search:479) INFO: total log probability: -1.11
2024-10-27 18:00:05,599 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:00:05,599 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:05,599 (beam_search:483) INFO: best hypo: ▁I▁THINK▁IT▁IS▁LIKE▁THE▁AND▁THEN▁THE▁SWITCH▁AND▁THE▁BATTERY

2024-10-27 18:00:05,602 (asr_inference:509) INFO: speech length: 153808
2024-10-27 18:00:11,469 (beam_search:428) INFO: decoder input length: 119
2024-10-27 18:00:11,469 (beam_search:429) INFO: max output length: 119
2024-10-27 18:00:11,469 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:11,840 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:11,840 (beam_search:476) INFO:  -2.58 * 1.0 =  -2.58 for ctc
2024-10-27 18:00:11,840 (beam_search:479) INFO: total log probability: -2.58
2024-10-27 18:00:11,840 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:00:11,840 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:11,840 (beam_search:483) INFO: best hypo: ▁UM▁THE▁BECAUSE▁UM▁COMES▁BACK▁TO▁BUT▁IT▁GOES▁FOR▁LIKE▁A▁AND▁IT▁COMES▁BACK

2024-10-27 18:00:11,843 (asr_inference:509) INFO: speech length: 54256
2024-10-27 18:00:13,811 (beam_search:428) INFO: decoder input length: 41
2024-10-27 18:00:13,812 (beam_search:429) INFO: max output length: 41
2024-10-27 18:00:13,812 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:13,849 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:13,849 (beam_search:476) INFO:  -1.19 * 1.0 =  -1.19 for ctc
2024-10-27 18:00:13,849 (beam_search:479) INFO: total log probability: -1.19
2024-10-27 18:00:13,849 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:00:13,849 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:13,849 (beam_search:483) INFO: best hypo: ▁UM▁IT▁OUT▁OF▁THE

2024-10-27 18:00:13,852 (asr_inference:509) INFO: speech length: 83184
2024-10-27 18:00:16,958 (beam_search:428) INFO: decoder input length: 64
2024-10-27 18:00:16,958 (beam_search:429) INFO: max output length: 64
2024-10-27 18:00:16,958 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:17,048 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:17,048 (beam_search:476) INFO:  -2.11 * 1.0 =  -2.11 for ctc
2024-10-27 18:00:17,048 (beam_search:479) INFO: total log probability: -2.11
2024-10-27 18:00:17,048 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:00:17,048 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:17,048 (beam_search:483) INFO: best hypo: ▁I▁HAVE▁BEEN▁DOING▁THINGS▁WITH▁AND▁A

2024-10-27 18:00:17,050 (asr_inference:509) INFO: speech length: 138848
2024-10-27 18:00:22,166 (beam_search:428) INFO: decoder input length: 107
2024-10-27 18:00:22,166 (beam_search:429) INFO: max output length: 107
2024-10-27 18:00:22,166 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:22,619 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:22,619 (beam_search:476) INFO:  -3.24 * 1.0 =  -3.24 for ctc
2024-10-27 18:00:22,620 (beam_search:479) INFO: total log probability: -3.24
2024-10-27 18:00:22,620 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:00:22,620 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:22,620 (beam_search:483) INFO: best hypo: ▁WE▁HAVE▁A▁LIGHT▁AND▁WIRES▁AND▁WE▁PUT▁THE▁IN▁THIS▁AND▁THEN▁THE▁WIRES▁TO▁IT▁AND▁THEN▁THE▁TO▁THE▁LIGHT▁TOO

2024-10-27 18:00:22,622 (asr_inference:509) INFO: speech length: 249216
2024-10-27 18:00:32,674 (beam_search:428) INFO: decoder input length: 194
2024-10-27 18:00:32,674 (beam_search:429) INFO: max output length: 194
2024-10-27 18:00:32,674 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:34,004 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:34,005 (beam_search:476) INFO:  -3.27 * 1.0 =  -3.27 for ctc
2024-10-27 18:00:34,005 (beam_search:479) INFO: total log probability: -3.27
2024-10-27 18:00:34,005 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:00:34,005 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:34,005 (beam_search:483) INFO: best hypo: ▁UM▁WE▁JUST▁HAD▁THESE▁LITTLE▁THAT▁ARE▁LIKES▁AND▁ONCE▁WE▁PUT▁EM▁IN▁AND▁UM▁THERE'S▁A▁THAT▁GOES▁WITH▁THE▁LIGHT▁SO▁THEN▁IT▁HAS▁TO▁TOUCH▁THE▁BOTTOM▁THING

2024-10-27 18:00:34,008 (asr_inference:509) INFO: speech length: 162512
2024-10-27 18:00:40,254 (beam_search:428) INFO: decoder input length: 126
2024-10-27 18:00:40,254 (beam_search:429) INFO: max output length: 126
2024-10-27 18:00:40,254 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:40,674 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:40,674 (beam_search:476) INFO:  -2.18 * 1.0 =  -2.18 for ctc
2024-10-27 18:00:40,674 (beam_search:479) INFO: total log probability: -2.18
2024-10-27 18:00:40,674 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:00:40,674 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:40,674 (beam_search:483) INFO: best hypo: ▁UM▁THE▁WIRES▁HAVE▁TO▁BE▁IN▁AND▁THE▁THES▁HAVE▁TO▁BE▁FOR▁THE▁LIGHT▁TO▁TOUCH

2024-10-27 18:00:40,677 (asr_inference:509) INFO: speech length: 92112
2024-10-27 18:00:44,039 (beam_search:428) INFO: decoder input length: 71
2024-10-27 18:00:44,039 (beam_search:429) INFO: max output length: 71
2024-10-27 18:00:44,039 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:44,179 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:44,180 (beam_search:476) INFO:  -1.83 * 1.0 =  -1.83 for ctc
2024-10-27 18:00:44,180 (beam_search:479) INFO: total log probability: -1.83
2024-10-27 18:00:44,180 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:00:44,180 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:44,180 (beam_search:483) INFO: best hypo: ▁THE▁WIRES▁HAVE▁TO▁SO▁THE▁ELECTRICITY▁CAN▁FLOW▁IN▁THE▁LIGHT

2024-10-27 18:00:44,182 (asr_inference:509) INFO: speech length: 180992
2024-10-27 18:00:51,281 (beam_search:428) INFO: decoder input length: 140
2024-10-27 18:00:51,281 (beam_search:429) INFO: max output length: 140
2024-10-27 18:00:51,281 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:52,260 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:52,260 (beam_search:476) INFO:  -1.88 * 1.0 =  -1.88 for ctc
2024-10-27 18:00:52,260 (beam_search:479) INFO: total log probability: -1.88
2024-10-27 18:00:52,260 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:00:52,260 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:52,260 (beam_search:483) INFO: best hypo: ▁ONE▁HAS▁TO▁BE▁ON▁THE▁NEGATIVE▁SIDE▁AND▁THE▁OTHER▁HAS▁TO▁BE▁ON▁THE▁POSITIVE▁AND▁THEN▁THE▁OTHER▁ENDS▁OF▁THE▁WIRES▁HAVE▁TO▁BE▁IN▁THE▁LITTLES▁SO▁IT▁CONNECTS▁TO▁THE▁LIGHT

2024-10-27 18:00:52,263 (asr_inference:509) INFO: speech length: 137040
2024-10-27 18:00:57,303 (beam_search:428) INFO: decoder input length: 106
2024-10-27 18:00:57,303 (beam_search:429) INFO: max output length: 106
2024-10-27 18:00:57,303 (beam_search:430) INFO: min output length: 0
2024-10-27 18:00:57,614 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:00:57,615 (beam_search:476) INFO:  -1.19 * 1.0 =  -1.19 for ctc
2024-10-27 18:00:57,615 (beam_search:479) INFO: total log probability: -1.19
2024-10-27 18:00:57,615 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:00:57,615 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:00:57,615 (beam_search:483) INFO: best hypo: ▁UM▁WELL▁THE▁THE▁ELECTRICITY▁COMES▁OUT▁FROM▁THE▁NEGATIVE▁AND▁THEN▁COMES▁BACK▁INTO▁THE▁POSITIVE

2024-10-27 18:00:57,617 (asr_inference:509) INFO: speech length: 119024
2024-10-27 18:01:01,977 (beam_search:428) INFO: decoder input length: 92
2024-10-27 18:01:01,977 (beam_search:429) INFO: max output length: 92
2024-10-27 18:01:01,977 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:02,187 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:02,187 (beam_search:476) INFO:  -1.62 * 1.0 =  -1.62 for ctc
2024-10-27 18:01:02,187 (beam_search:479) INFO: total log probability: -1.62
2024-10-27 18:01:02,187 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:01:02,187 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:02,187 (beam_search:483) INFO: best hypo: ▁THE▁ALL▁OF▁THE▁WIRES▁ARE▁AT▁HOOKED▁UP▁TO▁THE▁LITTLE

2024-10-27 18:01:02,189 (asr_inference:509) INFO: speech length: 155168
2024-10-27 18:01:07,953 (beam_search:428) INFO: decoder input length: 120
2024-10-27 18:01:07,953 (beam_search:429) INFO: max output length: 120
2024-10-27 18:01:07,953 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:08,527 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:08,527 (beam_search:476) INFO:  -1.82 * 1.0 =  -1.82 for ctc
2024-10-27 18:01:08,527 (beam_search:479) INFO: total log probability: -1.82
2024-10-27 18:01:08,527 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:01:08,527 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:08,527 (beam_search:483) INFO: best hypo: ▁IF▁THEY'RE▁NOTED▁UP▁THE▁ELECTRICITY▁CAN'T▁GO▁FROM▁THE▁FROM▁THE▁BATTERY▁AND▁IT▁CAN'T▁GET▁TO▁THE▁LIGHT

2024-10-27 18:01:08,529 (asr_inference:509) INFO: speech length: 90560
2024-10-27 18:01:11,942 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:01:11,942 (beam_search:429) INFO: max output length: 70
2024-10-27 18:01:11,942 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:12,121 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:12,121 (beam_search:476) INFO:  -1.45 * 1.0 =  -1.45 for ctc
2024-10-27 18:01:12,121 (beam_search:479) INFO: total log probability: -1.45
2024-10-27 18:01:12,122 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:01:12,122 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:12,122 (beam_search:483) INFO: best hypo: ▁THEY▁HELP▁HOLD▁IT▁BECAUSE▁YOU▁HAVE▁TO▁HOLD▁THE▁LIGHT▁YOU▁PUT▁THE▁WIRES▁ON

2024-10-27 18:01:12,124 (asr_inference:509) INFO: speech length: 248576
2024-10-27 18:01:22,170 (beam_search:428) INFO: decoder input length: 193
2024-10-27 18:01:22,171 (beam_search:429) INFO: max output length: 193
2024-10-27 18:01:22,171 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:23,410 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:23,411 (beam_search:476) INFO:  -2.68 * 1.0 =  -2.68 for ctc
2024-10-27 18:01:23,411 (beam_search:479) INFO: total log probability: -2.68
2024-10-27 18:01:23,411 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:01:23,411 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:23,411 (beam_search:483) INFO: best hypo: ▁THE▁LIGHTS▁INTO▁THE▁LITTLE▁A▁LITTLE▁SO▁THEN▁ITS▁THE▁BOTTOM▁THES▁THE▁BOTTOM▁SO▁WHEN▁THE▁WIRES▁ARE▁UM▁IN▁IT▁THE▁THE▁ELECTRICITY▁GOES▁TO▁THE▁BOTTOM▁AND▁UP

2024-10-27 18:01:23,414 (asr_inference:509) INFO: speech length: 130992
2024-10-27 18:01:28,230 (beam_search:428) INFO: decoder input length: 101
2024-10-27 18:01:28,230 (beam_search:429) INFO: max output length: 101
2024-10-27 18:01:28,230 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:28,437 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:28,438 (beam_search:476) INFO:  -2.24 * 1.0 =  -2.24 for ctc
2024-10-27 18:01:28,438 (beam_search:479) INFO: total log probability: -2.24
2024-10-27 18:01:28,438 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:01:28,438 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:28,438 (beam_search:483) INFO: best hypo: ▁THE▁ARE▁AND▁THE▁BOTTOM▁IS▁TOUCHING▁THE▁METAL▁ON▁TO▁THE

2024-10-27 18:01:28,440 (asr_inference:509) INFO: speech length: 22848
2024-10-27 18:01:29,520 (beam_search:428) INFO: decoder input length: 17
2024-10-27 18:01:29,520 (beam_search:429) INFO: max output length: 17
2024-10-27 18:01:29,520 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:29,534 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:29,534 (beam_search:476) INFO:  -0.29 * 1.0 =  -0.29 for ctc
2024-10-27 18:01:29,534 (beam_search:479) INFO: total log probability: -0.29
2024-10-27 18:01:29,534 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:01:29,534 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:29,534 (beam_search:483) INFO: best hypo: ▁THEY'RE

2024-10-27 18:01:29,536 (asr_inference:509) INFO: speech length: 86400
2024-10-27 18:01:32,715 (beam_search:428) INFO: decoder input length: 66
2024-10-27 18:01:32,715 (beam_search:429) INFO: max output length: 66
2024-10-27 18:01:32,715 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:32,832 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:32,832 (beam_search:476) INFO:  -1.69 * 1.0 =  -1.69 for ctc
2024-10-27 18:01:32,832 (beam_search:479) INFO: total log probability: -1.69
2024-10-27 18:01:32,832 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:01:32,832 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:32,832 (beam_search:483) INFO: best hypo: ▁UM▁WELL▁AND▁THERE'S▁TWO▁THE▁IS▁BRIGHTER

2024-10-27 18:01:32,835 (asr_inference:509) INFO: speech length: 59648
2024-10-27 18:01:35,076 (beam_search:428) INFO: decoder input length: 46
2024-10-27 18:01:35,076 (beam_search:429) INFO: max output length: 46
2024-10-27 18:01:35,076 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:35,126 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:35,127 (beam_search:476) INFO:  -1.51 * 1.0 =  -1.51 for ctc
2024-10-27 18:01:35,127 (beam_search:479) INFO: total log probability: -1.51
2024-10-27 18:01:35,127 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:01:35,127 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:35,127 (beam_search:483) INFO: best hypo: ▁BECAUSE▁TWO▁THERE▁IS▁MORE▁POWER

2024-10-27 18:01:35,129 (asr_inference:509) INFO: speech length: 29408
2024-10-27 18:01:36,395 (beam_search:428) INFO: decoder input length: 22
2024-10-27 18:01:36,395 (beam_search:429) INFO: max output length: 22
2024-10-27 18:01:36,395 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:36,417 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:36,417 (beam_search:476) INFO:  -0.93 * 1.0 =  -0.93 for ctc
2024-10-27 18:01:36,417 (beam_search:479) INFO: total log probability: -0.93
2024-10-27 18:01:36,417 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:01:36,417 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:36,417 (beam_search:483) INFO: best hypo: ▁THEY'RE▁LESS▁LIGHT

2024-10-27 18:01:36,420 (asr_inference:509) INFO: speech length: 58688
2024-10-27 18:01:38,579 (beam_search:428) INFO: decoder input length: 45
2024-10-27 18:01:38,579 (beam_search:429) INFO: max output length: 45
2024-10-27 18:01:38,579 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:38,676 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:38,676 (beam_search:476) INFO:  -1.24 * 1.0 =  -1.24 for ctc
2024-10-27 18:01:38,676 (beam_search:479) INFO: total log probability: -1.24
2024-10-27 18:01:38,677 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:01:38,677 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:38,677 (beam_search:483) INFO: best hypo: ▁THEY'RE▁NOT▁THEY'RE▁NOT▁AS▁BRIGHT▁AS▁THE▁OTHER▁ONES

2024-10-27 18:01:38,680 (asr_inference:509) INFO: speech length: 53168
2024-10-27 18:01:40,669 (beam_search:428) INFO: decoder input length: 41
2024-10-27 18:01:40,669 (beam_search:429) INFO: max output length: 41
2024-10-27 18:01:40,669 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:40,718 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:40,719 (beam_search:476) INFO:  -0.98 * 1.0 =  -0.98 for ctc
2024-10-27 18:01:40,719 (beam_search:479) INFO: total log probability: -0.98
2024-10-27 18:01:40,719 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:01:40,719 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:40,719 (beam_search:483) INFO: best hypo: ▁THERE▁ARE▁TWOS▁AND▁ONE▁BATTERY

2024-10-27 18:01:40,722 (asr_inference:509) INFO: speech length: 68368
2024-10-27 18:01:43,199 (beam_search:428) INFO: decoder input length: 52
2024-10-27 18:01:43,199 (beam_search:429) INFO: max output length: 52
2024-10-27 18:01:43,199 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:43,262 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:43,262 (beam_search:476) INFO:  -0.56 * 1.0 =  -0.56 for ctc
2024-10-27 18:01:43,262 (beam_search:479) INFO: total log probability: -0.56
2024-10-27 18:01:43,262 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:01:43,262 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:43,262 (beam_search:483) INFO: best hypo: ▁I▁THEY'RE▁ALL▁CONNECTED▁AND

2024-10-27 18:01:43,264 (asr_inference:509) INFO: speech length: 123888
2024-10-27 18:01:47,699 (beam_search:428) INFO: decoder input length: 96
2024-10-27 18:01:47,699 (beam_search:429) INFO: max output length: 96
2024-10-27 18:01:47,700 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:48,015 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:48,016 (beam_search:476) INFO:  -2.73 * 1.0 =  -2.73 for ctc
2024-10-27 18:01:48,016 (beam_search:479) INFO: total log probability: -2.73
2024-10-27 18:01:48,016 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:01:48,016 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:48,016 (beam_search:483) INFO: best hypo: ▁IF▁THEY'RE▁NOT▁AT▁TOUCHING▁THE▁BOTTOM▁PART▁THE▁ELECTRICITY▁CAN'T▁GET▁TO▁THE▁LIGHT▁BULB

2024-10-27 18:01:48,019 (asr_inference:509) INFO: speech length: 19648
2024-10-27 18:01:48,993 (beam_search:428) INFO: decoder input length: 14
2024-10-27 18:01:48,993 (beam_search:429) INFO: max output length: 14
2024-10-27 18:01:48,993 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:49,001 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:49,001 (beam_search:476) INFO:  -0.59 * 1.0 =  -0.59 for ctc
2024-10-27 18:01:49,001 (beam_search:479) INFO: total log probability: -0.59
2024-10-27 18:01:49,001 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:01:49,001 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:49,001 (beam_search:483) INFO: best hypo: ▁YEAH

2024-10-27 18:01:49,003 (asr_inference:509) INFO: speech length: 144112
2024-10-27 18:01:54,227 (beam_search:428) INFO: decoder input length: 112
2024-10-27 18:01:54,228 (beam_search:429) INFO: max output length: 112
2024-10-27 18:01:54,228 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:54,698 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:54,698 (beam_search:476) INFO:  -3.03 * 1.0 =  -3.03 for ctc
2024-10-27 18:01:54,698 (beam_search:479) INFO: total log probability: -3.03
2024-10-27 18:01:54,698 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:01:54,698 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:54,699 (beam_search:483) INFO: best hypo: ▁THE▁ELECTRICITY▁GOES▁FROM▁THE▁NEGATIVE▁AND▁IT▁GOES▁THROUGH▁THE▁LIGHT▁BULB▁THEN▁COMES▁INTO▁THE▁OTHER▁ONE▁AND▁COMES▁BACK▁TO▁THE▁BATTERY

2024-10-27 18:01:54,702 (asr_inference:509) INFO: speech length: 81904
2024-10-27 18:01:58,000 (beam_search:428) INFO: decoder input length: 63
2024-10-27 18:01:58,000 (beam_search:429) INFO: max output length: 63
2024-10-27 18:01:58,000 (beam_search:430) INFO: min output length: 0
2024-10-27 18:01:58,098 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:01:58,098 (beam_search:476) INFO:  -1.66 * 1.0 =  -1.66 for ctc
2024-10-27 18:01:58,098 (beam_search:479) INFO: total log probability: -1.66
2024-10-27 18:01:58,098 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:01:58,098 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:01:58,098 (beam_search:483) INFO: best hypo: ▁THE▁ENERGY▁BECAUSE▁THE▁NEGATIVE▁IS▁ON▁THE▁SIDE

2024-10-27 18:01:58,101 (asr_inference:509) INFO: speech length: 54656
2024-10-27 18:02:00,165 (beam_search:428) INFO: decoder input length: 42
2024-10-27 18:02:00,165 (beam_search:429) INFO: max output length: 42
2024-10-27 18:02:00,166 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:00,210 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:00,210 (beam_search:476) INFO:  -0.87 * 1.0 =  -0.87 for ctc
2024-10-27 18:02:00,210 (beam_search:479) INFO: total log probability: -0.87
2024-10-27 18:02:00,210 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:02:00,210 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:00,210 (beam_search:483) INFO: best hypo: ▁THE▁THE▁ENERGY▁THE▁OTHER▁WAY

2024-10-27 18:02:00,212 (asr_inference:509) INFO: speech length: 148368
2024-10-27 18:02:05,739 (beam_search:428) INFO: decoder input length: 115
2024-10-27 18:02:05,739 (beam_search:429) INFO: max output length: 115
2024-10-27 18:02:05,739 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:06,001 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:06,001 (beam_search:476) INFO:  -1.00 * 1.0 =  -1.00 for ctc
2024-10-27 18:02:06,001 (beam_search:479) INFO: total log probability: -1.00
2024-10-27 18:02:06,001 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:02:06,001 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:06,001 (beam_search:483) INFO: best hypo: ▁THE▁OTHER▁LIKE▁THE▁ELECTRICITY▁GOES▁TO▁THE▁RIGHT▁INTO▁THE▁OTHER▁SIDE

2024-10-27 18:02:06,004 (asr_inference:509) INFO: speech length: 138336
2024-10-27 18:02:11,049 (beam_search:428) INFO: decoder input length: 107
2024-10-27 18:02:11,049 (beam_search:429) INFO: max output length: 107
2024-10-27 18:02:11,049 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:11,299 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:11,300 (beam_search:476) INFO:  -1.04 * 1.0 =  -1.04 for ctc
2024-10-27 18:02:11,300 (beam_search:479) INFO: total log probability: -1.04
2024-10-27 18:02:11,300 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:02:11,300 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:11,300 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THERE'S▁UM▁IT'S▁ABOUT▁UM▁LIGHT▁BULBS▁AND

2024-10-27 18:02:11,302 (asr_inference:509) INFO: speech length: 110608
2024-10-27 18:02:15,302 (beam_search:428) INFO: decoder input length: 85
2024-10-27 18:02:15,302 (beam_search:429) INFO: max output length: 85
2024-10-27 18:02:15,302 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:15,517 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:15,517 (beam_search:476) INFO:  -2.28 * 1.0 =  -2.28 for ctc
2024-10-27 18:02:15,517 (beam_search:479) INFO: total log probability: -2.28
2024-10-27 18:02:15,517 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:02:15,517 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:15,517 (beam_search:483) INFO: best hypo: ▁IN▁CLASS▁WE▁HAVE▁A▁PAPER▁THAT▁HAS▁THESES▁AND▁THEY'RE▁ON

2024-10-27 18:02:15,520 (asr_inference:509) INFO: speech length: 231616
2024-10-27 18:02:25,429 (beam_search:428) INFO: decoder input length: 180
2024-10-27 18:02:25,429 (beam_search:429) INFO: max output length: 180
2024-10-27 18:02:25,429 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:26,074 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:26,074 (beam_search:476) INFO:  -4.01 * 1.0 =  -4.01 for ctc
2024-10-27 18:02:26,074 (beam_search:479) INFO: total log probability: -4.01
2024-10-27 18:02:26,074 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:02:26,074 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:26,074 (beam_search:483) INFO: best hypo: ▁THE▁WITH▁ARE▁THES▁AND▁THE▁ARE▁ARE▁THE▁WIRE▁AND▁THE▁AND▁THE▁ONE▁IS▁THE▁BATTERY

2024-10-27 18:02:26,077 (asr_inference:509) INFO: speech length: 82240
2024-10-27 18:02:29,076 (beam_search:428) INFO: decoder input length: 63
2024-10-27 18:02:29,076 (beam_search:429) INFO: max output length: 63
2024-10-27 18:02:29,077 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:29,207 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:29,207 (beam_search:476) INFO:  -1.38 * 1.0 =  -1.38 for ctc
2024-10-27 18:02:29,207 (beam_search:479) INFO: total log probability: -1.38
2024-10-27 18:02:29,207 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:02:29,208 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:29,208 (beam_search:483) INFO: best hypo: ▁UM▁I'M▁NOT▁SURE▁BUT▁I▁THINK▁IT'S▁AND▁TWO

2024-10-27 18:02:29,210 (asr_inference:509) INFO: speech length: 50128
2024-10-27 18:02:31,031 (beam_search:428) INFO: decoder input length: 38
2024-10-27 18:02:31,031 (beam_search:429) INFO: max output length: 38
2024-10-27 18:02:31,031 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:31,072 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:31,073 (beam_search:476) INFO:  -1.07 * 1.0 =  -1.07 for ctc
2024-10-27 18:02:31,073 (beam_search:479) INFO: total log probability: -1.07
2024-10-27 18:02:31,073 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:02:31,073 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:31,073 (beam_search:483) INFO: best hypo: ▁UM▁I'M▁NOTE

2024-10-27 18:02:31,075 (asr_inference:509) INFO: speech length: 98256
2024-10-27 18:02:34,744 (beam_search:428) INFO: decoder input length: 76
2024-10-27 18:02:34,744 (beam_search:429) INFO: max output length: 76
2024-10-27 18:02:34,744 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:34,851 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:34,851 (beam_search:476) INFO:  -1.01 * 1.0 =  -1.01 for ctc
2024-10-27 18:02:34,851 (beam_search:479) INFO: total log probability: -1.01
2024-10-27 18:02:34,851 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:02:34,851 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:34,852 (beam_search:483) INFO: best hypo: ▁WE▁A▁MAGNET▁THAT▁CAN▁TURN▁AND▁ON

2024-10-27 18:02:34,854 (asr_inference:509) INFO: speech length: 311088
2024-10-27 18:02:48,194 (beam_search:428) INFO: decoder input length: 242
2024-10-27 18:02:48,194 (beam_search:429) INFO: max output length: 242
2024-10-27 18:02:48,194 (beam_search:430) INFO: min output length: 0
2024-10-27 18:02:49,952 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:02:49,952 (beam_search:476) INFO:  -8.56 * 1.0 =  -8.56 for ctc
2024-10-27 18:02:49,952 (beam_search:479) INFO: total log probability: -8.56
2024-10-27 18:02:49,952 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:02:49,952 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:02:49,952 (beam_search:483) INFO: best hypo: ▁WELL▁WE▁HAD▁A▁RIVET▁AND▁A▁SHORT▁WIRE▁AND▁A▁LONG▁WIRE▁WE▁ALSO▁HAD▁A▁BATTERY▁AND▁WE▁FIRST▁TRIED▁TO▁ALL▁OF▁EM▁TOGETHER▁AND▁WE▁HAD▁TO▁WRAP▁AROUND▁THE▁LIKE▁OR▁TIMES▁TO▁THE

2024-10-27 18:02:49,955 (asr_inference:509) INFO: speech length: 270608
2024-10-27 18:03:01,056 (beam_search:428) INFO: decoder input length: 210
2024-10-27 18:03:01,056 (beam_search:429) INFO: max output length: 210
2024-10-27 18:03:01,056 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:02,552 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:02,552 (beam_search:476) INFO:  -4.50 * 1.0 =  -4.50 for ctc
2024-10-27 18:03:02,552 (beam_search:479) INFO: total log probability: -4.50
2024-10-27 18:03:02,552 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:03:02,552 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:02,552 (beam_search:483) INFO: best hypo: ▁WE▁UM▁HAD▁A▁SWITCH▁AND▁LITTLE▁WASHERS▁TOO▁AND▁WE▁JUST▁PUT▁EM▁IN▁AND▁WRAPPED▁IT▁AROUND▁AND▁CONNECTED▁ALL▁OF▁THEM▁TOGETHER▁TO▁TRY▁TO▁MAKE▁IT▁WORK▁AND▁PICK▁UP▁THE▁WASHERS

2024-10-27 18:03:02,555 (asr_inference:509) INFO: speech length: 52848
2024-10-27 18:03:04,483 (beam_search:428) INFO: decoder input length: 40
2024-10-27 18:03:04,484 (beam_search:429) INFO: max output length: 40
2024-10-27 18:03:04,484 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:04,510 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:04,510 (beam_search:476) INFO:  -0.52 * 1.0 =  -0.52 for ctc
2024-10-27 18:03:04,510 (beam_search:479) INFO: total log probability: -0.52
2024-10-27 18:03:04,510 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:03:04,511 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:04,511 (beam_search:483) INFO: best hypo: ▁UH▁WAIT▁WHAT

2024-10-27 18:03:04,513 (asr_inference:509) INFO: speech length: 164224
2024-10-27 18:03:10,909 (beam_search:428) INFO: decoder input length: 127
2024-10-27 18:03:10,909 (beam_search:429) INFO: max output length: 127
2024-10-27 18:03:10,909 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:11,465 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:11,465 (beam_search:476) INFO:  -2.45 * 1.0 =  -2.45 for ctc
2024-10-27 18:03:11,465 (beam_search:479) INFO: total log probability: -2.45
2024-10-27 18:03:11,465 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:03:11,465 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:11,465 (beam_search:483) INFO: best hypo: ▁THEY▁PICKED▁UP▁THE▁WASHERS▁FROM▁THE▁LITTLE▁RIVET▁AND▁WE▁HAVE▁TO▁HAVE▁A▁LOT▁OF▁POWER▁SO▁WED▁IT▁A

2024-10-27 18:03:11,468 (asr_inference:509) INFO: speech length: 112544
2024-10-27 18:03:15,664 (beam_search:428) INFO: decoder input length: 87
2024-10-27 18:03:15,664 (beam_search:429) INFO: max output length: 87
2024-10-27 18:03:15,664 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:15,877 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:15,878 (beam_search:476) INFO:  -0.80 * 1.0 =  -0.80 for ctc
2024-10-27 18:03:15,878 (beam_search:479) INFO: total log probability: -0.80
2024-10-27 18:03:15,878 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:03:15,878 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:15,878 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁BATTERYVE▁THE▁UM▁UM▁THE▁POWER▁GO▁TO▁THE▁THE▁RIVET

2024-10-27 18:03:15,880 (asr_inference:509) INFO: speech length: 143328
2024-10-27 18:03:21,404 (beam_search:428) INFO: decoder input length: 111
2024-10-27 18:03:21,405 (beam_search:429) INFO: max output length: 111
2024-10-27 18:03:21,405 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:21,972 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:21,972 (beam_search:476) INFO:  -2.83 * 1.0 =  -2.83 for ctc
2024-10-27 18:03:21,972 (beam_search:479) INFO: total log probability: -2.83
2024-10-27 18:03:21,972 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:03:21,972 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:21,973 (beam_search:483) INFO: best hypo: ▁WE▁ALSO▁HAD▁TO▁CONNECT▁THE▁WIRES▁TO▁THE▁THAT'S▁HOW▁IT▁CAN▁TURN▁OFF▁AND▁ON▁SO▁THEN▁IT▁DOESN'T▁BACK▁TO▁THE▁BATTERY

2024-10-27 18:03:21,975 (asr_inference:509) INFO: speech length: 36880
2024-10-27 18:03:23,459 (beam_search:428) INFO: decoder input length: 28
2024-10-27 18:03:23,459 (beam_search:429) INFO: max output length: 28
2024-10-27 18:03:23,459 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:23,474 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:23,474 (beam_search:476) INFO:  -0.54 * 1.0 =  -0.54 for ctc
2024-10-27 18:03:23,474 (beam_search:479) INFO: total log probability: -0.54
2024-10-27 18:03:23,474 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:03:23,474 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:23,474 (beam_search:483) INFO: best hypo: 'S

2024-10-27 18:03:23,476 (asr_inference:509) INFO: speech length: 101280
2024-10-27 18:03:27,035 (beam_search:428) INFO: decoder input length: 78
2024-10-27 18:03:27,036 (beam_search:429) INFO: max output length: 78
2024-10-27 18:03:27,036 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:27,220 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:27,220 (beam_search:476) INFO:  -3.36 * 1.0 =  -3.36 for ctc
2024-10-27 18:03:27,220 (beam_search:479) INFO: total log probability: -3.36
2024-10-27 18:03:27,220 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:03:27,220 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:27,220 (beam_search:483) INFO: best hypo: ▁WHEN▁THE▁SWITCH▁IS▁CLOSED▁THE▁THE▁LITTLE▁RIVET▁PICKS▁UP▁THE▁WASHERS

2024-10-27 18:03:27,223 (asr_inference:509) INFO: speech length: 101280
2024-10-27 18:03:30,948 (beam_search:428) INFO: decoder input length: 78
2024-10-27 18:03:30,948 (beam_search:429) INFO: max output length: 78
2024-10-27 18:03:30,948 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:31,166 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:31,166 (beam_search:476) INFO:  -1.22 * 1.0 =  -1.22 for ctc
2024-10-27 18:03:31,166 (beam_search:479) INFO: total log probability: -1.22
2024-10-27 18:03:31,166 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:03:31,166 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:31,166 (beam_search:483) INFO: best hypo: ▁THE▁RIVET▁IS▁UM▁A▁MAGNET▁KIND▁OF▁SO▁THEN▁IT▁PICKS▁UP▁THE▁WASHERS

2024-10-27 18:03:31,169 (asr_inference:509) INFO: speech length: 122256
2024-10-27 18:03:35,578 (beam_search:428) INFO: decoder input length: 95
2024-10-27 18:03:35,578 (beam_search:429) INFO: max output length: 95
2024-10-27 18:03:35,578 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:35,915 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:35,915 (beam_search:476) INFO:  -1.82 * 1.0 =  -1.82 for ctc
2024-10-27 18:03:35,915 (beam_search:479) INFO: total log probability: -1.82
2024-10-27 18:03:35,915 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:03:35,915 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:35,915 (beam_search:483) INFO: best hypo: ▁WELL▁I'M▁NOTE▁SURE▁I▁THINK▁IT'S▁A▁METAL▁BUT▁I'M▁NOTE▁SURE

2024-10-27 18:03:35,917 (asr_inference:509) INFO: speech length: 33648
2024-10-27 18:03:37,240 (beam_search:428) INFO: decoder input length: 25
2024-10-27 18:03:37,240 (beam_search:429) INFO: max output length: 25
2024-10-27 18:03:37,240 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:37,261 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:37,261 (beam_search:476) INFO:  -0.61 * 1.0 =  -0.61 for ctc
2024-10-27 18:03:37,261 (beam_search:479) INFO: total log probability: -0.61
2024-10-27 18:03:37,261 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:03:37,261 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:37,261 (beam_search:483) INFO: best hypo: ▁THEY▁WON'T

2024-10-27 18:03:37,263 (asr_inference:509) INFO: speech length: 69168
2024-10-27 18:03:39,830 (beam_search:428) INFO: decoder input length: 53
2024-10-27 18:03:39,830 (beam_search:429) INFO: max output length: 53
2024-10-27 18:03:39,830 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:39,897 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:39,897 (beam_search:476) INFO:  -1.18 * 1.0 =  -1.18 for ctc
2024-10-27 18:03:39,897 (beam_search:479) INFO: total log probability: -1.18
2024-10-27 18:03:39,897 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:03:39,897 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:39,897 (beam_search:483) INFO: best hypo: ▁THE▁RIVET▁ISING▁UP▁THE▁WASHERS

2024-10-27 18:03:39,900 (asr_inference:509) INFO: speech length: 86896
2024-10-27 18:03:43,105 (beam_search:428) INFO: decoder input length: 67
2024-10-27 18:03:43,105 (beam_search:429) INFO: max output length: 67
2024-10-27 18:03:43,105 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:43,247 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:43,247 (beam_search:476) INFO:  -1.94 * 1.0 =  -1.94 for ctc
2024-10-27 18:03:43,247 (beam_search:479) INFO: total log probability: -1.94
2024-10-27 18:03:43,248 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:03:43,248 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:43,248 (beam_search:483) INFO: best hypo: ▁WE▁THAT▁WOULD▁BECAUSE▁THERE'S▁NO▁POWER▁GOING▁INTO▁IT

2024-10-27 18:03:43,251 (asr_inference:509) INFO: speech length: 164224
2024-10-27 18:03:49,413 (beam_search:428) INFO: decoder input length: 127
2024-10-27 18:03:49,413 (beam_search:429) INFO: max output length: 127
2024-10-27 18:03:49,413 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:49,870 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:49,870 (beam_search:476) INFO:  -1.96 * 1.0 =  -1.96 for ctc
2024-10-27 18:03:49,870 (beam_search:479) INFO: total log probability: -1.96
2024-10-27 18:03:49,870 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:03:49,870 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:49,870 (beam_search:483) INFO: best hypo: ▁I▁THINK▁THAT▁WHEN▁THE▁POWER▁GOES▁TO▁THE▁METAL▁IT▁IT▁JUST▁SOMETHING▁TO▁GET▁THE▁TO▁GO▁UP

2024-10-27 18:03:49,872 (asr_inference:509) INFO: speech length: 41664
2024-10-27 18:03:51,496 (beam_search:428) INFO: decoder input length: 32
2024-10-27 18:03:51,496 (beam_search:429) INFO: max output length: 32
2024-10-27 18:03:51,497 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:51,521 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:51,521 (beam_search:476) INFO:  -1.02 * 1.0 =  -1.02 for ctc
2024-10-27 18:03:51,521 (beam_search:479) INFO: total log probability: -1.02
2024-10-27 18:03:51,521 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:03:51,521 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:51,521 (beam_search:483) INFO: best hypo: ▁WE▁USE▁JUST▁MAGNETS

2024-10-27 18:03:51,523 (asr_inference:509) INFO: speech length: 107792
2024-10-27 18:03:55,338 (beam_search:428) INFO: decoder input length: 83
2024-10-27 18:03:55,338 (beam_search:429) INFO: max output length: 83
2024-10-27 18:03:55,338 (beam_search:430) INFO: min output length: 0
2024-10-27 18:03:55,615 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:03:55,616 (beam_search:476) INFO:  -2.19 * 1.0 =  -2.19 for ctc
2024-10-27 18:03:55,616 (beam_search:479) INFO: total log probability: -2.19
2024-10-27 18:03:55,616 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:03:55,616 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:03:55,616 (beam_search:483) INFO: best hypo: ▁I▁DON'T▁THINK▁SO▁BUT▁I'▁NOT▁SURE▁IF▁I'M▁IT'S▁JUST▁IN

2024-10-27 18:03:55,618 (asr_inference:509) INFO: speech length: 164400
2024-10-27 18:04:01,845 (beam_search:428) INFO: decoder input length: 127
2024-10-27 18:04:01,845 (beam_search:429) INFO: max output length: 127
2024-10-27 18:04:01,845 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:02,371 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:02,371 (beam_search:476) INFO:  -2.37 * 1.0 =  -2.37 for ctc
2024-10-27 18:04:02,373 (beam_search:479) INFO: total log probability: -2.37
2024-10-27 18:04:02,374 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:04:02,374 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:02,374 (beam_search:483) INFO: best hypo: ▁THE▁ELECTRICITY▁ALWAYS▁OUT▁THE▁NEGATIVE▁SIDE▁AND▁COMES▁BACK▁IN▁THE▁UM▁POSITIVE▁SO▁IT▁GOES▁LIKE▁IN▁A▁BACK▁TO▁IT

2024-10-27 18:04:02,376 (asr_inference:509) INFO: speech length: 25504
2024-10-27 18:04:03,468 (beam_search:428) INFO: decoder input length: 19
2024-10-27 18:04:03,468 (beam_search:429) INFO: max output length: 19
2024-10-27 18:04:03,468 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:03,487 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:03,487 (beam_search:476) INFO:  -0.04 * 1.0 =  -0.04 for ctc
2024-10-27 18:04:03,487 (beam_search:479) INFO: total log probability: -0.04
2024-10-27 18:04:03,487 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 18:04:03,487 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:03,487 (beam_search:483) INFO: best hypo: ▁UM▁WHAT▁DID▁YOU▁SAY

2024-10-27 18:04:03,489 (asr_inference:509) INFO: speech length: 133664
2024-10-27 18:04:08,535 (beam_search:428) INFO: decoder input length: 103
2024-10-27 18:04:08,535 (beam_search:429) INFO: max output length: 103
2024-10-27 18:04:08,535 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:08,944 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:08,944 (beam_search:476) INFO:  -2.95 * 1.0 =  -2.95 for ctc
2024-10-27 18:04:08,944 (beam_search:479) INFO: total log probability: -2.95
2024-10-27 18:04:08,944 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:04:08,944 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:08,944 (beam_search:483) INFO: best hypo: ▁WELL▁IF▁UM▁THE▁DOESN'T▁IT▁WON'T▁AND▁IF▁A▁SWITCH▁ISN'T▁IT▁WON'T

2024-10-27 18:04:08,946 (asr_inference:509) INFO: speech length: 96400
2024-10-27 18:04:12,494 (beam_search:428) INFO: decoder input length: 74
2024-10-27 18:04:12,494 (beam_search:429) INFO: max output length: 74
2024-10-27 18:04:12,494 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:12,572 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:12,572 (beam_search:476) INFO:  -0.72 * 1.0 =  -0.72 for ctc
2024-10-27 18:04:12,572 (beam_search:479) INFO: total log probability: -0.72
2024-10-27 18:04:12,572 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:04:12,572 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:12,572 (beam_search:483) INFO: best hypo: ▁UM▁WE▁ALSO▁JUST▁I

2024-10-27 18:04:12,576 (asr_inference:509) INFO: speech length: 117488
2024-10-27 18:04:16,856 (beam_search:428) INFO: decoder input length: 91
2024-10-27 18:04:16,857 (beam_search:429) INFO: max output length: 91
2024-10-27 18:04:16,857 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:17,057 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:17,057 (beam_search:476) INFO:  -1.67 * 1.0 =  -1.67 for ctc
2024-10-27 18:04:17,057 (beam_search:479) INFO: total log probability: -1.67
2024-10-27 18:04:17,057 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:04:17,057 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:17,057 (beam_search:483) INFO: best hypo: ▁WE▁WE▁JUST▁TRIED▁UM▁THE▁WIRE▁TOUCHING▁INSTEAD▁OF▁IT▁AROUND▁AND

2024-10-27 18:04:17,059 (asr_inference:509) INFO: speech length: 135264
2024-10-27 18:04:22,002 (beam_search:428) INFO: decoder input length: 105
2024-10-27 18:04:22,002 (beam_search:429) INFO: max output length: 105
2024-10-27 18:04:22,002 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:22,325 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:22,325 (beam_search:476) INFO:  -2.34 * 1.0 =  -2.34 for ctc
2024-10-27 18:04:22,325 (beam_search:479) INFO: total log probability: -2.34
2024-10-27 18:04:22,326 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:04:22,326 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:22,326 (beam_search:483) INFO: best hypo: 'S▁NOT▁ENOUGH▁ELECTRICITY▁GOING▁TO▁THE▁RIVET▁SO▁THEN▁IT▁CAN'T▁UP▁THE▁WASHERS

2024-10-27 18:04:22,328 (asr_inference:509) INFO: speech length: 120576
2024-10-27 18:04:26,699 (beam_search:428) INFO: decoder input length: 93
2024-10-27 18:04:26,699 (beam_search:429) INFO: max output length: 93
2024-10-27 18:04:26,699 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:26,851 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:26,851 (beam_search:476) INFO:  -2.60 * 1.0 =  -2.60 for ctc
2024-10-27 18:04:26,851 (beam_search:479) INFO: total log probability: -2.60
2024-10-27 18:04:26,851 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:04:26,851 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:26,852 (beam_search:483) INFO: best hypo: ▁WE▁FOR▁FROM▁AND▁US▁IT▁AROUND▁THE▁RIVET

2024-10-27 18:04:26,854 (asr_inference:509) INFO: speech length: 83504
2024-10-27 18:04:29,913 (beam_search:428) INFO: decoder input length: 64
2024-10-27 18:04:29,913 (beam_search:429) INFO: max output length: 64
2024-10-27 18:04:29,913 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:29,980 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:29,980 (beam_search:476) INFO:  -0.98 * 1.0 =  -0.98 for ctc
2024-10-27 18:04:29,980 (beam_search:479) INFO: total log probability: -0.98
2024-10-27 18:04:29,980 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:04:29,980 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:29,980 (beam_search:483) INFO: best hypo: ▁THE▁UM▁THEY▁TO▁THE▁RIVET

2024-10-27 18:04:29,982 (asr_inference:509) INFO: speech length: 68400
2024-10-27 18:04:32,495 (beam_search:428) INFO: decoder input length: 52
2024-10-27 18:04:32,495 (beam_search:429) INFO: max output length: 52
2024-10-27 18:04:32,495 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:32,565 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:32,566 (beam_search:476) INFO:  -1.39 * 1.0 =  -1.39 for ctc
2024-10-27 18:04:32,566 (beam_search:479) INFO: total log probability: -1.39
2024-10-27 18:04:32,566 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:04:32,566 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:32,566 (beam_search:483) INFO: best hypo: ▁WE'VE▁BEEN▁WITH▁UM▁ON▁ELECTRICITY

2024-10-27 18:04:32,568 (asr_inference:509) INFO: speech length: 233664
2024-10-27 18:04:42,269 (beam_search:428) INFO: decoder input length: 182
2024-10-27 18:04:42,269 (beam_search:429) INFO: max output length: 182
2024-10-27 18:04:42,269 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:42,906 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:42,907 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 18:04:42,907 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 18:04:42,907 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:04:42,907 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:42,907 (beam_search:483) INFO: best hypo: ▁ELECTRICITY▁IS▁WHAT▁WE▁USED▁TO▁HAVE▁TO▁HAVE▁LIGHT▁WITH▁AND▁AND▁WE▁USE▁A▁LOT▁OF▁ELECTRICITY

2024-10-27 18:04:42,909 (asr_inference:509) INFO: speech length: 22848
2024-10-27 18:04:44,017 (beam_search:428) INFO: decoder input length: 17
2024-10-27 18:04:44,017 (beam_search:429) INFO: max output length: 17
2024-10-27 18:04:44,017 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:44,027 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:44,027 (beam_search:476) INFO:  -1.05 * 1.0 =  -1.05 for ctc
2024-10-27 18:04:44,028 (beam_search:479) INFO: total log probability: -1.05
2024-10-27 18:04:44,028 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:04:44,028 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:44,028 (beam_search:483) INFO: best hypo: ▁UM▁WE

2024-10-27 18:04:44,030 (asr_inference:509) INFO: speech length: 97504
2024-10-27 18:04:47,543 (beam_search:428) INFO: decoder input length: 75
2024-10-27 18:04:47,543 (beam_search:429) INFO: max output length: 75
2024-10-27 18:04:47,543 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:47,724 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:47,724 (beam_search:476) INFO:  -2.45 * 1.0 =  -2.45 for ctc
2024-10-27 18:04:47,724 (beam_search:479) INFO: total log probability: -2.45
2024-10-27 18:04:47,724 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:04:47,724 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:47,725 (beam_search:483) INFO: best hypo: ▁WE'VE▁BEEN▁AND▁THE▁CIRCUITS▁MAKE▁THE▁MAKE▁THE▁LIGHTS▁ON

2024-10-27 18:04:47,727 (asr_inference:509) INFO: speech length: 88976
2024-10-27 18:04:51,022 (beam_search:428) INFO: decoder input length: 69
2024-10-27 18:04:51,022 (beam_search:429) INFO: max output length: 69
2024-10-27 18:04:51,022 (beam_search:430) INFO: min output length: 0
2024-10-27 18:04:51,108 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:04:51,108 (beam_search:476) INFO:  -1.31 * 1.0 =  -1.31 for ctc
2024-10-27 18:04:51,108 (beam_search:479) INFO: total log probability: -1.31
2024-10-27 18:04:51,108 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:04:51,108 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:04:51,108 (beam_search:483) INFO: best hypo: ▁YEAH▁A▁CIRCUIT▁IS▁A▁OF▁ELECTRICITY

2024-10-27 18:04:51,110 (asr_inference:509) INFO: speech length: 308512
2024-10-27 18:05:04,572 (beam_search:428) INFO: decoder input length: 240
2024-10-27 18:05:04,572 (beam_search:429) INFO: max output length: 240
2024-10-27 18:05:04,572 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:06,005 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:06,005 (beam_search:476) INFO:  -3.96 * 1.0 =  -3.96 for ctc
2024-10-27 18:05:06,005 (beam_search:479) INFO: total log probability: -3.96
2024-10-27 18:05:06,005 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:05:06,005 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:06,005 (beam_search:483) INFO: best hypo: ▁WELL▁UM▁A▁CIRCUIT▁IS▁IS▁UM▁WELL▁IF▁YOU▁WANT▁TO▁LIGHT▁SOMETHING▁OR▁MAKE▁SOMETHING▁THEN▁A▁CIRCUIT▁IS▁THE▁THAT▁THE▁IN▁AND▁IT'S▁LIKE▁A

2024-10-27 18:05:06,008 (asr_inference:509) INFO: speech length: 16000
2024-10-27 18:05:06,804 (beam_search:428) INFO: decoder input length: 11
2024-10-27 18:05:06,805 (beam_search:429) INFO: max output length: 11
2024-10-27 18:05:06,805 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:06,814 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:06,815 (beam_search:476) INFO:  -0.46 * 1.0 =  -0.46 for ctc
2024-10-27 18:05:06,815 (beam_search:479) INFO: total log probability: -0.46
2024-10-27 18:05:06,815 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:05:06,815 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:06,815 (beam_search:483) INFO: best hypo: ▁IT▁IS

2024-10-27 18:05:06,817 (asr_inference:509) INFO: speech length: 205856
2024-10-27 18:05:14,779 (beam_search:428) INFO: decoder input length: 160
2024-10-27 18:05:14,779 (beam_search:429) INFO: max output length: 160
2024-10-27 18:05:14,779 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:15,329 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:15,329 (beam_search:476) INFO:  -1.18 * 1.0 =  -1.18 for ctc
2024-10-27 18:05:15,329 (beam_search:479) INFO: total log probability: -1.18
2024-10-27 18:05:15,329 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:05:15,329 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:15,329 (beam_search:483) INFO: best hypo: ▁UM▁WELL▁THE▁IT'S▁THE▁WAY▁THAT▁THE▁IT'S▁THE▁WAY▁THAT▁THE▁THAT▁THE

2024-10-27 18:05:15,332 (asr_inference:509) INFO: speech length: 24176
2024-10-27 18:05:16,493 (beam_search:428) INFO: decoder input length: 18
2024-10-27 18:05:16,493 (beam_search:429) INFO: max output length: 18
2024-10-27 18:05:16,493 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:16,502 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:16,502 (beam_search:476) INFO:  -1.46 * 1.0 =  -1.46 for ctc
2024-10-27 18:05:16,502 (beam_search:479) INFO: total log probability: -1.46
2024-10-27 18:05:16,502 (beam_search:480) INFO: normalized log probability: -0.49
2024-10-27 18:05:16,502 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:16,502 (beam_search:483) INFO: best hypo: ▁THE

2024-10-27 18:05:16,505 (asr_inference:509) INFO: speech length: 213552
2024-10-27 18:05:25,004 (beam_search:428) INFO: decoder input length: 166
2024-10-27 18:05:25,005 (beam_search:429) INFO: max output length: 166
2024-10-27 18:05:25,005 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:25,960 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:25,960 (beam_search:476) INFO:  -2.20 * 1.0 =  -2.20 for ctc
2024-10-27 18:05:25,960 (beam_search:479) INFO: total log probability: -2.20
2024-10-27 18:05:25,960 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:05:25,960 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:25,960 (beam_search:483) INFO: best hypo: ▁TO▁HAVE▁A▁CIRCUIT▁YOU▁HAVE▁TO▁GET▁YOU▁HAVE▁TO▁HAVE▁WIRES▁AND▁THEY▁HAVE▁TO▁BE▁CONNECTED▁TO▁A▁BATTERY▁AND▁AN▁ELECTRICITY▁WHICH▁COULD▁BE▁A▁LIGHT▁BULB

2024-10-27 18:05:25,963 (asr_inference:509) INFO: speech length: 236224
2024-10-27 18:05:35,723 (beam_search:428) INFO: decoder input length: 184
2024-10-27 18:05:35,723 (beam_search:429) INFO: max output length: 184
2024-10-27 18:05:35,723 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:36,854 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:36,854 (beam_search:476) INFO:  -3.39 * 1.0 =  -3.39 for ctc
2024-10-27 18:05:36,854 (beam_search:479) INFO: total log probability: -3.39
2024-10-27 18:05:36,854 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:05:36,854 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:36,854 (beam_search:483) INFO: best hypo: ▁WELL▁IF▁YOU▁LIGHT▁SOMETHING▁THEN▁YOU▁HAVE▁TO▁HAVE▁WIRES▁THAT▁ARE▁CONNECTED▁TO▁A▁BATTERY▁AND▁THE▁WIRES▁ALSO▁HAVE▁TO▁BE▁CONNECTED▁TO▁THE▁LIGHT▁BULB▁IN▁IT▁TO▁LIGHT

2024-10-27 18:05:36,857 (asr_inference:509) INFO: speech length: 12080
2024-10-27 18:05:37,502 (beam_search:428) INFO: decoder input length: 8
2024-10-27 18:05:37,502 (beam_search:429) INFO: max output length: 8
2024-10-27 18:05:37,502 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:37,509 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:37,509 (beam_search:476) INFO:  -0.26 * 1.0 =  -0.26 for ctc
2024-10-27 18:05:37,509 (beam_search:479) INFO: total log probability: -0.26
2024-10-27 18:05:37,509 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:05:37,509 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:37,509 (beam_search:483) INFO: best hypo: 'S

2024-10-27 18:05:37,511 (asr_inference:509) INFO: speech length: 97536
2024-10-27 18:05:40,894 (beam_search:428) INFO: decoder input length: 75
2024-10-27 18:05:40,894 (beam_search:429) INFO: max output length: 75
2024-10-27 18:05:40,894 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:41,013 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:41,013 (beam_search:476) INFO:  -0.17 * 1.0 =  -0.17 for ctc
2024-10-27 18:05:41,013 (beam_search:479) INFO: total log probability: -0.17
2024-10-27 18:05:41,013 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:05:41,013 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:41,014 (beam_search:483) INFO: best hypo: ▁UM▁THEN▁THE▁THEN▁THE▁LIGHT▁BULB▁LIGHTS

2024-10-27 18:05:41,016 (asr_inference:509) INFO: speech length: 24560
2024-10-27 18:05:42,092 (beam_search:428) INFO: decoder input length: 18
2024-10-27 18:05:42,092 (beam_search:429) INFO: max output length: 18
2024-10-27 18:05:42,092 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:42,103 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:42,103 (beam_search:476) INFO:  -0.50 * 1.0 =  -0.50 for ctc
2024-10-27 18:05:42,103 (beam_search:479) INFO: total log probability: -0.50
2024-10-27 18:05:42,103 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:05:42,103 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:42,103 (beam_search:483) INFO: best hypo: S

2024-10-27 18:05:42,106 (asr_inference:509) INFO: speech length: 173936
2024-10-27 18:05:48,707 (beam_search:428) INFO: decoder input length: 135
2024-10-27 18:05:48,708 (beam_search:429) INFO: max output length: 135
2024-10-27 18:05:48,708 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:49,257 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:49,257 (beam_search:476) INFO:  -2.13 * 1.0 =  -2.13 for ctc
2024-10-27 18:05:49,257 (beam_search:479) INFO: total log probability: -2.13
2024-10-27 18:05:49,258 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:05:49,258 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:49,258 (beam_search:483) INFO: best hypo: ▁SO▁YOU▁HAVE▁A▁CIRCUIT▁AND▁THE▁WIRES▁ARE▁CONNECTED▁TO▁THE▁BATTERY▁AND▁THE▁LIGHT▁BULB▁THEN▁THE▁LIGHT▁BULB▁WILL▁LIGHT

2024-10-27 18:05:49,260 (asr_inference:509) INFO: speech length: 13392
2024-10-27 18:05:49,973 (beam_search:428) INFO: decoder input length: 9
2024-10-27 18:05:49,973 (beam_search:429) INFO: max output length: 9
2024-10-27 18:05:49,973 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:49,980 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:49,980 (beam_search:476) INFO:  -0.20 * 1.0 =  -0.20 for ctc
2024-10-27 18:05:49,981 (beam_search:479) INFO: total log probability: -0.20
2024-10-27 18:05:49,981 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:05:49,981 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:49,981 (beam_search:483) INFO: best hypo: ▁YOU▁CAN

2024-10-27 18:05:49,983 (asr_inference:509) INFO: speech length: 160160
2024-10-27 18:05:56,123 (beam_search:428) INFO: decoder input length: 124
2024-10-27 18:05:56,123 (beam_search:429) INFO: max output length: 124
2024-10-27 18:05:56,123 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:56,505 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:56,505 (beam_search:476) INFO:  -1.30 * 1.0 =  -1.30 for ctc
2024-10-27 18:05:56,505 (beam_search:479) INFO: total log probability: -1.30
2024-10-27 18:05:56,505 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:05:56,505 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:56,506 (beam_search:483) INFO: best hypo: ▁WELL▁YOU▁CAN▁LIGHT▁A▁LIGHT▁BULB▁YOU▁CAN▁LIGHT▁MORE▁LIGHT▁BULBS▁IF▁YOU▁MAKE▁THE▁CIRCUIT

2024-10-27 18:05:56,508 (asr_inference:509) INFO: speech length: 88768
2024-10-27 18:05:59,809 (beam_search:428) INFO: decoder input length: 68
2024-10-27 18:05:59,809 (beam_search:429) INFO: max output length: 68
2024-10-27 18:05:59,809 (beam_search:430) INFO: min output length: 0
2024-10-27 18:05:59,890 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:05:59,891 (beam_search:476) INFO:  -1.34 * 1.0 =  -1.34 for ctc
2024-10-27 18:05:59,891 (beam_search:479) INFO: total log probability: -1.34
2024-10-27 18:05:59,891 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:05:59,891 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:05:59,891 (beam_search:483) INFO: best hypo: ▁MAKE▁THE▁THEN▁YOU▁WILL▁NEED▁MORE

2024-10-27 18:05:59,894 (asr_inference:509) INFO: speech length: 212528
2024-10-27 18:06:08,463 (beam_search:428) INFO: decoder input length: 165
2024-10-27 18:06:08,463 (beam_search:429) INFO: max output length: 165
2024-10-27 18:06:08,463 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:08,854 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:08,854 (beam_search:476) INFO:  -2.77 * 1.0 =  -2.77 for ctc
2024-10-27 18:06:08,854 (beam_search:479) INFO: total log probability: -2.77
2024-10-27 18:06:08,854 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:06:08,854 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:08,854 (beam_search:483) INFO: best hypo: ▁MORE▁YOU▁NEED▁MORE▁WIRES▁AND▁MORE▁LIGHT▁BULBS▁AND▁THAT'S

2024-10-27 18:06:08,857 (asr_inference:509) INFO: speech length: 82560
2024-10-27 18:06:11,961 (beam_search:428) INFO: decoder input length: 63
2024-10-27 18:06:11,961 (beam_search:429) INFO: max output length: 63
2024-10-27 18:06:11,961 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:12,059 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:12,059 (beam_search:476) INFO:  -0.72 * 1.0 =  -0.72 for ctc
2024-10-27 18:06:12,059 (beam_search:479) INFO: total log probability: -0.72
2024-10-27 18:06:12,059 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:06:12,059 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:12,059 (beam_search:483) INFO: best hypo: ▁NO▁BUT▁I▁THINK▁THAT▁IT▁WILL▁LIKE▁THAT

2024-10-27 18:06:12,062 (asr_inference:509) INFO: speech length: 21216
2024-10-27 18:06:13,034 (beam_search:428) INFO: decoder input length: 16
2024-10-27 18:06:13,034 (beam_search:429) INFO: max output length: 16
2024-10-27 18:06:13,034 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:13,047 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:13,047 (beam_search:476) INFO:  -1.04 * 1.0 =  -1.04 for ctc
2024-10-27 18:06:13,047 (beam_search:479) INFO: total log probability: -1.04
2024-10-27 18:06:13,047 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:06:13,047 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:13,047 (beam_search:483) INFO: best hypo: ▁IT▁WILL▁WORK

2024-10-27 18:06:13,050 (asr_inference:509) INFO: speech length: 25888
2024-10-27 18:06:14,129 (beam_search:428) INFO: decoder input length: 19
2024-10-27 18:06:14,130 (beam_search:429) INFO: max output length: 19
2024-10-27 18:06:14,130 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:14,149 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:14,150 (beam_search:476) INFO:  -2.36 * 1.0 =  -2.36 for ctc
2024-10-27 18:06:14,150 (beam_search:479) INFO: total log probability: -2.36
2024-10-27 18:06:14,150 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 18:06:14,150 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:14,150 (beam_search:483) INFO: best hypo: ▁WE▁DID▁WITH▁LIKE▁THIS

2024-10-27 18:06:14,153 (asr_inference:509) INFO: speech length: 17328
2024-10-27 18:06:14,973 (beam_search:428) INFO: decoder input length: 13
2024-10-27 18:06:14,973 (beam_search:429) INFO: max output length: 13
2024-10-27 18:06:14,973 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:14,984 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:14,984 (beam_search:476) INFO:  -0.55 * 1.0 =  -0.55 for ctc
2024-10-27 18:06:14,984 (beam_search:479) INFO: total log probability: -0.55
2024-10-27 18:06:14,984 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:06:14,984 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:14,984 (beam_search:483) INFO: best hypo: ▁THAT'S

2024-10-27 18:06:14,987 (asr_inference:509) INFO: speech length: 131424
2024-10-27 18:06:19,830 (beam_search:428) INFO: decoder input length: 102
2024-10-27 18:06:19,831 (beam_search:429) INFO: max output length: 102
2024-10-27 18:06:19,831 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:20,041 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:20,041 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 18:06:20,041 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 18:06:20,041 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:06:20,042 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:20,042 (beam_search:483) INFO: best hypo: ▁THE▁WAY▁BECAUSE▁THE▁ENERGY▁IS▁STILL▁GOING▁OUT▁OF▁THE▁NEGATIVE

2024-10-27 18:06:20,044 (asr_inference:509) INFO: speech length: 72544
2024-10-27 18:06:22,687 (beam_search:428) INFO: decoder input length: 56
2024-10-27 18:06:22,687 (beam_search:429) INFO: max output length: 56
2024-10-27 18:06:22,687 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:22,790 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:22,791 (beam_search:476) INFO:  -1.03 * 1.0 =  -1.03 for ctc
2024-10-27 18:06:22,791 (beam_search:479) INFO: total log probability: -1.03
2024-10-27 18:06:22,791 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:06:22,791 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:22,791 (beam_search:483) INFO: best hypo: ▁YOU▁CAN▁STILL▁MAKE▁UM▁A▁CIRCUIT▁WITH▁TWO▁BATTERIES

2024-10-27 18:06:22,794 (asr_inference:509) INFO: speech length: 376896
2024-10-27 18:06:39,941 (beam_search:428) INFO: decoder input length: 293
2024-10-27 18:06:39,941 (beam_search:429) INFO: max output length: 293
2024-10-27 18:06:39,941 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:42,230 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:42,230 (beam_search:476) INFO:  -5.16 * 1.0 =  -5.16 for ctc
2024-10-27 18:06:42,230 (beam_search:479) INFO: total log probability: -5.16
2024-10-27 18:06:42,230 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:06:42,230 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:42,230 (beam_search:483) INFO: best hypo: ▁YOU▁JUST▁HAVE▁TO▁GET▁MORE▁WIRES▁AND▁THE▁WIRES▁TO▁THE▁BATTERIES▁AND▁THEN▁YOU▁CAN▁STILL▁MAKE▁A▁CIRCUIT▁BUT▁IF▁YOU▁DO▁IT▁LIKE▁THIS▁THEN▁THE▁LIGHT▁BULB▁WILL▁BE▁WILL▁HAVE▁WILL▁BE▁EVEN▁MORE

2024-10-27 18:06:42,233 (asr_inference:509) INFO: speech length: 49680
2024-10-27 18:06:44,146 (beam_search:428) INFO: decoder input length: 38
2024-10-27 18:06:44,146 (beam_search:429) INFO: max output length: 38
2024-10-27 18:06:44,146 (beam_search:430) INFO: min output length: 0
2024-10-27 18:06:44,203 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:06:44,204 (beam_search:476) INFO:  -1.91 * 1.0 =  -1.91 for ctc
2024-10-27 18:06:44,204 (beam_search:479) INFO: total log probability: -1.91
2024-10-27 18:06:44,204 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:06:44,204 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:06:44,204 (beam_search:483) INFO: best hypo: ▁IT▁WILL▁BE▁EVEN▁BRIGHTER▁THAN▁WITH▁ONE▁BATTERY

2024-10-27 18:06:44,207 (asr_inference:509) INFO: speech length: 449008
2024-10-27 18:07:07,192 (beam_search:428) INFO: decoder input length: 350
2024-10-27 18:07:07,192 (beam_search:429) INFO: max output length: 350
2024-10-27 18:07:07,192 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:10,757 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:10,757 (beam_search:476) INFO:  -6.32 * 1.0 =  -6.32 for ctc
2024-10-27 18:07:10,757 (beam_search:479) INFO: total log probability: -6.32
2024-10-27 18:07:10,757 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:07:10,757 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:10,757 (beam_search:483) INFO: best hypo: ▁WELL▁YOU▁JUST▁A▁WIRE▁IN▁WELL▁YOU▁NEED▁YOU▁NEED▁A▁TO▁WELL▁YOU▁TO▁HAVE▁A▁PLACE▁TO▁THE▁YOU▁NEED▁TO▁HAVE▁A▁BATTERY▁AND▁THEN▁YOU▁PUT▁THE▁BATTERY▁IN▁AND▁THEN▁YOU▁GET▁THE▁OTHER▁WIRES▁AND▁THEN▁YOU▁THE▁WIRES▁TO▁TO▁THE▁BATTERY

2024-10-27 18:07:10,761 (asr_inference:509) INFO: speech length: 90368
2024-10-27 18:07:13,980 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:07:13,980 (beam_search:429) INFO: max output length: 70
2024-10-27 18:07:13,980 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:14,093 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:14,094 (beam_search:476) INFO:  -1.19 * 1.0 =  -1.19 for ctc
2024-10-27 18:07:14,094 (beam_search:479) INFO: total log probability: -1.19
2024-10-27 18:07:14,094 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:07:14,094 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:14,094 (beam_search:483) INFO: best hypo: ▁THE▁BATTERY▁IS▁BEING▁USED▁TO▁LIGHT▁TO▁LIGHT

2024-10-27 18:07:14,096 (asr_inference:509) INFO: speech length: 171504
2024-10-27 18:07:20,822 (beam_search:428) INFO: decoder input length: 133
2024-10-27 18:07:20,822 (beam_search:429) INFO: max output length: 133
2024-10-27 18:07:20,822 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:21,181 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:21,181 (beam_search:476) INFO:  -1.64 * 1.0 =  -1.64 for ctc
2024-10-27 18:07:21,181 (beam_search:479) INFO: total log probability: -1.64
2024-10-27 18:07:21,181 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:07:21,181 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:21,181 (beam_search:483) INFO: best hypo: ▁IN▁THIS▁THERE▁ARE▁TWO▁THAT▁ARE▁BEING▁USED▁TO▁UM▁LIGHT▁TWO▁LIGHT▁BULBS

2024-10-27 18:07:21,184 (asr_inference:509) INFO: speech length: 156192
2024-10-27 18:07:27,100 (beam_search:428) INFO: decoder input length: 121
2024-10-27 18:07:27,100 (beam_search:429) INFO: max output length: 121
2024-10-27 18:07:27,100 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:27,630 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:27,631 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 18:07:27,631 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 18:07:27,631 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:07:27,631 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:27,631 (beam_search:483) INFO: best hypo: ▁YOU▁ON▁THE▁BATTERY▁AND▁THE▁LIGHT▁BULBS▁DON'T▁LIGHT▁THAT'S▁BECAUSE▁THE▁BATTERY▁IS▁IN▁IN▁THE▁WRONG▁PLACE

2024-10-27 18:07:27,633 (asr_inference:509) INFO: speech length: 78240
2024-10-27 18:07:30,493 (beam_search:428) INFO: decoder input length: 60
2024-10-27 18:07:30,493 (beam_search:429) INFO: max output length: 60
2024-10-27 18:07:30,493 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:30,547 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:30,547 (beam_search:476) INFO:  -1.79 * 1.0 =  -1.79 for ctc
2024-10-27 18:07:30,547 (beam_search:479) INFO: total log probability: -1.79
2024-10-27 18:07:30,547 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:07:30,547 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:30,547 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁WILL▁BE▁IN

2024-10-27 18:07:30,550 (asr_inference:509) INFO: speech length: 100560
2024-10-27 18:07:34,156 (beam_search:428) INFO: decoder input length: 78
2024-10-27 18:07:34,157 (beam_search:429) INFO: max output length: 78
2024-10-27 18:07:34,157 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:34,325 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:34,325 (beam_search:476) INFO:  -2.44 * 1.0 =  -2.44 for ctc
2024-10-27 18:07:34,325 (beam_search:479) INFO: total log probability: -2.44
2024-10-27 18:07:34,325 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:07:34,325 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:34,325 (beam_search:483) INFO: best hypo: ▁IN▁THE▁WRONG▁IN▁THE▁WRONG▁PLACE▁BECAUSE▁ONE▁SIDE▁SHOULD▁BE

2024-10-27 18:07:34,328 (asr_inference:509) INFO: speech length: 113248
2024-10-27 18:07:38,528 (beam_search:428) INFO: decoder input length: 87
2024-10-27 18:07:38,528 (beam_search:429) INFO: max output length: 87
2024-10-27 18:07:38,528 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:38,663 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:38,663 (beam_search:476) INFO:  -1.48 * 1.0 =  -1.48 for ctc
2024-10-27 18:07:38,663 (beam_search:479) INFO: total log probability: -1.48
2024-10-27 18:07:38,663 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:07:38,663 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:38,663 (beam_search:483) INFO: best hypo: ▁AND▁THE▁OTHER▁SIDE▁BE▁ON▁THE▁POSITIVE▁SIDE

2024-10-27 18:07:38,665 (asr_inference:509) INFO: speech length: 141136
2024-10-27 18:07:44,023 (beam_search:428) INFO: decoder input length: 109
2024-10-27 18:07:44,024 (beam_search:429) INFO: max output length: 109
2024-10-27 18:07:44,024 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:44,212 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:44,212 (beam_search:476) INFO:  -2.47 * 1.0 =  -2.47 for ctc
2024-10-27 18:07:44,212 (beam_search:479) INFO: total log probability: -2.47
2024-10-27 18:07:44,212 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:07:44,212 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:44,212 (beam_search:483) INFO: best hypo: ▁THEY▁HAVE▁TO▁BE▁IN▁THE▁RIGHT▁FOR▁IT▁TO

2024-10-27 18:07:44,215 (asr_inference:509) INFO: speech length: 93376
2024-10-27 18:07:47,533 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:07:47,534 (beam_search:429) INFO: max output length: 72
2024-10-27 18:07:47,534 (beam_search:430) INFO: min output length: 0
2024-10-27 18:07:47,612 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:07:47,612 (beam_search:476) INFO:  -1.20 * 1.0 =  -1.20 for ctc
2024-10-27 18:07:47,612 (beam_search:479) INFO: total log probability: -1.20
2024-10-27 18:07:47,612 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:07:47,612 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:07:47,613 (beam_search:483) INFO: best hypo: ▁UM▁IT▁WOULD▁HAVE▁NO▁DIFFERENCE

2024-10-27 18:07:47,615 (asr_inference:509) INFO: speech length: 288512
2024-10-27 18:08:00,279 (beam_search:428) INFO: decoder input length: 224
2024-10-27 18:08:00,280 (beam_search:429) INFO: max output length: 224
2024-10-27 18:08:00,280 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:01,614 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:01,615 (beam_search:476) INFO:  -6.14 * 1.0 =  -6.14 for ctc
2024-10-27 18:08:01,615 (beam_search:479) INFO: total log probability: -6.14
2024-10-27 18:08:01,615 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:08:01,615 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:01,615 (beam_search:483) INFO: best hypo: ▁IN▁THIS'S▁ANOTHER▁CIRCUIT▁AND▁ALL▁THE▁ENERGY▁IS▁STILL▁RUNNING▁OUT▁OF▁THE▁NEGATIVE▁SIDE▁OF▁THE▁BATTERY▁AND▁THE▁LIGHTS▁ARE▁BRIGHTER▁BECAUSE▁THEY'RE▁TWO

2024-10-27 18:08:01,617 (asr_inference:509) INFO: speech length: 229520
2024-10-27 18:08:10,937 (beam_search:428) INFO: decoder input length: 178
2024-10-27 18:08:10,937 (beam_search:429) INFO: max output length: 178
2024-10-27 18:08:10,937 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:11,745 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:11,745 (beam_search:476) INFO:  -3.04 * 1.0 =  -3.04 for ctc
2024-10-27 18:08:11,745 (beam_search:479) INFO: total log probability: -3.04
2024-10-27 18:08:11,745 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:08:11,745 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:11,745 (beam_search:483) INFO: best hypo: ▁UM▁IF▁THE▁PLUS▁SIDE▁IS▁I▁MEAN▁THE▁NEGATIVE▁SIDE▁IS▁TOUCHING▁THE▁OTHER▁NEGATIVE▁SIDE▁THEN▁UM▁THEN▁IT▁WON'T▁BECAUSE

2024-10-27 18:08:11,748 (asr_inference:509) INFO: speech length: 193744
2024-10-27 18:08:19,399 (beam_search:428) INFO: decoder input length: 150
2024-10-27 18:08:19,399 (beam_search:429) INFO: max output length: 150
2024-10-27 18:08:19,399 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:19,845 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:19,845 (beam_search:476) INFO:  -3.26 * 1.0 =  -3.26 for ctc
2024-10-27 18:08:19,845 (beam_search:479) INFO: total log probability: -3.26
2024-10-27 18:08:19,845 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:08:19,845 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:19,845 (beam_search:483) INFO: best hypo: ▁IT▁BECAUSE▁THE▁CAUSE▁THE▁POSITIVE▁SIDE▁HAS▁TO▁BE▁ALWAYS▁HAS▁TO▁BEING▁ENERGY

2024-10-27 18:08:19,848 (asr_inference:509) INFO: speech length: 14832
2024-10-27 18:08:20,629 (beam_search:428) INFO: decoder input length: 11
2024-10-27 18:08:20,629 (beam_search:429) INFO: max output length: 11
2024-10-27 18:08:20,631 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:20,641 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:20,641 (beam_search:476) INFO:  -0.41 * 1.0 =  -0.41 for ctc
2024-10-27 18:08:20,641 (beam_search:479) INFO: total log probability: -0.41
2024-10-27 18:08:20,641 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:08:20,641 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:20,641 (beam_search:483) INFO: best hypo: ▁THAT'S

2024-10-27 18:08:20,643 (asr_inference:509) INFO: speech length: 19920
2024-10-27 18:08:21,586 (beam_search:428) INFO: decoder input length: 15
2024-10-27 18:08:21,586 (beam_search:429) INFO: max output length: 15
2024-10-27 18:08:21,586 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:21,600 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:21,600 (beam_search:476) INFO:  -0.24 * 1.0 =  -0.24 for ctc
2024-10-27 18:08:21,600 (beam_search:479) INFO: total log probability: -0.24
2024-10-27 18:08:21,601 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 18:08:21,601 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:21,601 (beam_search:483) INFO: best hypo: ▁YEAH▁THAT'S

2024-10-27 18:08:21,603 (asr_inference:509) INFO: speech length: 161792
2024-10-27 18:08:27,835 (beam_search:428) INFO: decoder input length: 125
2024-10-27 18:08:27,835 (beam_search:429) INFO: max output length: 125
2024-10-27 18:08:27,835 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:28,253 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:28,253 (beam_search:476) INFO:  -1.26 * 1.0 =  -1.26 for ctc
2024-10-27 18:08:28,253 (beam_search:479) INFO: total log probability: -1.26
2024-10-27 18:08:28,253 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:08:28,253 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:28,253 (beam_search:483) INFO: best hypo: ▁UM▁NO▁ITN'T▁SHOW▁THE▁ENERGY▁GOING▁TO▁THE▁LIGHT▁BUT▁UM▁I▁TO▁SAY▁THAT▁THE

2024-10-27 18:08:28,255 (asr_inference:509) INFO: speech length: 43392
2024-10-27 18:08:30,060 (beam_search:428) INFO: decoder input length: 33
2024-10-27 18:08:30,061 (beam_search:429) INFO: max output length: 33
2024-10-27 18:08:30,061 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:30,088 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:30,088 (beam_search:476) INFO:  -1.86 * 1.0 =  -1.86 for ctc
2024-10-27 18:08:30,088 (beam_search:479) INFO: total log probability: -1.86
2024-10-27 18:08:30,088 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 18:08:30,088 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:30,088 (beam_search:483) INFO: best hypo: ▁YES▁THIS▁PROBABLY▁WOULD

2024-10-27 18:08:30,091 (asr_inference:509) INFO: speech length: 58720
2024-10-27 18:08:32,395 (beam_search:428) INFO: decoder input length: 45
2024-10-27 18:08:32,395 (beam_search:429) INFO: max output length: 45
2024-10-27 18:08:32,395 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:32,422 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:32,422 (beam_search:476) INFO:  -1.12 * 1.0 =  -1.12 for ctc
2024-10-27 18:08:32,422 (beam_search:479) INFO: total log probability: -1.12
2024-10-27 18:08:32,422 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:08:32,422 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:32,422 (beam_search:483) INFO: best hypo: ▁THIS▁WOULD▁WORK

2024-10-27 18:08:32,424 (asr_inference:509) INFO: speech length: 10448
2024-10-27 18:08:33,055 (beam_search:428) INFO: decoder input length: 7
2024-10-27 18:08:33,055 (beam_search:429) INFO: max output length: 7
2024-10-27 18:08:33,055 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:33,063 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:33,063 (beam_search:476) INFO:  -0.93 * 1.0 =  -0.93 for ctc
2024-10-27 18:08:33,063 (beam_search:479) INFO: total log probability: -0.93
2024-10-27 18:08:33,063 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:08:33,063 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:33,063 (beam_search:483) INFO: best hypo: ▁YOU'RE

2024-10-27 18:08:33,065 (asr_inference:509) INFO: speech length: 195072
2024-10-27 18:08:40,949 (beam_search:428) INFO: decoder input length: 151
2024-10-27 18:08:40,949 (beam_search:429) INFO: max output length: 151
2024-10-27 18:08:40,949 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:41,616 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:41,617 (beam_search:476) INFO:  -4.00 * 1.0 =  -4.00 for ctc
2024-10-27 18:08:41,617 (beam_search:479) INFO: total log probability: -4.00
2024-10-27 18:08:41,617 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:08:41,617 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:41,617 (beam_search:483) INFO: best hypo: ▁UM▁THAT▁IS▁TWO▁UM▁UM▁TWO▁MAGNETS▁IF▁YOU▁HAVE▁A▁POLE▁AND▁A▁THEN▁THIS▁MEANS▁THAT▁THE▁TWO▁MAGNETS▁WILL▁TO▁EACH

2024-10-27 18:08:41,620 (asr_inference:509) INFO: speech length: 200304
2024-10-27 18:08:49,362 (beam_search:428) INFO: decoder input length: 155
2024-10-27 18:08:49,362 (beam_search:429) INFO: max output length: 155
2024-10-27 18:08:49,362 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:50,065 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:50,065 (beam_search:476) INFO:  -5.85 * 1.0 =  -5.85 for ctc
2024-10-27 18:08:50,065 (beam_search:479) INFO: total log probability: -5.85
2024-10-27 18:08:50,065 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:08:50,065 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:50,065 (beam_search:483) INFO: best hypo: ▁THE▁MAGNETS▁ON▁THE▁RIGHT▁SIDE▁OF▁THIS▁ARE▁WILL▁STICK▁TO▁EACH▁OTHER▁OR▁TO▁EACH▁OTHER▁BECAUSE▁THE▁NORTH▁AND▁ARE▁EACH▁OTHER

2024-10-27 18:08:50,068 (asr_inference:509) INFO: speech length: 178864
2024-10-27 18:08:57,195 (beam_search:428) INFO: decoder input length: 139
2024-10-27 18:08:57,195 (beam_search:429) INFO: max output length: 139
2024-10-27 18:08:57,195 (beam_search:430) INFO: min output length: 0
2024-10-27 18:08:57,841 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:08:57,842 (beam_search:476) INFO:  -5.38 * 1.0 =  -5.38 for ctc
2024-10-27 18:08:57,842 (beam_search:479) INFO: total log probability: -5.38
2024-10-27 18:08:57,842 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:08:57,842 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:08:57,842 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁TWO▁AND▁SOUTH▁WERE▁EACH▁OTHER▁SO▁THAT▁MEANS▁AS▁I▁TOLD▁YOU▁THAT▁AND▁SOUTH▁TO▁EACH▁OTHER▁OR▁SO▁IT▁WILL▁ATTRACT

2024-10-27 18:08:57,844 (asr_inference:509) INFO: speech length: 172560
2024-10-27 18:09:04,724 (beam_search:428) INFO: decoder input length: 134
2024-10-27 18:09:04,724 (beam_search:429) INFO: max output length: 134
2024-10-27 18:09:04,724 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:05,361 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:05,361 (beam_search:476) INFO:  -2.58 * 1.0 =  -2.58 for ctc
2024-10-27 18:09:05,361 (beam_search:479) INFO: total log probability: -2.58
2024-10-27 18:09:05,362 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:09:05,362 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:05,362 (beam_search:483) INFO: best hypo: ▁WELL▁IN▁THE▁THERE▁A▁LOT▁OF▁THERE▁ARE▁A▁LOT▁OF▁OBJECTS▁AND▁THE▁MAGNET▁WILL▁ONLY▁STICK▁TO▁A▁OF▁THE▁TO▁A▁OF▁THE

2024-10-27 18:09:05,364 (asr_inference:509) INFO: speech length: 13696
2024-10-27 18:09:06,082 (beam_search:428) INFO: decoder input length: 10
2024-10-27 18:09:06,082 (beam_search:429) INFO: max output length: 10
2024-10-27 18:09:06,082 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:06,088 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:06,088 (beam_search:476) INFO:  -1.65 * 1.0 =  -1.65 for ctc
2024-10-27 18:09:06,088 (beam_search:479) INFO: total log probability: -1.65
2024-10-27 18:09:06,088 (beam_search:480) INFO: normalized log probability: -0.55
2024-10-27 18:09:06,088 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:06,088 (beam_search:483) INFO: best hypo: ▁AND

2024-10-27 18:09:06,091 (asr_inference:509) INFO: speech length: 196160
2024-10-27 18:09:13,650 (beam_search:428) INFO: decoder input length: 152
2024-10-27 18:09:13,650 (beam_search:429) INFO: max output length: 152
2024-10-27 18:09:13,650 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:14,340 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:14,340 (beam_search:476) INFO:  -4.17 * 1.0 =  -4.17 for ctc
2024-10-27 18:09:14,340 (beam_search:479) INFO: total log probability: -4.17
2024-10-27 18:09:14,340 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:09:14,340 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:14,340 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁MA▁I▁THINK▁THAT▁THE▁MAGNET▁WILL▁STICK▁TO▁THE▁WIRE▁SCREEN▁THE▁THE▁NAIL▁THE▁PAPER▁THE▁WASHER▁AND▁THAT'S▁ALL

2024-10-27 18:09:14,343 (asr_inference:509) INFO: speech length: 146272
2024-10-27 18:09:19,932 (beam_search:428) INFO: decoder input length: 113
2024-10-27 18:09:19,932 (beam_search:429) INFO: max output length: 113
2024-10-27 18:09:19,932 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:20,261 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:20,261 (beam_search:476) INFO:  -1.58 * 1.0 =  -1.58 for ctc
2024-10-27 18:09:20,261 (beam_search:479) INFO: total log probability: -1.58
2024-10-27 18:09:20,261 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:09:20,261 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:20,261 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁STEEL▁WIRE▁SCREEN▁IS▁TO▁THE▁MAGNET▁AND▁THAT'S▁WHAT'S

2024-10-27 18:09:20,263 (asr_inference:509) INFO: speech length: 22656
2024-10-27 18:09:21,313 (beam_search:428) INFO: decoder input length: 17
2024-10-27 18:09:21,313 (beam_search:429) INFO: max output length: 17
2024-10-27 18:09:21,313 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:21,325 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:21,325 (beam_search:476) INFO:  -0.47 * 1.0 =  -0.47 for ctc
2024-10-27 18:09:21,325 (beam_search:479) INFO: total log probability: -0.47
2024-10-27 18:09:21,325 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:09:21,325 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:21,325 (beam_search:483) INFO: best hypo: ▁THAT▁IS

2024-10-27 18:09:21,327 (asr_inference:509) INFO: speech length: 91376
2024-10-27 18:09:24,659 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:09:24,659 (beam_search:429) INFO: max output length: 70
2024-10-27 18:09:24,659 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:24,767 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:24,767 (beam_search:476) INFO:  -2.50 * 1.0 =  -2.50 for ctc
2024-10-27 18:09:24,767 (beam_search:479) INFO: total log probability: -2.50
2024-10-27 18:09:24,767 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:09:24,767 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:24,768 (beam_search:483) INFO: best hypo: ▁IT▁TO▁ANY▁STEEL▁TO▁ANY▁STEEL▁OR▁MADE

2024-10-27 18:09:24,770 (asr_inference:509) INFO: speech length: 15040
2024-10-27 18:09:25,520 (beam_search:428) INFO: decoder input length: 11
2024-10-27 18:09:25,520 (beam_search:429) INFO: max output length: 11
2024-10-27 18:09:25,520 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:25,526 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:25,526 (beam_search:476) INFO:  -0.14 * 1.0 =  -0.14 for ctc
2024-10-27 18:09:25,526 (beam_search:479) INFO: total log probability: -0.14
2024-10-27 18:09:25,526 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:09:25,526 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:25,526 (beam_search:483) INFO: best hypo: ▁IT

2024-10-27 18:09:25,528 (asr_inference:509) INFO: speech length: 6864
2024-10-27 18:09:26,067 (beam_search:428) INFO: decoder input length: 4
2024-10-27 18:09:26,067 (beam_search:429) INFO: max output length: 4
2024-10-27 18:09:26,067 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:26,071 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:26,071 (beam_search:476) INFO:  -0.82 * 1.0 =  -0.82 for ctc
2024-10-27 18:09:26,071 (beam_search:479) INFO: total log probability: -0.82
2024-10-27 18:09:26,072 (beam_search:480) INFO: normalized log probability: -0.41
2024-10-27 18:09:26,072 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:26,072 (beam_search:483) INFO: best hypo: 

2024-10-27 18:09:26,074 (asr_inference:509) INFO: speech length: 319792
2024-10-27 18:09:40,388 (beam_search:428) INFO: decoder input length: 249
2024-10-27 18:09:40,389 (beam_search:429) INFO: max output length: 249
2024-10-27 18:09:40,389 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:41,892 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:41,892 (beam_search:476) INFO:  -6.91 * 1.0 =  -6.91 for ctc
2024-10-27 18:09:41,892 (beam_search:479) INFO: total log probability: -6.91
2024-10-27 18:09:41,892 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:09:41,892 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:41,892 (beam_search:483) INFO: best hypo: ▁UM▁WELL▁BE▁OTHER▁THINGS▁IN▁THE▁PICTURE▁ARE▁A▁SPONGE▁A▁A▁A▁PAPER▁UM▁TWO▁A▁STICK▁TWO▁NAILS▁UM▁A▁OF▁A▁STRAW▁A▁SPONGE▁AND▁THAT'S

2024-10-27 18:09:41,895 (asr_inference:509) INFO: speech length: 46912
2024-10-27 18:09:43,705 (beam_search:428) INFO: decoder input length: 36
2024-10-27 18:09:43,705 (beam_search:429) INFO: max output length: 36
2024-10-27 18:09:43,705 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:43,768 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:43,768 (beam_search:476) INFO:  -1.82 * 1.0 =  -1.82 for ctc
2024-10-27 18:09:43,768 (beam_search:479) INFO: total log probability: -1.82
2024-10-27 18:09:43,768 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:09:43,768 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:43,768 (beam_search:483) INFO: best hypo: ▁NO▁BECAUSE▁THE▁SPONGE▁IS▁NOT▁MADE▁OF▁STEEL▁OR▁IRON

2024-10-27 18:09:43,770 (asr_inference:509) INFO: speech length: 156512
2024-10-27 18:09:49,712 (beam_search:428) INFO: decoder input length: 121
2024-10-27 18:09:49,712 (beam_search:429) INFO: max output length: 121
2024-10-27 18:09:49,712 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:50,051 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:50,051 (beam_search:476) INFO:  -1.79 * 1.0 =  -1.79 for ctc
2024-10-27 18:09:50,051 (beam_search:479) INFO: total log probability: -1.79
2024-10-27 18:09:50,051 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:09:50,051 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:50,051 (beam_search:483) INFO: best hypo: ▁UM▁SO▁THE▁SO▁THE▁MAGNET▁IS▁TO▁A▁NAIL▁AND▁THE▁NAIL▁IS▁TO▁A

2024-10-27 18:09:50,054 (asr_inference:509) INFO: speech length: 57344
2024-10-27 18:09:52,230 (beam_search:428) INFO: decoder input length: 44
2024-10-27 18:09:52,230 (beam_search:429) INFO: max output length: 44
2024-10-27 18:09:52,230 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:52,290 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:52,290 (beam_search:476) INFO:  -1.38 * 1.0 =  -1.38 for ctc
2024-10-27 18:09:52,290 (beam_search:479) INFO: total log probability: -1.38
2024-10-27 18:09:52,290 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:09:52,290 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:52,290 (beam_search:483) INFO: best hypo: ▁UM▁I▁FORGOT▁THE▁OF▁WHAT'S

2024-10-27 18:09:52,292 (asr_inference:509) INFO: speech length: 167024
2024-10-27 18:09:58,703 (beam_search:428) INFO: decoder input length: 129
2024-10-27 18:09:58,704 (beam_search:429) INFO: max output length: 129
2024-10-27 18:09:58,704 (beam_search:430) INFO: min output length: 0
2024-10-27 18:09:59,024 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:09:59,025 (beam_search:476) INFO:  -1.83 * 1.0 =  -1.83 for ctc
2024-10-27 18:09:59,025 (beam_search:479) INFO: total log probability: -1.83
2024-10-27 18:09:59,025 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:09:59,025 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:09:59,025 (beam_search:483) INFO: best hypo: ▁UM▁THE▁NAIL▁THE▁MAGNET▁AND▁THE▁AND▁THE▁PAPER▁CLIP▁ARE▁FORCE▁OF

2024-10-27 18:09:59,027 (asr_inference:509) INFO: speech length: 110000
2024-10-27 18:10:03,023 (beam_search:428) INFO: decoder input length: 85
2024-10-27 18:10:03,023 (beam_search:429) INFO: max output length: 85
2024-10-27 18:10:03,023 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:03,262 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:03,262 (beam_search:476) INFO:  -1.91 * 1.0 =  -1.91 for ctc
2024-10-27 18:10:03,262 (beam_search:479) INFO: total log probability: -1.91
2024-10-27 18:10:03,262 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:10:03,262 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:03,262 (beam_search:483) INFO: best hypo: ▁UM▁YES▁IT▁IS▁BECAUSE▁I▁BELIEVE▁THAT▁A▁PAPER▁IS▁MADE▁OF▁IRON▁OR▁STEEL

2024-10-27 18:10:03,265 (asr_inference:509) INFO: speech length: 47280
2024-10-27 18:10:05,076 (beam_search:428) INFO: decoder input length: 36
2024-10-27 18:10:05,076 (beam_search:429) INFO: max output length: 36
2024-10-27 18:10:05,076 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:05,133 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:05,133 (beam_search:476) INFO:  -0.82 * 1.0 =  -0.82 for ctc
2024-10-27 18:10:05,133 (beam_search:479) INFO: total log probability: -0.82
2024-10-27 18:10:05,133 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:10:05,133 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:05,133 (beam_search:483) INFO: best hypo: ▁IT▁IS▁BECAUSE▁IT▁IS▁MADE▁OF▁IRON▁OR▁STEEL

2024-10-27 18:10:05,135 (asr_inference:509) INFO: speech length: 410800
2024-10-27 18:10:26,754 (beam_search:428) INFO: decoder input length: 320
2024-10-27 18:10:26,754 (beam_search:429) INFO: max output length: 320
2024-10-27 18:10:26,754 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:29,010 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:29,011 (beam_search:476) INFO:  -6.47 * 1.0 =  -6.47 for ctc
2024-10-27 18:10:29,011 (beam_search:479) INFO: total log probability: -6.47
2024-10-27 18:10:29,011 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:10:29,011 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:29,011 (beam_search:483) INFO: best hypo: ▁AS▁YOU▁CAN▁SEE▁THE▁THE▁FORCE▁OF▁THE▁OF▁IS▁UM▁IS▁IN▁THIS▁PICTURE▁BECAUSE▁UM▁THE▁FORCE▁OF▁OF▁TWO▁IS▁GOING▁THROUGH▁THE▁UM▁THE▁THIN▁PIECE▁OF▁PAPER▁OR▁THE▁OBJECT

2024-10-27 18:10:29,014 (asr_inference:509) INFO: speech length: 110576
2024-10-27 18:10:33,030 (beam_search:428) INFO: decoder input length: 85
2024-10-27 18:10:33,031 (beam_search:429) INFO: max output length: 85
2024-10-27 18:10:33,031 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:33,201 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:33,201 (beam_search:476) INFO:  -1.86 * 1.0 =  -1.86 for ctc
2024-10-27 18:10:33,201 (beam_search:479) INFO: total log probability: -1.86
2024-10-27 18:10:33,201 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:10:33,201 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:33,201 (beam_search:483) INFO: best hypo: ▁WELL▁A▁MAGNET▁IS▁MADE▁OF▁ANYTHING▁IS▁MADE▁OF▁OR▁STEEL

2024-10-27 18:10:33,204 (asr_inference:509) INFO: speech length: 167264
2024-10-27 18:10:39,746 (beam_search:428) INFO: decoder input length: 130
2024-10-27 18:10:39,746 (beam_search:429) INFO: max output length: 130
2024-10-27 18:10:39,746 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:40,272 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:40,272 (beam_search:476) INFO:  -3.93 * 1.0 =  -3.93 for ctc
2024-10-27 18:10:40,272 (beam_search:479) INFO: total log probability: -3.93
2024-10-27 18:10:40,272 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:10:40,272 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:40,272 (beam_search:483) INFO: best hypo: ▁AND▁IT▁I▁MEAN▁A▁MAGNET▁ONLY▁TO▁A▁MAGNET▁ONLY▁STICKS▁TO▁UM▁ANYTHING▁THAT'S▁MADE▁OF▁IRON▁OR▁STEEL

2024-10-27 18:10:40,275 (asr_inference:509) INFO: speech length: 191232
2024-10-27 18:10:48,029 (beam_search:428) INFO: decoder input length: 148
2024-10-27 18:10:48,029 (beam_search:429) INFO: max output length: 148
2024-10-27 18:10:48,029 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:48,519 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:48,519 (beam_search:476) INFO:  -2.90 * 1.0 =  -2.90 for ctc
2024-10-27 18:10:48,519 (beam_search:479) INFO: total log probability: -2.90
2024-10-27 18:10:48,519 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:10:48,519 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:48,519 (beam_search:483) INFO: best hypo: ▁WELL▁UM▁IT▁CAN▁STICK▁TO▁STEEL▁AND▁BUT▁SOME▁IT▁CAN▁NOT▁STICK▁TO▁SO▁IT▁IS

2024-10-27 18:10:48,522 (asr_inference:509) INFO: speech length: 15424
2024-10-27 18:10:49,406 (beam_search:428) INFO: decoder input length: 11
2024-10-27 18:10:49,406 (beam_search:429) INFO: max output length: 11
2024-10-27 18:10:49,406 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:49,414 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:49,414 (beam_search:476) INFO:  -0.18 * 1.0 =  -0.18 for ctc
2024-10-27 18:10:49,414 (beam_search:479) INFO: total log probability: -0.18
2024-10-27 18:10:49,415 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 18:10:49,415 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:49,415 (beam_search:483) INFO: best hypo: ▁THEY▁WOULD

2024-10-27 18:10:49,417 (asr_inference:509) INFO: speech length: 107152
2024-10-27 18:10:53,310 (beam_search:428) INFO: decoder input length: 83
2024-10-27 18:10:53,310 (beam_search:429) INFO: max output length: 83
2024-10-27 18:10:53,310 (beam_search:430) INFO: min output length: 0
2024-10-27 18:10:53,444 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:10:53,444 (beam_search:476) INFO:  -1.70 * 1.0 =  -1.70 for ctc
2024-10-27 18:10:53,444 (beam_search:479) INFO: total log probability: -1.70
2024-10-27 18:10:53,444 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:10:53,444 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:10:53,444 (beam_search:483) INFO: best hypo: ▁NO▁NOT▁EXACTLY▁BUT▁WE▁DID▁TO▁THIS

2024-10-27 18:10:53,446 (asr_inference:509) INFO: speech length: 249120
2024-10-27 18:11:03,700 (beam_search:428) INFO: decoder input length: 194
2024-10-27 18:11:03,701 (beam_search:429) INFO: max output length: 194
2024-10-27 18:11:03,701 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:04,363 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:04,363 (beam_search:476) INFO:  -3.86 * 1.0 =  -3.86 for ctc
2024-10-27 18:11:04,363 (beam_search:479) INFO: total log probability: -3.86
2024-10-27 18:11:04,363 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:11:04,363 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:04,364 (beam_search:483) INFO: best hypo: ▁THE▁MAGNET▁IS▁THESE▁TWO▁MAGNETS▁HAVE▁A▁OF▁AND▁IT▁HAS▁TO▁BE▁OF▁FOR▁THE▁TWO

2024-10-27 18:11:04,367 (asr_inference:509) INFO: speech length: 17984
2024-10-27 18:11:05,337 (beam_search:428) INFO: decoder input length: 13
2024-10-27 18:11:05,337 (beam_search:429) INFO: max output length: 13
2024-10-27 18:11:05,337 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:05,346 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:05,346 (beam_search:476) INFO:  -0.06 * 1.0 =  -0.06 for ctc
2024-10-27 18:11:05,346 (beam_search:479) INFO: total log probability: -0.06
2024-10-27 18:11:05,346 (beam_search:480) INFO: normalized log probability: -0.01
2024-10-27 18:11:05,346 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:05,346 (beam_search:483) INFO: best hypo: ▁THEY▁ARE

2024-10-27 18:11:05,349 (asr_inference:509) INFO: speech length: 85056
2024-10-27 18:11:08,512 (beam_search:428) INFO: decoder input length: 65
2024-10-27 18:11:08,512 (beam_search:429) INFO: max output length: 65
2024-10-27 18:11:08,512 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:08,588 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:08,588 (beam_search:476) INFO:  -1.37 * 1.0 =  -1.37 for ctc
2024-10-27 18:11:08,588 (beam_search:479) INFO: total log probability: -1.37
2024-10-27 18:11:08,588 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:11:08,588 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:08,588 (beam_search:483) INFO: best hypo: ▁UM▁THE▁MAGNETS▁BOTH▁HAVE▁A▁OF

2024-10-27 18:11:08,590 (asr_inference:509) INFO: speech length: 21472
2024-10-27 18:11:09,531 (beam_search:428) INFO: decoder input length: 16
2024-10-27 18:11:09,531 (beam_search:429) INFO: max output length: 16
2024-10-27 18:11:09,531 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:09,540 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:09,540 (beam_search:476) INFO:  -0.71 * 1.0 =  -0.71 for ctc
2024-10-27 18:11:09,540 (beam_search:479) INFO: total log probability: -0.71
2024-10-27 18:11:09,540 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:11:09,540 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:09,541 (beam_search:483) INFO: best hypo: ▁OKAY▁YOU

2024-10-27 18:11:09,542 (asr_inference:509) INFO: speech length: 56784
2024-10-27 18:11:11,768 (beam_search:428) INFO: decoder input length: 43
2024-10-27 18:11:11,768 (beam_search:429) INFO: max output length: 43
2024-10-27 18:11:11,768 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:11,806 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:11,806 (beam_search:476) INFO:  -1.10 * 1.0 =  -1.10 for ctc
2024-10-27 18:11:11,806 (beam_search:479) INFO: total log probability: -1.10
2024-10-27 18:11:11,806 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:11:11,806 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:11,806 (beam_search:483) INFO: best hypo: ▁WE▁HAVE▁BEEN▁ON▁MAGNETISM

2024-10-27 18:11:11,809 (asr_inference:509) INFO: speech length: 291664
2024-10-27 18:11:24,218 (beam_search:428) INFO: decoder input length: 227
2024-10-27 18:11:24,218 (beam_search:429) INFO: max output length: 227
2024-10-27 18:11:24,218 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:25,229 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:25,230 (beam_search:476) INFO:  -4.69 * 1.0 =  -4.69 for ctc
2024-10-27 18:11:25,230 (beam_search:479) INFO: total log probability: -4.69
2024-10-27 18:11:25,230 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:11:25,230 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:25,230 (beam_search:483) INFO: best hypo: ▁WE▁ARE▁ABOUT▁THE▁OF▁AND▁OF▁AND▁THE▁OF▁IS▁TWO▁TWO▁MAGNETS▁UM▁STICK▁TOGETHER▁AND▁THE▁OF▁IS▁TWO▁MAGNETS▁FROM

2024-10-27 18:11:25,233 (asr_inference:509) INFO: speech length: 167824
2024-10-27 18:11:31,706 (beam_search:428) INFO: decoder input length: 130
2024-10-27 18:11:31,706 (beam_search:429) INFO: max output length: 130
2024-10-27 18:11:31,706 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:32,037 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:32,037 (beam_search:476) INFO:  -2.56 * 1.0 =  -2.56 for ctc
2024-10-27 18:11:32,037 (beam_search:479) INFO: total log probability: -2.56
2024-10-27 18:11:32,037 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:11:32,037 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:32,037 (beam_search:483) INFO: best hypo: ▁UM▁THE▁IS▁WHERE▁THE▁THE▁TWO▁AND▁POLES▁ATTRACT▁TO▁EACH▁OTHER

2024-10-27 18:11:32,040 (asr_inference:509) INFO: speech length: 278848
2024-10-27 18:11:43,860 (beam_search:428) INFO: decoder input length: 217
2024-10-27 18:11:43,860 (beam_search:429) INFO: max output length: 217
2024-10-27 18:11:43,860 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:45,124 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:45,125 (beam_search:476) INFO:  -6.55 * 1.0 =  -6.55 for ctc
2024-10-27 18:11:45,125 (beam_search:479) INFO: total log probability: -6.55
2024-10-27 18:11:45,125 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:11:45,125 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:45,125 (beam_search:483) INFO: best hypo: ▁THERE▁ARE▁THINGS▁ON▁A▁MAGNET▁CALLED▁AND▁SOUTH▁AND▁WHEN▁THERE▁ARE▁TWO▁AND▁THEN▁THEY▁TO▁EACH▁OTHER▁BUT▁WHEN▁THERE▁ARE▁TWO▁OR▁TWO▁THEY▁FROM▁EACH▁OTHER

2024-10-27 18:11:45,128 (asr_inference:509) INFO: speech length: 50736
2024-10-27 18:11:47,095 (beam_search:428) INFO: decoder input length: 39
2024-10-27 18:11:47,095 (beam_search:429) INFO: max output length: 39
2024-10-27 18:11:47,095 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:47,144 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:47,144 (beam_search:476) INFO:  -2.10 * 1.0 =  -2.10 for ctc
2024-10-27 18:11:47,144 (beam_search:479) INFO: total log probability: -2.10
2024-10-27 18:11:47,144 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:11:47,144 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:47,144 (beam_search:483) INFO: best hypo: ▁UM▁NO▁WE▁HAVE▁NOT▁ON▁LIGHT

2024-10-27 18:11:47,146 (asr_inference:509) INFO: speech length: 21312
2024-10-27 18:11:48,096 (beam_search:428) INFO: decoder input length: 16
2024-10-27 18:11:48,096 (beam_search:429) INFO: max output length: 16
2024-10-27 18:11:48,096 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:48,110 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:48,110 (beam_search:476) INFO:  -0.11 * 1.0 =  -0.11 for ctc
2024-10-27 18:11:48,110 (beam_search:479) INFO: total log probability: -0.11
2024-10-27 18:11:48,111 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:11:48,111 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:48,111 (beam_search:483) INFO: best hypo: ▁GOOD▁HOW▁ARE▁YOU

2024-10-27 18:11:48,114 (asr_inference:509) INFO: speech length: 261056
2024-10-27 18:11:58,905 (beam_search:428) INFO: decoder input length: 203
2024-10-27 18:11:58,905 (beam_search:429) INFO: max output length: 203
2024-10-27 18:11:58,905 (beam_search:430) INFO: min output length: 0
2024-10-27 18:11:59,795 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:11:59,795 (beam_search:476) INFO:  -4.29 * 1.0 =  -4.29 for ctc
2024-10-27 18:11:59,795 (beam_search:479) INFO: total log probability: -4.29
2024-10-27 18:11:59,795 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:11:59,795 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:11:59,795 (beam_search:483) INFO: best hypo: ▁SO▁TODAY▁IN▁WE▁HAD▁TO▁WE▁HAD▁TO▁TO▁USE▁A▁TO▁UP▁TO▁PICK▁UP▁UM▁TO▁PICK▁UP▁SOME▁LITTLE

2024-10-27 18:11:59,798 (asr_inference:509) INFO: speech length: 154496
2024-10-27 18:12:05,593 (beam_search:428) INFO: decoder input length: 120
2024-10-27 18:12:05,593 (beam_search:429) INFO: max output length: 120
2024-10-27 18:12:05,593 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:05,983 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:05,983 (beam_search:476) INFO:  -2.51 * 1.0 =  -2.51 for ctc
2024-10-27 18:12:05,983 (beam_search:479) INFO: total log probability: -2.51
2024-10-27 18:12:05,983 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:12:05,983 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:05,983 (beam_search:483) INFO: best hypo: ▁UM▁YES▁THERE▁WAS▁THE▁MORE▁TIMES▁YOU▁THE▁WIRE▁AROUND▁THE▁THE▁MORE▁POWER▁IT▁WOULD▁HAVET

2024-10-27 18:12:05,986 (asr_inference:509) INFO: speech length: 26272
2024-10-27 18:12:07,123 (beam_search:428) INFO: decoder input length: 20
2024-10-27 18:12:07,123 (beam_search:429) INFO: max output length: 20
2024-10-27 18:12:07,123 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:07,132 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:07,132 (beam_search:476) INFO:  -0.66 * 1.0 =  -0.66 for ctc
2024-10-27 18:12:07,132 (beam_search:479) INFO: total log probability: -0.66
2024-10-27 18:12:07,132 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:12:07,132 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:07,132 (beam_search:483) INFO: best hypo: ▁WASHERS

2024-10-27 18:12:07,135 (asr_inference:509) INFO: speech length: 275472
2024-10-27 18:12:18,656 (beam_search:428) INFO: decoder input length: 214
2024-10-27 18:12:18,657 (beam_search:429) INFO: max output length: 214
2024-10-27 18:12:18,657 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:20,181 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:20,182 (beam_search:476) INFO:  -4.22 * 1.0 =  -4.22 for ctc
2024-10-27 18:12:20,182 (beam_search:479) INFO: total log probability: -4.22
2024-10-27 18:12:20,182 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:12:20,182 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:20,182 (beam_search:483) INFO: best hypo: ▁WELL▁IF▁YOUD▁WELL▁THE▁MORE▁YOUD▁THE▁WELL▁THE▁MORE▁YOUD▁THE▁WIRE▁ABOUT▁AROUND▁THE▁RIVET▁THEN▁IT▁WOULD▁THEN▁THERE▁WOULD▁BE▁SOME▁MORE▁POWER▁EACH▁TIME▁YOUD▁IT▁AROUND

2024-10-27 18:12:20,184 (asr_inference:509) INFO: speech length: 30384
2024-10-27 18:12:21,417 (beam_search:428) INFO: decoder input length: 23
2024-10-27 18:12:21,417 (beam_search:429) INFO: max output length: 23
2024-10-27 18:12:21,417 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:21,436 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:21,436 (beam_search:476) INFO:  -0.43 * 1.0 =  -0.43 for ctc
2024-10-27 18:12:21,436 (beam_search:479) INFO: total log probability: -0.43
2024-10-27 18:12:21,436 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:12:21,436 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:21,436 (beam_search:483) INFO: best hypo: ▁YEAH▁THAT'S

2024-10-27 18:12:21,438 (asr_inference:509) INFO: speech length: 322080
2024-10-27 18:12:35,812 (beam_search:428) INFO: decoder input length: 251
2024-10-27 18:12:35,812 (beam_search:429) INFO: max output length: 251
2024-10-27 18:12:35,812 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:36,824 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:36,824 (beam_search:476) INFO:  -6.25 * 1.0 =  -6.25 for ctc
2024-10-27 18:12:36,824 (beam_search:479) INFO: total log probability: -6.25
2024-10-27 18:12:36,824 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:12:36,824 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:36,825 (beam_search:483) INFO: best hypo: ▁WELL▁WE▁WERE▁LEARNING▁ABOUT▁AND▁ELECTRICITY▁WERE▁LEARNING▁ABOUT▁THE▁MAGNETISM▁ABOUT▁MAGNETISM▁AND▁WE▁ABOUT▁MAGNETISM▁AND▁AT▁THE▁TIME

2024-10-27 18:12:36,827 (asr_inference:509) INFO: speech length: 316960
2024-10-27 18:12:50,893 (beam_search:428) INFO: decoder input length: 247
2024-10-27 18:12:50,893 (beam_search:429) INFO: max output length: 247
2024-10-27 18:12:50,893 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:52,298 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:52,298 (beam_search:476) INFO:  -3.69 * 1.0 =  -3.69 for ctc
2024-10-27 18:12:52,298 (beam_search:479) INFO: total log probability: -3.69
2024-10-27 18:12:52,298 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:12:52,298 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:52,298 (beam_search:483) INFO: best hypo: ▁WE▁USED▁ELECTRICITY▁TO▁POWER▁THE▁WE▁USED▁THE▁ELECTRICITY▁TO▁POWER▁THE▁STEEL▁RIVET▁AND▁AND▁WE▁ALSO▁USED▁MAGNETISM▁TO▁MAKE▁THE▁STEEL▁RIVET▁PICK▁UP▁THE▁WASHERS

2024-10-27 18:12:52,300 (asr_inference:509) INFO: speech length: 103232
2024-10-27 18:12:56,041 (beam_search:428) INFO: decoder input length: 80
2024-10-27 18:12:56,041 (beam_search:429) INFO: max output length: 80
2024-10-27 18:12:56,041 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:56,180 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:56,180 (beam_search:476) INFO:  -3.00 * 1.0 =  -3.00 for ctc
2024-10-27 18:12:56,180 (beam_search:479) INFO: total log probability: -3.00
2024-10-27 18:12:56,180 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:12:56,180 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:56,180 (beam_search:483) INFO: best hypo: ▁WITHOUT▁THE▁RIVET▁CAN▁NOT▁BE▁USED▁A▁MAGNET

2024-10-27 18:12:56,182 (asr_inference:509) INFO: speech length: 41744
2024-10-27 18:12:57,847 (beam_search:428) INFO: decoder input length: 32
2024-10-27 18:12:57,847 (beam_search:429) INFO: max output length: 32
2024-10-27 18:12:57,847 (beam_search:430) INFO: min output length: 0
2024-10-27 18:12:57,881 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:12:57,882 (beam_search:476) INFO:  -1.41 * 1.0 =  -1.41 for ctc
2024-10-27 18:12:57,882 (beam_search:479) INFO: total log probability: -1.41
2024-10-27 18:12:57,882 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:12:57,882 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:12:57,882 (beam_search:483) INFO: best hypo: ▁AND▁THEY▁BOTH▁WORKED▁WITH▁OTHER

2024-10-27 18:12:57,884 (asr_inference:509) INFO: speech length: 462336
2024-10-27 18:13:22,096 (beam_search:428) INFO: decoder input length: 360
2024-10-27 18:13:22,096 (beam_search:429) INFO: max output length: 360
2024-10-27 18:13:22,096 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:24,945 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:24,945 (beam_search:476) INFO:  -6.79 * 1.0 =  -6.79 for ctc
2024-10-27 18:13:24,945 (beam_search:479) INFO: total log probability: -6.79
2024-10-27 18:13:24,945 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:13:24,945 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:24,946 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁SWITCH▁ON▁AND▁WHEN▁THE▁IS▁TURNED▁ON▁THE▁CAN▁FLOW▁AND▁AND▁THE▁WHEN▁THE▁WAS▁ON▁THEN▁THE▁THE▁SWITCH▁WAS▁ON▁THEN▁THE▁STEEL▁RIVET▁COULD▁COULD▁AND▁PICK▁UP▁SOME▁OF▁THE▁METAL▁WASHERS

2024-10-27 18:13:24,949 (asr_inference:509) INFO: speech length: 36928
2024-10-27 18:13:26,425 (beam_search:428) INFO: decoder input length: 28
2024-10-27 18:13:26,425 (beam_search:429) INFO: max output length: 28
2024-10-27 18:13:26,425 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:26,444 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:26,444 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 18:13:26,444 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 18:13:26,444 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:13:26,444 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:26,444 (beam_search:483) INFO: best hypo: ▁THAT▁IS▁RIGHT

2024-10-27 18:13:26,447 (asr_inference:509) INFO: speech length: 209504
2024-10-27 18:13:34,942 (beam_search:428) INFO: decoder input length: 163
2024-10-27 18:13:34,942 (beam_search:429) INFO: max output length: 163
2024-10-27 18:13:34,942 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:35,556 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:35,556 (beam_search:476) INFO:  -3.76 * 1.0 =  -3.76 for ctc
2024-10-27 18:13:35,556 (beam_search:479) INFO: total log probability: -3.76
2024-10-27 18:13:35,556 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:13:35,556 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:35,556 (beam_search:483) INFO: best hypo: ▁BECAUSE▁IT▁TO▁TO▁A▁MAGNET▁AND▁PICK▁UP▁THINGS▁AND▁I▁THINK▁THAT▁THAT'S▁WHY▁YOU▁AN

2024-10-27 18:13:35,559 (asr_inference:509) INFO: speech length: 40272
2024-10-27 18:13:37,078 (beam_search:428) INFO: decoder input length: 30
2024-10-27 18:13:37,078 (beam_search:429) INFO: max output length: 30
2024-10-27 18:13:37,078 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:37,091 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:37,091 (beam_search:476) INFO:  -0.52 * 1.0 =  -0.52 for ctc
2024-10-27 18:13:37,091 (beam_search:479) INFO: total log probability: -0.52
2024-10-27 18:13:37,091 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:13:37,091 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:37,091 (beam_search:483) INFO: best hypo: ▁IS

2024-10-27 18:13:37,093 (asr_inference:509) INFO: speech length: 15952
2024-10-27 18:13:37,884 (beam_search:428) INFO: decoder input length: 11
2024-10-27 18:13:37,884 (beam_search:429) INFO: max output length: 11
2024-10-27 18:13:37,884 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:37,894 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:37,894 (beam_search:476) INFO:  -0.57 * 1.0 =  -0.57 for ctc
2024-10-27 18:13:37,894 (beam_search:479) INFO: total log probability: -0.57
2024-10-27 18:13:37,894 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:13:37,894 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:37,894 (beam_search:483) INFO: best hypo: ▁YOU'RE

2024-10-27 18:13:37,896 (asr_inference:509) INFO: speech length: 11104
2024-10-27 18:13:38,557 (beam_search:428) INFO: decoder input length: 8
2024-10-27 18:13:38,557 (beam_search:429) INFO: max output length: 8
2024-10-27 18:13:38,557 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:38,567 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:38,567 (beam_search:476) INFO:  -0.46 * 1.0 =  -0.46 for ctc
2024-10-27 18:13:38,567 (beam_search:479) INFO: total log probability: -0.46
2024-10-27 18:13:38,567 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:13:38,567 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:38,567 (beam_search:483) INFO: best hypo: ▁I'M▁GOOD

2024-10-27 18:13:38,569 (asr_inference:509) INFO: speech length: 67600
2024-10-27 18:13:41,139 (beam_search:428) INFO: decoder input length: 52
2024-10-27 18:13:41,140 (beam_search:429) INFO: max output length: 52
2024-10-27 18:13:41,140 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:41,202 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:41,202 (beam_search:476) INFO:  -0.49 * 1.0 =  -0.49 for ctc
2024-10-27 18:13:41,202 (beam_search:479) INFO: total log probability: -0.49
2024-10-27 18:13:41,202 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:13:41,202 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:41,202 (beam_search:483) INFO: best hypo: ▁UM▁WE'RE▁THAT▁HOW▁MANY

2024-10-27 18:13:41,205 (asr_inference:509) INFO: speech length: 74800
2024-10-27 18:13:44,044 (beam_search:428) INFO: decoder input length: 57
2024-10-27 18:13:44,044 (beam_search:429) INFO: max output length: 57
2024-10-27 18:13:44,044 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:44,080 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:44,081 (beam_search:476) INFO:  -0.73 * 1.0 =  -0.73 for ctc
2024-10-27 18:13:44,081 (beam_search:479) INFO: total log probability: -0.73
2024-10-27 18:13:44,081 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:13:44,081 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:44,081 (beam_search:483) INFO: best hypo: ▁IT▁TO▁SEE

2024-10-27 18:13:44,084 (asr_inference:509) INFO: speech length: 61040
2024-10-27 18:13:46,378 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:13:46,378 (beam_search:429) INFO: max output length: 47
2024-10-27 18:13:46,378 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:46,445 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:46,446 (beam_search:476) INFO:  -1.82 * 1.0 =  -1.82 for ctc
2024-10-27 18:13:46,446 (beam_search:479) INFO: total log probability: -1.82
2024-10-27 18:13:46,446 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:13:46,446 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:46,446 (beam_search:483) INFO: best hypo: ▁WHEN▁THE▁MAGNETS▁DON'T▁STICK▁TOGETHER▁ANYMORE

2024-10-27 18:13:46,448 (asr_inference:509) INFO: speech length: 191760
2024-10-27 18:13:53,978 (beam_search:428) INFO: decoder input length: 149
2024-10-27 18:13:53,979 (beam_search:429) INFO: max output length: 149
2024-10-27 18:13:53,979 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:54,515 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:54,515 (beam_search:476) INFO:  -4.54 * 1.0 =  -4.54 for ctc
2024-10-27 18:13:54,515 (beam_search:479) INFO: total log probability: -4.54
2024-10-27 18:13:54,515 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:13:54,515 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:54,515 (beam_search:483) INFO: best hypo: ▁ONE▁ONE▁AND▁THE▁TWO▁UM▁AND▁MAGNETS▁AND▁THIS▁OTHER▁THING▁THAT▁WE▁DIDN'T▁USE▁IN

2024-10-27 18:13:54,519 (asr_inference:509) INFO: speech length: 108240
2024-10-27 18:13:58,765 (beam_search:428) INFO: decoder input length: 84
2024-10-27 18:13:58,765 (beam_search:429) INFO: max output length: 84
2024-10-27 18:13:58,765 (beam_search:430) INFO: min output length: 0
2024-10-27 18:13:58,978 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:13:58,978 (beam_search:476) INFO:  -2.82 * 1.0 =  -2.82 for ctc
2024-10-27 18:13:58,978 (beam_search:479) INFO: total log probability: -2.82
2024-10-27 18:13:58,978 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:13:58,978 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:13:58,978 (beam_search:483) INFO: best hypo: ▁THE▁TWO▁MAGNETS▁TOGETHER▁AND▁ONE▁IS▁GOING▁INTO▁A▁ITS▁THE▁TWO▁MAGNETS

2024-10-27 18:13:58,980 (asr_inference:509) INFO: speech length: 85584
2024-10-27 18:14:02,140 (beam_search:428) INFO: decoder input length: 66
2024-10-27 18:14:02,140 (beam_search:429) INFO: max output length: 66
2024-10-27 18:14:02,140 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:02,238 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:02,238 (beam_search:476) INFO:  -1.95 * 1.0 =  -1.95 for ctc
2024-10-27 18:14:02,238 (beam_search:479) INFO: total log probability: -1.95
2024-10-27 18:14:02,239 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:14:02,239 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:02,239 (beam_search:483) INFO: best hypo: ▁IT'S▁HOW▁MANY▁IT▁TO▁TWO▁MAGNETS

2024-10-27 18:14:02,241 (asr_inference:509) INFO: speech length: 182080
2024-10-27 18:14:09,338 (beam_search:428) INFO: decoder input length: 141
2024-10-27 18:14:09,338 (beam_search:429) INFO: max output length: 141
2024-10-27 18:14:09,338 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:09,907 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:09,907 (beam_search:476) INFO:  -2.62 * 1.0 =  -2.62 for ctc
2024-10-27 18:14:09,907 (beam_search:479) INFO: total log probability: -2.62
2024-10-27 18:14:09,907 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:14:09,907 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:09,907 (beam_search:483) INFO: best hypo: ▁IT'SING▁ME▁THAT▁THE'S▁A▁LOT▁OF▁ON▁B▁ON▁B▁CUP▁B▁AND▁THAT▁IT'S

2024-10-27 18:14:09,909 (asr_inference:509) INFO: speech length: 98992
2024-10-27 18:14:13,523 (beam_search:428) INFO: decoder input length: 76
2024-10-27 18:14:13,523 (beam_search:429) INFO: max output length: 76
2024-10-27 18:14:13,523 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:13,732 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:13,732 (beam_search:476) INFO:  -1.93 * 1.0 =  -1.93 for ctc
2024-10-27 18:14:13,732 (beam_search:479) INFO: total log probability: -1.93
2024-10-27 18:14:13,732 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:14:13,732 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:13,732 (beam_search:483) INFO: best hypo: ▁THAT▁IT'S▁TAKING▁THAT▁MANY▁WASHERS▁HOW▁MANY▁ARE▁IN▁THE▁CUP▁TO▁THE

2024-10-27 18:14:13,735 (asr_inference:509) INFO: speech length: 85088
2024-10-27 18:14:16,840 (beam_search:428) INFO: decoder input length: 65
2024-10-27 18:14:16,840 (beam_search:429) INFO: max output length: 65
2024-10-27 18:14:16,840 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:16,939 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:16,939 (beam_search:476) INFO:  -2.03 * 1.0 =  -2.03 for ctc
2024-10-27 18:14:16,939 (beam_search:479) INFO: total log probability: -2.03
2024-10-27 18:14:16,939 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:14:16,939 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:16,939 (beam_search:483) INFO: best hypo: ▁THEY'RE▁BEING▁FROM▁OTHER▁BY▁THE▁WASHERS

2024-10-27 18:14:16,942 (asr_inference:509) INFO: speech length: 80992
2024-10-27 18:14:19,830 (beam_search:428) INFO: decoder input length: 62
2024-10-27 18:14:19,830 (beam_search:429) INFO: max output length: 62
2024-10-27 18:14:19,830 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:19,877 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:19,877 (beam_search:476) INFO:  -1.35 * 1.0 =  -1.35 for ctc
2024-10-27 18:14:19,878 (beam_search:479) INFO: total log probability: -1.35
2024-10-27 18:14:19,878 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:14:19,878 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:19,878 (beam_search:483) INFO: best hypo: ▁THEY▁ARE▁BEING▁AND

2024-10-27 18:14:19,880 (asr_inference:509) INFO: speech length: 40336
2024-10-27 18:14:21,425 (beam_search:428) INFO: decoder input length: 31
2024-10-27 18:14:21,425 (beam_search:429) INFO: max output length: 31
2024-10-27 18:14:21,425 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:21,449 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:21,449 (beam_search:476) INFO:  -0.81 * 1.0 =  -0.81 for ctc
2024-10-27 18:14:21,449 (beam_search:479) INFO: total log probability: -0.81
2024-10-27 18:14:21,449 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:14:21,449 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:21,449 (beam_search:483) INFO: best hypo: ▁WAS▁TO▁THE▁MAGNETS

2024-10-27 18:14:21,451 (asr_inference:509) INFO: speech length: 92720
2024-10-27 18:14:24,864 (beam_search:428) INFO: decoder input length: 71
2024-10-27 18:14:24,864 (beam_search:429) INFO: max output length: 71
2024-10-27 18:14:24,864 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:24,997 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:24,997 (beam_search:476) INFO:  -2.43 * 1.0 =  -2.43 for ctc
2024-10-27 18:14:24,997 (beam_search:479) INFO: total log probability: -2.43
2024-10-27 18:14:24,997 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:14:24,997 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:24,997 (beam_search:483) INFO: best hypo: ▁WHEN▁YOU▁THE▁WASHER▁THE▁ONE▁WASHERS▁THE▁TWO▁MAGNETS

2024-10-27 18:14:25,000 (asr_inference:509) INFO: speech length: 30576
2024-10-27 18:14:26,317 (beam_search:428) INFO: decoder input length: 23
2024-10-27 18:14:26,317 (beam_search:429) INFO: max output length: 23
2024-10-27 18:14:26,317 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:26,327 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:26,327 (beam_search:476) INFO:  -0.14 * 1.0 =  -0.14 for ctc
2024-10-27 18:14:26,327 (beam_search:479) INFO: total log probability: -0.14
2024-10-27 18:14:26,327 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:14:26,327 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:26,327 (beam_search:483) INFO: best hypo: ▁AND

2024-10-27 18:14:26,329 (asr_inference:509) INFO: speech length: 62576
2024-10-27 18:14:28,663 (beam_search:428) INFO: decoder input length: 48
2024-10-27 18:14:28,663 (beam_search:429) INFO: max output length: 48
2024-10-27 18:14:28,663 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:28,722 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:28,722 (beam_search:476) INFO:  -1.17 * 1.0 =  -1.17 for ctc
2024-10-27 18:14:28,722 (beam_search:479) INFO: total log probability: -1.17
2024-10-27 18:14:28,722 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:14:28,722 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:28,722 (beam_search:483) INFO: best hypo: ▁WE▁DID▁AND▁WE▁DID▁THE▁WITH

2024-10-27 18:14:28,724 (asr_inference:509) INFO: speech length: 29840
2024-10-27 18:14:30,036 (beam_search:428) INFO: decoder input length: 22
2024-10-27 18:14:30,036 (beam_search:429) INFO: max output length: 22
2024-10-27 18:14:30,036 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:30,053 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:30,053 (beam_search:476) INFO:  -0.16 * 1.0 =  -0.16 for ctc
2024-10-27 18:14:30,053 (beam_search:479) INFO: total log probability: -0.16
2024-10-27 18:14:30,053 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:14:30,053 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:30,053 (beam_search:483) INFO: best hypo: ▁I▁CAN▁THE

2024-10-27 18:14:30,056 (asr_inference:509) INFO: speech length: 43184
2024-10-27 18:14:31,687 (beam_search:428) INFO: decoder input length: 33
2024-10-27 18:14:31,687 (beam_search:429) INFO: max output length: 33
2024-10-27 18:14:31,687 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:31,699 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:31,699 (beam_search:476) INFO:  -0.55 * 1.0 =  -0.55 for ctc
2024-10-27 18:14:31,699 (beam_search:479) INFO: total log probability: -0.55
2024-10-27 18:14:31,699 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:14:31,699 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:31,700 (beam_search:483) INFO: best hypo: ▁IT

2024-10-27 18:14:31,702 (asr_inference:509) INFO: speech length: 45584
2024-10-27 18:14:33,518 (beam_search:428) INFO: decoder input length: 35
2024-10-27 18:14:33,518 (beam_search:429) INFO: max output length: 35
2024-10-27 18:14:33,518 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:33,541 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:33,541 (beam_search:476) INFO:  -0.38 * 1.0 =  -0.38 for ctc
2024-10-27 18:14:33,541 (beam_search:479) INFO: total log probability: -0.38
2024-10-27 18:14:33,541 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:14:33,541 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:33,541 (beam_search:483) INFO: best hypo: ▁IT▁TO▁THE

2024-10-27 18:14:33,544 (asr_inference:509) INFO: speech length: 107120
2024-10-27 18:14:37,353 (beam_search:428) INFO: decoder input length: 83
2024-10-27 18:14:37,353 (beam_search:429) INFO: max output length: 83
2024-10-27 18:14:37,353 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:37,456 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:37,456 (beam_search:476) INFO:  -1.64 * 1.0 =  -1.64 for ctc
2024-10-27 18:14:37,456 (beam_search:479) INFO: total log probability: -1.64
2024-10-27 18:14:37,456 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:14:37,456 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:37,456 (beam_search:483) INFO: best hypo: ▁HOW▁MANY▁METAL▁WASHERS▁IT▁TO▁THE

2024-10-27 18:14:37,459 (asr_inference:509) INFO: speech length: 109152
2024-10-27 18:14:41,404 (beam_search:428) INFO: decoder input length: 84
2024-10-27 18:14:41,405 (beam_search:429) INFO: max output length: 84
2024-10-27 18:14:41,405 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:41,565 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:41,565 (beam_search:476) INFO:  -2.68 * 1.0 =  -2.68 for ctc
2024-10-27 18:14:41,565 (beam_search:479) INFO: total log probability: -2.68
2024-10-27 18:14:41,565 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:14:41,565 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:41,565 (beam_search:483) INFO: best hypo: ▁THE▁THE▁WASHERS▁THE▁MORE▁LIKELY▁IT▁TO▁THE▁MAGNETIC

2024-10-27 18:14:41,568 (asr_inference:509) INFO: speech length: 43600
2024-10-27 18:14:43,226 (beam_search:428) INFO: decoder input length: 33
2024-10-27 18:14:43,226 (beam_search:429) INFO: max output length: 33
2024-10-27 18:14:43,226 (beam_search:430) INFO: min output length: 0
2024-10-27 18:14:43,259 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:14:43,259 (beam_search:476) INFO:  -1.33 * 1.0 =  -1.33 for ctc
2024-10-27 18:14:43,259 (beam_search:479) INFO: total log probability: -1.33
2024-10-27 18:14:43,259 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:14:43,259 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:14:43,259 (beam_search:483) INFO: best hypo: ▁I▁WHAT▁WAS▁IMPORTANT▁WAS

2024-10-27 18:14:43,261 (asr_inference:509) INFO: speech length: 448320
2024-10-27 18:15:06,698 (beam_search:428) INFO: decoder input length: 349
2024-10-27 18:15:06,698 (beam_search:429) INFO: max output length: 349
2024-10-27 18:15:06,698 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:10,763 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:10,763 (beam_search:476) INFO:  -8.26 * 1.0 =  -8.26 for ctc
2024-10-27 18:15:10,763 (beam_search:479) INFO: total log probability: -8.26
2024-10-27 18:15:10,763 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:15:10,763 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:10,763 (beam_search:483) INFO: best hypo: ▁THAT▁THE▁METAL▁THAT▁THE▁IRON▁WASED▁UM▁BY▁THE▁FROM▁THE▁MAGNET▁IT▁SO▁IT▁A▁TEMPORARY▁MAGNET▁THAT▁JUSTED▁IT▁AND▁UM▁ALSO▁S▁IF▁YOU▁USE▁ON▁A▁PENCIL▁YOU▁IF▁YOU▁PUT▁A▁NEGATIVE▁SIDEST▁A▁POSITIVE▁SIDE▁IT'LL▁SO▁THERE'S▁ANOTHER▁MAGNETIC▁THAT▁YOU▁CAN▁USE

2024-10-27 18:15:10,766 (asr_inference:509) INFO: speech length: 261840
2024-10-27 18:15:21,545 (beam_search:428) INFO: decoder input length: 204
2024-10-27 18:15:21,545 (beam_search:429) INFO: max output length: 204
2024-10-27 18:15:21,545 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:22,193 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:22,193 (beam_search:476) INFO:  -2.65 * 1.0 =  -2.65 for ctc
2024-10-27 18:15:22,193 (beam_search:479) INFO: total log probability: -2.65
2024-10-27 18:15:22,193 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:15:22,193 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:22,193 (beam_search:483) INFO: best hypo: ▁UM▁IT▁IT▁IT▁USES▁UM▁IT▁TO▁MAKE▁THE▁A▁MAGNETST▁IT▁AND▁UM

2024-10-27 18:15:22,196 (asr_inference:509) INFO: speech length: 442560
2024-10-27 18:15:45,008 (beam_search:428) INFO: decoder input length: 345
2024-10-27 18:15:45,008 (beam_search:429) INFO: max output length: 345
2024-10-27 18:15:45,008 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:50,065 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:50,065 (beam_search:476) INFO:  -8.46 * 1.0 =  -8.46 for ctc
2024-10-27 18:15:50,065 (beam_search:479) INFO: total log probability: -8.46
2024-10-27 18:15:50,065 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:15:50,065 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:50,065 (beam_search:483) INFO: best hypo: ▁WELL▁WE▁DIDN'T▁USE▁MAGNETS▁SO▁THEY'T▁PUSH▁EACH▁OTHER▁BUT▁UM▁IT▁WAS▁KIND▁OF▁BUT▁WE▁DIDN'T▁USE▁ANOTHER▁MAGNET▁NOT▁REALLY▁BECAUSE▁WELL▁WE▁WE▁USED▁MORE▁IRON▁UM▁TO▁FIND▁THE▁MAGNETS▁MORE▁THAN▁OTHER▁MAGNETS▁CAUSE▁WE▁WEREN'T▁TO▁USE▁MAGNET▁CAUSE▁IT▁WOULD▁MAKE▁IT▁A▁EASIER▁AND▁WE▁WOULDN'T▁OUT▁WHAT▁IN▁THAT▁THAT▁WE▁HAD▁HAD▁IRON

2024-10-27 18:15:50,069 (asr_inference:509) INFO: speech length: 18960
2024-10-27 18:15:51,034 (beam_search:428) INFO: decoder input length: 14
2024-10-27 18:15:51,034 (beam_search:429) INFO: max output length: 14
2024-10-27 18:15:51,034 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:51,043 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:51,043 (beam_search:476) INFO:  -1.25 * 1.0 =  -1.25 for ctc
2024-10-27 18:15:51,043 (beam_search:479) INFO: total log probability: -1.25
2024-10-27 18:15:51,043 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 18:15:51,043 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:51,043 (beam_search:483) INFO: best hypo: ▁HAD▁A

2024-10-27 18:15:51,045 (asr_inference:509) INFO: speech length: 16976
2024-10-27 18:15:51,908 (beam_search:428) INFO: decoder input length: 12
2024-10-27 18:15:51,908 (beam_search:429) INFO: max output length: 12
2024-10-27 18:15:51,908 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:51,920 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:51,920 (beam_search:476) INFO:  -0.55 * 1.0 =  -0.55 for ctc
2024-10-27 18:15:51,920 (beam_search:479) INFO: total log probability: -0.55
2024-10-27 18:15:51,920 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:15:51,920 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:51,920 (beam_search:483) INFO: best hypo: 'M▁GOOD

2024-10-27 18:15:51,922 (asr_inference:509) INFO: speech length: 95200
2024-10-27 18:15:55,513 (beam_search:428) INFO: decoder input length: 73
2024-10-27 18:15:55,514 (beam_search:429) INFO: max output length: 73
2024-10-27 18:15:55,514 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:55,594 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:55,594 (beam_search:476) INFO:  -1.26 * 1.0 =  -1.26 for ctc
2024-10-27 18:15:55,594 (beam_search:479) INFO: total log probability: -1.26
2024-10-27 18:15:55,595 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:15:55,595 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:55,595 (beam_search:483) INFO: best hypo: ▁THE▁MOTOR▁UM▁HAD▁THE▁THE

2024-10-27 18:15:55,597 (asr_inference:509) INFO: speech length: 27824
2024-10-27 18:15:56,828 (beam_search:428) INFO: decoder input length: 21
2024-10-27 18:15:56,828 (beam_search:429) INFO: max output length: 21
2024-10-27 18:15:56,828 (beam_search:430) INFO: min output length: 0
2024-10-27 18:15:56,851 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:15:56,851 (beam_search:476) INFO:  -2.32 * 1.0 =  -2.32 for ctc
2024-10-27 18:15:56,851 (beam_search:479) INFO: total log probability: -2.32
2024-10-27 18:15:56,851 (beam_search:480) INFO: normalized log probability: -0.29
2024-10-27 18:15:56,851 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:15:56,851 (beam_search:483) INFO: best hypo: ▁IT'S▁START▁WITH▁IT

2024-10-27 18:15:56,854 (asr_inference:509) INFO: speech length: 126912
2024-10-27 18:16:01,424 (beam_search:428) INFO: decoder input length: 98
2024-10-27 18:16:01,424 (beam_search:429) INFO: max output length: 98
2024-10-27 18:16:01,424 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:01,689 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:01,689 (beam_search:476) INFO:  -3.03 * 1.0 =  -3.03 for ctc
2024-10-27 18:16:01,689 (beam_search:479) INFO: total log probability: -3.03
2024-10-27 18:16:01,689 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:16:01,689 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:01,689 (beam_search:483) INFO: best hypo: ▁THE▁HAD▁A▁LITTLE▁FLAG▁ON▁IT▁AND▁UM▁WHEN▁WHEN▁YOU▁HAD▁IT▁CONNECTED▁IT

2024-10-27 18:16:01,692 (asr_inference:509) INFO: speech length: 70176
2024-10-27 18:16:04,235 (beam_search:428) INFO: decoder input length: 54
2024-10-27 18:16:04,235 (beam_search:429) INFO: max output length: 54
2024-10-27 18:16:04,235 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:04,323 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:04,323 (beam_search:476) INFO:  -0.41 * 1.0 =  -0.41 for ctc
2024-10-27 18:16:04,323 (beam_search:479) INFO: total log probability: -0.41
2024-10-27 18:16:04,323 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:16:04,323 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:04,323 (beam_search:483) INFO: best hypo: ▁WE▁HAD▁TO▁HAVE▁THE▁WIRE▁UM▁ON▁THE▁BATTERY

2024-10-27 18:16:04,325 (asr_inference:509) INFO: speech length: 475104
2024-10-27 18:16:29,275 (beam_search:428) INFO: decoder input length: 370
2024-10-27 18:16:29,276 (beam_search:429) INFO: max output length: 370
2024-10-27 18:16:29,276 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:34,476 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:34,476 (beam_search:476) INFO:  -8.18 * 1.0 =  -8.18 for ctc
2024-10-27 18:16:34,476 (beam_search:479) INFO: total log probability: -8.18
2024-10-27 18:16:34,476 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:16:34,476 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:34,476 (beam_search:483) INFO: best hypo: ▁ON▁SIDE▁OF▁THE▁BATTERY▁WE▁HAD▁TO▁PUT▁A▁THE▁OF▁THE▁WIRE▁WHERE▁IT'S▁NOT▁ON▁THE▁OTHER▁BECAUSE▁IF▁IT▁WAS▁ON▁THE▁OTHER▁THEN▁IT▁WOULDN'T▁IT▁HAD▁TO▁BE▁THE▁ENERGY▁OUT▁THE▁ENERGY▁FROM▁THE▁WIRES▁WAS▁ELECTRICITY▁AND▁IT▁CAME▁THROUGH▁THE▁WIRES▁AND▁TO▁THE▁MOTOR▁AND▁AND▁WHEN▁IT▁INSIDE▁THE▁MOTOR▁IT▁AROUND▁THE▁FLAG▁REALLY

2024-10-27 18:16:34,479 (asr_inference:509) INFO: speech length: 296080
2024-10-27 18:16:47,187 (beam_search:428) INFO: decoder input length: 230
2024-10-27 18:16:47,188 (beam_search:429) INFO: max output length: 230
2024-10-27 18:16:47,188 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:49,267 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:49,267 (beam_search:476) INFO:  -6.46 * 1.0 =  -6.46 for ctc
2024-10-27 18:16:49,267 (beam_search:479) INFO: total log probability: -6.46
2024-10-27 18:16:49,267 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:16:49,267 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:49,267 (beam_search:483) INFO: best hypo: ▁THERE▁THERE▁WAS▁I'M▁NOT▁SURE▁CAUSE▁THE▁WIRES▁WAS▁CONNECTED▁IT▁EITHER▁WENT▁THROUGH▁CAUSE▁WAS▁TAPE▁ON▁THE▁SIDES▁SO▁YOU'T▁REALLY▁SEE▁OR▁UM▁IT▁EITHER▁WAS▁TWO▁WIRES▁AROUND▁OR▁JUST▁ONE▁WIRE▁GOING▁ALL▁THE▁WAY▁THROUGH▁AROUND

2024-10-27 18:16:49,270 (asr_inference:509) INFO: speech length: 140384
2024-10-27 18:16:54,706 (beam_search:428) INFO: decoder input length: 109
2024-10-27 18:16:54,706 (beam_search:429) INFO: max output length: 109
2024-10-27 18:16:54,706 (beam_search:430) INFO: min output length: 0
2024-10-27 18:16:54,980 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:16:54,980 (beam_search:476) INFO:  -1.87 * 1.0 =  -1.87 for ctc
2024-10-27 18:16:54,980 (beam_search:479) INFO: total log probability: -1.87
2024-10-27 18:16:54,980 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:16:54,980 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:16:54,980 (beam_search:483) INFO: best hypo: ▁THE▁BATTERY▁A▁BATTERY▁AND▁IT▁WELL▁WHAT▁DO▁YOU▁MEAN▁BY▁THAT

2024-10-27 18:16:54,983 (asr_inference:509) INFO: speech length: 415024
2024-10-27 18:17:14,207 (beam_search:428) INFO: decoder input length: 323
2024-10-27 18:17:14,207 (beam_search:429) INFO: max output length: 323
2024-10-27 18:17:14,207 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:17,510 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:17,510 (beam_search:476) INFO:  -5.98 * 1.0 =  -5.98 for ctc
2024-10-27 18:17:17,511 (beam_search:479) INFO: total log probability: -5.98
2024-10-27 18:17:17,511 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:17:17,511 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:17,511 (beam_search:483) INFO: best hypo: ▁IT▁GOES▁FROM▁THE▁OF▁THE▁BATTERY▁WHERE▁IT▁THE▁ELECTRICITY▁GO▁UM▁OUT▁OF▁THE▁BATTERY▁AND▁TO▁THE▁WIRES▁AND▁THEN▁IT▁GOES▁AROUND▁AND▁INTO▁THE▁MOTOR▁SO▁IT▁UM▁THE▁ELECTRICITY▁IN▁A▁CIRCUIT▁AND▁UM▁IT▁FLOWS▁THROUGH▁THE▁WIRES▁AND▁ONTO▁THE▁MOTOR▁WHICH▁MAKES▁IT

2024-10-27 18:17:17,513 (asr_inference:509) INFO: speech length: 15680
2024-10-27 18:17:18,265 (beam_search:428) INFO: decoder input length: 11
2024-10-27 18:17:18,265 (beam_search:429) INFO: max output length: 11
2024-10-27 18:17:18,265 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:18,275 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:18,275 (beam_search:476) INFO:  -1.37 * 1.0 =  -1.37 for ctc
2024-10-27 18:17:18,275 (beam_search:479) INFO: total log probability: -1.37
2024-10-27 18:17:18,275 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:17:18,275 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:18,275 (beam_search:483) INFO: best hypo: ▁I▁THINK▁SO

2024-10-27 18:17:18,277 (asr_inference:509) INFO: speech length: 123504
2024-10-27 18:17:22,715 (beam_search:428) INFO: decoder input length: 95
2024-10-27 18:17:22,715 (beam_search:429) INFO: max output length: 95
2024-10-27 18:17:22,715 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:22,978 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:22,978 (beam_search:476) INFO:  -2.60 * 1.0 =  -2.60 for ctc
2024-10-27 18:17:22,978 (beam_search:479) INFO: total log probability: -2.60
2024-10-27 18:17:22,978 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:17:22,978 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:22,978 (beam_search:483) INFO: best hypo: ▁THE▁ELECTRICITY▁IS▁FROM▁THE▁BATTERY▁TO▁THE▁MOTOR▁AND▁AND▁LET▁THE▁MOTOR▁RUN

2024-10-27 18:17:22,982 (asr_inference:509) INFO: speech length: 150560
2024-10-27 18:17:28,715 (beam_search:428) INFO: decoder input length: 117
2024-10-27 18:17:28,716 (beam_search:429) INFO: max output length: 117
2024-10-27 18:17:28,716 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:29,258 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:29,258 (beam_search:476) INFO:  -2.62 * 1.0 =  -2.62 for ctc
2024-10-27 18:17:29,258 (beam_search:479) INFO: total log probability: -2.62
2024-10-27 18:17:29,259 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:17:29,259 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:29,259 (beam_search:483) INFO: best hypo: ▁THAT▁IS▁THE▁WE▁DID▁IN▁WE▁USED▁UM▁THE▁BATTERY▁TO▁GO▁LIKE▁THAT▁INTO▁THE▁LIGHT▁BULB▁AND▁LIGHT▁UP▁THE▁LIGHT▁BULB

2024-10-27 18:17:29,261 (asr_inference:509) INFO: speech length: 13392
2024-10-27 18:17:30,145 (beam_search:428) INFO: decoder input length: 9
2024-10-27 18:17:30,146 (beam_search:429) INFO: max output length: 9
2024-10-27 18:17:30,146 (beam_search:430) INFO: min output length: 0
2024-10-27 18:17:30,152 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:17:30,152 (beam_search:476) INFO:  -0.23 * 1.0 =  -0.23 for ctc
2024-10-27 18:17:30,152 (beam_search:479) INFO: total log probability: -0.23
2024-10-27 18:17:30,152 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:17:30,152 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:17:30,152 (beam_search:483) INFO: best hypo: ▁THE

2024-10-27 18:17:30,155 (asr_inference:509) INFO: speech length: 479040
2024-10-27 18:17:56,176 (beam_search:428) INFO: decoder input length: 373
2024-10-27 18:17:56,176 (beam_search:429) INFO: max output length: 373
2024-10-27 18:17:56,176 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:01,815 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:01,816 (beam_search:476) INFO:  -8.28 * 1.0 =  -8.28 for ctc
2024-10-27 18:18:01,816 (beam_search:479) INFO: total log probability: -8.28
2024-10-27 18:18:01,816 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:18:01,816 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:01,816 (beam_search:483) INFO: best hypo: ▁THAT▁IS▁UM▁THE▁ON▁AND▁OFF▁THE▁THING▁THE▁IS▁CONNECTED▁BECAUSE▁THE▁ELECTRICITY▁IF▁THAT▁MOTOR▁IS▁NOT▁IF▁THAT▁SWITCH▁IS▁NOT▁CONNECTED▁TO▁THE▁THING▁THE▁ELECTRICITY▁CAN▁NOT▁FLOW▁THROUGH▁IT▁WILL▁NOT▁UM▁GO▁THROUGH▁THE▁AND▁CONNECT▁TO▁THE▁MOTOR▁BECAUSE▁IT▁IT▁IS▁IT▁WILL▁GO▁OUT▁AND▁IT▁WILL▁NOT▁THE▁OTHER▁SO▁THAT'S▁KIND▁OF▁LIKE▁AN▁ON▁AND▁OFF▁SWITCH▁SO▁IT'S

2024-10-27 18:18:01,820 (asr_inference:509) INFO: speech length: 353696
2024-10-27 18:18:17,689 (beam_search:428) INFO: decoder input length: 275
2024-10-27 18:18:17,689 (beam_search:429) INFO: max output length: 275
2024-10-27 18:18:17,689 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:20,705 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:20,705 (beam_search:476) INFO:  -7.62 * 1.0 =  -7.62 for ctc
2024-10-27 18:18:20,706 (beam_search:479) INFO: total log probability: -7.62
2024-10-27 18:18:20,706 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:18:20,706 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:20,706 (beam_search:483) INFO: best hypo: ▁THE▁UM▁WILL▁TAKE▁THE▁AND▁IT▁WILL▁TURN▁THEM▁OFF▁BECAUSE▁IF▁WHEN▁THE▁ELECTRICITY▁IS▁FLOWING▁THROUGH▁ITLL▁FLOW▁THROUGH▁THE▁METAL▁AND▁IF▁IT▁DOESN'T▁THE▁OTHER▁THING▁THEN▁IT▁WILL▁NOT▁BE▁TO▁GO▁THROUGH▁AND▁MAKE▁AND▁LIGHT▁UP▁THE▁AND▁MAKE▁THE▁MOTOR▁GO▁AND▁ROUND

2024-10-27 18:18:20,708 (asr_inference:509) INFO: speech length: 289216
2024-10-27 18:18:33,025 (beam_search:428) INFO: decoder input length: 225
2024-10-27 18:18:33,025 (beam_search:429) INFO: max output length: 225
2024-10-27 18:18:33,025 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:35,866 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:35,866 (beam_search:476) INFO:  -3.70 * 1.0 =  -3.70 for ctc
2024-10-27 18:18:35,866 (beam_search:479) INFO: total log probability: -3.70
2024-10-27 18:18:35,866 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:18:35,866 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:35,866 (beam_search:483) INFO: best hypo: ▁THAT'S▁IT'S▁KINDA▁LIKE▁AN▁AND▁BECAUSE▁IF▁YOU▁CONNECT▁THE▁SWITCH▁TO▁THE▁OTHER▁WIRE▁THEN▁THE▁ELECTRICITY▁IS▁TO▁GO▁THROUGH▁AND▁IT▁WILL▁TURN▁ON▁THE▁MOTOR▁BUT▁WHEN▁IT'S▁NOT▁THEN▁IT'S▁NOT▁CONNECTED▁AND▁IT▁WON'T▁BE▁TO▁GO▁THROUGH▁TO▁THE▁OTHER▁SIDE▁OF▁THE▁WIRE▁WHERE▁IT▁GETS▁CONNECTED▁TO▁THE▁MOTOR

2024-10-27 18:18:35,869 (asr_inference:509) INFO: speech length: 129792
2024-10-27 18:18:40,587 (beam_search:428) INFO: decoder input length: 100
2024-10-27 18:18:40,587 (beam_search:429) INFO: max output length: 100
2024-10-27 18:18:40,587 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:40,858 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:40,859 (beam_search:476) INFO:  -3.19 * 1.0 =  -3.19 for ctc
2024-10-27 18:18:40,859 (beam_search:479) INFO: total log probability: -3.19
2024-10-27 18:18:40,859 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:18:40,859 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:40,859 (beam_search:483) INFO: best hypo: ▁YOU▁WOULD▁PUT▁THE▁BATTERY▁INTO▁THE▁RIGHT▁AT▁THE▁BOTTOM▁AND▁THEN▁YOU▁WOULD

2024-10-27 18:18:40,861 (asr_inference:509) INFO: speech length: 46256
2024-10-27 18:18:42,712 (beam_search:428) INFO: decoder input length: 35
2024-10-27 18:18:42,712 (beam_search:429) INFO: max output length: 35
2024-10-27 18:18:42,712 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:42,734 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:42,734 (beam_search:476) INFO:  -0.08 * 1.0 =  -0.08 for ctc
2024-10-27 18:18:42,734 (beam_search:479) INFO: total log probability: -0.08
2024-10-27 18:18:42,734 (beam_search:480) INFO: normalized log probability: -0.02
2024-10-27 18:18:42,734 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:42,734 (beam_search:483) INFO: best hypo: ▁I▁SEE▁THAT

2024-10-27 18:18:42,737 (asr_inference:509) INFO: speech length: 67120
2024-10-27 18:18:45,238 (beam_search:428) INFO: decoder input length: 51
2024-10-27 18:18:45,238 (beam_search:429) INFO: max output length: 51
2024-10-27 18:18:45,238 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:45,307 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:45,307 (beam_search:476) INFO:  -2.04 * 1.0 =  -2.04 for ctc
2024-10-27 18:18:45,307 (beam_search:479) INFO: total log probability: -2.04
2024-10-27 18:18:45,307 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:18:45,307 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:45,307 (beam_search:483) INFO: best hypo: ▁THE'S▁IN▁IS▁A▁LIGHT▁BULB

2024-10-27 18:18:45,309 (asr_inference:509) INFO: speech length: 169696
2024-10-27 18:18:52,027 (beam_search:428) INFO: decoder input length: 132
2024-10-27 18:18:52,027 (beam_search:429) INFO: max output length: 132
2024-10-27 18:18:52,027 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:52,394 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:52,395 (beam_search:476) INFO:  -1.41 * 1.0 =  -1.41 for ctc
2024-10-27 18:18:52,395 (beam_search:479) INFO: total log probability: -1.41
2024-10-27 18:18:52,395 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:18:52,395 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:52,395 (beam_search:483) INFO: best hypo: ▁THE▁BATTERY▁GETS▁INSIDE▁THE▁TWO▁THINGS▁AND▁THE▁ELECTRICITY▁THROUGH▁INTO▁THE▁LIGHT▁BULB

2024-10-27 18:18:52,397 (asr_inference:509) INFO: speech length: 116240
2024-10-27 18:18:56,696 (beam_search:428) INFO: decoder input length: 90
2024-10-27 18:18:56,696 (beam_search:429) INFO: max output length: 90
2024-10-27 18:18:56,696 (beam_search:430) INFO: min output length: 0
2024-10-27 18:18:57,091 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:18:57,092 (beam_search:476) INFO:  -2.09 * 1.0 =  -2.09 for ctc
2024-10-27 18:18:57,092 (beam_search:479) INFO: total log probability: -2.09
2024-10-27 18:18:57,092 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:18:57,092 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:18:57,092 (beam_search:483) INFO: best hypo: ▁IT'S▁A▁CIRCUIT▁BUT▁IT'S▁A▁A▁IT▁IT'S▁A▁CIRCUIT▁BUT▁IT▁LOOKS▁KIND▁OF▁LIKE▁A▁RIGHT▁NOW

2024-10-27 18:18:57,094 (asr_inference:509) INFO: speech length: 339968
2024-10-27 18:19:11,719 (beam_search:428) INFO: decoder input length: 265
2024-10-27 18:19:11,719 (beam_search:429) INFO: max output length: 265
2024-10-27 18:19:11,719 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:13,700 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:13,700 (beam_search:476) INFO:  -6.94 * 1.0 =  -6.94 for ctc
2024-10-27 18:19:13,700 (beam_search:479) INFO: total log probability: -6.94
2024-10-27 18:19:13,700 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:19:13,700 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:13,700 (beam_search:483) INFO: best hypo: ▁WE'VE▁UM▁WE▁HAVE▁BEEN▁WELL▁WE'RE▁TO▁LIGHT▁TWO▁LIGHT▁BULBS▁IN▁ONE▁CIRCUIT▁NOW▁SO▁WE▁ARE▁TO▁LIGHT▁THEM▁WITH▁UM▁AND▁USE▁ONE▁BATTERY▁AND▁MAKE▁AND▁ONE▁CIRCUIT

2024-10-27 18:19:13,703 (asr_inference:509) INFO: speech length: 253136
2024-10-27 18:19:24,011 (beam_search:428) INFO: decoder input length: 197
2024-10-27 18:19:24,011 (beam_search:429) INFO: max output length: 197
2024-10-27 18:19:24,011 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:25,702 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:25,702 (beam_search:476) INFO:  -7.52 * 1.0 =  -7.52 for ctc
2024-10-27 18:19:25,702 (beam_search:479) INFO: total log probability: -7.52
2024-10-27 18:19:25,703 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:19:25,703 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:25,703 (beam_search:483) INFO: best hypo: ▁THE▁HAD▁TO▁BE▁RIGHT▁BECAUSE▁IF▁IT▁WAS▁A▁POSITIVE▁SIDE▁GOING▁TO▁ANOTHER▁POSITIVE▁SIDE▁IT▁WOULDN'T▁OUT▁IT'T▁THE▁ELECTRICITY▁FLOW▁THROUGH▁BUT▁IF▁IT'S▁A▁SIDE▁TO▁A▁POSITIVE▁SIDE▁IT▁YOU▁CAN▁IT▁FLOW▁THROUGH

2024-10-27 18:19:25,705 (asr_inference:509) INFO: speech length: 286528
2024-10-27 18:19:37,779 (beam_search:428) INFO: decoder input length: 223
2024-10-27 18:19:37,779 (beam_search:429) INFO: max output length: 223
2024-10-27 18:19:37,779 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:39,584 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:39,584 (beam_search:476) INFO:  -5.36 * 1.0 =  -5.36 for ctc
2024-10-27 18:19:39,584 (beam_search:479) INFO: total log probability: -5.36
2024-10-27 18:19:39,584 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:19:39,584 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:39,584 (beam_search:483) INFO: best hypo: ▁THE▁ELECTRICITY▁IS▁FLOWING▁THROUGH▁THE▁WIRES▁AND▁INTO▁THE▁UM▁AND▁THE▁LITTLE▁WIRE▁THE▁PART▁OF▁THE▁WIRE▁IS▁WITH▁THE▁WIRE▁INSIDE▁THE▁LIGHT▁BULB▁UM▁AND▁IT'S▁GOING▁UP▁AND▁LET▁THE▁THE▁ENERGY▁LIGHT▁IT▁UP

2024-10-27 18:19:39,587 (asr_inference:509) INFO: speech length: 361040
2024-10-27 18:19:55,882 (beam_search:428) INFO: decoder input length: 281
2024-10-27 18:19:55,882 (beam_search:429) INFO: max output length: 281
2024-10-27 18:19:55,882 (beam_search:430) INFO: min output length: 0
2024-10-27 18:19:59,410 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:19:59,410 (beam_search:476) INFO:  -3.65 * 1.0 =  -3.65 for ctc
2024-10-27 18:19:59,410 (beam_search:479) INFO: total log probability: -3.65
2024-10-27 18:19:59,410 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:19:59,410 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:19:59,410 (beam_search:483) INFO: best hypo: ▁THE▁ARE▁AS▁ONE▁WITH▁UM▁THEY'RE▁CONNECTED▁SO▁AND▁UM▁IT'S▁ON▁ONE▁SIDE▁OF▁THE▁BATTERY▁ON▁THE▁NEGATIVE▁SIDE▁OF▁THE▁BATTERY▁IT'S▁GOING▁UP▁TO▁INTO▁THE▁LIGHT▁BULB▁AND▁IT'S▁LIGHTING▁THE▁LIGHT▁BULB▁UP▁ON▁THE▁OTHER▁SIDE▁IT'S▁JUST▁GOING▁THROUGH▁TO▁GET▁TO▁THE▁OTHER▁OF▁THE▁BATTERY

2024-10-27 18:19:59,413 (asr_inference:509) INFO: speech length: 367312
2024-10-27 18:20:16,396 (beam_search:428) INFO: decoder input length: 286
2024-10-27 18:20:16,396 (beam_search:429) INFO: max output length: 286
2024-10-27 18:20:16,396 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:19,790 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:19,790 (beam_search:476) INFO:  -5.77 * 1.0 =  -5.77 for ctc
2024-10-27 18:20:19,790 (beam_search:479) INFO: total log probability: -5.77
2024-10-27 18:20:19,790 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:20:19,790 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:19,790 (beam_search:483) INFO: best hypo: ▁THE▁SIDE▁IS▁THE▁NEGATIVE▁SIDE▁AND▁THEY'RE▁TOUCHING▁EACH▁OTHER▁THEN▁THEY'RE▁ABLE▁TO▁TO▁EACH▁OTHER▁SO▁IT'S▁KIND▁OF▁LIKE▁ANOTHER▁WIRE▁CONNECTING▁TO▁THEM▁TOGETHER▁UM▁BUT▁IT'S▁JUST▁THEY'RE▁TOUCHING▁OTHER▁AND▁THEN▁ON▁THE▁OTHER▁SIDES▁IT'S▁TO▁USE▁AND▁IT▁MORE▁POWER

2024-10-27 18:20:19,793 (asr_inference:509) INFO: speech length: 14528
2024-10-27 18:20:20,530 (beam_search:428) INFO: decoder input length: 10
2024-10-27 18:20:20,530 (beam_search:429) INFO: max output length: 10
2024-10-27 18:20:20,530 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:20,538 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:20,538 (beam_search:476) INFO:  -0.55 * 1.0 =  -0.55 for ctc
2024-10-27 18:20:20,538 (beam_search:479) INFO: total log probability: -0.55
2024-10-27 18:20:20,538 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:20:20,538 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:20,538 (beam_search:483) INFO: best hypo: ▁I'

2024-10-27 18:20:20,540 (asr_inference:509) INFO: speech length: 232496
2024-10-27 18:20:29,781 (beam_search:428) INFO: decoder input length: 181
2024-10-27 18:20:29,782 (beam_search:429) INFO: max output length: 181
2024-10-27 18:20:29,782 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:31,069 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:31,069 (beam_search:476) INFO:  -5.09 * 1.0 =  -5.09 for ctc
2024-10-27 18:20:31,069 (beam_search:479) INFO: total log probability: -5.09
2024-10-27 18:20:31,070 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:20:31,070 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:31,070 (beam_search:483) INFO: best hypo: ▁YEAH▁THERE▁COULD▁BE▁A▁WIRE▁IN▁THE▁TWO▁THINGS▁AND▁IT▁WOULD▁IT▁WOULD▁BE▁THERE▁COULD▁BE▁A▁WIRE▁IN▁THE▁TWO▁BATTERIES▁AND▁IT▁WOULD▁THE▁SAME▁AS▁IF▁THEY▁WERE▁TOUCHING▁EACH▁OTHER

2024-10-27 18:20:31,073 (asr_inference:509) INFO: speech length: 468256
2024-10-27 18:20:54,154 (beam_search:428) INFO: decoder input length: 365
2024-10-27 18:20:54,155 (beam_search:429) INFO: max output length: 365
2024-10-27 18:20:54,155 (beam_search:430) INFO: min output length: 0
2024-10-27 18:20:59,391 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:20:59,392 (beam_search:476) INFO:  -8.62 * 1.0 =  -8.62 for ctc
2024-10-27 18:20:59,392 (beam_search:479) INFO: total log probability: -8.62
2024-10-27 18:20:59,392 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:20:59,392 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:20:59,392 (beam_search:483) INFO: best hypo: 'S▁WHAT▁WE▁DID▁TODAY▁WE▁WOULD▁HAVE▁THE▁WIRE▁IN▁THE▁TWO▁WELL▁IT▁THE▁ELECTRICITY▁IS▁FLOWING▁THROUGH▁THE▁WIRE▁SO▁UM▁THERE'S▁ONE▁BATTERY▁AND▁THEN'S▁A▁WIRE▁BETWEEN▁IT▁AND'S▁ANOTHER▁BATTERY▁THAT▁UM▁AND▁ITLL▁GO▁THROUGH▁JUST▁LIKE▁UM▁IF▁THE▁BATTERY'S▁TOUCHING▁EACH▁OTHER▁AND▁IT'LL▁GO▁AROUND▁AND▁LIGHT▁UP▁EACH▁LIGHT▁BULB

2024-10-27 18:20:59,395 (asr_inference:509) INFO: speech length: 53184
2024-10-27 18:21:01,337 (beam_search:428) INFO: decoder input length: 41
2024-10-27 18:21:01,337 (beam_search:429) INFO: max output length: 41
2024-10-27 18:21:01,337 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:01,409 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:01,409 (beam_search:476) INFO:  -1.02 * 1.0 =  -1.02 for ctc
2024-10-27 18:21:01,409 (beam_search:479) INFO: total log probability: -1.02
2024-10-27 18:21:01,410 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:21:01,410 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:01,410 (beam_search:483) INFO: best hypo: ▁THROUGH▁THE▁WIRES▁AND▁LIGHTS▁UP▁THE▁LIGHT▁BULB

2024-10-27 18:21:01,412 (asr_inference:509) INFO: speech length: 17600
2024-10-27 18:21:02,287 (beam_search:428) INFO: decoder input length: 13
2024-10-27 18:21:02,287 (beam_search:429) INFO: max output length: 13
2024-10-27 18:21:02,287 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:02,296 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:02,296 (beam_search:476) INFO:  -0.74 * 1.0 =  -0.74 for ctc
2024-10-27 18:21:02,296 (beam_search:479) INFO: total log probability: -0.74
2024-10-27 18:21:02,296 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:21:02,296 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:02,296 (beam_search:483) INFO: best hypo: ▁YEAH▁THEY

2024-10-27 18:21:02,298 (asr_inference:509) INFO: speech length: 189056
2024-10-27 18:21:09,762 (beam_search:428) INFO: decoder input length: 147
2024-10-27 18:21:09,763 (beam_search:429) INFO: max output length: 147
2024-10-27 18:21:09,763 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:10,500 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:10,501 (beam_search:476) INFO:  -2.09 * 1.0 =  -2.09 for ctc
2024-10-27 18:21:10,501 (beam_search:479) INFO: total log probability: -2.09
2024-10-27 18:21:10,501 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:21:10,501 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:10,501 (beam_search:483) INFO: best hypo: ▁THE▁SAME▁IT▁DIDN'T▁LIKE▁TAKE▁A▁WIRE▁OUT▁OR▁AND▁I▁CAN'T▁SEE▁A▁SWITCH▁SO▁I▁DON'T▁KNOW

2024-10-27 18:21:10,503 (asr_inference:509) INFO: speech length: 27744
2024-10-27 18:21:11,678 (beam_search:428) INFO: decoder input length: 21
2024-10-27 18:21:11,678 (beam_search:429) INFO: max output length: 21
2024-10-27 18:21:11,678 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:11,707 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:11,708 (beam_search:476) INFO:  -1.75 * 1.0 =  -1.75 for ctc
2024-10-27 18:21:11,708 (beam_search:479) INFO: total log probability: -1.75
2024-10-27 18:21:11,708 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:21:11,708 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:11,708 (beam_search:483) INFO: best hypo: ▁I▁SEE▁IT▁WHEN▁THEY'RE▁ON

2024-10-27 18:21:11,710 (asr_inference:509) INFO: speech length: 338336
2024-10-27 18:21:26,380 (beam_search:428) INFO: decoder input length: 263
2024-10-27 18:21:26,380 (beam_search:429) INFO: max output length: 263
2024-10-27 18:21:26,380 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:28,984 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:28,984 (beam_search:476) INFO:  -6.84 * 1.0 =  -6.84 for ctc
2024-10-27 18:21:28,984 (beam_search:479) INFO: total log probability: -6.84
2024-10-27 18:21:28,984 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:21:28,984 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:28,984 (beam_search:483) INFO: best hypo: ▁OH▁UM▁IT'S▁THE▁AND▁THE▁NEGATIVE▁SIDE▁ARE▁UM▁SO▁THE▁NEGATIVE▁CAN'T▁GO▁THROUGH▁UM▁WITH▁ANOTHER▁NEGATIVE▁JUST▁LIKE▁A▁POSITIVE▁CAN'T▁GO▁THROUGH▁WITH▁ANOTHER▁POSITIVE▁SO▁UM▁IT▁CAN'T▁THE▁ELECTRICITY▁GO▁THROUGH▁IT▁HAS▁TO▁PLUS▁AND▁A

2024-10-27 18:21:28,986 (asr_inference:509) INFO: speech length: 22672
2024-10-27 18:21:29,966 (beam_search:428) INFO: decoder input length: 17
2024-10-27 18:21:29,966 (beam_search:429) INFO: max output length: 17
2024-10-27 18:21:29,966 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:29,977 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:29,977 (beam_search:476) INFO:  -0.52 * 1.0 =  -0.52 for ctc
2024-10-27 18:21:29,977 (beam_search:479) INFO: total log probability: -0.52
2024-10-27 18:21:29,977 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:21:29,977 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:29,977 (beam_search:483) INFO: best hypo: ▁WAS▁OFF

2024-10-27 18:21:29,980 (asr_inference:509) INFO: speech length: 276928
2024-10-27 18:21:41,483 (beam_search:428) INFO: decoder input length: 215
2024-10-27 18:21:41,483 (beam_search:429) INFO: max output length: 215
2024-10-27 18:21:41,483 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:43,036 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:43,036 (beam_search:476) INFO:  -1.13 * 1.0 =  -1.13 for ctc
2024-10-27 18:21:43,036 (beam_search:479) INFO: total log probability: -1.13
2024-10-27 18:21:43,036 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:21:43,036 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:43,036 (beam_search:483) INFO: best hypo: ▁THE▁ONE▁LIGHT▁BULB▁WAS▁OFF▁UM▁IT▁IT▁WAS▁JUST▁ONE▁WIRE▁THAT▁WAS▁CONNECTING▁FROM▁ONE▁BATTERY▁UM▁SO▁IT▁WAS▁GOING▁THROUGH▁UM▁TO▁THAT▁ONE▁LIGHT▁BULB▁AND▁NOT▁TO▁THE▁OTHER

2024-10-27 18:21:43,039 (asr_inference:509) INFO: speech length: 232016
2024-10-27 18:21:52,378 (beam_search:428) INFO: decoder input length: 180
2024-10-27 18:21:52,378 (beam_search:429) INFO: max output length: 180
2024-10-27 18:21:52,378 (beam_search:430) INFO: min output length: 0
2024-10-27 18:21:53,489 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:21:53,489 (beam_search:476) INFO:  -2.46 * 1.0 =  -2.46 for ctc
2024-10-27 18:21:53,489 (beam_search:479) INFO: total log probability: -2.46
2024-10-27 18:21:53,489 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:21:53,489 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:21:53,489 (beam_search:483) INFO: best hypo: ▁ON▁IT▁IT▁WAS▁ON▁BUT▁WHEN▁I▁ON▁IT▁IN▁UM▁AT▁THE▁UH▁WHEN▁THERE▁WAS'T▁A▁D▁CELL▁THEN▁UM▁IT▁WAS▁ONE▁OF▁THEM▁WAS▁OFF

2024-10-27 18:21:53,492 (asr_inference:509) INFO: speech length: 264256
2024-10-27 18:22:04,707 (beam_search:428) INFO: decoder input length: 205
2024-10-27 18:22:04,707 (beam_search:429) INFO: max output length: 205
2024-10-27 18:22:04,707 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:06,636 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:06,636 (beam_search:476) INFO:  -5.74 * 1.0 =  -5.74 for ctc
2024-10-27 18:22:06,636 (beam_search:479) INFO: total log probability: -5.74
2024-10-27 18:22:06,636 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:22:06,636 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:06,636 (beam_search:483) INFO: best hypo: ▁UM▁YEAH▁I▁THINK▁THEY▁ARE▁BOTH▁ON▁BUT▁UM▁THE▁BATTERY▁CAN'T▁AS▁MUCH▁POWER▁TO▁BOTH▁OF▁THEM▁UM▁IT▁CAN▁ONLY▁A▁LITTLE▁OF▁POWER▁SO▁THEY'RE▁THEY'RE▁NOT▁AS▁BRIGHT▁AS▁THE▁ONE▁WITH▁TWO▁BATTERIES▁THEY▁GET▁A

2024-10-27 18:22:06,639 (asr_inference:509) INFO: speech length: 124400
2024-10-27 18:22:11,120 (beam_search:428) INFO: decoder input length: 96
2024-10-27 18:22:11,120 (beam_search:429) INFO: max output length: 96
2024-10-27 18:22:11,120 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:11,510 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:11,510 (beam_search:476) INFO:  -3.25 * 1.0 =  -3.25 for ctc
2024-10-27 18:22:11,510 (beam_search:479) INFO: total log probability: -3.25
2024-10-27 18:22:11,510 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:22:11,510 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:11,510 (beam_search:483) INFO: best hypo: ▁THERE▁THREE▁LIGHTS▁THEN▁UM▁THE▁WOULD▁KIND▁OF▁BE▁LIKE▁ONE▁FOR▁TWO▁LIGHTS▁THEY▁WOULDN'T▁WORK▁AS▁WELL

2024-10-27 18:22:11,513 (asr_inference:509) INFO: speech length: 67712
2024-10-27 18:22:14,019 (beam_search:428) INFO: decoder input length: 52
2024-10-27 18:22:14,019 (beam_search:429) INFO: max output length: 52
2024-10-27 18:22:14,019 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:14,089 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:14,089 (beam_search:476) INFO:  -1.20 * 1.0 =  -1.20 for ctc
2024-10-27 18:22:14,089 (beam_search:479) INFO: total log probability: -1.20
2024-10-27 18:22:14,089 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:22:14,089 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:14,089 (beam_search:483) INFO: best hypo: ▁YEAH▁WE▁WOULD▁NEED▁ONE▁MORE▁BATTERY▁I

2024-10-27 18:22:14,092 (asr_inference:509) INFO: speech length: 15072
2024-10-27 18:22:14,817 (beam_search:428) INFO: decoder input length: 11
2024-10-27 18:22:14,817 (beam_search:429) INFO: max output length: 11
2024-10-27 18:22:14,817 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:14,820 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:14,821 (beam_search:476) INFO:  -0.77 * 1.0 =  -0.77 for ctc
2024-10-27 18:22:14,821 (beam_search:479) INFO: total log probability: -0.77
2024-10-27 18:22:14,821 (beam_search:480) INFO: normalized log probability: -0.39
2024-10-27 18:22:14,821 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:14,821 (beam_search:483) INFO: best hypo: 

2024-10-27 18:22:14,823 (asr_inference:509) INFO: speech length: 233008
2024-10-27 18:22:24,214 (beam_search:428) INFO: decoder input length: 181
2024-10-27 18:22:24,214 (beam_search:429) INFO: max output length: 181
2024-10-27 18:22:24,214 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:25,253 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:25,253 (beam_search:476) INFO:  -5.60 * 1.0 =  -5.60 for ctc
2024-10-27 18:22:25,253 (beam_search:479) INFO: total log probability: -5.60
2024-10-27 18:22:25,253 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:22:25,253 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:25,253 (beam_search:483) INFO: best hypo: ▁IN▁SCIENCE▁WE▁USED▁MAGNETS▁AND▁WED▁INSIDE▁OUR▁BOX▁TO▁UM▁MAKE▁THAT▁COULD▁SEE▁EM▁AND▁THEN▁WE▁USED▁METAL▁TO▁TRY▁TO▁OUT▁WHERE▁THE▁MAGNETS▁WERE

2024-10-27 18:22:25,256 (asr_inference:509) INFO: speech length: 420432
2024-10-27 18:22:44,898 (beam_search:428) INFO: decoder input length: 327
2024-10-27 18:22:44,898 (beam_search:429) INFO: max output length: 327
2024-10-27 18:22:44,898 (beam_search:430) INFO: min output length: 0
2024-10-27 18:22:48,534 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:22:48,534 (beam_search:476) INFO:  -7.42 * 1.0 =  -7.42 for ctc
2024-10-27 18:22:48,534 (beam_search:479) INFO: total log probability: -7.42
2024-10-27 18:22:48,534 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:22:48,534 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:22:48,534 (beam_search:483) INFO: best hypo: ▁WELL▁SOME▁USED▁A▁AND▁THEY▁WOULD▁PUT▁IT▁ON▁AND▁THEN▁THEED▁THEY▁WOULD▁USE▁A▁MAGNET▁AND▁TRY▁TO▁FIND▁OUT▁THE▁OTHER▁MAGNETS▁WERE▁AND▁THEN▁OTHER▁GROUPS▁JUST▁THE▁AND▁UM▁PUT▁THEM▁OVER▁THE▁OVER▁THE▁BOX▁IN▁DIFFERENT▁AND▁UM▁THEN▁THEY▁OUT▁THEY▁WOULD▁DRAW▁ON▁A▁PIECE▁OF

2024-10-27 18:22:48,538 (asr_inference:509) INFO: speech length: 442176
2024-10-27 18:23:10,691 (beam_search:428) INFO: decoder input length: 344
2024-10-27 18:23:10,692 (beam_search:429) INFO: max output length: 344
2024-10-27 18:23:10,692 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:14,341 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:14,341 (beam_search:476) INFO:  -7.18 * 1.0 =  -7.18 for ctc
2024-10-27 18:23:14,341 (beam_search:479) INFO: total log probability: -7.18
2024-10-27 18:23:14,341 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:23:14,341 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:14,342 (beam_search:483) INFO: best hypo: ▁UM▁YOU▁COULD▁FEEL▁WHERE▁THEY▁ARE▁AND▁YOU▁WOULD▁USE▁DIFFERENT▁MATERIALS▁YOU▁USE▁WASHERS▁AND▁PAPER▁AND▁YOU▁WOULD▁SOME▁HAVE▁A▁THEY▁WOULD▁THEY▁WOULD▁HAVE▁A▁AND▁SO▁THEN▁YOU▁WOULD▁KNOW▁WHERE▁THE▁MAGNET▁IS▁AND▁SOME▁WERE▁AND▁SO▁YOU▁DIDN'T▁KNOW▁WHERE▁THE▁MAGNET▁WAS▁AS

2024-10-27 18:23:14,344 (asr_inference:509) INFO: speech length: 42208
2024-10-27 18:23:15,963 (beam_search:428) INFO: decoder input length: 32
2024-10-27 18:23:15,964 (beam_search:429) INFO: max output length: 32
2024-10-27 18:23:15,964 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:15,989 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:15,989 (beam_search:476) INFO:  -0.84 * 1.0 =  -0.84 for ctc
2024-10-27 18:23:15,989 (beam_search:479) INFO: total log probability: -0.84
2024-10-27 18:23:15,989 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:23:15,989 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:15,989 (beam_search:483) INFO: best hypo: ▁THE▁FORCE▁OF▁WAS

2024-10-27 18:23:15,991 (asr_inference:509) INFO: speech length: 11440
2024-10-27 18:23:16,659 (beam_search:428) INFO: decoder input length: 8
2024-10-27 18:23:16,659 (beam_search:429) INFO: max output length: 8
2024-10-27 18:23:16,659 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:16,668 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:16,668 (beam_search:476) INFO:  -0.17 * 1.0 =  -0.17 for ctc
2024-10-27 18:23:16,668 (beam_search:479) INFO: total log probability: -0.17
2024-10-27 18:23:16,668 (beam_search:480) INFO: normalized log probability: -0.03
2024-10-27 18:23:16,668 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:16,668 (beam_search:483) INFO: best hypo: ▁YOU'RE

2024-10-27 18:23:16,670 (asr_inference:509) INFO: speech length: 41232
2024-10-27 18:23:18,247 (beam_search:428) INFO: decoder input length: 31
2024-10-27 18:23:18,247 (beam_search:429) INFO: max output length: 31
2024-10-27 18:23:18,247 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:18,280 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:18,280 (beam_search:476) INFO:  -2.70 * 1.0 =  -2.70 for ctc
2024-10-27 18:23:18,280 (beam_search:479) INFO: total log probability: -2.70
2024-10-27 18:23:18,280 (beam_search:480) INFO: normalized log probability: -0.34
2024-10-27 18:23:18,280 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:18,280 (beam_search:483) INFO: best hypo: ▁NO▁WILL▁NOT▁ATTRACT▁THE▁MAGNET

2024-10-27 18:23:18,282 (asr_inference:509) INFO: speech length: 146416
2024-10-27 18:23:23,671 (beam_search:428) INFO: decoder input length: 113
2024-10-27 18:23:23,671 (beam_search:429) INFO: max output length: 113
2024-10-27 18:23:23,671 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:24,136 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:24,136 (beam_search:476) INFO:  -3.63 * 1.0 =  -3.63 for ctc
2024-10-27 18:23:24,136 (beam_search:479) INFO: total log probability: -3.63
2024-10-27 18:23:24,136 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:23:24,136 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:24,136 (beam_search:483) INFO: best hypo: ▁THE▁THAT▁THE▁THE▁MAGNET▁TO▁HAVE▁TO▁HAVE▁IRON▁OR▁STEEL▁IN▁THEM▁AND▁A▁PLASTIC▁CHIP▁NOT▁HAVE▁IRON▁OR▁STEEL

2024-10-27 18:23:24,139 (asr_inference:509) INFO: speech length: 80416
2024-10-27 18:23:27,077 (beam_search:428) INFO: decoder input length: 62
2024-10-27 18:23:27,077 (beam_search:429) INFO: max output length: 62
2024-10-27 18:23:27,077 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:27,146 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:27,146 (beam_search:476) INFO:  -2.82 * 1.0 =  -2.82 for ctc
2024-10-27 18:23:27,146 (beam_search:479) INFO: total log probability: -2.82
2024-10-27 18:23:27,146 (beam_search:480) INFO: normalized log probability: -0.35
2024-10-27 18:23:27,146 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:27,146 (beam_search:483) INFO: best hypo: ▁IS▁ARE▁AND▁WHEN▁ARE▁AWAY

2024-10-27 18:23:27,149 (asr_inference:509) INFO: speech length: 60432
2024-10-27 18:23:29,332 (beam_search:428) INFO: decoder input length: 46
2024-10-27 18:23:29,333 (beam_search:429) INFO: max output length: 46
2024-10-27 18:23:29,333 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:29,418 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:29,419 (beam_search:476) INFO:  -1.85 * 1.0 =  -1.85 for ctc
2024-10-27 18:23:29,419 (beam_search:479) INFO: total log probability: -1.85
2024-10-27 18:23:29,419 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:23:29,419 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:29,419 (beam_search:483) INFO: best hypo: ▁THE▁MAGNET▁CAN▁GO▁THROUGH▁THINGS▁THAT▁I▁IT▁COULD▁GO

2024-10-27 18:23:29,421 (asr_inference:509) INFO: speech length: 442128
2024-10-27 18:23:50,520 (beam_search:428) INFO: decoder input length: 344
2024-10-27 18:23:50,520 (beam_search:429) INFO: max output length: 344
2024-10-27 18:23:50,520 (beam_search:430) INFO: min output length: 0
2024-10-27 18:23:54,773 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:23:54,773 (beam_search:476) INFO:  -9.72 * 1.0 =  -9.72 for ctc
2024-10-27 18:23:54,773 (beam_search:479) INFO: total log probability: -9.72
2024-10-27 18:23:54,773 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:23:54,773 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:23:54,773 (beam_search:483) INFO: best hypo: ▁WE▁DID▁THAT▁IN▁ANOTHER▁SCIENCE▁EXPERIMENT▁WE▁HAD▁A▁UM▁A▁AND▁WE▁PUT▁PLASTIC▁IN▁THERE▁AND▁THEN▁WE▁HAD▁A▁MAGNET▁IN▁ONE▁CUP▁AND▁A▁MAGNET▁THE▁CUP▁AND▁WE▁WOULD▁PUT▁PLASTIC▁IN▁BETWEEN▁THE▁TWO▁MAGNETS▁AND▁WE▁HAD▁WASHERS▁IN▁THE▁OTHER▁AND▁WE▁S▁WOULD▁SEE▁HOW▁MANY▁WASHERS▁IT▁TOOK▁TO▁SEPARATE▁THE▁FORCE▁OF

2024-10-27 18:23:54,776 (asr_inference:509) INFO: speech length: 303840
2024-10-27 18:24:07,679 (beam_search:428) INFO: decoder input length: 236
2024-10-27 18:24:07,679 (beam_search:429) INFO: max output length: 236
2024-10-27 18:24:07,679 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:09,473 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:09,473 (beam_search:476) INFO:  -3.98 * 1.0 =  -3.98 for ctc
2024-10-27 18:24:09,473 (beam_search:479) INFO: total log probability: -3.98
2024-10-27 18:24:09,473 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:24:09,473 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:09,473 (beam_search:483) INFO: best hypo: ▁THE▁TWO▁THE▁MAGNET▁IN▁A▁IS▁ATTRACTED▁TO▁THE▁MAGNET▁THE▁CUP▁AND▁SO▁THE▁OTHER▁B▁IS▁JUST▁IN▁THE▁AIR▁BECAUSE▁OF▁THE▁MAGNET▁AND▁A▁IT'SING▁IT▁DOWN▁AND▁THAT'S▁WHAT

2024-10-27 18:24:09,477 (asr_inference:509) INFO: speech length: 238512
2024-10-27 18:24:19,076 (beam_search:428) INFO: decoder input length: 185
2024-10-27 18:24:19,076 (beam_search:429) INFO: max output length: 185
2024-10-27 18:24:19,076 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:20,244 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:20,245 (beam_search:476) INFO:  -4.62 * 1.0 =  -4.62 for ctc
2024-10-27 18:24:20,245 (beam_search:479) INFO: total log probability: -4.62
2024-10-27 18:24:20,245 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:24:20,245 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:20,245 (beam_search:483) INFO: best hypo: ▁THE▁WASHERS▁WHEN▁YOU▁PUT▁THEM▁IN▁THE▁WEIGHT▁OF▁THE▁WASHERS▁IS▁TO▁BREAK▁THE▁FORCE▁OF▁A▁AND▁THE▁MAGNET▁AND▁SO▁THE▁MORE▁WASHERS▁YOU▁PUT▁IN▁THE▁THE▁THE▁FORCE▁GETS

2024-10-27 18:24:20,248 (asr_inference:509) INFO: speech length: 166320
2024-10-27 18:24:26,638 (beam_search:428) INFO: decoder input length: 129
2024-10-27 18:24:26,638 (beam_search:429) INFO: max output length: 129
2024-10-27 18:24:26,638 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:27,270 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:27,270 (beam_search:476) INFO:  -3.40 * 1.0 =  -3.40 for ctc
2024-10-27 18:24:27,270 (beam_search:479) INFO: total log probability: -3.40
2024-10-27 18:24:27,270 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:24:27,270 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:27,270 (beam_search:483) INFO: best hypo: ▁THE▁OF▁WASHERS▁WOULD▁GO▁DOWN▁BECAUSE▁OF▁THE▁WEIGHT▁WELL▁THE▁FORCE▁WAS▁EACH▁TIME▁SO▁THE▁WEIGHT▁WOULDN'T▁HAVE▁TO▁BE▁AS▁MUCH

2024-10-27 18:24:27,273 (asr_inference:509) INFO: speech length: 41440
2024-10-27 18:24:28,876 (beam_search:428) INFO: decoder input length: 31
2024-10-27 18:24:28,876 (beam_search:429) INFO: max output length: 31
2024-10-27 18:24:28,876 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:28,905 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:28,905 (beam_search:476) INFO:  -0.73 * 1.0 =  -0.73 for ctc
2024-10-27 18:24:28,905 (beam_search:479) INFO: total log probability: -0.73
2024-10-27 18:24:28,905 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:24:28,905 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:28,905 (beam_search:483) INFO: best hypo: ▁I▁HAVEED▁LEARNING▁ABOUT

2024-10-27 18:24:28,908 (asr_inference:509) INFO: speech length: 94896
2024-10-27 18:24:32,397 (beam_search:428) INFO: decoder input length: 73
2024-10-27 18:24:32,398 (beam_search:429) INFO: max output length: 73
2024-10-27 18:24:32,398 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:32,613 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:32,613 (beam_search:476) INFO:  -4.32 * 1.0 =  -4.32 for ctc
2024-10-27 18:24:32,614 (beam_search:479) INFO: total log probability: -4.32
2024-10-27 18:24:32,614 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:24:32,614 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:32,614 (beam_search:483) INFO: best hypo: ▁UM▁WE▁A▁BOX▁AND▁WE▁PUT▁MAGNETS▁SOMEWHERE▁DIDN'T▁THINK▁PEOPLE▁WOULD▁FIND▁IT

2024-10-27 18:24:32,616 (asr_inference:509) INFO: speech length: 22336
2024-10-27 18:24:33,696 (beam_search:428) INFO: decoder input length: 16
2024-10-27 18:24:33,697 (beam_search:429) INFO: max output length: 16
2024-10-27 18:24:33,697 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:33,704 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:33,705 (beam_search:476) INFO:  -0.91 * 1.0 =  -0.91 for ctc
2024-10-27 18:24:33,705 (beam_search:479) INFO: total log probability: -0.91
2024-10-27 18:24:33,705 (beam_search:480) INFO: normalized log probability: -0.30
2024-10-27 18:24:33,705 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:33,705 (beam_search:483) INFO: best hypo: ▁A

2024-10-27 18:24:33,708 (asr_inference:509) INFO: speech length: 14192
2024-10-27 18:24:34,494 (beam_search:428) INFO: decoder input length: 10
2024-10-27 18:24:34,494 (beam_search:429) INFO: max output length: 10
2024-10-27 18:24:34,494 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:34,501 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:34,501 (beam_search:476) INFO:  -0.52 * 1.0 =  -0.52 for ctc
2024-10-27 18:24:34,501 (beam_search:479) INFO: total log probability: -0.52
2024-10-27 18:24:34,501 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:24:34,501 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:34,501 (beam_search:483) INFO: best hypo: ▁A

2024-10-27 18:24:34,504 (asr_inference:509) INFO: speech length: 278784
2024-10-27 18:24:46,455 (beam_search:428) INFO: decoder input length: 217
2024-10-27 18:24:46,456 (beam_search:429) INFO: max output length: 217
2024-10-27 18:24:46,456 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:48,218 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:48,219 (beam_search:476) INFO:  -6.61 * 1.0 =  -6.61 for ctc
2024-10-27 18:24:48,219 (beam_search:479) INFO: total log probability: -6.61
2024-10-27 18:24:48,219 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:24:48,219 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:48,219 (beam_search:483) INFO: best hypo: ▁UM▁WELL▁WE▁WITH▁PEOPLE▁AND▁WE▁USED▁METALS▁OR▁A▁AND▁WE▁IT▁AROUND▁THE▁BOX▁AND▁THEN▁WE▁UM▁PUT▁IT▁WE▁WHEN▁WE▁WE▁THE▁MAGNET▁BE▁WE▁IT▁ON▁THE▁PLACE▁THE▁ON▁THE▁BOX▁ON▁THE▁PAPER

2024-10-27 18:24:48,221 (asr_inference:509) INFO: speech length: 55840
2024-10-27 18:24:50,364 (beam_search:428) INFO: decoder input length: 43
2024-10-27 18:24:50,364 (beam_search:429) INFO: max output length: 43
2024-10-27 18:24:50,364 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:50,414 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:50,415 (beam_search:476) INFO:  -1.37 * 1.0 =  -1.37 for ctc
2024-10-27 18:24:50,415 (beam_search:479) INFO: total log probability: -1.37
2024-10-27 18:24:50,415 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:24:50,415 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:50,415 (beam_search:483) INFO: best hypo: ▁THES▁THE▁MAGNET▁THEY▁WOULD▁UP

2024-10-27 18:24:50,417 (asr_inference:509) INFO: speech length: 164640
2024-10-27 18:24:57,092 (beam_search:428) INFO: decoder input length: 128
2024-10-27 18:24:57,092 (beam_search:429) INFO: max output length: 128
2024-10-27 18:24:57,092 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:57,448 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:57,448 (beam_search:476) INFO:  -1.46 * 1.0 =  -1.46 for ctc
2024-10-27 18:24:57,448 (beam_search:479) INFO: total log probability: -1.46
2024-10-27 18:24:57,448 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:24:57,448 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:57,448 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁MAGNET▁IS▁TO▁UM▁THEY▁WOULD▁UP▁BECAUSE▁IT▁WAS▁GOING▁INTO▁THE▁UM

2024-10-27 18:24:57,451 (asr_inference:509) INFO: speech length: 60160
2024-10-27 18:24:59,617 (beam_search:428) INFO: decoder input length: 46
2024-10-27 18:24:59,617 (beam_search:429) INFO: max output length: 46
2024-10-27 18:24:59,617 (beam_search:430) INFO: min output length: 0
2024-10-27 18:24:59,679 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:24:59,679 (beam_search:476) INFO:  -3.34 * 1.0 =  -3.34 for ctc
2024-10-27 18:24:59,679 (beam_search:479) INFO: total log probability: -3.34
2024-10-27 18:24:59,679 (beam_search:480) INFO: normalized log probability: -0.33
2024-10-27 18:24:59,679 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:24:59,679 (beam_search:483) INFO: best hypo: ▁BECAUSE▁A▁MAGNET'S▁TO▁AND▁STEEL

2024-10-27 18:24:59,682 (asr_inference:509) INFO: speech length: 114624
2024-10-27 18:25:04,103 (beam_search:428) INFO: decoder input length: 89
2024-10-27 18:25:04,103 (beam_search:429) INFO: max output length: 89
2024-10-27 18:25:04,103 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:04,308 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:04,308 (beam_search:476) INFO:  -2.45 * 1.0 =  -2.45 for ctc
2024-10-27 18:25:04,308 (beam_search:479) INFO: total log probability: -2.45
2024-10-27 18:25:04,308 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:25:04,308 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:04,308 (beam_search:483) INFO: best hypo: ▁UM▁NO▁IT▁HAS▁TO▁BE▁KIND▁OF▁A▁LIKE▁PAPER▁OR▁A▁VERY

2024-10-27 18:25:04,310 (asr_inference:509) INFO: speech length: 256352
2024-10-27 18:25:15,123 (beam_search:428) INFO: decoder input length: 199
2024-10-27 18:25:15,123 (beam_search:429) INFO: max output length: 199
2024-10-27 18:25:15,123 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:16,718 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:16,718 (beam_search:476) INFO:  -3.82 * 1.0 =  -3.82 for ctc
2024-10-27 18:25:16,718 (beam_search:479) INFO: total log probability: -3.82
2024-10-27 18:25:16,718 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:25:16,718 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:16,718 (beam_search:483) INFO: best hypo: ▁UM▁LIKE▁IF▁IT▁WAS▁IT'T▁GO▁THROUGH▁BECAUSE▁IT▁WAS▁TOO▁AND▁THE▁MA▁UM▁MAGNETIC▁CAN'T▁GO▁THROUGH▁THE▁BUT▁IT▁COULD▁GO▁THROUGH▁A▁PIECE▁OF▁PAPER▁OR▁BECAUSE▁IT▁AND▁IT▁WASN'T▁AS

2024-10-27 18:25:16,721 (asr_inference:509) INFO: speech length: 266800
2024-10-27 18:25:27,741 (beam_search:428) INFO: decoder input length: 207
2024-10-27 18:25:27,741 (beam_search:429) INFO: max output length: 207
2024-10-27 18:25:27,741 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:29,149 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:29,149 (beam_search:476) INFO:  -5.53 * 1.0 =  -5.53 for ctc
2024-10-27 18:25:29,149 (beam_search:479) INFO: total log probability: -5.53
2024-10-27 18:25:29,149 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:25:29,149 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:29,150 (beam_search:483) INFO: best hypo: ▁AND▁IF▁YOU▁IN▁MORE▁WASHERS▁UM▁THE▁MAGNETIC▁THE▁MAGNET▁THE▁WASHERS▁WILL▁DOWN▁UM▁THAN▁THE▁FORCE▁OF▁THE▁MAGNET▁AND▁THAT'LL▁CAUSE▁TO▁MAKE▁THE▁MOVE▁THE▁BOTTOM▁BY▁THE▁MAGNET

2024-10-27 18:25:29,153 (asr_inference:509) INFO: speech length: 467664
2024-10-27 18:25:51,592 (beam_search:428) INFO: decoder input length: 364
2024-10-27 18:25:51,592 (beam_search:429) INFO: max output length: 364
2024-10-27 18:25:51,592 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:54,999 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:54,999 (beam_search:476) INFO:  -7.13 * 1.0 =  -7.13 for ctc
2024-10-27 18:25:54,999 (beam_search:479) INFO: total log probability: -7.13
2024-10-27 18:25:54,999 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:25:54,999 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:54,999 (beam_search:483) INFO: best hypo: ▁UM▁IT'SING▁ME▁THAT▁WITH▁ZERO▁IT▁WOULD▁GET▁UP▁TO▁NINETEEN▁WASHERS▁AND▁WITH▁ONE▁IT'D▁UP▁TO▁NINE▁WASHERS▁UM▁WITH▁THREE▁SPACERS▁IT▁WOULD▁GET▁FIVE▁UM▁FOUR▁IT▁WOULD▁GET▁FIVE▁ALSO▁AND▁WITH▁FIVE▁IT▁WOULD▁GET▁THREE

2024-10-27 18:25:55,003 (asr_inference:509) INFO: speech length: 109344
2024-10-27 18:25:59,090 (beam_search:428) INFO: decoder input length: 84
2024-10-27 18:25:59,090 (beam_search:429) INFO: max output length: 84
2024-10-27 18:25:59,090 (beam_search:430) INFO: min output length: 0
2024-10-27 18:25:59,313 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:25:59,313 (beam_search:476) INFO:  -1.80 * 1.0 =  -1.80 for ctc
2024-10-27 18:25:59,313 (beam_search:479) INFO: total log probability: -1.80
2024-10-27 18:25:59,313 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:25:59,313 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:25:59,313 (beam_search:483) INFO: best hypo: ▁UM▁PROBABLY▁MAYBE▁ABOUT▁OR▁SEVEN▁BECAUSE▁IT▁WOULD▁HAVE▁TO▁BE▁THAN▁THE▁THREE

2024-10-27 18:25:59,316 (asr_inference:509) INFO: speech length: 102832
2024-10-27 18:26:03,103 (beam_search:428) INFO: decoder input length: 79
2024-10-27 18:26:03,103 (beam_search:429) INFO: max output length: 79
2024-10-27 18:26:03,103 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:03,323 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:03,324 (beam_search:476) INFO:  -1.15 * 1.0 =  -1.15 for ctc
2024-10-27 18:26:03,324 (beam_search:479) INFO: total log probability: -1.15
2024-10-27 18:26:03,324 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:26:03,324 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:03,324 (beam_search:483) INFO: best hypo: ▁IT▁WOULD▁BE▁LESS▁WASHERS▁BUT▁LESS▁THAN▁THE▁ONE▁BECAUSE▁THERE'S▁MORE▁WASHERS

2024-10-27 18:26:03,327 (asr_inference:509) INFO: speech length: 229536
2024-10-27 18:26:12,781 (beam_search:428) INFO: decoder input length: 178
2024-10-27 18:26:12,781 (beam_search:429) INFO: max output length: 178
2024-10-27 18:26:12,781 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:13,963 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:13,963 (beam_search:476) INFO:  -7.04 * 1.0 =  -7.04 for ctc
2024-10-27 18:26:13,963 (beam_search:479) INFO: total log probability: -7.04
2024-10-27 18:26:13,963 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:26:13,964 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:13,964 (beam_search:483) INFO: best hypo: ▁USING▁THE▁IT▁UM▁KIND▁OF▁BECAUSE▁IT▁WOULD▁TO▁THE▁BUT▁THERE'▁A▁OF▁THINGS▁IN▁THE▁ROOM▁AND▁IT▁WAS▁REALLY▁TO▁SOMETHING▁METAL▁AND▁IT▁WASN'T▁TO▁THE▁UM

2024-10-27 18:26:13,966 (asr_inference:509) INFO: speech length: 305984
2024-10-27 18:26:26,827 (beam_search:428) INFO: decoder input length: 238
2024-10-27 18:26:26,827 (beam_search:429) INFO: max output length: 238
2024-10-27 18:26:26,828 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:28,471 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:28,472 (beam_search:476) INFO:  -4.41 * 1.0 =  -4.41 for ctc
2024-10-27 18:26:28,472 (beam_search:479) INFO: total log probability: -4.41
2024-10-27 18:26:28,472 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:26:28,472 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:28,472 (beam_search:483) INFO: best hypo: ▁UM▁THE▁IS▁TO▁THE▁OR▁AND▁IT'S▁USED▁TO▁MAGNETIC▁BUT▁TO▁IT'S▁IT'S▁MAGNETIC▁AND▁UM▁IT▁IT'S▁WHEN▁IT'S▁UM▁I▁PUT▁WHEN

2024-10-27 18:26:28,475 (asr_inference:509) INFO: speech length: 134928
2024-10-27 18:26:33,666 (beam_search:428) INFO: decoder input length: 104
2024-10-27 18:26:33,666 (beam_search:429) INFO: max output length: 104
2024-10-27 18:26:33,666 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:34,072 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:34,072 (beam_search:476) INFO:  -3.03 * 1.0 =  -3.03 for ctc
2024-10-27 18:26:34,072 (beam_search:479) INFO: total log probability: -3.03
2024-10-27 18:26:34,072 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:26:34,072 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:34,072 (beam_search:483) INFO: best hypo: 'S▁BY▁THAN▁THE▁MAGNET▁IT▁WILL▁POINT▁TO▁THAT▁INSTEAD▁OF▁THE▁MAGNET▁AND▁THAT'S▁WHAT▁WAS▁ME

2024-10-27 18:26:34,074 (asr_inference:509) INFO: speech length: 199184
2024-10-27 18:26:41,969 (beam_search:428) INFO: decoder input length: 155
2024-10-27 18:26:41,970 (beam_search:429) INFO: max output length: 155
2024-10-27 18:26:41,970 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:42,415 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:42,415 (beam_search:476) INFO:  -2.90 * 1.0 =  -2.90 for ctc
2024-10-27 18:26:42,415 (beam_search:479) INFO: total log probability: -2.90
2024-10-27 18:26:42,415 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:26:42,415 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:42,415 (beam_search:483) INFO: best hypo: ▁UM▁THES▁THAN▁THE▁AND▁A▁OF▁STEEL▁MAYBE▁OR▁SOMETHING▁LIKE▁MAYBE▁LIKE▁OR

2024-10-27 18:26:42,418 (asr_inference:509) INFO: speech length: 152832
2024-10-27 18:26:48,185 (beam_search:428) INFO: decoder input length: 118
2024-10-27 18:26:48,185 (beam_search:429) INFO: max output length: 118
2024-10-27 18:26:48,185 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:48,607 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:48,607 (beam_search:476) INFO:  -2.45 * 1.0 =  -2.45 for ctc
2024-10-27 18:26:48,607 (beam_search:479) INFO: total log probability: -2.45
2024-10-27 18:26:48,607 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:26:48,607 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:48,607 (beam_search:483) INFO: best hypo: ▁A▁PAPER▁BECAUSE▁IF▁IT▁TO▁THAT▁WHERE▁THE▁MAGNET▁WAS▁THEN▁YOU'D▁KNOW▁WHERE▁IT▁WAS▁YEAH

2024-10-27 18:26:48,610 (asr_inference:509) INFO: speech length: 176032
2024-10-27 18:26:55,452 (beam_search:428) INFO: decoder input length: 137
2024-10-27 18:26:55,452 (beam_search:429) INFO: max output length: 137
2024-10-27 18:26:55,452 (beam_search:430) INFO: min output length: 0
2024-10-27 18:26:55,976 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:26:55,976 (beam_search:476) INFO:  -1.74 * 1.0 =  -1.74 for ctc
2024-10-27 18:26:55,976 (beam_search:479) INFO: total log probability: -1.74
2024-10-27 18:26:55,977 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:26:55,977 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:26:55,977 (beam_search:483) INFO: best hypo: ▁WELL▁UM▁IF▁THE▁MAGNET▁CAN▁ATTRACT▁TO▁THE▁PAPER▁CLIP▁IT▁WILL▁STICK▁TO▁THE▁PAPER▁CLIP▁OF▁AND▁UM

2024-10-27 18:26:55,979 (asr_inference:509) INFO: speech length: 128800
2024-10-27 18:27:00,878 (beam_search:428) INFO: decoder input length: 100
2024-10-27 18:27:00,879 (beam_search:429) INFO: max output length: 100
2024-10-27 18:27:00,879 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:01,353 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:01,353 (beam_search:476) INFO:  -3.59 * 1.0 =  -3.59 for ctc
2024-10-27 18:27:01,353 (beam_search:479) INFO: total log probability: -3.59
2024-10-27 18:27:01,353 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:27:01,353 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:01,353 (beam_search:483) INFO: best hypo: ▁AND▁WHEN▁IT▁TO▁THE▁PAPER▁CLIP▁YOU▁CAN▁THE▁FORCE▁WHEN▁YOU▁THE▁PAPER▁CLIP▁AND▁THEN▁YOU'LL▁KNOW▁THAT▁THE▁MAGNET'S▁THERE

2024-10-27 18:27:01,355 (asr_inference:509) INFO: speech length: 285808
2024-10-27 18:27:13,146 (beam_search:428) INFO: decoder input length: 222
2024-10-27 18:27:13,147 (beam_search:429) INFO: max output length: 222
2024-10-27 18:27:13,147 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:13,886 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:13,886 (beam_search:476) INFO:  -5.03 * 1.0 =  -5.03 for ctc
2024-10-27 18:27:13,886 (beam_search:479) INFO: total log probability: -5.03
2024-10-27 18:27:13,886 (beam_search:480) INFO: normalized log probability: -0.24
2024-10-27 18:27:13,886 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:13,886 (beam_search:483) INFO: best hypo: ▁OF▁CAUSE▁OPPOSITE▁ATTRACT▁AND▁THE▁SIDE▁AND▁SO▁IF▁UH▁I▁DON'T▁KNOW▁IF▁THE▁UM

2024-10-27 18:27:13,889 (asr_inference:509) INFO: speech length: 439088
2024-10-27 18:27:34,607 (beam_search:428) INFO: decoder input length: 342
2024-10-27 18:27:34,608 (beam_search:429) INFO: max output length: 342
2024-10-27 18:27:34,608 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:38,233 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:38,233 (beam_search:476) INFO:  -8.72 * 1.0 =  -8.72 for ctc
2024-10-27 18:27:38,233 (beam_search:479) INFO: total log probability: -8.72
2024-10-27 18:27:38,234 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:27:38,234 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:38,234 (beam_search:483) INFO: best hypo: ▁WELL▁A▁MAGNET▁HAS▁TWO▁DIFFERENT▁SIDES▁NORTH▁AND▁AND▁A▁AND▁A▁NORTH▁OR▁AND▁A▁BUT▁A▁AND▁A▁AND▁A▁AND▁A▁NORTH▁SIDE▁BECAUSE▁THEY'RE▁THE▁SAME▁SIDE▁OF▁THE▁MAGNET▁AND▁THEY▁WON'T▁STICK▁BECAUSE▁THEY'RE▁THE▁SAME▁BUT▁DIFFERENT▁WILL▁UM▁UH▁STICK

2024-10-27 18:27:38,237 (asr_inference:509) INFO: speech length: 161504
2024-10-27 18:27:44,652 (beam_search:428) INFO: decoder input length: 125
2024-10-27 18:27:44,652 (beam_search:429) INFO: max output length: 125
2024-10-27 18:27:44,652 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:45,126 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:45,127 (beam_search:476) INFO:  -4.24 * 1.0 =  -4.24 for ctc
2024-10-27 18:27:45,127 (beam_search:479) INFO: total log probability: -4.24
2024-10-27 18:27:45,127 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:27:45,127 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:45,127 (beam_search:483) INFO: best hypo: ▁THE▁MAGNETS▁ARE▁AND▁OR▁AND▁THEY'LL▁UM▁ATTRACT▁BUT▁IF▁THEY'RE▁AND▁OR▁AND'LL

2024-10-27 18:27:45,129 (asr_inference:509) INFO: speech length: 10144
2024-10-27 18:27:45,832 (beam_search:428) INFO: decoder input length: 7
2024-10-27 18:27:45,832 (beam_search:429) INFO: max output length: 7
2024-10-27 18:27:45,832 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:45,837 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:45,837 (beam_search:476) INFO:  -0.31 * 1.0 =  -0.31 for ctc
2024-10-27 18:27:45,837 (beam_search:479) INFO: total log probability: -0.31
2024-10-27 18:27:45,837 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:27:45,837 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:45,837 (beam_search:483) INFO: best hypo: ▁YOU

2024-10-27 18:27:45,839 (asr_inference:509) INFO: speech length: 168576
2024-10-27 18:27:52,656 (beam_search:428) INFO: decoder input length: 131
2024-10-27 18:27:52,656 (beam_search:429) INFO: max output length: 131
2024-10-27 18:27:52,656 (beam_search:430) INFO: min output length: 0
2024-10-27 18:27:53,257 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:27:53,257 (beam_search:476) INFO:  -2.29 * 1.0 =  -2.29 for ctc
2024-10-27 18:27:53,257 (beam_search:479) INFO: total log probability: -2.29
2024-10-27 18:27:53,257 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:27:53,257 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:27:53,257 (beam_search:483) INFO: best hypo: ▁WELL▁IN▁SCIENCE▁WE'VE▁BEEN▁LEARNING▁ABOUT▁AND▁MAGNETISM▁AND▁SO▁WE'VE▁BEEN▁OUT▁HOW▁WE▁CAN▁USE▁A▁BATTERY▁TO▁LIGHT

2024-10-27 18:27:53,260 (asr_inference:509) INFO: speech length: 455360
2024-10-27 18:28:15,190 (beam_search:428) INFO: decoder input length: 355
2024-10-27 18:28:15,190 (beam_search:429) INFO: max output length: 355
2024-10-27 18:28:15,190 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:20,057 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:20,058 (beam_search:476) INFO:  -6.41 * 1.0 =  -6.41 for ctc
2024-10-27 18:28:20,058 (beam_search:479) INFO: total log probability: -6.41
2024-10-27 18:28:20,058 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:28:20,058 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:20,058 (beam_search:483) INFO: best hypo: ▁UM▁SO▁WE▁USED▁A▁MOTOR▁AND▁WEED▁UP▁TO▁A▁CIRCUIT▁AND▁THEN▁UM▁WE▁HAD▁WIRES▁AND▁WE▁HAD▁A▁UM▁A▁MOTOR▁AND▁WE▁PUT▁THE▁MOTOR▁IN▁THE▁ON▁THE▁AND▁THE▁BATTERY▁IN▁THE▁FOR▁THE▁BATTERY▁AND▁THEN▁UM▁WE▁THE▁WIRES▁THROUGH▁THE▁AND▁THE▁UM▁ON▁AND▁OFF▁SWITCH▁AND▁THEN▁WE▁COULD▁TURN▁THE▁LIGHT▁BULB▁ON▁OR▁OFF

2024-10-27 18:28:20,061 (asr_inference:509) INFO: speech length: 266176
2024-10-27 18:28:31,343 (beam_search:428) INFO: decoder input length: 207
2024-10-27 18:28:31,343 (beam_search:429) INFO: max output length: 207
2024-10-27 18:28:31,343 (beam_search:430) INFO: min output length: 0
2024-10-27 18:28:32,862 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:28:32,862 (beam_search:476) INFO:  -5.95 * 1.0 =  -5.95 for ctc
2024-10-27 18:28:32,862 (beam_search:479) INFO: total log probability: -5.95
2024-10-27 18:28:32,862 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:28:32,862 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:28:32,862 (beam_search:483) INFO: best hypo: ▁WELL▁THERE▁WAS▁A▁THAT▁HAD▁IN▁IT▁WHERE▁THE▁BATTERY▁WOULD▁FIT▁IN▁AND▁THE▁WOULD▁FIT▁IN▁AND▁AND▁WOULD▁FIT▁AND▁THEN▁OTHER▁THAT▁WE▁DIDN'T▁USE▁WE▁WHERE▁THEY▁WENT▁BECAUSE▁THEY▁WOULD

2024-10-27 18:28:32,865 (asr_inference:509) INFO: speech length: 473008
2024-10-27 18:28:56,030 (beam_search:428) INFO: decoder input length: 369
2024-10-27 18:28:56,030 (beam_search:429) INFO: max output length: 369
2024-10-27 18:28:56,030 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:01,537 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:01,538 (beam_search:476) INFO:  -8.30 * 1.0 =  -8.30 for ctc
2024-10-27 18:29:01,538 (beam_search:479) INFO: total log probability: -8.30
2024-10-27 18:29:01,538 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:29:01,538 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:01,538 (beam_search:483) INFO: best hypo: ▁YOU▁WOULDN'T▁USE▁THE▁MOTOR▁YOU▁WOULD▁TAKE▁THE▁MOTOR▁AWAY▁AND▁THEN▁PUT▁THE▁UM▁LIGHT▁BULB▁ON▁A▁LIGHT▁BULB▁CIRCUIT▁AND▁THAT▁COULD▁GO▁IN▁THE▁IN▁BETWEEN▁THE▁MOTOR▁AND▁THE▁UM▁BATTERY▁AND▁SO▁THEN▁YOU▁WOULD▁TAKE▁THE▁WIRES▁AND▁ATTACH▁IT▁TO▁THE▁UM▁LIGHT▁BULBS▁UM▁LITTLE▁PLACE▁THAT▁IT▁WOULD▁GO▁ON▁AND▁THEN▁IT▁WOULD▁TURN▁ON▁IF▁YOU▁HAD▁THE▁ON▁AND▁OFF▁SWITCH

2024-10-27 18:29:01,540 (asr_inference:509) INFO: speech length: 162592
2024-10-27 18:29:07,976 (beam_search:428) INFO: decoder input length: 126
2024-10-27 18:29:07,976 (beam_search:429) INFO: max output length: 126
2024-10-27 18:29:07,976 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:08,463 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:08,464 (beam_search:476) INFO:  -2.82 * 1.0 =  -2.82 for ctc
2024-10-27 18:29:08,464 (beam_search:479) INFO: total log probability: -2.82
2024-10-27 18:29:08,464 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:29:08,464 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:08,464 (beam_search:483) INFO: best hypo: ▁YOU▁WOULD▁THE▁WIRES▁CONNECTED▁TO▁THE▁TO▁THE▁BATTERY▁AND▁THEN▁THE▁BATTERY▁WOULD▁MAKE▁IT▁RUN▁UM▁IN▁A▁CIRCLE

2024-10-27 18:29:08,467 (asr_inference:509) INFO: speech length: 110368
2024-10-27 18:29:12,599 (beam_search:428) INFO: decoder input length: 85
2024-10-27 18:29:12,599 (beam_search:429) INFO: max output length: 85
2024-10-27 18:29:12,599 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:12,845 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:12,845 (beam_search:476) INFO:  -0.71 * 1.0 =  -0.71 for ctc
2024-10-27 18:29:12,845 (beam_search:479) INFO: total log probability: -0.71
2024-10-27 18:29:12,845 (beam_search:480) INFO: normalized log probability: -0.04
2024-10-27 18:29:12,845 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:12,845 (beam_search:483) INFO: best hypo: ▁THE▁WIRES▁TO▁THE▁BATTERY▁AND▁THEN▁THE▁OTHER▁WIRES▁TO▁THE▁ON▁AND▁OFF▁SWITCH

2024-10-27 18:29:12,847 (asr_inference:509) INFO: speech length: 286272
2024-10-27 18:29:24,875 (beam_search:428) INFO: decoder input length: 223
2024-10-27 18:29:24,875 (beam_search:429) INFO: max output length: 223
2024-10-27 18:29:24,875 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:27,146 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:27,147 (beam_search:476) INFO:  -7.26 * 1.0 =  -7.26 for ctc
2024-10-27 18:29:27,147 (beam_search:479) INFO: total log probability: -7.26
2024-10-27 18:29:27,147 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:29:27,147 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:27,147 (beam_search:483) INFO: best hypo: ▁THE▁AND▁SWITCH▁IS▁IMPORTANT▁BECAUSE▁IF▁YOU▁DON'T▁HAVE▁THE▁AND▁OFF▁SWITCH▁CONNECTED▁THE▁ENERGY'S▁JUST▁GONNA▁OFF▁THE▁END▁OR▁IT▁WOULD▁JUST▁BUT▁THEN▁IF▁IT'S▁CONNECTED▁IT▁GOES▁BACK▁THROUGH▁THE▁WIRES▁AND▁ONTO▁THE▁MOTOR▁WHICH▁MAKES▁THE▁MOTOR▁RUN

2024-10-27 18:29:27,149 (asr_inference:509) INFO: speech length: 137664
2024-10-27 18:29:32,172 (beam_search:428) INFO: decoder input length: 107
2024-10-27 18:29:32,172 (beam_search:429) INFO: max output length: 107
2024-10-27 18:29:32,172 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:32,624 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:32,624 (beam_search:476) INFO:  -2.74 * 1.0 =  -2.74 for ctc
2024-10-27 18:29:32,624 (beam_search:479) INFO: total log probability: -2.74
2024-10-27 18:29:32,624 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:29:32,624 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:32,624 (beam_search:483) INFO: best hypo: ▁THAT▁MEANS▁THAT▁THE▁ENERGY▁THROUGH▁THE▁ON▁AND▁AND▁IF▁YOU▁DON'T▁HAVE▁THE▁AND▁CONNECTED▁IT▁WON'T▁THROUGH

2024-10-27 18:29:32,626 (asr_inference:509) INFO: speech length: 122416
2024-10-27 18:29:37,122 (beam_search:428) INFO: decoder input length: 95
2024-10-27 18:29:37,122 (beam_search:429) INFO: max output length: 95
2024-10-27 18:29:37,123 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:37,549 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:37,549 (beam_search:476) INFO:  -3.94 * 1.0 =  -3.94 for ctc
2024-10-27 18:29:37,549 (beam_search:479) INFO: total log probability: -3.94
2024-10-27 18:29:37,549 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:29:37,549 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:37,549 (beam_search:483) INFO: best hypo: ▁WELL▁IF▁THE▁SWITCH▁IS▁IN▁THE▁WRONG▁THE▁MOTOR'S▁NOT▁GONNA▁RUN▁AND▁IF▁IT'S▁IN▁THE▁RIGHT▁THE▁MOTOR▁RUN

2024-10-27 18:29:37,552 (asr_inference:509) INFO: speech length: 123312
2024-10-27 18:29:42,148 (beam_search:428) INFO: decoder input length: 95
2024-10-27 18:29:42,148 (beam_search:429) INFO: max output length: 95
2024-10-27 18:29:42,148 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:42,510 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:42,510 (beam_search:476) INFO:  -2.25 * 1.0 =  -2.25 for ctc
2024-10-27 18:29:42,511 (beam_search:479) INFO: total log probability: -2.25
2024-10-27 18:29:42,511 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:29:42,511 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:42,511 (beam_search:483) INFO: best hypo: ▁IF▁THE▁MOTOR'S▁UM▁AND▁IF▁THE▁WIRES'T▁CONNECTED▁TO▁THE▁BATTERY▁IT▁ALSO▁WON'T▁RUN

2024-10-27 18:29:42,513 (asr_inference:509) INFO: speech length: 32064
2024-10-27 18:29:43,882 (beam_search:428) INFO: decoder input length: 24
2024-10-27 18:29:43,882 (beam_search:429) INFO: max output length: 24
2024-10-27 18:29:43,882 (beam_search:430) INFO: min output length: 0
2024-10-27 18:29:43,913 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:29:43,913 (beam_search:476) INFO:  -0.91 * 1.0 =  -0.91 for ctc
2024-10-27 18:29:43,913 (beam_search:479) INFO: total log probability: -0.91
2024-10-27 18:29:43,913 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:29:43,913 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:29:43,913 (beam_search:483) INFO: best hypo: ▁IT'S▁NOT▁IN▁THE▁RIGHT

2024-10-27 18:29:43,916 (asr_inference:509) INFO: speech length: 385152
2024-10-27 18:30:01,116 (beam_search:428) INFO: decoder input length: 300
2024-10-27 18:30:01,116 (beam_search:429) INFO: max output length: 300
2024-10-27 18:30:01,116 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:04,409 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:04,409 (beam_search:476) INFO:  -5.13 * 1.0 =  -5.13 for ctc
2024-10-27 18:30:04,409 (beam_search:479) INFO: total log probability: -5.13
2024-10-27 18:30:04,409 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:30:04,409 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:04,409 (beam_search:483) INFO: best hypo: ▁BECAUSE▁IT'S▁OUT▁AND▁NOT▁CONNECTED▁TO▁THE▁WIRE▁AND▁WHEN▁IT'S▁CONNECTED▁TO▁THE▁WIRE▁THE▁ENERGY▁THROUGH▁INTO▁THE▁UM▁WIRE▁TO▁THE▁MOTOR▁BUT▁IT'S▁NOT▁UM▁IT▁THE▁ENERGY▁JUST▁AND▁IT▁CAN'T▁GO▁ANYWHERE▁SO▁IT▁CAN'T▁MAKE▁THE▁MOTOR▁RUN

2024-10-27 18:30:04,412 (asr_inference:509) INFO: speech length: 145504
2024-10-27 18:30:09,952 (beam_search:428) INFO: decoder input length: 113
2024-10-27 18:30:09,952 (beam_search:429) INFO: max output length: 113
2024-10-27 18:30:09,952 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:10,173 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:10,174 (beam_search:476) INFO:  -1.44 * 1.0 =  -1.44 for ctc
2024-10-27 18:30:10,174 (beam_search:479) INFO: total log probability: -1.44
2024-10-27 18:30:10,174 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:30:10,174 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:10,174 (beam_search:483) INFO: best hypo: ▁UM▁ALSO▁IN▁SCIENCE▁WE▁HAVE▁A▁UM▁OH▁WE▁DO

2024-10-27 18:30:10,177 (asr_inference:509) INFO: speech length: 199504
2024-10-27 18:30:18,192 (beam_search:428) INFO: decoder input length: 155
2024-10-27 18:30:18,192 (beam_search:429) INFO: max output length: 155
2024-10-27 18:30:18,192 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:19,005 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:19,005 (beam_search:476) INFO:  -3.19 * 1.0 =  -3.19 for ctc
2024-10-27 18:30:19,005 (beam_search:479) INFO: total log probability: -3.19
2024-10-27 18:30:19,005 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:30:19,005 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:19,005 (beam_search:483) INFO: best hypo: ▁WELL▁THE▁LIGHT▁BULBS▁IN▁THE▁LIGHT▁BULB▁CIRCUIT▁THING▁AND▁THE▁REASON▁IT'S▁NOT▁LIGHTING▁UP▁IS▁BECAUSE▁THE▁ON▁IS▁IS▁OFF▁AND▁NOT▁ON

2024-10-27 18:30:19,009 (asr_inference:509) INFO: speech length: 304960
2024-10-27 18:30:32,253 (beam_search:428) INFO: decoder input length: 237
2024-10-27 18:30:32,253 (beam_search:429) INFO: max output length: 237
2024-10-27 18:30:32,253 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:34,156 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:34,156 (beam_search:476) INFO:  -5.60 * 1.0 =  -5.60 for ctc
2024-10-27 18:30:34,156 (beam_search:479) INFO: total log probability: -5.60
2024-10-27 18:30:34,156 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:30:34,156 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:34,156 (beam_search:483) INFO: best hypo: ▁THE▁ELECTRICITY▁FLOWS▁THROUGH▁THE▁CIRCUIT▁BECAUSE▁IT▁COMES▁FROM▁THE▁BATTERY▁AND▁THEN▁IT▁COMES▁OUT▁THROUGH▁THE▁WIRES▁THE▁AND▁THEN▁TO▁THE▁THAT▁THE▁LIGHT▁BULBS▁ON▁AND▁THEN▁IT▁UP▁INTO▁THE▁LIGHT▁BULB▁AND▁LIGHTS▁IT

2024-10-27 18:30:34,160 (asr_inference:509) INFO: speech length: 61536
2024-10-27 18:30:36,510 (beam_search:428) INFO: decoder input length: 47
2024-10-27 18:30:36,511 (beam_search:429) INFO: max output length: 47
2024-10-27 18:30:36,511 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:36,590 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:36,590 (beam_search:476) INFO:  -1.53 * 1.0 =  -1.53 for ctc
2024-10-27 18:30:36,590 (beam_search:479) INFO: total log probability: -1.53
2024-10-27 18:30:36,590 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:30:36,590 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:36,591 (beam_search:483) INFO: best hypo: ▁IT▁FROM▁THE▁NEGATIVE▁SIDE▁TO▁THE▁POSITIVE▁SIDE

2024-10-27 18:30:36,593 (asr_inference:509) INFO: speech length: 79424
2024-10-27 18:30:39,585 (beam_search:428) INFO: decoder input length: 61
2024-10-27 18:30:39,586 (beam_search:429) INFO: max output length: 61
2024-10-27 18:30:39,586 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:39,738 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:39,738 (beam_search:476) INFO:  -1.20 * 1.0 =  -1.20 for ctc
2024-10-27 18:30:39,738 (beam_search:479) INFO: total log probability: -1.20
2024-10-27 18:30:39,738 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:30:39,738 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:39,738 (beam_search:483) INFO: best hypo: ▁IT'S▁THE▁WIRE▁THAT'S▁THE▁BULB▁TO▁MAKE▁THE▁BULB▁LIGHT▁UP

2024-10-27 18:30:39,740 (asr_inference:509) INFO: speech length: 195456
2024-10-27 18:30:47,370 (beam_search:428) INFO: decoder input length: 152
2024-10-27 18:30:47,370 (beam_search:429) INFO: max output length: 152
2024-10-27 18:30:47,370 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:48,322 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:48,322 (beam_search:476) INFO:  -1.67 * 1.0 =  -1.67 for ctc
2024-10-27 18:30:48,322 (beam_search:479) INFO: total log probability: -1.67
2024-10-27 18:30:48,322 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:30:48,322 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:48,322 (beam_search:483) INFO: best hypo: ▁THAT▁YOU▁CAN▁SEE▁THE▁WIRE▁THAT'S▁CONNECTED▁TO▁THE▁OF▁THE▁BULB▁AND▁THEN▁IT▁CONNECTS▁TO▁THE▁BOTTOM▁OF▁THE▁BULB▁AND▁THAT'S▁HOW▁IT▁LIGHTS▁IT

2024-10-27 18:30:48,324 (asr_inference:509) INFO: speech length: 194448
2024-10-27 18:30:56,012 (beam_search:428) INFO: decoder input length: 151
2024-10-27 18:30:56,012 (beam_search:429) INFO: max output length: 151
2024-10-27 18:30:56,012 (beam_search:430) INFO: min output length: 0
2024-10-27 18:30:56,702 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:30:56,702 (beam_search:476) INFO:  -3.06 * 1.0 =  -3.06 for ctc
2024-10-27 18:30:56,702 (beam_search:479) INFO: total log probability: -3.06
2024-10-27 18:30:56,702 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:30:56,702 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:30:56,702 (beam_search:483) INFO: best hypo: ▁UM▁THE▁IS▁A▁CIRCLE▁AND▁THE▁ENERGY▁WOULD▁FLOW▁IN▁A▁CIRCUIT▁WHICH▁MEANS▁THE▁ENERGY▁WOULD▁FLOW▁IN▁A▁THROUGH▁THE▁LIGHT▁BULB

2024-10-27 18:30:56,705 (asr_inference:509) INFO: speech length: 357280
2024-10-27 18:31:12,540 (beam_search:428) INFO: decoder input length: 278
2024-10-27 18:31:12,540 (beam_search:429) INFO: max output length: 278
2024-10-27 18:31:12,540 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:15,061 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:15,061 (beam_search:476) INFO:  -4.79 * 1.0 =  -4.79 for ctc
2024-10-27 18:31:15,061 (beam_search:479) INFO: total log probability: -4.79
2024-10-27 18:31:15,061 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:31:15,061 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:15,062 (beam_search:483) INFO: best hypo: ▁NO▁YOU▁CAN'T▁PUT▁THE▁WIRES▁ON▁THE▁BULB▁YOU▁HAVE▁TO▁PUT▁THEM▁AT▁THE▁BOTTOM▁THE▁UM▁WHERE▁THE▁LITTLE▁CYLINDER▁IS▁OR▁ELSE▁BECAUSE▁THAT'S▁WHERE▁THE▁WIRE▁CONNECTED▁SO▁THAT'S▁HOW▁IT▁WILL▁LIGHT▁THE▁LIGHT▁BULB

2024-10-27 18:31:15,064 (asr_inference:509) INFO: speech length: 45616
2024-10-27 18:31:16,846 (beam_search:428) INFO: decoder input length: 35
2024-10-27 18:31:16,847 (beam_search:429) INFO: max output length: 35
2024-10-27 18:31:16,847 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:16,895 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:16,896 (beam_search:476) INFO:  -1.41 * 1.0 =  -1.41 for ctc
2024-10-27 18:31:16,896 (beam_search:479) INFO: total log probability: -1.41
2024-10-27 18:31:16,896 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:31:16,896 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:16,896 (beam_search:483) INFO: best hypo: ▁YOU▁WOULD▁BOTH▁AT▁THE▁BOTTOM▁OF▁THE

2024-10-27 18:31:16,899 (asr_inference:509) INFO: speech length: 142288
2024-10-27 18:31:22,276 (beam_search:428) INFO: decoder input length: 110
2024-10-27 18:31:22,276 (beam_search:429) INFO: max output length: 110
2024-10-27 18:31:22,276 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:22,648 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:22,648 (beam_search:476) INFO:  -3.34 * 1.0 =  -3.34 for ctc
2024-10-27 18:31:22,648 (beam_search:479) INFO: total log probability: -3.34
2024-10-27 18:31:22,648 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:31:22,648 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:22,648 (beam_search:483) INFO: best hypo: ▁IT▁WOULD▁PROBABLY▁LIGHT▁IT▁BUT▁IT'S▁NOT▁TOUCHING▁UM▁YOU▁MIGHT▁HAVE▁A▁TO▁LIGHT▁THE

2024-10-27 18:31:22,650 (asr_inference:509) INFO: speech length: 73872
2024-10-27 18:31:25,393 (beam_search:428) INFO: decoder input length: 57
2024-10-27 18:31:25,393 (beam_search:429) INFO: max output length: 57
2024-10-27 18:31:25,393 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:25,533 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:25,533 (beam_search:476) INFO:  -0.89 * 1.0 =  -0.89 for ctc
2024-10-27 18:31:25,533 (beam_search:479) INFO: total log probability: -0.89
2024-10-27 18:31:25,533 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:31:25,533 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:25,533 (beam_search:483) INFO: best hypo: ▁ONE'S▁CONNECTED▁AT▁THE▁BOTTOM▁AND▁ONE▁WIRE'S▁CONNECTED▁AT▁THE▁SIDE

2024-10-27 18:31:25,536 (asr_inference:509) INFO: speech length: 177760
2024-10-27 18:31:32,475 (beam_search:428) INFO: decoder input length: 138
2024-10-27 18:31:32,475 (beam_search:429) INFO: max output length: 138
2024-10-27 18:31:32,475 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:32,900 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:32,900 (beam_search:476) INFO:  -3.01 * 1.0 =  -3.01 for ctc
2024-10-27 18:31:32,900 (beam_search:479) INFO: total log probability: -3.01
2024-10-27 18:31:32,900 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:31:32,900 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:32,900 (beam_search:483) INFO: best hypo: ▁UH▁WE▁MAKE▁UM▁WE▁TOOK▁UM▁THE▁ONE▁IS'TD▁TO▁BE▁ANOTHER▁WIRE

2024-10-27 18:31:32,902 (asr_inference:509) INFO: speech length: 204544
2024-10-27 18:31:41,009 (beam_search:428) INFO: decoder input length: 159
2024-10-27 18:31:41,009 (beam_search:429) INFO: max output length: 159
2024-10-27 18:31:41,009 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:41,967 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:41,967 (beam_search:476) INFO:  -7.11 * 1.0 =  -7.11 for ctc
2024-10-27 18:31:41,967 (beam_search:479) INFO: total log probability: -7.11
2024-10-27 18:31:41,967 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:31:41,967 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:41,967 (beam_search:483) INFO: best hypo: ▁THED▁AROUND▁THES▁IT▁AND▁UM▁THAT▁KIND▁IT▁OTHERE▁IT▁KIND▁OF▁WOULD▁BE▁JUST▁IT▁WOULDN'T▁BE▁A▁MAGNET▁IF▁IT▁WAS▁A▁MAGNET▁AT

2024-10-27 18:31:41,970 (asr_inference:509) INFO: speech length: 117680
2024-10-27 18:31:46,283 (beam_search:428) INFO: decoder input length: 91
2024-10-27 18:31:46,283 (beam_search:429) INFO: max output length: 91
2024-10-27 18:31:46,283 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:46,541 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:46,541 (beam_search:476) INFO:  -1.75 * 1.0 =  -1.75 for ctc
2024-10-27 18:31:46,541 (beam_search:479) INFO: total log probability: -1.75
2024-10-27 18:31:46,541 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:31:46,541 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:46,541 (beam_search:483) INFO: best hypo: ▁UM▁YEAH▁CAUSE▁WE▁TRIED▁THAT▁A▁TIMES▁AND▁IT▁DIDN'T▁REALLY▁OUT▁WELL

2024-10-27 18:31:46,543 (asr_inference:509) INFO: speech length: 52928
2024-10-27 18:31:48,498 (beam_search:428) INFO: decoder input length: 40
2024-10-27 18:31:48,498 (beam_search:429) INFO: max output length: 40
2024-10-27 18:31:48,498 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:48,548 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:48,548 (beam_search:476) INFO:  -1.91 * 1.0 =  -1.91 for ctc
2024-10-27 18:31:48,548 (beam_search:479) INFO: total log probability: -1.91
2024-10-27 18:31:48,548 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:31:48,548 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:48,548 (beam_search:483) INFO: best hypo: ▁YES▁WE▁DID▁MAKE▁IT▁A▁MAGNET

2024-10-27 18:31:48,550 (asr_inference:509) INFO: speech length: 14496
2024-10-27 18:31:49,282 (beam_search:428) INFO: decoder input length: 10
2024-10-27 18:31:49,282 (beam_search:429) INFO: max output length: 10
2024-10-27 18:31:49,282 (beam_search:430) INFO: min output length: 0
2024-10-27 18:31:49,289 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:31:49,289 (beam_search:476) INFO:  -0.64 * 1.0 =  -0.64 for ctc
2024-10-27 18:31:49,289 (beam_search:479) INFO: total log probability: -0.64
2024-10-27 18:31:49,289 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:31:49,289 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:31:49,289 (beam_search:483) INFO: best hypo: ▁THE

2024-10-27 18:31:49,291 (asr_inference:509) INFO: speech length: 401072
2024-10-27 18:32:09,470 (beam_search:428) INFO: decoder input length: 312
2024-10-27 18:32:09,470 (beam_search:429) INFO: max output length: 312
2024-10-27 18:32:09,470 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:12,107 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:12,108 (beam_search:476) INFO:  -6.87 * 1.0 =  -6.87 for ctc
2024-10-27 18:32:12,108 (beam_search:479) INFO: total log probability: -6.87
2024-10-27 18:32:12,108 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:32:12,108 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:12,108 (beam_search:483) INFO: best hypo: ▁MAYBE▁ELECTRICITY▁WE▁DIDN'T▁REALLY▁TALK▁ABOUT▁THIS▁BUT▁IT▁KIND▁OF▁INTO▁UM▁IT▁LIKE▁THROUGH▁THE▁WIRE▁IT▁A▁LITTLE▁BIT▁MAGNETISM▁WELL▁IF▁IT'S▁ON▁A▁A▁NAIL▁OBJECT▁THEN▁WOULD▁IT▁BE▁TO▁MAGNETISM▁THROUGH▁IT

2024-10-27 18:32:12,110 (asr_inference:509) INFO: speech length: 48208
2024-10-27 18:32:13,947 (beam_search:428) INFO: decoder input length: 37
2024-10-27 18:32:13,947 (beam_search:429) INFO: max output length: 37
2024-10-27 18:32:13,947 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:13,981 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:13,981 (beam_search:476) INFO:  -0.60 * 1.0 =  -0.60 for ctc
2024-10-27 18:32:13,981 (beam_search:479) INFO: total log probability: -0.60
2024-10-27 18:32:13,981 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:32:13,981 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:13,981 (beam_search:483) INFO: best hypo: ▁SMALL▁THEY'RE▁CALLED

2024-10-27 18:32:13,984 (asr_inference:509) INFO: speech length: 272320
2024-10-27 18:32:25,612 (beam_search:428) INFO: decoder input length: 212
2024-10-27 18:32:25,613 (beam_search:429) INFO: max output length: 212
2024-10-27 18:32:25,613 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:26,961 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:26,961 (beam_search:476) INFO:  -3.62 * 1.0 =  -3.62 for ctc
2024-10-27 18:32:26,961 (beam_search:479) INFO: total log probability: -3.62
2024-10-27 18:32:26,961 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:32:26,961 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:26,961 (beam_search:483) INFO: best hypo: ▁LIKE▁IN▁A▁IT'LL▁UP▁WITH▁A▁ELECTROMAGNET▁AND▁THEN▁WHEN▁IT▁NEEDS▁TO▁DROP▁EM▁ELSE▁THEN▁THEY▁UM▁THEY▁LIKE▁CONNECT▁A▁SWITCH▁LIKE▁WE▁DID▁ON▁THIS▁AND▁IT▁WILL

2024-10-27 18:32:26,964 (asr_inference:509) INFO: speech length: 41664
2024-10-27 18:32:28,586 (beam_search:428) INFO: decoder input length: 32
2024-10-27 18:32:28,587 (beam_search:429) INFO: max output length: 32
2024-10-27 18:32:28,587 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:28,609 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:28,609 (beam_search:476) INFO:  -1.12 * 1.0 =  -1.12 for ctc
2024-10-27 18:32:28,609 (beam_search:479) INFO: total log probability: -1.12
2024-10-27 18:32:28,609 (beam_search:480) INFO: normalized log probability: -0.22
2024-10-27 18:32:28,609 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:28,609 (beam_search:483) INFO: best hypo: ▁NO▁WOULD▁NOT

2024-10-27 18:32:28,612 (asr_inference:509) INFO: speech length: 180336
2024-10-27 18:32:35,448 (beam_search:428) INFO: decoder input length: 140
2024-10-27 18:32:35,449 (beam_search:429) INFO: max output length: 140
2024-10-27 18:32:35,449 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:35,985 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:35,986 (beam_search:476) INFO:  -2.43 * 1.0 =  -2.43 for ctc
2024-10-27 18:32:35,986 (beam_search:479) INFO: total log probability: -2.43
2024-10-27 18:32:35,986 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:32:35,986 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:35,986 (beam_search:483) INFO: best hypo: ▁THAT▁IS▁THE▁ELECTRICITY▁WOULD▁GO▁TO▁THE▁AND▁THEN▁IT▁HAS▁ELECTRICITY▁IN▁IT▁AND▁IT▁WILL▁PICK▁UP▁METAL

2024-10-27 18:32:35,988 (asr_inference:509) INFO: speech length: 106240
2024-10-27 18:32:40,080 (beam_search:428) INFO: decoder input length: 82
2024-10-27 18:32:40,080 (beam_search:429) INFO: max output length: 82
2024-10-27 18:32:40,080 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:40,332 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:40,332 (beam_search:476) INFO:  -3.02 * 1.0 =  -3.02 for ctc
2024-10-27 18:32:40,332 (beam_search:479) INFO: total log probability: -3.02
2024-10-27 18:32:40,332 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:32:40,332 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:40,332 (beam_search:483) INFO: best hypo: ▁THE▁WE▁DID▁IT▁IS▁OUR▁SWITCH▁WAS▁A▁LITTLE▁BIT▁LONGER▁WELL▁YEAH▁UH▁YOU▁COULD

2024-10-27 18:32:40,335 (asr_inference:509) INFO: speech length: 352320
2024-10-27 18:32:56,801 (beam_search:428) INFO: decoder input length: 274
2024-10-27 18:32:56,801 (beam_search:429) INFO: max output length: 274
2024-10-27 18:32:56,801 (beam_search:430) INFO: min output length: 0
2024-10-27 18:32:59,282 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:32:59,282 (beam_search:476) INFO:  -5.60 * 1.0 =  -5.60 for ctc
2024-10-27 18:32:59,282 (beam_search:479) INFO: total log probability: -5.60
2024-10-27 18:32:59,282 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:32:59,282 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:32:59,282 (beam_search:483) INFO: best hypo: ▁YOU▁COULD▁IT▁A▁LOT▁MORE▁AND▁UM▁YOU▁COULD▁THEN▁YOU▁CAN▁UM▁UH▁IF▁YOU▁WRAP▁IT▁A▁LOT▁MORE▁THEN▁IT▁WILL▁IT▁MORE▁SO▁YOU▁YOU▁CAN▁MAKE▁IT▁BUT▁IF▁IT'SLY▁THEN▁IT'S▁JUST▁BE▁VERY▁IT

2024-10-27 18:32:59,285 (asr_inference:509) INFO: speech length: 321760
2024-10-27 18:33:14,221 (beam_search:428) INFO: decoder input length: 250
2024-10-27 18:33:14,221 (beam_search:429) INFO: max output length: 250
2024-10-27 18:33:14,221 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:17,038 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:17,038 (beam_search:476) INFO:  -5.08 * 1.0 =  -5.08 for ctc
2024-10-27 18:33:17,038 (beam_search:479) INFO: total log probability: -5.08
2024-10-27 18:33:17,038 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:33:17,038 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:17,038 (beam_search:483) INFO: best hypo: ▁BECAUSE▁IT'S▁IT▁MORE▁AND▁IT'S▁CAUSE▁WHEN▁WIRED▁AROUND▁A▁BAR▁IT▁WON'T▁REALLY▁GO▁THROUGH▁BUT▁IF▁THERE'S▁SOMETHING▁AROUND▁IT▁IT▁WILL▁BEING▁IT▁SO▁THAT▁IT▁JUST▁SO▁IT'LL▁GO▁THROUGH▁THAT▁AND▁NOT▁NOT▁BE▁TO▁JUST▁GET▁AWAY▁FROM▁IT

2024-10-27 18:33:17,043 (asr_inference:509) INFO: speech length: 262400
2024-10-27 18:33:28,337 (beam_search:428) INFO: decoder input length: 204
2024-10-27 18:33:28,338 (beam_search:429) INFO: max output length: 204
2024-10-27 18:33:28,338 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:30,232 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:30,232 (beam_search:476) INFO:  -9.26 * 1.0 =  -9.26 for ctc
2024-10-27 18:33:30,232 (beam_search:479) INFO: total log probability: -9.26
2024-10-27 18:33:30,232 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:33:30,232 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:30,232 (beam_search:483) INFO: best hypo: ▁IT▁WOULDN'T▁AS▁WELL▁IT▁WOULD▁BE▁AND▁MAGNETISM▁BECAUSE▁IF▁YOU▁ONLYD▁HALF▁OF▁IT▁IT'S▁GONNA▁GO▁KIND▁IT▁IT'S▁NOT▁GONNA▁BE▁THROUGH▁AND▁IT'S▁JUST▁GONNA▁LIKE▁DROP▁ALL▁THE▁MAGNETS▁IT'S▁ONLY▁A

2024-10-27 18:33:30,235 (asr_inference:509) INFO: speech length: 135248
2024-10-27 18:33:35,251 (beam_search:428) INFO: decoder input length: 105
2024-10-27 18:33:35,251 (beam_search:429) INFO: max output length: 105
2024-10-27 18:33:35,251 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:35,467 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:35,467 (beam_search:476) INFO:  -2.45 * 1.0 =  -2.45 for ctc
2024-10-27 18:33:35,467 (beam_search:479) INFO: total log probability: -2.45
2024-10-27 18:33:35,467 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:33:35,467 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:35,467 (beam_search:483) INFO: best hypo: ▁UM▁CONNECTS▁IT▁SO▁THE▁MAGNET▁CAN▁NOT▁SO▁THE

2024-10-27 18:33:35,469 (asr_inference:509) INFO: speech length: 93440
2024-10-27 18:33:38,971 (beam_search:428) INFO: decoder input length: 72
2024-10-27 18:33:38,972 (beam_search:429) INFO: max output length: 72
2024-10-27 18:33:38,972 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:39,093 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:39,094 (beam_search:476) INFO:  -1.33 * 1.0 =  -1.33 for ctc
2024-10-27 18:33:39,094 (beam_search:479) INFO: total log probability: -1.33
2024-10-27 18:33:39,094 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:33:39,094 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:39,094 (beam_search:483) INFO: best hypo: ▁GET▁THROUGH▁TO▁THE▁UM▁IT'LL▁GO▁THROUGH

2024-10-27 18:33:39,096 (asr_inference:509) INFO: speech length: 283376
2024-10-27 18:33:51,689 (beam_search:428) INFO: decoder input length: 220
2024-10-27 18:33:51,689 (beam_search:429) INFO: max output length: 220
2024-10-27 18:33:51,689 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:53,219 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:53,219 (beam_search:476) INFO:  -3.30 * 1.0 =  -3.30 for ctc
2024-10-27 18:33:53,219 (beam_search:479) INFO: total log probability: -3.30
2024-10-27 18:33:53,219 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:33:53,219 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:53,219 (beam_search:483) INFO: best hypo: ▁UM▁IT▁BECAUSE▁YOUD▁THAT▁THE▁ELECTRICITY▁WOULD▁FLOW▁THROUGH▁THE▁WIRES▁ONLY▁FROM▁ONE▁SIDE▁AND▁IF▁YOU▁HAD▁POSITIVE▁POSITIVE▁IT'T▁WORK▁BECAUSE▁YOU▁HAVE▁TO▁HAVE▁A▁NEGATIVE▁A▁POSITIVE

2024-10-27 18:33:53,222 (asr_inference:509) INFO: speech length: 17520
2024-10-27 18:33:54,102 (beam_search:428) INFO: decoder input length: 13
2024-10-27 18:33:54,102 (beam_search:429) INFO: max output length: 13
2024-10-27 18:33:54,102 (beam_search:430) INFO: min output length: 0
2024-10-27 18:33:54,116 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:33:54,117 (beam_search:476) INFO:  -1.49 * 1.0 =  -1.49 for ctc
2024-10-27 18:33:54,117 (beam_search:479) INFO: total log probability: -1.49
2024-10-27 18:33:54,117 (beam_search:480) INFO: normalized log probability: -0.21
2024-10-27 18:33:54,117 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:33:54,117 (beam_search:483) INFO: best hypo: 'T▁GO▁I▁MEAN

2024-10-27 18:33:54,120 (asr_inference:509) INFO: speech length: 217280
2024-10-27 18:34:03,158 (beam_search:428) INFO: decoder input length: 169
2024-10-27 18:34:03,158 (beam_search:429) INFO: max output length: 169
2024-10-27 18:34:03,158 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:03,974 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:03,975 (beam_search:476) INFO:  -3.86 * 1.0 =  -3.86 for ctc
2024-10-27 18:34:03,975 (beam_search:479) INFO: total log probability: -3.86
2024-10-27 18:34:03,975 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:34:03,975 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:03,975 (beam_search:483) INFO: best hypo: ▁UH▁IF▁SOMETHING▁IS▁CONNECTED▁AND▁THEN▁YOU▁IT▁LIKE▁WITHS▁IF▁THE▁TWO▁ARE▁CONNECTED▁TOGETHER▁THEN▁AND▁YOU▁TAKE▁IT▁IT'S

2024-10-27 18:34:03,977 (asr_inference:509) INFO: speech length: 83520
2024-10-27 18:34:06,969 (beam_search:428) INFO: decoder input length: 64
2024-10-27 18:34:06,969 (beam_search:429) INFO: max output length: 64
2024-10-27 18:34:06,969 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:07,087 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:07,087 (beam_search:476) INFO:  -2.47 * 1.0 =  -2.47 for ctc
2024-10-27 18:34:07,087 (beam_search:479) INFO: total log probability: -2.47
2024-10-27 18:34:07,087 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:34:07,087 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:07,087 (beam_search:483) INFO: best hypo: ▁THE▁MORE▁WIRE▁YOU▁THE▁MORE▁THE▁MORE▁GETED▁UP

2024-10-27 18:34:07,089 (asr_inference:509) INFO: speech length: 20688
2024-10-27 18:34:08,075 (beam_search:428) INFO: decoder input length: 15
2024-10-27 18:34:08,075 (beam_search:429) INFO: max output length: 15
2024-10-27 18:34:08,075 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:08,086 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:08,086 (beam_search:476) INFO:  -0.58 * 1.0 =  -0.58 for ctc
2024-10-27 18:34:08,086 (beam_search:479) INFO: total log probability: -0.58
2024-10-27 18:34:08,086 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:34:08,086 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:08,086 (beam_search:483) INFO: best hypo: ▁YES▁I

2024-10-27 18:34:08,089 (asr_inference:509) INFO: speech length: 24128
2024-10-27 18:34:09,270 (beam_search:428) INFO: decoder input length: 18
2024-10-27 18:34:09,270 (beam_search:429) INFO: max output length: 18
2024-10-27 18:34:09,270 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:09,278 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:09,278 (beam_search:476) INFO:  -0.83 * 1.0 =  -0.83 for ctc
2024-10-27 18:34:09,278 (beam_search:479) INFO: total log probability: -0.83
2024-10-27 18:34:09,278 (beam_search:480) INFO: normalized log probability: -0.28
2024-10-27 18:34:09,278 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:09,278 (beam_search:483) INFO: best hypo: ▁WILL

2024-10-27 18:34:09,281 (asr_inference:509) INFO: speech length: 106064
2024-10-27 18:34:13,211 (beam_search:428) INFO: decoder input length: 82
2024-10-27 18:34:13,211 (beam_search:429) INFO: max output length: 82
2024-10-27 18:34:13,211 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:13,413 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:13,413 (beam_search:476) INFO:  -1.06 * 1.0 =  -1.06 for ctc
2024-10-27 18:34:13,413 (beam_search:479) INFO: total log probability: -1.06
2024-10-27 18:34:13,413 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:34:13,413 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:13,413 (beam_search:483) INFO: best hypo: 'S▁ISING▁UP▁THE▁WASHERS▁BECAUSE▁IT'S▁ATTRACTED▁TO▁THEM

2024-10-27 18:34:13,417 (asr_inference:509) INFO: speech length: 15984
2024-10-27 18:34:14,278 (beam_search:428) INFO: decoder input length: 11
2024-10-27 18:34:14,279 (beam_search:429) INFO: max output length: 11
2024-10-27 18:34:14,279 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:14,287 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:14,287 (beam_search:476) INFO:  -0.91 * 1.0 =  -0.91 for ctc
2024-10-27 18:34:14,287 (beam_search:479) INFO: total log probability: -0.91
2024-10-27 18:34:14,287 (beam_search:480) INFO: normalized log probability: -0.23
2024-10-27 18:34:14,287 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:14,287 (beam_search:483) INFO: best hypo: ▁IS▁GET

2024-10-27 18:34:14,290 (asr_inference:509) INFO: speech length: 35360
2024-10-27 18:34:15,672 (beam_search:428) INFO: decoder input length: 27
2024-10-27 18:34:15,672 (beam_search:429) INFO: max output length: 27
2024-10-27 18:34:15,672 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:15,695 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:15,695 (beam_search:476) INFO:  -1.48 * 1.0 =  -1.48 for ctc
2024-10-27 18:34:15,695 (beam_search:479) INFO: total log probability: -1.48
2024-10-27 18:34:15,695 (beam_search:480) INFO: normalized log probability: -0.25
2024-10-27 18:34:15,695 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:15,695 (beam_search:483) INFO: best hypo: ▁IS▁GETTINGED▁THROUGH

2024-10-27 18:34:15,698 (asr_inference:509) INFO: speech length: 30320
2024-10-27 18:34:16,916 (beam_search:428) INFO: decoder input length: 23
2024-10-27 18:34:16,916 (beam_search:429) INFO: max output length: 23
2024-10-27 18:34:16,916 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:16,927 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:16,927 (beam_search:476) INFO:  -0.93 * 1.0 =  -0.93 for ctc
2024-10-27 18:34:16,927 (beam_search:479) INFO: total log probability: -0.93
2024-10-27 18:34:16,927 (beam_search:480) INFO: normalized log probability: -0.31
2024-10-27 18:34:16,927 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:16,927 (beam_search:483) INFO: best hypo: ▁FOR

2024-10-27 18:34:16,929 (asr_inference:509) INFO: speech length: 77312
2024-10-27 18:34:19,757 (beam_search:428) INFO: decoder input length: 59
2024-10-27 18:34:19,757 (beam_search:429) INFO: max output length: 59
2024-10-27 18:34:19,757 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:19,849 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:19,850 (beam_search:476) INFO:  -1.12 * 1.0 =  -1.12 for ctc
2024-10-27 18:34:19,850 (beam_search:479) INFO: total log probability: -1.12
2024-10-27 18:34:19,850 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:34:19,850 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:19,850 (beam_search:483) INFO: best hypo: ▁THAT▁THE▁ELECTRICITY▁WAS▁FLOWING▁BECAUSE▁IT▁GOT▁HOT

2024-10-27 18:34:19,852 (asr_inference:509) INFO: speech length: 151472
2024-10-27 18:34:25,714 (beam_search:428) INFO: decoder input length: 117
2024-10-27 18:34:25,714 (beam_search:429) INFO: max output length: 117
2024-10-27 18:34:25,714 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:26,169 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:26,169 (beam_search:476) INFO:  -3.20 * 1.0 =  -3.20 for ctc
2024-10-27 18:34:26,169 (beam_search:479) INFO: total log probability: -3.20
2024-10-27 18:34:26,169 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:34:26,169 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:26,169 (beam_search:483) INFO: best hypo: ▁UM▁THE▁IT▁GOT▁HOT▁WAS▁THE▁ELECTRICITY▁WAS▁FLOWING▁THROUGH▁IT▁AND▁SO▁IT▁GOT▁SO▁STRONG▁IT▁TO▁GET▁HOT

2024-10-27 18:34:26,172 (asr_inference:509) INFO: speech length: 212704
2024-10-27 18:34:34,766 (beam_search:428) INFO: decoder input length: 165
2024-10-27 18:34:34,766 (beam_search:429) INFO: max output length: 165
2024-10-27 18:34:34,766 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:35,581 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:35,581 (beam_search:476) INFO:  -2.98 * 1.0 =  -2.98 for ctc
2024-10-27 18:34:35,581 (beam_search:479) INFO: total log probability: -2.98
2024-10-27 18:34:35,581 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:34:35,581 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:35,581 (beam_search:483) INFO: best hypo: ▁UM▁THE▁ELECTRICITY'S▁BECAUSE▁THERE'S▁A▁LOT▁OF▁WIRED▁AROUND▁AND▁IT'S▁ALL▁FLOWING▁THROUGH▁THE▁SO▁THAT▁ITS

2024-10-27 18:34:35,584 (asr_inference:509) INFO: speech length: 79728
2024-10-27 18:34:38,545 (beam_search:428) INFO: decoder input length: 61
2024-10-27 18:34:38,546 (beam_search:429) INFO: max output length: 61
2024-10-27 18:34:38,546 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:38,603 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:38,603 (beam_search:476) INFO:  -0.66 * 1.0 =  -0.66 for ctc
2024-10-27 18:34:38,603 (beam_search:479) INFO: total log probability: -0.66
2024-10-27 18:34:38,603 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:34:38,603 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:38,603 (beam_search:483) INFO: best hypo: ▁THE▁ELECTRICITY▁FROM▁THE▁BATTERY

2024-10-27 18:34:38,605 (asr_inference:509) INFO: speech length: 226768
2024-10-27 18:34:47,965 (beam_search:428) INFO: decoder input length: 176
2024-10-27 18:34:47,966 (beam_search:429) INFO: max output length: 176
2024-10-27 18:34:47,966 (beam_search:430) INFO: min output length: 0
2024-10-27 18:34:49,210 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:34:49,210 (beam_search:476) INFO:  -3.31 * 1.0 =  -3.31 for ctc
2024-10-27 18:34:49,210 (beam_search:479) INFO: total log probability: -3.31
2024-10-27 18:34:49,210 (beam_search:480) INFO: normalized log probability: -0.09
2024-10-27 18:34:49,210 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:34:49,210 (beam_search:483) INFO: best hypo: ▁THE▁BATTERY▁IS▁OF▁ELECTRICITY▁AND▁IT▁MAKES▁RUN▁SO▁IF▁YOU▁HOOK▁A▁WIRE▁UP▁TO▁IT▁THE▁ELECTRICITY'S▁GOING▁GO▁THROUGH▁THE▁WIRE▁AND▁TURN▁ON▁WHATEVER▁YOU▁WANT▁TO▁TURN▁ON

2024-10-27 18:34:49,213 (asr_inference:509) INFO: speech length: 299712
2024-10-27 18:35:02,266 (beam_search:428) INFO: decoder input length: 233
2024-10-27 18:35:02,266 (beam_search:429) INFO: max output length: 233
2024-10-27 18:35:02,266 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:04,553 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:04,553 (beam_search:476) INFO:  -4.25 * 1.0 =  -4.25 for ctc
2024-10-27 18:35:04,553 (beam_search:479) INFO: total log probability: -4.25
2024-10-27 18:35:04,554 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:35:04,554 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:04,554 (beam_search:483) INFO: best hypo: ▁IN▁THIS▁PICTURE▁YOU▁HAVE▁THE▁WIRESED▁UP▁TO▁THE▁BATTERY▁AND▁HOOKED▁UP▁TO▁THE▁SWITCH▁AND▁THEN▁YOU▁HAVE▁THE▁WIRE▁HOOKED▁UP▁TO▁THE▁BATTERY▁ANDD▁THE▁AND▁THEN▁YOU▁HAVE▁IT▁UMED▁UP▁TO▁THE▁SWITCH▁AND▁THE▁ELECTRICITY'S▁ON▁THE

2024-10-27 18:35:04,556 (asr_inference:509) INFO: speech length: 127200
2024-10-27 18:35:09,288 (beam_search:428) INFO: decoder input length: 98
2024-10-27 18:35:09,288 (beam_search:429) INFO: max output length: 98
2024-10-27 18:35:09,288 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:09,556 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:09,556 (beam_search:476) INFO:  -0.87 * 1.0 =  -0.87 for ctc
2024-10-27 18:35:09,556 (beam_search:479) INFO: total log probability: -0.87
2024-10-27 18:35:09,556 (beam_search:480) INFO: normalized log probability: -0.05
2024-10-27 18:35:09,556 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:09,556 (beam_search:483) INFO: best hypo: ▁THE▁THE▁UM▁THE▁ELECTRICITY'S▁GETTING▁AND▁IT'S▁MORE▁AND▁MORE▁WASHERS

2024-10-27 18:35:09,559 (asr_inference:509) INFO: speech length: 99712
2024-10-27 18:35:13,256 (beam_search:428) INFO: decoder input length: 77
2024-10-27 18:35:13,256 (beam_search:429) INFO: max output length: 77
2024-10-27 18:35:13,256 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:13,391 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:13,392 (beam_search:476) INFO:  -2.12 * 1.0 =  -2.12 for ctc
2024-10-27 18:35:13,392 (beam_search:479) INFO: total log probability: -2.12
2024-10-27 18:35:13,392 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:35:13,392 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:13,392 (beam_search:483) INFO: best hypo: ▁IT▁IN▁BECAUSE▁IT▁WE▁DID▁THIS▁JUST▁IN

2024-10-27 18:35:13,395 (asr_inference:509) INFO: speech length: 188480
2024-10-27 18:35:21,039 (beam_search:428) INFO: decoder input length: 146
2024-10-27 18:35:21,039 (beam_search:429) INFO: max output length: 146
2024-10-27 18:35:21,039 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:21,581 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:21,581 (beam_search:476) INFO:  -2.59 * 1.0 =  -2.59 for ctc
2024-10-27 18:35:21,581 (beam_search:479) INFO: total log probability: -2.59
2024-10-27 18:35:21,582 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:35:21,582 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:21,582 (beam_search:483) INFO: best hypo: ▁YOU▁CAN▁TURN▁ON▁AND▁OFF▁UM▁A▁SWITCH▁AND▁THEN▁YOU▁CAN▁TURN▁ON▁AND▁OFF▁IT▁JUST▁IS▁ON

2024-10-27 18:35:21,584 (asr_inference:509) INFO: speech length: 178784
2024-10-27 18:35:28,375 (beam_search:428) INFO: decoder input length: 139
2024-10-27 18:35:28,375 (beam_search:429) INFO: max output length: 139
2024-10-27 18:35:28,376 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:29,129 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:29,130 (beam_search:476) INFO:  -3.85 * 1.0 =  -3.85 for ctc
2024-10-27 18:35:29,130 (beam_search:479) INFO: total log probability: -3.85
2024-10-27 18:35:29,130 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:35:29,130 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:29,130 (beam_search:483) INFO: best hypo: ▁UM▁IT'S▁IMPORTANT▁BECAUSE▁YOU▁NEED▁A▁BATTERY'S▁I▁MEAN▁A▁MAGNET▁THAT'S▁GOING▁BUT▁YOU▁NEED▁TO▁BE▁ABLE▁TO▁A▁BATTERY▁OFF

2024-10-27 18:35:29,132 (asr_inference:509) INFO: speech length: 67008
2024-10-27 18:35:31,611 (beam_search:428) INFO: decoder input length: 51
2024-10-27 18:35:31,611 (beam_search:429) INFO: max output length: 51
2024-10-27 18:35:31,611 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:31,676 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:31,676 (beam_search:476) INFO:  -0.56 * 1.0 =  -0.56 for ctc
2024-10-27 18:35:31,676 (beam_search:479) INFO: total log probability: -0.56
2024-10-27 18:35:31,676 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:35:31,676 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:31,676 (beam_search:483) INFO: best hypo: ▁WE'VE▁BEEN▁LEARNING▁ABOUT▁AND

2024-10-27 18:35:31,680 (asr_inference:509) INFO: speech length: 190208
2024-10-27 18:35:39,231 (beam_search:428) INFO: decoder input length: 148
2024-10-27 18:35:39,231 (beam_search:429) INFO: max output length: 148
2024-10-27 18:35:39,231 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:39,629 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:39,629 (beam_search:476) INFO:  -3.40 * 1.0 =  -3.40 for ctc
2024-10-27 18:35:39,629 (beam_search:479) INFO: total log probability: -3.40
2024-10-27 18:35:39,629 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:35:39,629 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:39,629 (beam_search:483) INFO: best hypo: ▁THEY▁ARE▁THEY▁ARE▁THAN▁A▁MAGNET▁BECAUSE▁THERE'S▁ELECTRICITY▁AND▁MAKES▁MORE

2024-10-27 18:35:39,632 (asr_inference:509) INFO: speech length: 171824
2024-10-27 18:35:46,269 (beam_search:428) INFO: decoder input length: 133
2024-10-27 18:35:46,269 (beam_search:429) INFO: max output length: 133
2024-10-27 18:35:46,269 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:46,799 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:46,800 (beam_search:476) INFO:  -1.74 * 1.0 =  -1.74 for ctc
2024-10-27 18:35:46,800 (beam_search:479) INFO: total log probability: -1.74
2024-10-27 18:35:46,800 (beam_search:480) INFO: normalized log probability: -0.07
2024-10-27 18:35:46,800 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:46,800 (beam_search:483) INFO: best hypo: ▁THAT▁THE▁WIRE▁IS▁AROUND▁THE▁ELECTROMAGNET▁TO▁SO▁TO▁MAKE▁IT▁AN▁ELECTROMAGNET▁SO▁THAT▁IT▁CAN▁PICK▁UP▁THE▁WASHERS

2024-10-27 18:35:46,802 (asr_inference:509) INFO: speech length: 191760
2024-10-27 18:35:54,489 (beam_search:428) INFO: decoder input length: 149
2024-10-27 18:35:54,490 (beam_search:429) INFO: max output length: 149
2024-10-27 18:35:54,490 (beam_search:430) INFO: min output length: 0
2024-10-27 18:35:55,155 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:35:55,155 (beam_search:476) INFO:  -3.88 * 1.0 =  -3.88 for ctc
2024-10-27 18:35:55,155 (beam_search:479) INFO: total log probability: -3.88
2024-10-27 18:35:55,155 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:35:55,155 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:35:55,155 (beam_search:483) INFO: best hypo: ▁IT▁ISD▁AROUND▁A▁CONNECTED▁TO▁A▁SWITCH▁AND▁A▁BATTERY▁TO▁MAKE▁IT▁UM▁AN▁ELECTROMAGNET▁SO▁THAT▁IT▁CAN▁PICK▁UP▁STUFF

2024-10-27 18:35:55,157 (asr_inference:509) INFO: speech length: 300240
2024-10-27 18:36:08,532 (beam_search:428) INFO: decoder input length: 234
2024-10-27 18:36:08,532 (beam_search:429) INFO: max output length: 234
2024-10-27 18:36:08,532 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:10,576 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:10,576 (beam_search:476) INFO:  -4.59 * 1.0 =  -4.59 for ctc
2024-10-27 18:36:10,576 (beam_search:479) INFO: total log probability: -4.59
2024-10-27 18:36:10,576 (beam_search:480) INFO: normalized log probability: -0.10
2024-10-27 18:36:10,576 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:10,577 (beam_search:483) INFO: best hypo: ▁THE▁IS▁A▁NAIL▁IF▁THE▁WIRE'S▁NOT▁AROUND▁IT▁BECAUSE▁WHEN▁IT'S▁AROUND▁IT▁ITS▁THE▁ELECTRICITY▁TO▁GO▁THROUGH▁AND▁THEN▁ITS▁UP▁ON▁THES▁ON▁THE▁NAIL▁SO▁THAT▁IT▁IS▁TO▁PICK▁UP▁UM▁WASHERS

2024-10-27 18:36:10,579 (asr_inference:509) INFO: speech length: 24448
2024-10-27 18:36:11,665 (beam_search:428) INFO: decoder input length: 18
2024-10-27 18:36:11,665 (beam_search:429) INFO: max output length: 18
2024-10-27 18:36:11,665 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:11,675 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:11,675 (beam_search:476) INFO:  -1.08 * 1.0 =  -1.08 for ctc
2024-10-27 18:36:11,675 (beam_search:479) INFO: total log probability: -1.08
2024-10-27 18:36:11,675 (beam_search:480) INFO: normalized log probability: -0.27
2024-10-27 18:36:11,675 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:11,675 (beam_search:483) INFO: best hypo: ▁THAT▁CORRECT

2024-10-27 18:36:11,678 (asr_inference:509) INFO: speech length: 118000
2024-10-27 18:36:15,990 (beam_search:428) INFO: decoder input length: 91
2024-10-27 18:36:15,991 (beam_search:429) INFO: max output length: 91
2024-10-27 18:36:15,991 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:16,277 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:16,277 (beam_search:476) INFO:  -3.15 * 1.0 =  -3.15 for ctc
2024-10-27 18:36:16,277 (beam_search:479) INFO: total log probability: -3.15
2024-10-27 18:36:16,277 (beam_search:480) INFO: normalized log probability: -0.15
2024-10-27 18:36:16,277 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:16,277 (beam_search:483) INFO: best hypo: ▁WHEN▁IT▁IS▁OPEN▁IT▁DOES▁NOT▁PICK▁UP▁ELECTRICITY▁BUT▁WHEN▁IT▁IS▁CLOSED▁IT▁DOES▁UP▁ELECTRICITY

2024-10-27 18:36:16,280 (asr_inference:509) INFO: speech length: 210912
2024-10-27 18:36:24,599 (beam_search:428) INFO: decoder input length: 164
2024-10-27 18:36:24,599 (beam_search:429) INFO: max output length: 164
2024-10-27 18:36:24,599 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:25,989 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:25,990 (beam_search:476) INFO:  -5.53 * 1.0 =  -5.53 for ctc
2024-10-27 18:36:25,990 (beam_search:479) INFO: total log probability: -5.53
2024-10-27 18:36:25,990 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:36:25,990 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:25,990 (beam_search:483) INFO: best hypo: ▁YOU▁HAVE▁TO▁OF▁HOW▁MANY▁YOU▁WRAP▁THE▁WIRE▁AROUND▁THE▁NAIL▁BECAUSE▁IF▁YOU▁DON'T▁AROUND▁THAT▁MUCH▁IT'S▁IT'S▁NOT▁THAT▁BUT▁WHEN▁YOU▁IT▁AROUND▁A▁IT'S▁REALLY▁AND▁CAN▁PICK▁UP▁MORE

2024-10-27 18:36:25,992 (asr_inference:509) INFO: speech length: 230480
2024-10-27 18:36:35,308 (beam_search:428) INFO: decoder input length: 179
2024-10-27 18:36:35,308 (beam_search:429) INFO: max output length: 179
2024-10-27 18:36:35,308 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:36,645 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:36,645 (beam_search:476) INFO:  -4.51 * 1.0 =  -4.51 for ctc
2024-10-27 18:36:36,645 (beam_search:479) INFO: total log probability: -4.51
2024-10-27 18:36:36,645 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:36:36,645 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:36,645 (beam_search:483) INFO: best hypo: ▁THAT▁YOU▁IT▁TO▁THE▁AND▁THE▁SWITCH▁AND▁WHEN▁YOU▁WANT▁TO▁PICK▁IT▁UP▁YOU▁THE▁SWITCH▁AND▁WHEN▁YOU▁AND▁THEN▁WHEN▁YOU▁WANT▁TO▁IT▁YOU▁NOT▁YOU▁DO▁NOT▁OPEN▁THE▁SWITCH▁YOU▁IT

2024-10-27 18:36:36,649 (asr_inference:509) INFO: speech length: 119248
2024-10-27 18:36:40,944 (beam_search:428) INFO: decoder input length: 92
2024-10-27 18:36:40,945 (beam_search:429) INFO: max output length: 92
2024-10-27 18:36:40,945 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:41,147 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:41,147 (beam_search:476) INFO:  -0.89 * 1.0 =  -0.89 for ctc
2024-10-27 18:36:41,147 (beam_search:479) INFO: total log probability: -0.89
2024-10-27 18:36:41,147 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:36:41,147 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:41,147 (beam_search:483) INFO: best hypo: ▁THE▁ELECTRICITY▁IS▁TO▁AND▁IT▁IS▁TO▁PICK▁UP▁UM▁WASHERS

2024-10-27 18:36:41,149 (asr_inference:509) INFO: speech length: 23424
2024-10-27 18:36:42,206 (beam_search:428) INFO: decoder input length: 17
2024-10-27 18:36:42,206 (beam_search:429) INFO: max output length: 17
2024-10-27 18:36:42,206 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:42,217 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:42,217 (beam_search:476) INFO:  -0.57 * 1.0 =  -0.57 for ctc
2024-10-27 18:36:42,217 (beam_search:479) INFO: total log probability: -0.57
2024-10-27 18:36:42,218 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:36:42,218 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:42,218 (beam_search:483) INFO: best hypo: ▁THAT▁IS

2024-10-27 18:36:42,220 (asr_inference:509) INFO: speech length: 40096
2024-10-27 18:36:43,797 (beam_search:428) INFO: decoder input length: 30
2024-10-27 18:36:43,797 (beam_search:429) INFO: max output length: 30
2024-10-27 18:36:43,797 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:43,826 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:43,826 (beam_search:476) INFO:  -2.93 * 1.0 =  -2.93 for ctc
2024-10-27 18:36:43,826 (beam_search:479) INFO: total log probability: -2.93
2024-10-27 18:36:43,826 (beam_search:480) INFO: normalized log probability: -0.42
2024-10-27 18:36:43,826 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:43,826 (beam_search:483) INFO: best hypo: ▁YOU▁CAN▁MAKE▁AN▁ELECTROMAGNET

2024-10-27 18:36:43,829 (asr_inference:509) INFO: speech length: 170256
2024-10-27 18:36:50,283 (beam_search:428) INFO: decoder input length: 132
2024-10-27 18:36:50,283 (beam_search:429) INFO: max output length: 132
2024-10-27 18:36:50,283 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:50,747 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:50,747 (beam_search:476) INFO:  -4.01 * 1.0 =  -4.01 for ctc
2024-10-27 18:36:50,747 (beam_search:479) INFO: total log probability: -4.01
2024-10-27 18:36:50,747 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:36:50,747 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:50,747 (beam_search:483) INFO: best hypo: ▁THE▁IS▁WHAT▁WHEN▁YOU▁PUT▁ELECTRICITY▁WIRES▁AND▁A▁TOGETHER▁AND▁A▁SWITCH▁YOU▁ARE▁ABLE▁TO▁MAKE▁A

2024-10-27 18:36:50,750 (asr_inference:509) INFO: speech length: 146064
2024-10-27 18:36:56,443 (beam_search:428) INFO: decoder input length: 113
2024-10-27 18:36:56,443 (beam_search:429) INFO: max output length: 113
2024-10-27 18:36:56,443 (beam_search:430) INFO: min output length: 0
2024-10-27 18:36:56,679 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:36:56,679 (beam_search:476) INFO:  -2.66 * 1.0 =  -2.66 for ctc
2024-10-27 18:36:56,679 (beam_search:479) INFO: total log probability: -2.66
2024-10-27 18:36:56,679 (beam_search:480) INFO: normalized log probability: -0.20
2024-10-27 18:36:56,679 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:36:56,679 (beam_search:483) INFO: best hypo: ▁THE▁WIRES▁THE▁ELECTRICITY▁TO▁FLOW▁THROUGH▁SO▁THE▁ELECTROMAGNET

2024-10-27 18:36:56,681 (asr_inference:509) INFO: speech length: 91152
2024-10-27 18:37:00,134 (beam_search:428) INFO: decoder input length: 70
2024-10-27 18:37:00,134 (beam_search:429) INFO: max output length: 70
2024-10-27 18:37:00,134 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:00,252 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:00,253 (beam_search:476) INFO:  -2.82 * 1.0 =  -2.82 for ctc
2024-10-27 18:37:00,253 (beam_search:479) INFO: total log probability: -2.82
2024-10-27 18:37:00,253 (beam_search:480) INFO: normalized log probability: -0.26
2024-10-27 18:37:00,253 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:00,253 (beam_search:483) INFO: best hypo: ▁MEANS▁LIKE▁THE▁WIRE▁IS▁INSIDE▁OF▁THE▁GREEN

2024-10-27 18:37:00,255 (asr_inference:509) INFO: speech length: 63552
2024-10-27 18:37:02,536 (beam_search:428) INFO: decoder input length: 49
2024-10-27 18:37:02,536 (beam_search:429) INFO: max output length: 49
2024-10-27 18:37:02,536 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:02,619 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:02,620 (beam_search:476) INFO:  -0.97 * 1.0 =  -0.97 for ctc
2024-10-27 18:37:02,620 (beam_search:479) INFO: total log probability: -0.97
2024-10-27 18:37:02,620 (beam_search:480) INFO: normalized log probability: -0.08
2024-10-27 18:37:02,620 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:02,620 (beam_search:483) INFO: best hypo: ▁THAT▁YOU▁IT▁AROUND▁AND▁YOU▁TURN▁ON▁A▁SWITCH

2024-10-27 18:37:02,622 (asr_inference:509) INFO: speech length: 76176
2024-10-27 18:37:05,394 (beam_search:428) INFO: decoder input length: 59
2024-10-27 18:37:05,394 (beam_search:429) INFO: max output length: 59
2024-10-27 18:37:05,394 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:05,473 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:05,473 (beam_search:476) INFO:  -1.30 * 1.0 =  -1.30 for ctc
2024-10-27 18:37:05,474 (beam_search:479) INFO: total log probability: -1.30
2024-10-27 18:37:05,474 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:37:05,474 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:05,474 (beam_search:483) INFO: best hypo: ▁WHAT▁DO▁YOU▁MEAN▁BY▁THE▁RIVET▁MAGNETIC

2024-10-27 18:37:05,476 (asr_inference:509) INFO: speech length: 272608
2024-10-27 18:37:17,388 (beam_search:428) INFO: decoder input length: 212
2024-10-27 18:37:17,388 (beam_search:429) INFO: max output length: 212
2024-10-27 18:37:17,388 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:18,364 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:18,364 (beam_search:476) INFO:  -4.77 * 1.0 =  -4.77 for ctc
2024-10-27 18:37:18,364 (beam_search:479) INFO: total log probability: -4.77
2024-10-27 18:37:18,364 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:37:18,364 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:18,364 (beam_search:483) INFO: best hypo: ▁NOT▁A▁THAT▁WE'VE▁REALLY▁BEEN▁IS▁THE▁THING▁WE▁DID▁IN▁SCIENCE▁WAS▁THAN▁THAT▁WE▁MADE▁A▁PARALLEL▁CIRCUIT

2024-10-27 18:37:18,368 (asr_inference:509) INFO: speech length: 168176
2024-10-27 18:37:26,072 (beam_search:428) INFO: decoder input length: 130
2024-10-27 18:37:26,072 (beam_search:429) INFO: max output length: 130
2024-10-27 18:37:26,072 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:26,288 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:26,288 (beam_search:476) INFO:  -2.14 * 1.0 =  -2.14 for ctc
2024-10-27 18:37:26,288 (beam_search:479) INFO: total log probability: -2.14
2024-10-27 18:37:26,289 (beam_search:480) INFO: normalized log probability: -0.19
2024-10-27 18:37:26,289 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:26,289 (beam_search:483) INFO: best hypo: ▁WE'VE▁BEEN▁WITH▁AND▁JUST▁WE▁WITH

2024-10-27 18:37:26,291 (asr_inference:509) INFO: speech length: 120752
2024-10-27 18:37:31,394 (beam_search:428) INFO: decoder input length: 93
2024-10-27 18:37:31,394 (beam_search:429) INFO: max output length: 93
2024-10-27 18:37:31,394 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:31,623 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:31,623 (beam_search:476) INFO:  -0.83 * 1.0 =  -0.83 for ctc
2024-10-27 18:37:31,623 (beam_search:479) INFO: total log probability: -0.83
2024-10-27 18:37:31,623 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:37:31,623 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:31,623 (beam_search:483) INFO: best hypo: ▁THE▁LIGHTS▁TURN▁OFF▁AND▁YEAH▁IT'S▁ABOUT▁IT

2024-10-27 18:37:31,626 (asr_inference:509) INFO: speech length: 86336
2024-10-27 18:37:35,387 (beam_search:428) INFO: decoder input length: 66
2024-10-27 18:37:35,387 (beam_search:429) INFO: max output length: 66
2024-10-27 18:37:35,387 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:35,486 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:35,487 (beam_search:476) INFO:  -1.55 * 1.0 =  -1.55 for ctc
2024-10-27 18:37:35,487 (beam_search:479) INFO: total log probability: -1.55
2024-10-27 18:37:35,487 (beam_search:480) INFO: normalized log probability: -0.17
2024-10-27 18:37:35,487 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:35,487 (beam_search:483) INFO: best hypo: ▁I▁SEE▁THE▁ON▁AND▁OFF▁TIME

2024-10-27 18:37:35,490 (asr_inference:509) INFO: speech length: 79616
2024-10-27 18:37:38,879 (beam_search:428) INFO: decoder input length: 61
2024-10-27 18:37:38,879 (beam_search:429) INFO: max output length: 61
2024-10-27 18:37:38,879 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:39,040 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:39,041 (beam_search:476) INFO:  -0.98 * 1.0 =  -0.98 for ctc
2024-10-27 18:37:39,041 (beam_search:479) INFO: total log probability: -0.98
2024-10-27 18:37:39,041 (beam_search:480) INFO: normalized log probability: -0.06
2024-10-27 18:37:39,041 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:39,041 (beam_search:483) INFO: best hypo: ▁IT▁DOESN'T▁THE▁CIRCUIT▁AND▁THE▁THE▁OTHER▁BULB▁NOT▁LIGHT

2024-10-27 18:37:39,044 (asr_inference:509) INFO: speech length: 158160
2024-10-27 18:37:47,328 (beam_search:428) INFO: decoder input length: 123
2024-10-27 18:37:47,328 (beam_search:429) INFO: max output length: 123
2024-10-27 18:37:47,328 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:48,106 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:48,107 (beam_search:476) INFO:  -4.06 * 1.0 =  -4.06 for ctc
2024-10-27 18:37:48,107 (beam_search:479) INFO: total log probability: -4.06
2024-10-27 18:37:48,107 (beam_search:480) INFO: normalized log probability: -0.12
2024-10-27 18:37:48,107 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:48,107 (beam_search:483) INFO: best hypo: ▁THE▁ELECTRICITY▁HAS▁TO▁THROUGH▁THE▁BULB▁AND▁IF▁THE▁BULB▁ISN'T▁THEN▁IT▁CAN'T▁THROUGH▁AND▁IT▁CAN'T▁THE▁ELECTRICITY▁CAN▁NOT▁POWER▁THE▁NEXT▁BULB

2024-10-27 18:37:48,109 (asr_inference:509) INFO: speech length: 188400
2024-10-27 18:37:56,216 (beam_search:428) INFO: decoder input length: 146
2024-10-27 18:37:56,216 (beam_search:429) INFO: max output length: 146
2024-10-27 18:37:56,216 (beam_search:430) INFO: min output length: 0
2024-10-27 18:37:57,171 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:37:57,172 (beam_search:476) INFO:  -3.61 * 1.0 =  -3.61 for ctc
2024-10-27 18:37:57,172 (beam_search:479) INFO: total log probability: -3.61
2024-10-27 18:37:57,172 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:37:57,172 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:37:57,172 (beam_search:483) INFO: best hypo: ▁THE▁ELECTRICITY▁DOESN'T▁THROUGH▁THAT▁BULB▁IT▁OUT▁AND▁THE▁OTHER▁BULB▁CAN'T▁GET▁ITS▁POWER▁AND▁THEN▁THE▁NOT▁AND▁BULBS▁GO▁OUT

2024-10-27 18:37:57,191 (asr_inference:509) INFO: speech length: 222416
2024-10-27 18:38:09,927 (beam_search:428) INFO: decoder input length: 173
2024-10-27 18:38:09,927 (beam_search:429) INFO: max output length: 173
2024-10-27 18:38:09,927 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:11,422 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:11,422 (beam_search:476) INFO:  -6.33 * 1.0 =  -6.33 for ctc
2024-10-27 18:38:11,422 (beam_search:479) INFO: total log probability: -6.33
2024-10-27 18:38:11,422 (beam_search:480) INFO: normalized log probability: -0.14
2024-10-27 18:38:11,422 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:11,422 (beam_search:483) INFO: best hypo: ▁WHAT'S▁GOING▁ON▁IS▁THAT▁IN▁THE▁BULB▁ELECTRICITY▁HAS▁TO▁GO▁ALL▁THE▁THROUGH▁AND▁WHEN▁IT▁OUT▁THAT▁MEANS▁OF▁THE▁WIRES▁INSIDE▁THE▁BULB▁SO▁IT▁DOESN'T▁GO▁ALL▁THE▁THROUGH▁TO▁THE▁OTHER▁BULB

2024-10-27 18:38:11,472 (asr_inference:509) INFO: speech length: 298048
2024-10-27 18:38:27,353 (beam_search:428) INFO: decoder input length: 232
2024-10-27 18:38:27,354 (beam_search:429) INFO: max output length: 232
2024-10-27 18:38:27,354 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:29,011 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:29,012 (beam_search:476) INFO:  -4.83 * 1.0 =  -4.83 for ctc
2024-10-27 18:38:29,012 (beam_search:479) INFO: total log probability: -4.83
2024-10-27 18:38:29,012 (beam_search:480) INFO: normalized log probability: -0.13
2024-10-27 18:38:29,012 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:29,012 (beam_search:483) INFO: best hypo: ▁UM▁THERE'S▁ELECTRICITY▁FLOWING▁IN▁TWO▁DIFFERENT▁PATHWAYS▁ONE▁FOR▁EACH▁BULB▁IF▁ONE▁OUT▁BOTH▁ONE▁WILL▁STILL▁STAY▁ON▁AS▁TO▁CIRCUIT▁WHERE▁THEY▁BOTH▁GO▁OUT▁IF▁ONE▁GOES▁OUT

2024-10-27 18:38:29,015 (asr_inference:509) INFO: speech length: 252640
2024-10-27 18:38:42,591 (beam_search:428) INFO: decoder input length: 196
2024-10-27 18:38:42,591 (beam_search:429) INFO: max output length: 196
2024-10-27 18:38:42,591 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:43,873 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:43,873 (beam_search:476) INFO:  -3.42 * 1.0 =  -3.42 for ctc
2024-10-27 18:38:43,873 (beam_search:479) INFO: total log probability: -3.42
2024-10-27 18:38:43,873 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:38:43,873 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:43,873 (beam_search:483) INFO: best hypo: ▁IF▁I▁ON▁A▁BULB▁IT▁OUT▁BUT▁SINCE▁IT'S▁A▁PARALLEL▁CIRCUITING▁THAT▁EACH▁BULB▁HAS▁ITS▁PATHWAY▁IT▁CAN▁ONE▁BULB▁CAN▁STILL▁ON

2024-10-27 18:38:43,887 (asr_inference:509) INFO: speech length: 100224
2024-10-27 18:38:48,637 (beam_search:428) INFO: decoder input length: 77
2024-10-27 18:38:48,637 (beam_search:429) INFO: max output length: 77
2024-10-27 18:38:48,637 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:48,745 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:48,746 (beam_search:476) INFO:  -1.63 * 1.0 =  -1.63 for ctc
2024-10-27 18:38:48,746 (beam_search:479) INFO: total log probability: -1.63
2024-10-27 18:38:48,746 (beam_search:480) INFO: normalized log probability: -0.18
2024-10-27 18:38:48,746 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:48,746 (beam_search:483) INFO: best hypo: ▁I▁THE▁ELECTRICITY▁IS▁FLOWING▁INSIDE▁THE

2024-10-27 18:38:48,759 (asr_inference:509) INFO: speech length: 168256
2024-10-27 18:38:57,236 (beam_search:428) INFO: decoder input length: 130
2024-10-27 18:38:57,236 (beam_search:429) INFO: max output length: 130
2024-10-27 18:38:57,236 (beam_search:430) INFO: min output length: 0
2024-10-27 18:38:57,968 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:38:57,968 (beam_search:476) INFO:  -4.52 * 1.0 =  -4.52 for ctc
2024-10-27 18:38:57,968 (beam_search:479) INFO: total log probability: -4.52
2024-10-27 18:38:57,968 (beam_search:480) INFO: normalized log probability: -0.16
2024-10-27 18:38:57,968 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:38:57,968 (beam_search:483) INFO: best hypo: ▁IN▁I▁THINK▁A▁PARALLEL▁CIRCUIT▁WOULD▁BE▁BETTER▁A▁LONG▁OF▁LIGHTS▁SO▁THAT▁ONE▁GOES▁OUT▁THERE'S▁STILL▁THAT▁ARE▁ON

2024-10-27 18:38:57,981 (asr_inference:509) INFO: speech length: 141248
2024-10-27 18:39:05,720 (beam_search:428) INFO: decoder input length: 109
2024-10-27 18:39:05,720 (beam_search:429) INFO: max output length: 109
2024-10-27 18:39:05,720 (beam_search:430) INFO: min output length: 0
2024-10-27 18:39:06,198 (beam_search:447) INFO: no hypothesis. Finish decoding.
2024-10-27 18:39:06,198 (beam_search:476) INFO:  -2.43 * 1.0 =  -2.43 for ctc
2024-10-27 18:39:06,198 (beam_search:479) INFO: total log probability: -2.43
2024-10-27 18:39:06,198 (beam_search:480) INFO: normalized log probability: -0.11
2024-10-27 18:39:06,198 (beam_search:481) INFO: total number of ended hypotheses: 1
2024-10-27 18:39:06,198 (beam_search:483) INFO: best hypo: ▁IF▁ONE▁BULB▁OUT▁THE▁POWER▁DOESN'T▁FLOW▁TO▁THE▁BULB▁IN▁AND▁IT▁CAN▁NOT▁THE▁CIRCUIT

2024-10-27 18:39:06,213 (asr_inference:509) INFO: speech length: 275856
=======
FileNotFoundError: [Errno 2] No such file or directory: '/data/mohan/workdir/espnet/egs2/myst/asr1/exp/librispeech_100_ctc_e_branchformer/exp/asr_train_asr_ctc_e_branchformer_e12_raw_en_bpe5000_sp/valid.cer_ctc.ave_10best.pth'
[lambda-Lambda-Vector:0/2] 2024-10-31 20:45:26,341 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[lambda-Lambda-Vector:0/2] 2024-10-31 20:45:26,346 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=5, axis=time)
  )
  (normalize): GlobalMVN(stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz, norm_means=True, norm_vars=True)
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 26.43 M
    Number of trainable parameters: 26.43 M (100.0%)
    Size: 105.74 MB
    Type: torch.float32
[lambda-Lambda-Vector:0/2] 2024-10-31 20:45:26,346 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 1.3333333333333336e-07
    maximize: False
    weight_decay: 1e-06
)
[lambda-Lambda-Vector:0/2] 2024-10-31 20:45:26,346 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[lambda-Lambda-Vector:0/2] 2024-10-31 20:45:26,346 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/config.yaml
[lambda-Lambda-Vector:0/2] 2024-10-31 20:45:26,409 (abs_task:1465) INFO: Loading pretrained params from /data/mohan/workdir/espnet/egs2/myst/asr1/exp/librispeech_100_ctc_e_branchformer/exp/asr_train_asr_ctc_e_branchformer_e12_raw_en_bpe5000_sp/valid.cer_ctc.ave_10best.pth:encoder:encoder
Process SpawnProcess-1:
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1466, in main_worker
    load_pretrained_model(
  File "/data/mohan/workdir/espnet/espnet2/torch_utils/load_pretrained_model.py", line 99, in load_pretrained_model
    src_state = torch.load(path, map_location=map_location)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/data/mohan/workdir/espnet/egs2/myst/asr1/exp/librispeech_100_ctc_e_branchformer/exp/asr_train_asr_ctc_e_branchformer_e12_raw_en_bpe5000_sp/valid.cer_ctc.ave_10best.pth'
W1031 20:45:27.053000 123363692857152 torch/multiprocessing/spawn.py:145] Terminating process 847750 via signal SIGTERM
>>>>>>> cea3af74175b4f1b718b2c4afeb0a5f028e5698c
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
<<<<<<< HEAD
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1174, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 1170, in main
    inference(**kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 843, in inference
    results = speech2text(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_inference.py", line 515, in __call__
    enc, enc_olens = self.asr_model.encode(**batch)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 378, in encode
    feats, feats_lengths = self._extract_feats(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 445, in _extract_feats
    feats, feats_lengths = self.frontend(speech, speech_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/frontend/s3prl.py", line 99, in forward
    feats, feats_lens = self.upstream(input, input_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/nn/upstream.py", line 209, in forward
    hidden_states = self.upstream(wavs_list)["hidden_states"]
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/interfaces.py", line 103, in __call__
    result = super().__call__(wavs, *args, **kwargs) or {}
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/expert.py", line 83, in forward
    features, feat_padding_mask = self.model.extract_features(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 389, in extract_features
    x, layer_results = self.encoder(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 592, in forward
    x, layer_results = self.extract_features(x, padding_mask, streaming_mask, layer)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 626, in extract_features
    x, z, pos_bias = layer(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/s3prl/upstream/wavlm/WavLM.py", line 742, in forward
    x = self.activation_fn(self.fc1(x))
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1498974) is killed by signal: Killed. 
# Accounting: time=5235 threads=1
# Ended (code 1) at Sun Oct 27 18:58:12 EDT 2024, elapsed time 5235 seconds
2024-10-28T00:15:06 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 4 --inference_asr_model valid.cer_ctc.ave.pth --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-28T00:15:07 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-28T00:15:07 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000
2024-10-28T00:15:07 (asr.sh:1508:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/run.sh'. You can resume the process from stage 12 using this script
2024-10-28T00:15:07 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/asr_inference.*.log'
2024-10-28T04:11:31 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/dev/logdir/calculate_rtf.log'
2024-10-28T04:11:33 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/test/logdir/asr_inference.*.log'
2024-10-28T08:52:45 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.ave/test/logdir/calculate_rtf.log'
2024-10-28T08:52:48 (asr.sh:1813:main) Successfully finished. [elapsed=31062s]
2024-10-28T16:43:32 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 4 --nj 16 --gpu_inference false --inference_nj 4 --inference_asr_model valid.cer_ctc.best.pth --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-28T16:43:32 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-28T16:43:32 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000
2024-10-28T16:43:32 (asr.sh:1508:main) Generate 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 12 using this script
2024-10-28T16:43:32 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/asr_inference.*.log'
2024-10-28T20:51:20 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/calculate_rtf.log'
2024-10-28T20:51:23 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/test/logdir/asr_inference.*.log'
2024-10-29T01:42:48 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_wavlm_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/test/logdir/calculate_rtf.log'
2024-10-29T01:42:51 (asr.sh:1813:main) Successfully finished. [elapsed=32359s]
=======
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1273, in main
    while not ProcessContext(processes, error_files).join():
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 177, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 1 terminated with exit code 1
# Accounting: time=14 threads=1
# Ended (code 1) at Thu Oct 31 20:45:27 PDT 2024, elapsed time 14 seconds

2024-10-31T20:46:59 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference true --inference_nj 2 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_onlyctc_finetune.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-10-31T20:46:59 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-10-31T20:46:59 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-10-31T20:46:59 (asr.sh:1407:main) Generate 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-10-31T20:46:59 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log'
2024-10-31 20:46:59,614 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log' --log exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000 --config conf/train_asr_onlyctc_finetune.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-10-31 20:46:59,624 (launch:237) INFO: single-node with 2gpu on distributed mode
2024-10-31 20:46:59,625 (launch:348) INFO: log file: exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/train.log
2024-11-01T10:28:55 (asr.sh:1813:main) Successfully finished. [elapsed=49316s]
2024-11-01T10:41:58 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 2 --nj 16 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_onlyctc_finetune.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-11-01T10:41:58 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-11-01T10:41:58 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000
2024-11-01T10:41:58 (asr.sh:1508:main) Generate 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 12 using this script
2024-11-01T10:41:58 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/asr_inference.*.log'
2024-11-01T11:35:32 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/calculate_rtf.log'
2024-11-01T11:35:33 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/test/logdir/asr_inference.*.log'
2024-11-01T12:47:00 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_onlyctc_finetune_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/test/logdir/calculate_rtf.log'
2024-11-01T12:47:01 (asr.sh:1813:main) Successfully finished. [elapsed=7503s]
2024-11-09T00:06:48 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 2 --nj 16 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_onlyctc_nospecaug.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-11-09T00:06:48 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-11-09T00:06:48 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2024-11-09T00:06:48 (asr.sh:1407:main) Generate 'exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2024-11-09T00:06:48 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/train.log'
2024-11-09 00:06:48,818 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/train.log' --log exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/train.log --ngpu 2 --num_nodes 1 --init_file_prefix exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000 --config conf/train_asr_onlyctc_nospecaug.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe5000/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2024-11-09 00:06:48,831 (launch:237) INFO: single-node with 2gpu on distributed mode
2024-11-09 00:06:48,831 (launch:348) INFO: log file: exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/train.log
2024-11-09T13:13:52 (asr.sh:1813:main) Successfully finished. [elapsed=47224s]
2024-11-09T14:18:03 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 2 --nj 16 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_onlyctc_nospecaug.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-11-09T14:18:03 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-11-09T14:18:03 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000
2024-11-09T14:18:03 (asr.sh:1508:main) Generate 'exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 12 using this script
2024-11-09T14:18:03 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/asr_inference.*.log'
2024-11-09T15:11:34 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/calculate_rtf.log'
2024-11-09T15:11:36 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/test/logdir/asr_inference.*.log'
2024-11-09T16:31:08 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 2 --nj 16 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_onlyctc_nospecaug.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text data/train/text --bpe_train_text data/train/text
2024-11-09T16:31:08 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2024-11-09T16:31:08 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000
2024-11-09T16:31:08 (asr.sh:1508:main) Generate 'exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 12 using this script
2024-11-09T16:31:08 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/test/logdir/asr_inference.*.log'
2024-11-09T17:43:02 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_onlyctc_nospecaug_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/test/logdir/calculate_rtf.log'
2024-11-09T17:43:03 (asr.sh:1813:main) Successfully finished. [elapsed=4315s]
<<<<<<< HEAD
2025-01-07T23:56:22 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 4 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2025-01-07T23:56:23 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-01-07T23:56:23 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-01-07T23:56:23 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-01-07T23:56:23 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log'
2025-01-07 23:56:23,155 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-01-07 23:56:23,168 (launch:237) INFO: single-node with 4gpu on distributed mode
2025-01-07 23:56:23,169 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log', '--gpu', '4', 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '4', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
=======
>>>>>>> cea3af74175b4f1b718b2c4afeb0a5f028e5698c
2025-01-09T04:23:57 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 4 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-01-09T04:23:57 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-01-09T04:23:57 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-01-09T04:23:57 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-01-09T04:23:57 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-01-09 04:23:58,042 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-01-09 04:23:58,058 (launch:237) INFO: single-node with 4gpu on distributed mode
2025-01-09 04:23:58,059 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', '--gpu', '4', 'exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '4', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
>>>>>>> 5ebf8f1b6dc51bfa26a032f85e8e4b6ca01d6936
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
<<<<<<< HEAD
################### The last 1000 lines of exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 4 --multiprocessing_distributed True 
# Started at Tue Jan  7 23:56:23 PST 2025
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 4 --multiprocessing_distributed True
Process SpawnProcess-4:
=======
################### The last 1000 lines of exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 4 --multiprocessing_distributed True 
# Started at Thu Jan  9 04:23:58 EST 2025
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 4 --multiprocessing_distributed True
usage: asr_train.py [-h] [--config CONFIG] [--print_config]
                    [--log_level {ERROR,WARNING,INFO,DEBUG,NOTSET}]
                    [--drop_last_iter DROP_LAST_ITER] [--dry_run DRY_RUN]
                    [--iterator_type {sequence,category,chunk,task,none}]
                    [--valid_iterator_type {sequence,category,chunk,task,none}]
                    [--output_dir OUTPUT_DIR] [--ngpu NGPU] [--seed SEED]
                    [--num_workers NUM_WORKERS] [--num_att_plot NUM_ATT_PLOT]
                    [--dist_backend DIST_BACKEND]
                    [--dist_init_method DIST_INIT_METHOD]
                    [--dist_world_size DIST_WORLD_SIZE]
                    [--dist_rank DIST_RANK] [--local_rank LOCAL_RANK]
                    [--dist_master_addr DIST_MASTER_ADDR]
                    [--dist_master_port DIST_MASTER_PORT]
                    [--dist_launcher {slurm,mpi,None}]
                    [--multiprocessing_distributed MULTIPROCESSING_DISTRIBUTED]
                    [--unused_parameters UNUSED_PARAMETERS]
                    [--sharded_ddp SHARDED_DDP]
                    [--use_deepspeed USE_DEEPSPEED]
                    [--deepspeed_config DEEPSPEED_CONFIG]
                    [--cudnn_enabled CUDNN_ENABLED]
                    [--cudnn_benchmark CUDNN_BENCHMARK]
                    [--cudnn_deterministic CUDNN_DETERMINISTIC]
                    [--use_tf32 USE_TF32] [--collect_stats COLLECT_STATS]
                    [--write_collected_feats WRITE_COLLECTED_FEATS]
                    [--max_epoch MAX_EPOCH] [--patience PATIENCE]
                    [--val_scheduler_criterion VAL_SCHEDULER_CRITERION VAL_SCHEDULER_CRITERION]
                    [--early_stopping_criterion EARLY_STOPPING_CRITERION EARLY_STOPPING_CRITERION EARLY_STOPPING_CRITERION]
                    [--best_model_criterion BEST_MODEL_CRITERION [BEST_MODEL_CRITERION ...]]
                    [--keep_nbest_models KEEP_NBEST_MODELS [KEEP_NBEST_MODELS ...]]
                    [--nbest_averaging_interval NBEST_AVERAGING_INTERVAL]
                    [--grad_clip GRAD_CLIP] [--grad_clip_type GRAD_CLIP_TYPE]
                    [--grad_noise GRAD_NOISE] [--accum_grad ACCUM_GRAD]
                    [--no_forward_run NO_FORWARD_RUN] [--resume RESUME]
                    [--train_dtype {float16,float32,float64}]
                    [--use_amp USE_AMP] [--log_interval LOG_INTERVAL]
                    [--use_matplotlib USE_MATPLOTLIB]
                    [--use_tensorboard USE_TENSORBOARD]
                    [--create_graph_in_tensorboard CREATE_GRAPH_IN_TENSORBOARD]
                    [--use_wandb USE_WANDB] [--wandb_project WANDB_PROJECT]
                    [--wandb_id WANDB_ID] [--wandb_entity WANDB_ENTITY]
                    [--wandb_name WANDB_NAME]
                    [--wandb_model_log_interval WANDB_MODEL_LOG_INTERVAL]
                    [--detect_anomaly DETECT_ANOMALY]
                    [--use_adapter USE_ADAPTER] [--adapter {lora,houlsby}]
                    [--save_strategy {all,adapter_only,required_grad_only}]
                    [--adapter_conf ADAPTER_CONF]
                    [--pretrain_path PRETRAIN_PATH]
                    [--init_param [INIT_PARAM ...]]
                    [--ignore_init_mismatch IGNORE_INIT_MISMATCH]
                    [--freeze_param [FREEZE_PARAM ...]]
                    [--num_iters_per_epoch NUM_ITERS_PER_EPOCH]
                    [--batch_size BATCH_SIZE]
                    [--valid_batch_size VALID_BATCH_SIZE]
                    [--batch_bins BATCH_BINS]
                    [--valid_batch_bins VALID_BATCH_BINS]
                    [--category_sample_size CATEGORY_SAMPLE_SIZE]
                    [--train_shape_file TRAIN_SHAPE_FILE]
                    [--valid_shape_file VALID_SHAPE_FILE]
                    [--batch_type {unsorted,sorted,folded,length,numel}]
                    [--valid_batch_type {unsorted,sorted,folded,length,numel,None}]
                    [--fold_length FOLD_LENGTH]
                    [--sort_in_batch {descending,ascending}]
                    [--shuffle_within_batch SHUFFLE_WITHIN_BATCH]
                    [--sort_batch {descending,ascending}]
                    [--multiple_iterator MULTIPLE_ITERATOR]
                    [--chunk_length CHUNK_LENGTH]
                    [--chunk_shift_ratio CHUNK_SHIFT_RATIO]
                    [--num_cache_chunks NUM_CACHE_CHUNKS]
                    [--chunk_excluded_key_prefixes CHUNK_EXCLUDED_KEY_PREFIXES [CHUNK_EXCLUDED_KEY_PREFIXES ...]]
                    [--chunk_default_fs CHUNK_DEFAULT_FS]
                    [--chunk_max_abs_length CHUNK_MAX_ABS_LENGTH]
                    [--chunk_discard_short_samples CHUNK_DISCARD_SHORT_SAMPLES]
                    [--train_data_path_and_name_and_type TRAIN_DATA_PATH_AND_NAME_AND_TYPE]
                    [--valid_data_path_and_name_and_type VALID_DATA_PATH_AND_NAME_AND_TYPE]
                    [--multi_task_dataset MULTI_TASK_DATASET]
                    [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS]
                    [--max_cache_size MAX_CACHE_SIZE]
                    [--max_cache_fd MAX_CACHE_FD]
                    [--allow_multi_rates ALLOW_MULTI_RATES]
                    [--valid_max_cache_size VALID_MAX_CACHE_SIZE]
                    [--exclude_weight_decay EXCLUDE_WEIGHT_DECAY]
                    [--exclude_weight_decay_conf EXCLUDE_WEIGHT_DECAY_CONF]
                    [--optim {adam,adamw,sgd,adadelta,adagrad,adamax,asgd,lbfgs,rmsprop,rprop,radam}]
                    [--optim_conf OPTIM_CONF]
                    [--scheduler {reducelronplateau,lambdalr,steplr,multisteplr,exponentiallr,cosineannealinglr,noamlr,warmuplr,piecewiselinearwarmuplr,warmupsteplr,warmupreducelronplateau,cycliclr,onecyclelr,cosineannealingwarmrestarts,cosineannealingwarmuprestarts,None}]
                    [--scheduler_conf SCHEDULER_CONF]
                    [--token_list TOKEN_LIST]
                    [--init {chainer,xavier_uniform,xavier_normal,kaiming_uniform,kaiming_normal,None}]
                    [--input_size INPUT_SIZE] [--ctc_conf CTC_CONF]
                    [--joint_net_conf JOINT_NET_CONF]
                    [--use_preprocessor USE_PREPROCESSOR]
                    [--use_lang_prompt USE_LANG_PROMPT]
                    [--use_nlp_prompt USE_NLP_PROMPT]
                    [--token_type {bpe,char,word,phn,hugging_face,whisper_en,whisper_multilingual}]
                    [--bpemodel BPEMODEL]
                    [--non_linguistic_symbols NON_LINGUISTIC_SYMBOLS]
                    [--cleaner {None,tacotron,jaconv,vietnamese,whisper_en,whisper_basic}]
                    [--g2p {None,g2p_en,g2p_en_no_space,pyopenjtalk,pyopenjtalk_kana,pyopenjtalk_accent,pyopenjtalk_accent_with_pause,pyopenjtalk_prosody,pypinyin_g2p,pypinyin_g2p_phone,pypinyin_g2p_phone_without_prosody,espeak_ng_arabic,espeak_ng_german,espeak_ng_french,espeak_ng_spanish,espeak_ng_russian,espeak_ng_greek,espeak_ng_finnish,espeak_ng_hungarian,espeak_ng_dutch,espeak_ng_english_us_vits,espeak_ng_hindi,espeak_ng_italian,espeak_ng_ukrainian,espeak_ng_polish,g2pk,g2pk_no_space,g2pk_explicit_space,korean_jaso,korean_jaso_no_space,g2p_is}]
                    [--speech_volume_normalize SPEECH_VOLUME_NORMALIZE]
                    [--rir_scp RIR_SCP] [--rir_apply_prob RIR_APPLY_PROB]
                    [--noise_scp NOISE_SCP]
                    [--noise_apply_prob NOISE_APPLY_PROB]
                    [--noise_db_range NOISE_DB_RANGE]
                    [--short_noise_thres SHORT_NOISE_THRES]
                    [--aux_ctc_tasks AUX_CTC_TASKS [AUX_CTC_TASKS ...]]
                    [--frontend {default,sliding_window,s3prl,hf_freeze_ctc,hf_train_fsq_ctc,fused,whisper}]
                    [--frontend_conf FRONTEND_CONF] [--specaug {specaug,None}]
                    [--specaug_conf SPECAUG_CONF]
                    [--normalize {global_mvn,utterance_mvn,None}]
                    [--normalize_conf NORMALIZE_CONF]
                    [--model {espnet,maskctc,pit_espnet}]
                    [--model_conf MODEL_CONF]
                    [--preencoder {sinc,linear,None}]
                    [--preencoder_conf PREENCODER_CONF]
                    [--encoder {conformer,transformer,transformer_multispkr,contextual_block_transformer,contextual_block_conformer,vgg_rnn,rnn,wav2vec2,hubert,hubert_pretrain,torchaudiohubert,longformer,branchformer,whisper,e_branchformer,avhubert,multiconv_conformer}]
                    [--encoder_conf ENCODER_CONF]
                    [--postencoder {hugging_face_transformers,length_adaptor,None}]
                    [--postencoder_conf POSTENCODER_CONF]
                    [--decoder {transformer,lightweight_conv,lightweight_conv2d,dynamic_conv,dynamic_conv2d,rnn,transducer,mlm,whisper,hugging_face_transformers,s4,None}]
                    [--decoder_conf DECODER_CONF]
                    [--preprocessor {default,multi}]
                    [--preprocessor_conf PREPROCESSOR_CONF]
asr_train.py: error: No such file: conf/train_asr_wavlm_ogi_myst_ebranchformer_onlyctc_lr1e-4.yaml
# Accounting: time=10 threads=1
# Ended (code 2) at Thu Jan  9 04:24:08 EST 2025, elapsed time 10 seconds

2025-01-09T04:25:00 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 4 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-01-09T04:25:01 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-01-09T04:25:01 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-01-09T04:25:01 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-01-09T04:25:01 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-01-09 04:25:01,266 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-01-09 04:25:01,282 (launch:237) INFO: single-node with 4gpu on distributed mode
2025-01-09 04:25:01,283 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', '--gpu', '4', 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '4', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 4 --multiprocessing_distributed True 
# Started at Thu Jan  9 04:25:01 EST 2025
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 4 --multiprocessing_distributed True
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[dl:0/4] 2025-01-09 04:25:20,753 (asr:527) INFO: Vocabulary size: 5000
[dl:0/4] 2025-01-09 04:25:23,668 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
[dl:0/4] 2025-01-09 04:25:24,758 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl:0/4] 2025-01-09 04:25:24,767 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): HfCTCFSQFrontend(
    (upstream): WavLMForCTC(
      (wavlm): WavLMModel(
        (feature_extractor): WavLMFeatureEncoder(
          (conv_layers): ModuleList(
            (0): WavLMLayerNormConvLayer(
              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (activation): GELUActivation()
            )
            (1-4): 4 x WavLMLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (activation): GELUActivation()
            )
            (5-6): 2 x WavLMLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (activation): GELUActivation()
            )
          )
        )
        (feature_projection): WavLMFeatureProjection(
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (projection): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (encoder): WavLMEncoderStableLayerNorm(
          (pos_conv_embed): WavLMPositionalConvEmbedding(
            (conv): ParametrizedConv1d(
              1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16
              (parametrizations): ModuleDict(
                (weight): ParametrizationList(
                  (0): _WeightNorm()
                )
              )
            )
            (padding): WavLMSamePadLayer()
            (activation): GELUActivation()
          )
          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (layers): ModuleList(
            (0): WavLMEncoderLayerStableLayerNorm(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
                (rel_attn_embed): Embedding(320, 16)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=4096, out_features=1024, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
            (1-23): 23 x WavLMEncoderLayerStableLayerNorm(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=4096, out_features=1024, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (lm_head): Linear(in_features=1024, out_features=42, bias=True)
    )
    (quantizer): FSQ(
      (project_in): Linear(in_features=1024, out_features=5, bias=True)
      (project_out): Linear(in_features=5, out_features=1024, bias=True)
    )
  )
  (preencoder): LinearProjection(
    (linear_out): Linear(in_features=1024, out_features=128, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=7936, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 342.86 M
    Number of trainable parameters: 338.65 M (98.8%)
    Size: 1.35 GB
    Type: torch.float32
[dl:0/4] 2025-01-09 04:25:24,767 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 6.666666666666668e-09
    maximize: False
    weight_decay: 1e-06
)
[dl:0/4] 2025-01-09 04:25:24,767 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[dl:0/4] 2025-01-09 04:25:24,767 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/config.yaml
[dl:0/4] 2025-01-09 04:25:24,926 (asr:499) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/4] 2025-01-09 04:25:25,621 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7918aa7234f0>)
[dl:0/4] 2025-01-09 04:25:25,621 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=5974, batch_bins=2000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/4] 2025-01-09 04:25:25,622 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=5974, mean=9.3, min=4, max=57
[dl:0/4] 2025-01-09 04:25:25,638 (asr:499) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/4] 2025-01-09 04:25:25,674 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7918aa3e3550>)
[dl:0/4] 2025-01-09 04:25:25,674 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=954, batch_bins=2000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/4] 2025-01-09 04:25:25,674 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=954, mean=9.5, min=4, max=55
[dl:0/4] 2025-01-09 04:25:25,686 (asr:499) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/4] 2025-01-09 04:25:25,693 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7918aa3fbd30>)
[dl:0/4] 2025-01-09 04:25:25,693 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl:0/4] 2025-01-09 04:25:25,693 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
dl:1650678:1650678 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650678:1650678 [0] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1650678:1650678 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1650678:1650678 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.20.5+cuda12.4
dl:1650681:1650681 [2] NCCL INFO cudaDriverVersion 12050
dl:1650681:1650681 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650681:1650681 [2] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1650681:1650681 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1650681:1650858 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650681:1650858 [2] NCCL INFO NET/IB : No device found.
dl:1650681:1650858 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650681:1650858 [2] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:1650681:1650858 [2] NCCL INFO Using non-device net plugin version 0
dl:1650681:1650858 [2] NCCL INFO Using network Socket
dl:1650681:1650858 [2] NCCL INFO comm 0x58699d80 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 49000 commId 0xce73e7998c3f4506 - Init START
dl:1650681:1650858 [2] NCCL INFO NVLS multicast support is not available on dev 2
dl:1650681:1650858 [2] NCCL INFO comm 0x58699d80 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
dl:1650681:1650858 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
dl:1650681:1650858 [2] NCCL INFO P2P Chunksize set to 131072
dl:1650681:1650858 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
dl:1650681:1650858 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
dl:1650681:1650858 [2] NCCL INFO Connected all rings
dl:1650681:1650858 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
dl:1650681:1650858 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
dl:1650681:1650858 [2] NCCL INFO Connected all trees
dl:1650681:1650858 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:1650681:1650858 [2] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:1650681:1650858 [2] NCCL INFO comm 0x58699d80 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 49000 commId 0xce73e7998c3f4506 - Init COMPLETE
dl:1650680:1650680 [1] NCCL INFO cudaDriverVersion 12050
dl:1650680:1650680 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650680:1650680 [1] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1650680:1650680 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1650680:1650856 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650680:1650856 [1] NCCL INFO NET/IB : No device found.
dl:1650680:1650856 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650680:1650856 [1] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:1650680:1650856 [1] NCCL INFO Using non-device net plugin version 0
dl:1650680:1650856 [1] NCCL INFO Using network Socket
dl:1650680:1650856 [1] NCCL INFO comm 0xc849470 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 21000 commId 0xce73e7998c3f4506 - Init START
dl:1650680:1650856 [1] NCCL INFO NVLS multicast support is not available on dev 1
dl:1650680:1650856 [1] NCCL INFO comm 0xc849470 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
dl:1650680:1650856 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
dl:1650680:1650856 [1] NCCL INFO P2P Chunksize set to 131072
dl:1650680:1650856 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
dl:1650680:1650856 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
dl:1650680:1650856 [1] NCCL INFO Connected all rings
dl:1650680:1650856 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
dl:1650680:1650856 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
dl:1650680:1650856 [1] NCCL INFO Connected all trees
dl:1650680:1650856 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:1650680:1650856 [1] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:1650680:1650856 [1] NCCL INFO comm 0xc849470 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 21000 commId 0xce73e7998c3f4506 - Init COMPLETE
[rank2]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
[rank1]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
dl:1650678:1650855 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650678:1650855 [0] NCCL INFO NET/IB : No device found.
dl:1650678:1650855 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650678:1650855 [0] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:1650678:1650855 [0] NCCL INFO Using non-device net plugin version 0
dl:1650678:1650855 [0] NCCL INFO Using network Socket
dl:1650678:1650855 [0] NCCL INFO comm 0x376cfbe0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 2000 commId 0xce73e7998c3f4506 - Init START
dl:1650678:1650855 [0] NCCL INFO NVLS multicast support is not available on dev 0
dl:1650678:1650855 [0] NCCL INFO comm 0x376cfbe0 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
dl:1650678:1650855 [0] NCCL INFO Channel 00/02 :    0   1   2   3
dl:1650678:1650855 [0] NCCL INFO Channel 01/02 :    0   1   2   3
dl:1650678:1650855 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
dl:1650678:1650855 [0] NCCL INFO P2P Chunksize set to 131072
dl:1650678:1650855 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
dl:1650678:1650855 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
dl:1650678:1650855 [0] NCCL INFO Connected all rings
dl:1650678:1650855 [0] NCCL INFO Connected all trees
dl:1650678:1650855 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:1650678:1650855 [0] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:1650678:1650855 [0] NCCL INFO comm 0x376cfbe0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 2000 commId 0xce73e7998c3f4506 - Init COMPLETE
[rank0]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
dl:1650683:1650683 [3] NCCL INFO cudaDriverVersion 12050
dl:1650683:1650683 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650683:1650683 [3] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1650683:1650683 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1650683:1650857 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650683:1650857 [3] NCCL INFO NET/IB : No device found.
dl:1650683:1650857 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1650683:1650857 [3] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:1650683:1650857 [3] NCCL INFO Using non-device net plugin version 0
dl:1650683:1650857 [3] NCCL INFO Using network Socket
dl:1650683:1650857 [3] NCCL INFO comm 0xa9247d0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 4a000 commId 0xce73e7998c3f4506 - Init START
dl:1650683:1650857 [3] NCCL INFO NVLS multicast support is not available on dev 3
dl:1650683:1650857 [3] NCCL INFO comm 0xa9247d0 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
dl:1650683:1650857 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
dl:1650683:1650857 [3] NCCL INFO P2P Chunksize set to 131072
dl:1650683:1650857 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM
dl:1650683:1650857 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/CUMEM
dl:1650683:1650857 [3] NCCL INFO Connected all rings
dl:1650683:1650857 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
dl:1650683:1650857 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
dl:1650683:1650857 [3] NCCL INFO Connected all trees
dl:1650683:1650857 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:1650683:1650857 [3] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:1650683:1650857 [3] NCCL INFO comm 0xa9247d0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 4a000 commId 0xce73e7998c3f4506 - Init COMPLETE
[rank3]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
[dl:0/4] 2025-01-09 04:25:26,502 (trainer:311) INFO: 1/70epoch started
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
Process SpawnProcess-1:
>>>>>>> 5ebf8f1b6dc51bfa26a032f85e8e4b6ca01d6936
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
<<<<<<< HEAD
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1283, in main_worker
    distributed_option.init_options()
  File "/data/mohan/workdir/espnet/espnet2/train/distributed_utils.py", line 60, in init_options
    raise RuntimeError(
RuntimeError: LOCAL_RANK=3 is bigger than the number of visible devices: 0
Process SpawnProcess-2:
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1283, in main_worker
    distributed_option.init_options()
  File "/data/mohan/workdir/espnet/espnet2/train/distributed_utils.py", line 60, in init_options
    raise RuntimeError(
RuntimeError: LOCAL_RANK=1 is bigger than the number of visible devices: 0
Process SpawnProcess-3:
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1283, in main_worker
    distributed_option.init_options()
  File "/data/mohan/workdir/espnet/espnet2/train/distributed_utils.py", line 60, in init_options
    raise RuntimeError(
RuntimeError: LOCAL_RANK=2 is bigger than the number of visible devices: 0
W0107 23:56:33.980000 124899948611392 torch/multiprocessing/spawn.py:145] Terminating process 1585013 via signal SIGTERM
W0107 23:56:33.980000 124899948611392 torch/multiprocessing/spawn.py:145] Terminating process 1585014 via signal SIGTERM
W0107 23:56:33.980000 124899948611392 torch/multiprocessing/spawn.py:145] Terminating process 1585015 via signal SIGTERM
=======
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 614, in train_one_epoch
    retval = model(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 237, in forward
    encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 402, in encode
    encoder_out, encoder_out_lens, _ = self.encoder(feats, feats_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/encoder/e_branchformer_encoder.py", line 495, in forward
    xs_pad, masks = self.encoders(xs_pad, masks)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet/nets/pytorch_backend/transformer/repeat.py", line 30, in forward
    args = m(*args)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/encoder/e_branchformer_encoder.py", line 157, in forward
    x2 = self.cgmlp(x2, mask)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/layers/cgmlp.py", line 118, in forward
    xs_pad = self.channel_proj2(xs_pad)  # linear_units/2 -> size
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 
dl:1650678:1650859 [0] NCCL INFO [Service thread] Connection closed by localRank 0
dl:1650678:1653049 [0] NCCL INFO comm 0x376cfbe0 rank 0 nranks 4 cudaDev 0 busId 2000 - Abort COMPLETE
W0109 04:30:19.545000 126123558676288 torch/multiprocessing/spawn.py:145] Terminating process 1650680 via signal SIGTERM
W0109 04:30:19.546000 126123558676288 torch/multiprocessing/spawn.py:145] Terminating process 1650681 via signal SIGTERM
W0109 04:30:19.546000 126123558676288 torch/multiprocessing/spawn.py:145] Terminating process 1650683 via signal SIGTERM
>>>>>>> 5ebf8f1b6dc51bfa26a032f85e8e4b6ca01d6936
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1273, in main
    while not ProcessContext(processes, error_files).join():
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 177, in join
    raise ProcessExitedException(
<<<<<<< HEAD
torch.multiprocessing.spawn.ProcessExitedException: process 3 terminated with exit code 1
# Accounting: time=11 threads=1
# Ended (code 1) at Tue Jan  7 23:56:34 PST 2025, elapsed time 11 seconds

2025-01-07T23:57:40 (asr.sh:283:main) ./asr.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2025-01-07T23:57:40 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-01-07T23:57:40 (asr.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-01-07T23:57:40 (asr.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-01-07T23:57:40 (asr.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log'
2025-01-07 23:57:40,772 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-01-07 23:57:40,785 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/train.log
2025-01-09T18:20:44 (asr.sh:1813:main) Successfully finished. [elapsed=152584s]
2025-01-09T19:12:29 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2025-01-09T19:12:29 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-01-09T19:12:29 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000
2025-01-09T19:12:29 (asr.sh:1508:main) Generate 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 12 using this script
2025-01-09T19:12:29 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/asr_inference.*.log'
2025-01-09T19:28:08 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 1 --nj 4 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2025-01-09T19:28:08 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-01-09T19:28:08 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000
2025-01-09T19:28:08 (asr.sh:1508:main) Generate 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 12 using this script
2025-01-09T19:28:08 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/asr_inference.*.log'
2025-01-09T21:17:01 (asr.sh:283:main) ./asr.sh --stage 12 --stop_stage 12 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --feats_normalize utterance_mvn --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_myst_ebranchformer_onlyctc.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets dev test --lm_train_text data/train/text --bpe_train_text data/train/text
2025-01-09T21:17:02 (asr.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-01-09T21:17:02 (asr.sh:1480:main) Stage 12: Decoding: training_dir=exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000
2025-01-09T21:17:02 (asr.sh:1508:main) Generate 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/run.sh'. You can resume the process from stage 12 using this script
2025-01-09T21:17:02 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/asr_inference.*.log'
2025-01-09T22:11:29 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/dev/logdir/calculate_rtf.log'
2025-01-09T22:11:30 (asr.sh:1575:main) Decoding started... log: 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/test/logdir/asr_inference.*.log'
2025-01-09T23:23:31 (asr.sh:1591:main) Calculating RTF & latency... log: 'exp/asr_train_asr_wavlm_myst_ebranchformer_onlyctc_raw_en_bpe5000/decode_asr_ctc_greedy_asr_model_valid.cer_ctc.best/test/logdir/calculate_rtf.log'
2025-01-09T23:23:32 (asr.sh:1813:main) Successfully finished. [elapsed=7591s]
=======
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with exit code 1
# Accounting: time=320 threads=1
# Ended (code 1) at Thu Jan  9 04:30:21 EST 2025, elapsed time 320 seconds

2025-01-09T04:32:57 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 4 --nj 4 --gpu_inference false --inference_asr_model valid.cer_ctc.best.pth --inference_nj 4 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-01-09T04:32:57 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-01-09T04:32:57 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-01-09T04:32:57 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-01-09T04:32:57 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-01-09 04:32:59,309 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 4 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-01-09 04:32:59,326 (launch:237) INFO: single-node with 4gpu on distributed mode
2025-01-09 04:32:59,330 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', '--gpu', '4', 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '4', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 4 --multiprocessing_distributed True 
# Started at Thu Jan  9 04:32:59 EST 2025
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 4 --multiprocessing_distributed True
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)
[dl:0/4] 2025-01-09 04:33:31,455 (asr:527) INFO: Vocabulary size: 5000
[dl:0/4] 2025-01-09 04:33:40,722 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
[dl:0/4] 2025-01-09 04:33:41,704 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[dl:0/4] 2025-01-09 04:33:41,714 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): HfCTCFSQFrontend(
    (upstream): WavLMForCTC(
      (wavlm): WavLMModel(
        (feature_extractor): WavLMFeatureEncoder(
          (conv_layers): ModuleList(
            (0): WavLMLayerNormConvLayer(
              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (activation): GELUActivation()
            )
            (1-4): 4 x WavLMLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (activation): GELUActivation()
            )
            (5-6): 2 x WavLMLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (activation): GELUActivation()
            )
          )
        )
        (feature_projection): WavLMFeatureProjection(
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (projection): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (encoder): WavLMEncoderStableLayerNorm(
          (pos_conv_embed): WavLMPositionalConvEmbedding(
            (conv): ParametrizedConv1d(
              1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16
              (parametrizations): ModuleDict(
                (weight): ParametrizationList(
                  (0): _WeightNorm()
                )
              )
            )
            (padding): WavLMSamePadLayer()
            (activation): GELUActivation()
          )
          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (layers): ModuleList(
            (0): WavLMEncoderLayerStableLayerNorm(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
                (rel_attn_embed): Embedding(320, 16)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=4096, out_features=1024, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
            (1-23): 23 x WavLMEncoderLayerStableLayerNorm(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=4096, out_features=1024, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (lm_head): Linear(in_features=1024, out_features=42, bias=True)
    )
    (quantizer): FSQ(
      (project_in): Linear(in_features=1024, out_features=5, bias=True)
      (project_out): Linear(in_features=5, out_features=1024, bias=True)
    )
  )
  (preencoder): LinearProjection(
    (linear_out): Linear(in_features=1024, out_features=128, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=7936, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 342.86 M
    Number of trainable parameters: 338.65 M (98.8%)
    Size: 1.35 GB
    Type: torch.float32
[dl:0/4] 2025-01-09 04:33:41,714 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 6.666666666666668e-09
    maximize: False
    weight_decay: 1e-06
)
[dl:0/4] 2025-01-09 04:33:41,714 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[dl:0/4] 2025-01-09 04:33:41,714 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_wavlm_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/config.yaml
[dl:0/4] 2025-01-09 04:33:41,884 (asr:499) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/4] 2025-01-09 04:33:42,637 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7e9387011210>)
[dl:0/4] 2025-01-09 04:33:42,637 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=9198, batch_bins=1000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/4] 2025-01-09 04:33:42,639 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=9198, mean=6.1, min=4, max=32
[dl:0/4] 2025-01-09 04:33:42,660 (asr:499) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/4] 2025-01-09 04:33:42,711 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7e938701c910>)
[dl:0/4] 2025-01-09 04:33:42,712 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=1483, batch_bins=1000000, sort_in_batch=descending, sort_batch=descending)
[dl:0/4] 2025-01-09 04:33:42,712 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=1483, mean=6.1, min=4, max=28
[dl:0/4] 2025-01-09 04:33:42,726 (asr:499) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[dl:0/4] 2025-01-09 04:33:42,734 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7e9381145d20>)
[dl:0/4] 2025-01-09 04:33:42,734 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[dl:0/4] 2025-01-09 04:33:42,735 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
dl:1653741:1653741 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653741:1653741 [0] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1653741:1653741 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1653741:1653741 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.20.5+cuda12.4
dl:1653743:1653743 [2] NCCL INFO cudaDriverVersion 12050
dl:1653743:1653743 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653743:1653743 [2] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1653743:1653743 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1653743:1653981 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653743:1653981 [2] NCCL INFO NET/IB : No device found.
dl:1653743:1653981 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653743:1653981 [2] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:1653743:1653981 [2] NCCL INFO Using non-device net plugin version 0
dl:1653743:1653981 [2] NCCL INFO Using network Socket
dl:1653743:1653981 [2] NCCL INFO comm 0xc12daf0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 49000 commId 0x408e9fc53d949dff - Init START
dl:1653743:1653981 [2] NCCL INFO NVLS multicast support is not available on dev 2
dl:1653743:1653981 [2] NCCL INFO comm 0xc12daf0 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
dl:1653743:1653981 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
dl:1653743:1653981 [2] NCCL INFO P2P Chunksize set to 131072
dl:1653743:1653981 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
dl:1653743:1653981 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
dl:1653743:1653981 [2] NCCL INFO Connected all rings
dl:1653743:1653981 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
dl:1653743:1653981 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
dl:1653743:1653981 [2] NCCL INFO Connected all trees
dl:1653743:1653981 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:1653743:1653981 [2] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:1653743:1653981 [2] NCCL INFO comm 0xc12daf0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 49000 commId 0x408e9fc53d949dff - Init COMPLETE
[rank2]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
dl:1653744:1653744 [3] NCCL INFO cudaDriverVersion 12050
dl:1653744:1653744 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653744:1653744 [3] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1653744:1653744 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1653744:1653979 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653744:1653979 [3] NCCL INFO NET/IB : No device found.
dl:1653744:1653979 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653744:1653979 [3] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:1653744:1653979 [3] NCCL INFO Using non-device net plugin version 0
dl:1653744:1653979 [3] NCCL INFO Using network Socket
dl:1653744:1653979 [3] NCCL INFO comm 0x4e1607d0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 4a000 commId 0x408e9fc53d949dff - Init START
dl:1653744:1653979 [3] NCCL INFO NVLS multicast support is not available on dev 3
dl:1653744:1653979 [3] NCCL INFO comm 0x4e1607d0 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
dl:1653744:1653979 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
dl:1653744:1653979 [3] NCCL INFO P2P Chunksize set to 131072
dl:1653744:1653979 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM
dl:1653744:1653979 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/CUMEM
dl:1653744:1653979 [3] NCCL INFO Connected all rings
dl:1653744:1653979 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
dl:1653744:1653979 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
dl:1653744:1653979 [3] NCCL INFO Connected all trees
dl:1653744:1653979 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:1653744:1653979 [3] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:1653744:1653979 [3] NCCL INFO comm 0x4e1607d0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 4a000 commId 0x408e9fc53d949dff - Init COMPLETE
[rank3]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
dl:1653741:1653978 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653741:1653978 [0] NCCL INFO NET/IB : No device found.
dl:1653741:1653978 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653741:1653978 [0] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:1653741:1653978 [0] NCCL INFO Using non-device net plugin version 0
dl:1653741:1653978 [0] NCCL INFO Using network Socket
dl:1653741:1653978 [0] NCCL INFO comm 0x44f87ea0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 2000 commId 0x408e9fc53d949dff - Init START
dl:1653741:1653978 [0] NCCL INFO NVLS multicast support is not available on dev 0
dl:1653741:1653978 [0] NCCL INFO comm 0x44f87ea0 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
dl:1653741:1653978 [0] NCCL INFO Channel 00/02 :    0   1   2   3
dl:1653741:1653978 [0] NCCL INFO Channel 01/02 :    0   1   2   3
dl:1653741:1653978 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
dl:1653741:1653978 [0] NCCL INFO P2P Chunksize set to 131072
dl:1653741:1653978 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
dl:1653741:1653978 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
dl:1653741:1653978 [0] NCCL INFO Connected all rings
dl:1653741:1653978 [0] NCCL INFO Connected all trees
dl:1653741:1653978 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:1653741:1653978 [0] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:1653741:1653978 [0] NCCL INFO comm 0x44f87ea0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 2000 commId 0x408e9fc53d949dff - Init COMPLETE
[rank0]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
dl:1653742:1653742 [1] NCCL INFO cudaDriverVersion 12050
dl:1653742:1653742 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653742:1653742 [1] NCCL INFO Bootstrap : Using enp67s0:128.97.91.100<0>
dl:1653742:1653742 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
dl:1653742:1653980 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653742:1653980 [1] NCCL INFO NET/IB : No device found.
dl:1653742:1653980 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
dl:1653742:1653980 [1] NCCL INFO NET/Socket : Using [0]enp67s0:128.97.91.100<0>
dl:1653742:1653980 [1] NCCL INFO Using non-device net plugin version 0
dl:1653742:1653980 [1] NCCL INFO Using network Socket
dl:1653742:1653980 [1] NCCL INFO comm 0x59ce0090 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 21000 commId 0x408e9fc53d949dff - Init START
dl:1653742:1653980 [1] NCCL INFO NVLS multicast support is not available on dev 1
dl:1653742:1653980 [1] NCCL INFO comm 0x59ce0090 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
dl:1653742:1653980 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
dl:1653742:1653980 [1] NCCL INFO P2P Chunksize set to 131072
dl:1653742:1653980 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
dl:1653742:1653980 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
dl:1653742:1653980 [1] NCCL INFO Connected all rings
dl:1653742:1653980 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
dl:1653742:1653980 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
dl:1653742:1653980 [1] NCCL INFO Connected all trees
dl:1653742:1653980 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
dl:1653742:1653980 [1] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
dl:1653742:1653980 [1] NCCL INFO comm 0x59ce0090 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 21000 commId 0x408e9fc53d949dff - Init COMPLETE
[rank1]:[W Utils.hpp:108] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarString)
[dl:0/4] 2025-01-09 04:33:43,402 (trainer:311) INFO: 1/70epoch started
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
[dl:0/4] 2025-01-09 04:39:26,872 (trainer:779) INFO: 1epoch:train:1-459batch: iter_time=4.084e-04, forward_time=0.207, loss_ctc=820.796, loss=820.796, backward_time=0.400, grad_norm=2.444e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.023, optim0_lr0=1.033e-07, train_time=11.994
[dl:0/4] 2025-01-09 04:44:24,242 (trainer:779) INFO: 1epoch:train:460-918batch: iter_time=2.724e-04, forward_time=0.202, loss_ctc=815.300, loss=815.300, backward_time=0.397, grad_norm=2.835e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.021, optim0_lr0=2.933e-07, train_time=10.376
[dl:0/4] 2025-01-09 04:49:35,130 (trainer:779) INFO: 1epoch:train:919-1377batch: iter_time=2.735e-04, forward_time=0.220, loss_ctc=792.735, loss=792.735, backward_time=0.405, grad_norm=5.410e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.021, optim0_lr0=4.867e-07, train_time=10.834
[dl:0/4] 2025-01-09 04:54:38,939 (trainer:779) INFO: 1epoch:train:1378-1836batch: iter_time=2.355e-04, forward_time=0.208, loss_ctc=651.194, loss=651.194, backward_time=0.400, grad_norm=5.732e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.021, optim0_lr0=6.767e-07, train_time=10.585
[dl:0/4] 2025-01-09 04:59:48,095 (trainer:779) INFO: 1epoch:train:1837-2295batch: iter_time=2.849e-04, forward_time=0.219, loss_ctc=560.796, loss=560.796, backward_time=0.400, grad_norm=4.010e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.022, optim0_lr0=8.667e-07, train_time=10.782
[dl:0/4] 2025-01-09 05:04:53,663 (trainer:779) INFO: 1epoch:train:2296-2754batch: iter_time=2.320e-04, forward_time=0.213, loss_ctc=480.817, loss=480.817, backward_time=0.400, grad_norm=2.325e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.020, optim0_lr0=1.060e-06, train_time=10.661
[dl:0/4] 2025-01-09 05:09:41,726 (trainer:779) INFO: 1epoch:train:2755-3213batch: iter_time=1.736e-04, forward_time=0.191, loss_ctc=419.804, loss=419.804, backward_time=0.386, grad_norm=1.545e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.016, optim0_lr0=1.250e-06, train_time=10.045
[dl:0/4] 2025-01-09 05:14:56,877 (trainer:779) INFO: 1epoch:train:3214-3672batch: iter_time=4.172e-04, forward_time=0.223, loss_ctc=383.714, loss=383.714, backward_time=0.406, grad_norm=1.326e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.022, optim0_lr0=1.440e-06, train_time=10.933
[dl:0/4] 2025-01-09 05:20:13,363 (trainer:779) INFO: 1epoch:train:3673-4131batch: iter_time=2.977e-04, forward_time=0.227, loss_ctc=335.541, loss=335.541, backward_time=0.403, grad_norm=1.116e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.024, optim0_lr0=1.633e-06, train_time=11.050
[dl:0/4] 2025-01-09 05:25:27,165 (trainer:779) INFO: 1epoch:train:4132-4590batch: iter_time=2.699e-04, forward_time=0.228, loss_ctc=349.711, loss=349.711, backward_time=0.403, grad_norm=1.068e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.025, optim0_lr0=1.823e-06, train_time=10.924
[dl:0/4] 2025-01-09 05:30:48,154 (trainer:779) INFO: 1epoch:train:4591-5049batch: iter_time=2.628e-04, forward_time=0.234, loss_ctc=346.744, loss=346.744, backward_time=0.409, grad_norm=1.034e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.025, optim0_lr0=2.013e-06, train_time=11.207
[dl:0/4] 2025-01-09 05:36:04,495 (trainer:779) INFO: 1epoch:train:5050-5508batch: iter_time=2.875e-04, forward_time=0.230, loss_ctc=339.174, loss=339.174, backward_time=0.404, grad_norm=1.001e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.024, optim0_lr0=2.207e-06, train_time=11.014
[dl:0/4] 2025-01-09 05:41:21,617 (trainer:779) INFO: 1epoch:train:5509-5967batch: iter_time=2.610e-04, forward_time=0.230, loss_ctc=320.948, loss=320.948, backward_time=0.403, grad_norm=996.920, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.038, optim0_lr0=2.397e-06, train_time=11.064
[dl:0/4] 2025-01-09 05:46:40,494 (trainer:779) INFO: 1epoch:train:5968-6426batch: iter_time=2.113e-04, forward_time=0.226, loss_ctc=237.084, loss=237.084, backward_time=0.409, grad_norm=1.664e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.022, optim0_lr0=2.587e-06, train_time=11.103
[dl:0/4] 2025-01-09 05:51:58,938 (trainer:779) INFO: 1epoch:train:6427-6885batch: iter_time=2.007e-04, forward_time=0.228, loss_ctc=178.305, loss=178.305, backward_time=0.406, grad_norm=496.213, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.021, optim0_lr0=2.780e-06, train_time=11.107
[dl:0/4] 2025-01-09 05:57:12,626 (trainer:779) INFO: 1epoch:train:6886-7344batch: iter_time=1.980e-04, forward_time=0.214, loss_ctc=172.182, loss=172.182, backward_time=0.401, grad_norm=394.727, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.020, optim0_lr0=2.973e-06, train_time=10.934
[dl:0/4] 2025-01-09 06:02:30,182 (trainer:779) INFO: 1epoch:train:7345-7803batch: iter_time=1.988e-04, forward_time=0.221, loss_ctc=171.158, loss=171.158, backward_time=0.403, grad_norm=360.756, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.020, optim0_lr0=3.163e-06, train_time=11.067
[dl:0/4] 2025-01-09 06:07:51,830 (trainer:779) INFO: 1epoch:train:7804-8262batch: iter_time=2.075e-04, forward_time=0.230, loss_ctc=168.643, loss=168.643, backward_time=0.406, grad_norm=321.227, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.021, optim0_lr0=3.353e-06, train_time=11.216
[dl:0/4] 2025-01-09 06:13:13,234 (trainer:779) INFO: 1epoch:train:8263-8721batch: iter_time=2.560e-04, forward_time=0.234, loss_ctc=164.246, loss=164.246, backward_time=0.404, grad_norm=282.617, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.023, optim0_lr0=3.547e-06, train_time=11.199
[dl:0/4] 2025-01-09 06:18:29,370 (trainer:779) INFO: 1epoch:train:8722-9180batch: iter_time=2.444e-04, forward_time=0.225, loss_ctc=155.266, loss=155.266, backward_time=0.403, grad_norm=244.800, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.024, optim0_lr0=3.737e-06, train_time=11.032
/data/mohan/workdir/espnet/espnet2/train/reporter.py:79: UserWarning: No valid stats found
  warnings.warn("No valid stats found")
/data/mohan/workdir/espnet/espnet2/train/reporter.py:79: UserWarning: No valid stats found
  warnings.warn("No valid stats found")
/data/mohan/workdir/espnet/espnet2/train/reporter.py:79: UserWarning: No valid stats found
  warnings.warn("No valid stats found")
/data/mohan/workdir/espnet/espnet2/train/reporter.py:79: UserWarning: No valid stats found
  warnings.warn("No valid stats found")
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
[dl:0/4] 2025-01-09 06:27:30,739 (trainer:365) INFO: 1epoch results: [train] iter_time=2.595e-04, forward_time=0.221, loss_ctc=392.333, loss=392.333, backward_time=0.402, grad_norm=1.727e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.023, optim0_lr0=1.923e-06, train_time=10.956, time=1 hour, 45 minutes and 8.35 seconds, total_count=9198, gpu_max_cached_mem_GB=15.240, [valid] loss_ctc=154.724, cer_ctc=1.000, loss_att=nan, acc=nan, cer=nan, wer=nan, loss=154.724, time=7 minutes and 6.43 seconds, total_count=1483, gpu_max_cached_mem_GB=15.240, [att_plot] time=1 minute and 32.49 seconds, total_count=0, gpu_max_cached_mem_GB=15.240
[dl:0/4] 2025-01-09 06:27:56,053 (trainer:433) INFO: The best model has been updated: valid.cer_ctc
[dl:0/4] 2025-01-09 06:27:56,086 (trainer:299) INFO: 2/70epoch started. Estimated time to finish: 5 days, 11 hours and 20 minutes
W0109 06:35:02.523000 136733754685248 torch/multiprocessing/spawn.py:145] Terminating process 1653742 via signal SIGTERM
W0109 06:35:02.641000 136733754685248 torch/multiprocessing/spawn.py:145] Terminating process 1653743 via signal SIGTERM
W0109 06:35:02.642000 136733754685248 torch/multiprocessing/spawn.py:145] Terminating process 1653744 via signal SIGTERM
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1273, in main
    while not ProcessContext(processes, error_files).join():
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 169, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGKILL
/data/mohan/anaconda3/envs/espnet/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 80 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
# Accounting: time=7333 threads=1
# Ended (code 1) at Thu Jan  9 06:35:12 EST 2025, elapsed time 7333 seconds

>>>>>>> 5ebf8f1b6dc51bfa26a032f85e8e4b6ca01d6936
2025-03-20T17:28:18 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-20T17:28:18 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-20T17:28:18 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-20T17:28:18 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-20T17:28:18 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-20 17:28:18,886 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-20 17:28:18,899 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', '--gpu', '1', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Thu Mar 20 17:28:18 PDT 2025
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
usage: asr_train.py [-h] [--config CONFIG] [--print_config]
                    [--log_level {ERROR,WARNING,INFO,DEBUG,NOTSET}]
                    [--drop_last_iter DROP_LAST_ITER] [--dry_run DRY_RUN]
                    [--iterator_type {sequence,category,chunk,task,none}]
                    [--valid_iterator_type {sequence,category,chunk,task,none}]
                    [--output_dir OUTPUT_DIR] [--ngpu NGPU] [--seed SEED]
                    [--num_workers NUM_WORKERS] [--num_att_plot NUM_ATT_PLOT]
                    [--dist_backend DIST_BACKEND]
                    [--dist_init_method DIST_INIT_METHOD]
                    [--dist_world_size DIST_WORLD_SIZE]
                    [--dist_rank DIST_RANK] [--local_rank LOCAL_RANK]
                    [--dist_master_addr DIST_MASTER_ADDR]
                    [--dist_master_port DIST_MASTER_PORT]
                    [--dist_launcher {slurm,mpi,None}]
                    [--multiprocessing_distributed MULTIPROCESSING_DISTRIBUTED]
                    [--unused_parameters UNUSED_PARAMETERS]
                    [--sharded_ddp SHARDED_DDP]
                    [--use_deepspeed USE_DEEPSPEED]
                    [--deepspeed_config DEEPSPEED_CONFIG]
                    [--cudnn_enabled CUDNN_ENABLED]
                    [--cudnn_benchmark CUDNN_BENCHMARK]
                    [--cudnn_deterministic CUDNN_DETERMINISTIC]
                    [--use_tf32 USE_TF32] [--collect_stats COLLECT_STATS]
                    [--write_collected_feats WRITE_COLLECTED_FEATS]
                    [--max_epoch MAX_EPOCH] [--patience PATIENCE]
                    [--val_scheduler_criterion VAL_SCHEDULER_CRITERION VAL_SCHEDULER_CRITERION]
                    [--early_stopping_criterion EARLY_STOPPING_CRITERION EARLY_STOPPING_CRITERION EARLY_STOPPING_CRITERION]
                    [--best_model_criterion BEST_MODEL_CRITERION [BEST_MODEL_CRITERION ...]]
                    [--keep_nbest_models KEEP_NBEST_MODELS [KEEP_NBEST_MODELS ...]]
                    [--nbest_averaging_interval NBEST_AVERAGING_INTERVAL]
                    [--grad_clip GRAD_CLIP] [--grad_clip_type GRAD_CLIP_TYPE]
                    [--grad_noise GRAD_NOISE] [--accum_grad ACCUM_GRAD]
                    [--no_forward_run NO_FORWARD_RUN] [--resume RESUME]
                    [--train_dtype {float16,float32,float64}]
                    [--use_amp USE_AMP] [--log_interval LOG_INTERVAL]
                    [--use_matplotlib USE_MATPLOTLIB]
                    [--use_tensorboard USE_TENSORBOARD]
                    [--create_graph_in_tensorboard CREATE_GRAPH_IN_TENSORBOARD]
                    [--use_wandb USE_WANDB] [--wandb_project WANDB_PROJECT]
                    [--wandb_id WANDB_ID] [--wandb_entity WANDB_ENTITY]
                    [--wandb_name WANDB_NAME]
                    [--wandb_model_log_interval WANDB_MODEL_LOG_INTERVAL]
                    [--detect_anomaly DETECT_ANOMALY]
                    [--use_adapter USE_ADAPTER] [--adapter {lora,houlsby}]
                    [--save_strategy {all,adapter_only,required_grad_only}]
                    [--adapter_conf ADAPTER_CONF]
                    [--pretrain_path PRETRAIN_PATH]
                    [--init_param [INIT_PARAM ...]]
                    [--ignore_init_mismatch IGNORE_INIT_MISMATCH]
                    [--freeze_param [FREEZE_PARAM ...]]
                    [--num_iters_per_epoch NUM_ITERS_PER_EPOCH]
                    [--batch_size BATCH_SIZE]
                    [--valid_batch_size VALID_BATCH_SIZE]
                    [--batch_bins BATCH_BINS]
                    [--valid_batch_bins VALID_BATCH_BINS]
                    [--category_sample_size CATEGORY_SAMPLE_SIZE]
                    [--train_shape_file TRAIN_SHAPE_FILE]
                    [--valid_shape_file VALID_SHAPE_FILE]
                    [--batch_type {unsorted,sorted,folded,length,numel}]
                    [--valid_batch_type {unsorted,sorted,folded,length,numel,None}]
                    [--fold_length FOLD_LENGTH]
                    [--sort_in_batch {descending,ascending}]
                    [--shuffle_within_batch SHUFFLE_WITHIN_BATCH]
                    [--sort_batch {descending,ascending}]
                    [--multiple_iterator MULTIPLE_ITERATOR]
                    [--chunk_length CHUNK_LENGTH]
                    [--chunk_shift_ratio CHUNK_SHIFT_RATIO]
                    [--num_cache_chunks NUM_CACHE_CHUNKS]
                    [--chunk_excluded_key_prefixes CHUNK_EXCLUDED_KEY_PREFIXES [CHUNK_EXCLUDED_KEY_PREFIXES ...]]
                    [--chunk_default_fs CHUNK_DEFAULT_FS]
                    [--chunk_max_abs_length CHUNK_MAX_ABS_LENGTH]
                    [--chunk_discard_short_samples CHUNK_DISCARD_SHORT_SAMPLES]
                    [--train_data_path_and_name_and_type TRAIN_DATA_PATH_AND_NAME_AND_TYPE]
                    [--valid_data_path_and_name_and_type VALID_DATA_PATH_AND_NAME_AND_TYPE]
                    [--multi_task_dataset MULTI_TASK_DATASET]
                    [--allow_variable_data_keys ALLOW_VARIABLE_DATA_KEYS]
                    [--max_cache_size MAX_CACHE_SIZE]
                    [--max_cache_fd MAX_CACHE_FD]
                    [--allow_multi_rates ALLOW_MULTI_RATES]
                    [--valid_max_cache_size VALID_MAX_CACHE_SIZE]
                    [--exclude_weight_decay EXCLUDE_WEIGHT_DECAY]
                    [--exclude_weight_decay_conf EXCLUDE_WEIGHT_DECAY_CONF]
                    [--optim {adam,adamw,sgd,adadelta,adagrad,adamax,asgd,lbfgs,rmsprop,rprop,radam}]
                    [--optim_conf OPTIM_CONF]
                    [--scheduler {reducelronplateau,lambdalr,steplr,multisteplr,exponentiallr,cosineannealinglr,noamlr,warmuplr,piecewiselinearwarmuplr,warmupsteplr,warmupreducelronplateau,cycliclr,onecyclelr,cosineannealingwarmrestarts,cosineannealingwarmuprestarts,None}]
                    [--scheduler_conf SCHEDULER_CONF]
                    [--token_list TOKEN_LIST]
                    [--init {chainer,xavier_uniform,xavier_normal,kaiming_uniform,kaiming_normal,None}]
                    [--input_size INPUT_SIZE] [--ctc_conf CTC_CONF]
                    [--joint_net_conf JOINT_NET_CONF]
                    [--use_preprocessor USE_PREPROCESSOR]
                    [--use_lang_prompt USE_LANG_PROMPT]
                    [--use_nlp_prompt USE_NLP_PROMPT]
                    [--token_type {bpe,char,word,phn,hugging_face,whisper_en,whisper_multilingual}]
                    [--bpemodel BPEMODEL]
                    [--non_linguistic_symbols NON_LINGUISTIC_SYMBOLS]
                    [--cleaner {None,tacotron,jaconv,vietnamese,whisper_en,whisper_basic}]
                    [--g2p {None,g2p_en,g2p_en_no_space,pyopenjtalk,pyopenjtalk_kana,pyopenjtalk_accent,pyopenjtalk_accent_with_pause,pyopenjtalk_prosody,pypinyin_g2p,pypinyin_g2p_phone,pypinyin_g2p_phone_without_prosody,espeak_ng_arabic,espeak_ng_german,espeak_ng_french,espeak_ng_spanish,espeak_ng_russian,espeak_ng_greek,espeak_ng_finnish,espeak_ng_hungarian,espeak_ng_dutch,espeak_ng_english_us_vits,espeak_ng_hindi,espeak_ng_italian,espeak_ng_ukrainian,espeak_ng_polish,g2pk,g2pk_no_space,g2pk_explicit_space,korean_jaso,korean_jaso_no_space,g2p_is}]
                    [--speech_volume_normalize SPEECH_VOLUME_NORMALIZE]
                    [--rir_scp RIR_SCP] [--rir_apply_prob RIR_APPLY_PROB]
                    [--noise_scp NOISE_SCP]
                    [--noise_apply_prob NOISE_APPLY_PROB]
                    [--noise_db_range NOISE_DB_RANGE]
                    [--short_noise_thres SHORT_NOISE_THRES]
                    [--aux_ctc_tasks AUX_CTC_TASKS [AUX_CTC_TASKS ...]]
                    [--frontend {default,sliding_window,s3prl,hf_freeze_ctc,hf_train_fsq_ctc,hf_train_fsq_240_ctc,hf_train_fsq_1000_ctc,fused,whisper}]
                    [--frontend_conf FRONTEND_CONF] [--specaug {specaug,None}]
                    [--specaug_conf SPECAUG_CONF]
                    [--normalize {global_mvn,utterance_mvn,None}]
                    [--normalize_conf NORMALIZE_CONF]
                    [--model {espnet,maskctc,pit_espnet}]
                    [--model_conf MODEL_CONF]
                    [--preencoder {sinc,linear,None}]
                    [--preencoder_conf PREENCODER_CONF]
                    [--encoder {conformer,transformer,transformer_multispkr,contextual_block_transformer,contextual_block_conformer,vgg_rnn,rnn,wav2vec2,hubert,hubert_pretrain,torchaudiohubert,longformer,branchformer,whisper,e_branchformer,avhubert,multiconv_conformer}]
                    [--encoder_conf ENCODER_CONF]
                    [--postencoder {hugging_face_transformers,length_adaptor,None}]
                    [--postencoder_conf POSTENCODER_CONF]
                    [--decoder {transformer,lightweight_conv,lightweight_conv2d,dynamic_conv,dynamic_conv2d,rnn,transducer,mlm,whisper,hugging_face_transformers,s4,None}]
                    [--decoder_conf DECODER_CONF]
                    [--preprocessor {default,multi}]
                    [--preprocessor_conf PREPROCESSOR_CONF]
asr_train.py: error: No such file: conf/train_asr_wavlm_base_plus_myst_fsq_240_ebranchformer_onlyctc_lr1e-4.yaml
# Accounting: time=7 threads=1
# Ended (code 2) at Thu Mar 20 17:28:25 PDT 2025, elapsed time 7 seconds

2025-03-20T17:28:48 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-20T17:28:48 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-20T17:28:48 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-20T17:28:48 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-20T17:28:48 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-20 17:28:48,883 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-20 17:28:48,897 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', '--gpu', '1', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Thu Mar 20 17:28:48 PDT 2025
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[lambda-Lambda-Vector] 2025-03-20 17:28:54,134 (asr:531) INFO: Vocabulary size: 5000
[lambda-Lambda-Vector] 2025-03-20 17:28:55,330 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
[lambda-Lambda-Vector] 2025-03-20 17:28:55,612 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[lambda-Lambda-Vector] 2025-03-20 17:28:55,619 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): HfCTCFSQFrontend(
    (upstream): WavLMForCTC(
      (wavlm): WavLMModel(
        (feature_extractor): WavLMFeatureEncoder(
          (conv_layers): ModuleList(
            (0): WavLMLayerNormConvLayer(
              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (activation): GELUActivation()
            )
            (1-4): 4 x WavLMLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (activation): GELUActivation()
            )
            (5-6): 2 x WavLMLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (activation): GELUActivation()
            )
          )
        )
        (feature_projection): WavLMFeatureProjection(
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (projection): Linear(in_features=512, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (encoder): WavLMEncoderStableLayerNorm(
          (pos_conv_embed): WavLMPositionalConvEmbedding(
            (conv): ParametrizedConv1d(
              1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16
              (parametrizations): ModuleDict(
                (weight): ParametrizationList(
                  (0): _WeightNorm()
                )
              )
            )
            (padding): WavLMSamePadLayer()
            (activation): GELUActivation()
          )
          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (layers): ModuleList(
            (0): WavLMEncoderLayerStableLayerNorm(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
                (rel_attn_embed): Embedding(320, 16)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=4096, out_features=1024, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
            (1-23): 23 x WavLMEncoderLayerStableLayerNorm(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=4096, out_features=1024, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (lm_head): Linear(in_features=1024, out_features=42, bias=True)
    )
    (quantizer): FSQ(
      (project_in): Linear(in_features=1024, out_features=5, bias=True)
      (project_out): Linear(in_features=5, out_features=1024, bias=True)
    )
  )
  (preencoder): LinearProjection(
    (linear_out): Linear(in_features=768, out_features=128, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=7936, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 342.83 M
    Number of trainable parameters: 338.62 M (98.8%)
    Size: 1.35 GB
    Type: torch.float32
[lambda-Lambda-Vector] 2025-03-20 17:28:55,619 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 6.666666666666668e-09
    maximize: False
    weight_decay: 1e-06
)
[lambda-Lambda-Vector] 2025-03-20 17:28:55,619 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[lambda-Lambda-Vector] 2025-03-20 17:28:55,619 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/config.yaml
[lambda-Lambda-Vector] 2025-03-20 17:28:55,742 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-20 17:28:56,038 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x74d06c589c60>)
[lambda-Lambda-Vector] 2025-03-20 17:28:56,038 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=911, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2025-03-20 17:28:56,039 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=911, mean=61.1, min=16, max=362
[lambda-Lambda-Vector] 2025-03-20 17:28:56,048 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-20 17:28:56,182 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x74d06c132860>)
[lambda-Lambda-Vector] 2025-03-20 17:28:56,182 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=145, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2025-03-20 17:28:56,183 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=145, mean=62.3, min=1, max=309
[lambda-Lambda-Vector] 2025-03-20 17:28:56,191 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-20 17:28:56,197 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x74d06c1320e0>)
[lambda-Lambda-Vector] 2025-03-20 17:28:56,197 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[lambda-Lambda-Vector] 2025-03-20 17:28:56,197 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[lambda-Lambda-Vector] 2025-03-20 17:28:56,198 (trainer:311) INFO: 1/70epoch started
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1210, in main
    cls.main_worker(args)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 614, in train_one_epoch
    retval = model(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 237, in forward
    encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 378, in encode
    feats, feats_lengths = self._extract_feats(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 445, in _extract_feats
    feats, feats_lengths = self.frontend(speech, speech_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/frontend/hf_fsq_train.py", line 100, in forward
    outputs = self.upstream.wavlm(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 1227, in forward
    encoder_outputs = self.encoder(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 807, in forward
    layer_outputs = layer(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 650, in forward
    hidden_states, attn_weights, position_bias = self.attention(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 473, in forward
    attn_output, attn_weights = self.torch_multi_head_self_attention(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 497, in torch_multi_head_self_attention
    attn_output, attn_weights = F.multi_head_attention_forward(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py", line 5504, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.78 GiB. GPU 
# Accounting: time=11 threads=1
# Ended (code 1) at Thu Mar 20 17:28:59 PDT 2025, elapsed time 11 seconds

2025-03-20T17:30:02 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-20T17:30:02 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-20T17:30:02 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-20T17:30:02 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-20T17:30:02 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-20 17:30:02,936 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-20 17:30:02,949 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', '--gpu', '1', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Thu Mar 20 17:30:02 PDT 2025
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[lambda-Lambda-Vector] 2025-03-20 17:30:08,292 (asr:531) INFO: Vocabulary size: 5000
[lambda-Lambda-Vector] 2025-03-20 17:30:08,705 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
[lambda-Lambda-Vector] 2025-03-20 17:30:08,895 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[lambda-Lambda-Vector] 2025-03-20 17:30:08,901 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): HfCTCFSQFrontend(
    (upstream): WavLMForCTC(
      (wavlm): WavLMModel(
        (feature_extractor): WavLMFeatureEncoder(
          (conv_layers): ModuleList(
            (0): WavLMGroupNormConvLayer(
              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
              (activation): GELUActivation()
              (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)
            )
            (1-4): 4 x WavLMNoLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
              (activation): GELUActivation()
            )
            (5-6): 2 x WavLMNoLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
              (activation): GELUActivation()
            )
          )
        )
        (feature_projection): WavLMFeatureProjection(
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (projection): Linear(in_features=512, out_features=768, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (encoder): WavLMEncoder(
          (pos_conv_embed): WavLMPositionalConvEmbedding(
            (conv): ParametrizedConv1d(
              768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16
              (parametrizations): ModuleDict(
                (weight): ParametrizationList(
                  (0): _WeightNorm()
                )
              )
            )
            (padding): WavLMSamePadLayer()
            (activation): GELUActivation()
          )
          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (layers): ModuleList(
            (0): WavLMEncoderLayer(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
                (rel_attn_embed): Embedding(320, 12)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=3072, out_features=768, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1-11): 11 x WavLMEncoderLayer(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=3072, out_features=768, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (lm_head): Linear(in_features=768, out_features=42, bias=True)
    )
    (quantizer): FSQ(
      (project_in): Linear(in_features=768, out_features=5, bias=True)
      (project_out): Linear(in_features=5, out_features=768, bias=True)
    )
  )
  (preencoder): LinearProjection(
    (linear_out): Linear(in_features=768, out_features=128, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=7936, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 121.74 M
    Number of trainable parameters: 117.54 M (96.5%)
    Size: 470.16 MB
    Type: torch.float32
[lambda-Lambda-Vector] 2025-03-20 17:30:08,901 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 6.666666666666668e-09
    maximize: False
    weight_decay: 1e-06
)
[lambda-Lambda-Vector] 2025-03-20 17:30:08,901 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[lambda-Lambda-Vector] 2025-03-20 17:30:08,901 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/config.yaml
[lambda-Lambda-Vector] 2025-03-20 17:30:09,018 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-20 17:30:09,315 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x74bf0fbb4160>)
[lambda-Lambda-Vector] 2025-03-20 17:30:09,315 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=911, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2025-03-20 17:30:09,315 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=911, mean=61.1, min=16, max=362
[lambda-Lambda-Vector] 2025-03-20 17:30:09,324 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-20 17:30:09,351 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x74bf0fbb27d0>)
[lambda-Lambda-Vector] 2025-03-20 17:30:09,351 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=145, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2025-03-20 17:30:09,351 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=145, mean=62.3, min=1, max=309
[lambda-Lambda-Vector] 2025-03-20 17:30:09,359 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-20 17:30:09,472 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x74bf0fbb0bb0>)
[lambda-Lambda-Vector] 2025-03-20 17:30:09,472 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[lambda-Lambda-Vector] 2025-03-20 17:30:09,473 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[lambda-Lambda-Vector] 2025-03-20 17:30:09,475 (trainer:311) INFO: 1/70epoch started
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1210, in main
    cls.main_worker(args)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 614, in train_one_epoch
    retval = model(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 237, in forward
    encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 378, in encode
    feats, feats_lengths = self._extract_feats(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 445, in _extract_feats
    feats, feats_lengths = self.frontend(speech, speech_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/frontend/hf_fsq_train.py", line 105, in forward
    feats = outputs[-1][self.layer]
IndexError: tuple index out of range
# Accounting: time=9 threads=1
# Ended (code 1) at Thu Mar 20 17:30:11 PDT 2025, elapsed time 9 seconds

2025-03-20T17:30:35 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-20T17:30:35 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-20T17:30:35 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-20T17:30:35 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-20T17:30:35 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-20 17:30:35,911 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-20 17:30:35,924 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', '--gpu', '1', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Thu Mar 20 17:30:35 PDT 2025
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[lambda-Lambda-Vector] 2025-03-20 17:30:41,228 (asr:531) INFO: Vocabulary size: 5000
[lambda-Lambda-Vector] 2025-03-20 17:30:41,637 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
[lambda-Lambda-Vector] 2025-03-20 17:30:41,828 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[lambda-Lambda-Vector] 2025-03-20 17:30:41,833 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): HfCTCFSQFrontend(
    (upstream): WavLMForCTC(
      (wavlm): WavLMModel(
        (feature_extractor): WavLMFeatureEncoder(
          (conv_layers): ModuleList(
            (0): WavLMGroupNormConvLayer(
              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
              (activation): GELUActivation()
              (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)
            )
            (1-4): 4 x WavLMNoLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
              (activation): GELUActivation()
            )
            (5-6): 2 x WavLMNoLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
              (activation): GELUActivation()
            )
          )
        )
        (feature_projection): WavLMFeatureProjection(
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (projection): Linear(in_features=512, out_features=768, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (encoder): WavLMEncoder(
          (pos_conv_embed): WavLMPositionalConvEmbedding(
            (conv): ParametrizedConv1d(
              768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16
              (parametrizations): ModuleDict(
                (weight): ParametrizationList(
                  (0): _WeightNorm()
                )
              )
            )
            (padding): WavLMSamePadLayer()
            (activation): GELUActivation()
          )
          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (layers): ModuleList(
            (0): WavLMEncoderLayer(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
                (rel_attn_embed): Embedding(320, 12)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=3072, out_features=768, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1-11): 11 x WavLMEncoderLayer(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=3072, out_features=768, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (lm_head): Linear(in_features=768, out_features=42, bias=True)
    )
    (quantizer): FSQ(
      (project_in): Linear(in_features=768, out_features=5, bias=True)
      (project_out): Linear(in_features=5, out_features=768, bias=True)
    )
  )
  (preencoder): LinearProjection(
    (linear_out): Linear(in_features=768, out_features=128, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=7936, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 121.74 M
    Number of trainable parameters: 117.54 M (96.5%)
    Size: 470.16 MB
    Type: torch.float32
[lambda-Lambda-Vector] 2025-03-20 17:30:41,833 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 6.666666666666668e-09
    maximize: False
    weight_decay: 1e-06
)
[lambda-Lambda-Vector] 2025-03-20 17:30:41,833 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[lambda-Lambda-Vector] 2025-03-20 17:30:41,833 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/config.yaml
[lambda-Lambda-Vector] 2025-03-20 17:30:41,952 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-20 17:30:42,256 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7b8c4bbcc400>)
[lambda-Lambda-Vector] 2025-03-20 17:30:42,256 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=911, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2025-03-20 17:30:42,256 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=911, mean=61.1, min=16, max=362
[lambda-Lambda-Vector] 2025-03-20 17:30:42,264 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-20 17:30:42,292 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7b8c4bb21750>)
[lambda-Lambda-Vector] 2025-03-20 17:30:42,292 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=145, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2025-03-20 17:30:42,292 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=145, mean=62.3, min=1, max=309
[lambda-Lambda-Vector] 2025-03-20 17:30:42,300 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-20 17:30:42,415 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7b8c4bb21d50>)
[lambda-Lambda-Vector] 2025-03-20 17:30:42,415 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[lambda-Lambda-Vector] 2025-03-20 17:30:42,415 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[lambda-Lambda-Vector] 2025-03-20 17:30:42,417 (trainer:311) INFO: 1/70epoch started
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1210, in main
    cls.main_worker(args)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 614, in train_one_epoch
    retval = model(**batch)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 237, in forward
    encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 378, in encode
    feats, feats_lengths = self._extract_feats(speech, speech_lengths)
  File "/data/mohan/workdir/espnet/espnet2/asr/espnet_model.py", line 445, in _extract_feats
    feats, feats_lengths = self.frontend(speech, speech_lengths)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/workdir/espnet/espnet2/asr/frontend/hf_fsq_train.py", line 100, in forward
    outputs = self.upstream.wavlm(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 1227, in forward
    encoder_outputs = self.encoder(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 722, in forward
    layer_outputs = layer(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 608, in forward
    hidden_states, attn_weights, position_bias = self.attention(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 473, in forward
    attn_output, attn_weights = self.torch_multi_head_self_attention(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py", line 497, in torch_multi_head_self_attention
    attn_output, attn_weights = F.multi_head_attention_forward(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py", line 5450, in multi_head_attention_forward
    attn_mask = attn_mask + key_padding_mask
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 
# Accounting: time=12 threads=1
# Ended (code 1) at Thu Mar 20 17:30:47 PDT 2025, elapsed time 12 seconds

2025-03-20T17:31:15 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-20T17:31:15 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-20T17:31:15 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-20T17:31:15 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-20T17:31:15 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-20 17:31:15,749 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-20 17:31:15,762 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
2025-03-20T17:34:26 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-20T17:34:26 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-20T17:34:26 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-20T17:34:26 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-20T17:34:26 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-20 17:34:26,183 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-20 17:34:26,196 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
2025-03-20T23:43:18 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-20T23:43:18 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-20T23:43:18 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-20T23:43:18 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-20T23:43:18 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-20 23:43:18,698 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-20 23:43:18,709 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
2025-03-21T15:37:38 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-21T15:37:38 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-21T15:37:38 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-21T15:37:38 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-21T15:37:38 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-21 15:37:38,883 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-21 15:37:38,894 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
2025-03-22T05:26:42 (asr_fsq.sh:1942:main) Successfully finished. [elapsed=49744s]
2025-03-29T15:00:56 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-29T15:00:56 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-29T15:00:56 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-29T15:00:56 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-29T15:00:56 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-29 15:00:56,900 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-29 15:00:56,913 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
run.pl: job failed, log is in exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', '--gpu', '1', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_unigram5000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_unigram5000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,sound', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000', '--config', 'conf/train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', 'dump/raw/train/wav.scp,speech,sound', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log ###################
# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Sat Mar 29 15:00:56 PDT 2025
#
/data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[lambda-Lambda-Vector] 2025-03-29 15:01:02,831 (asr:531) INFO: Vocabulary size: 5000
[lambda-Lambda-Vector] 2025-03-29 15:01:03,204 (espnet_model:169) WARNING: Set decoder to none as ctc_weight==1.0
[lambda-Lambda-Vector] 2025-03-29 15:01:04,169 (abs_task:1387) INFO: pytorch.version=2.3.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[lambda-Lambda-Vector] 2025-03-29 15:01:04,174 (abs_task:1388) INFO: Model structure:
ESPnetASRModel(
  (frontend): HfCTCFSQFrontend(
    (upstream): WavLMForCTC(
      (wavlm): WavLMModel(
        (feature_extractor): WavLMFeatureEncoder(
          (conv_layers): ModuleList(
            (0): WavLMGroupNormConvLayer(
              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
              (activation): GELUActivation()
              (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)
            )
            (1-4): 4 x WavLMNoLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
              (activation): GELUActivation()
            )
            (5-6): 2 x WavLMNoLayerNormConvLayer(
              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
              (activation): GELUActivation()
            )
          )
        )
        (feature_projection): WavLMFeatureProjection(
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (projection): Linear(in_features=512, out_features=768, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (encoder): WavLMEncoder(
          (pos_conv_embed): WavLMPositionalConvEmbedding(
            (conv): ParametrizedConv1d(
              768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16
              (parametrizations): ModuleDict(
                (weight): ParametrizationList(
                  (0): _WeightNorm()
                )
              )
            )
            (padding): WavLMSamePadLayer()
            (activation): GELUActivation()
          )
          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (layers): ModuleList(
            (0): WavLMEncoderLayer(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
                (rel_attn_embed): Embedding(320, 12)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=3072, out_features=768, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1-11): 11 x WavLMEncoderLayer(
              (attention): WavLMAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (feed_forward): WavLMFeedForward(
                (intermediate_dropout): Dropout(p=0.1, inplace=False)
                (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
                (output_dense): Linear(in_features=3072, out_features=768, bias=True)
                (output_dropout): Dropout(p=0.0, inplace=False)
              )
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (lm_head): Linear(in_features=768, out_features=42, bias=True)
    )
    (quantizer): FSQ(
      (project_in): Linear(in_features=768, out_features=3, bias=True)
      (project_out): Linear(in_features=3, out_features=768, bias=True)
    )
  )
  (preencoder): LinearProjection(
    (linear_out): Linear(in_features=768, out_features=128, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (encoder): EBranchformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=7936, out_features=256, bias=True)
        (1): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (2): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (3): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (4): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (5): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (6): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (7): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (8): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (9): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (10): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
      (11): EBranchformerEncoderLayer(
        (attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (q_norm): Identity()
          (k_norm): Identity()
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (cgmlp): ConvolutionalGatingMLP(
          (channel_proj1): Sequential(
            (0): Linear(in_features=256, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
          (csgu): ConvolutionalSpatialGatingUnit(
            (norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
            (act): Identity()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (channel_proj2): Linear(in_features=512, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mlp): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (depthwise_conv_fusion): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
        (merge_proj): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 121.74 M
    Number of trainable parameters: 117.54 M (96.5%)
    Size: 470.15 MB
    Type: torch.float32
[lambda-Lambda-Vector] 2025-03-29 15:01:04,174 (abs_task:1391) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 6.666666666666668e-09
    maximize: False
    weight_decay: 1e-06
)
[lambda-Lambda-Vector] 2025-03-29 15:01:04,174 (abs_task:1392) INFO: Scheduler: WarmupLR(warmup_steps=15000)
[lambda-Lambda-Vector] 2025-03-29 15:01:04,175 (abs_task:1401) INFO: Saving the configuration in exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/config.yaml
[lambda-Lambda-Vector] 2025-03-29 15:01:04,290 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-29 15:01:04,636 (abs_task:1810) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/train/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7e4d4f47b430>)
[lambda-Lambda-Vector] 2025-03-29 15:01:04,636 (abs_task:1811) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=1737, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2025-03-29 15:01:04,636 (abs_task:1812) INFO: [train] mini-batch sizes summary: N-batch=1737, mean=32.1, min=6, max=201
[lambda-Lambda-Vector] 2025-03-29 15:01:04,645 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-29 15:01:04,672 (abs_task:1810) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7e4d4f47bca0>)
[lambda-Lambda-Vector] 2025-03-29 15:01:04,672 (abs_task:1811) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=275, batch_bins=8000000, sort_in_batch=descending, sort_batch=descending)
[lambda-Lambda-Vector] 2025-03-29 15:01:04,672 (abs_task:1812) INFO: [valid] mini-batch sizes summary: N-batch=275, mean=32.9, min=5, max=156
[lambda-Lambda-Vector] 2025-03-29 15:01:04,681 (asr:503) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[lambda-Lambda-Vector] 2025-03-29 15:01:04,686 (abs_task:1810) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7e4d4f47a860>)
[lambda-Lambda-Vector] 2025-03-29 15:01:04,686 (abs_task:1811) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=9037, batch_size=1, key_file=exp/asr_stats_raw_en_bpe5000/valid/speech_shape, 
[lambda-Lambda-Vector] 2025-03-29 15:01:04,686 (abs_task:1812) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[lambda-Lambda-Vector] 2025-03-29 15:01:04,688 (trainer:311) INFO: 1/70epoch started
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/data/mohan/workdir/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1210, in main
    cls.main_worker(args)
  File "/data/mohan/workdir/espnet/espnet2/tasks/abs_task.py", line 1571, in main_worker
    cls.trainer.run(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/data/mohan/workdir/espnet/espnet2/train/trainer.py", line 677, in train_one_epoch
    scaler.scale(loss).backward()
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/data/mohan/anaconda3/envs/espnet/lib/python3.10/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 966.00 MiB. GPU 
# Accounting: time=39 threads=1
# Ended (code 1) at Sat Mar 29 15:01:35 PDT 2025, elapsed time 39 seconds

2025-03-29T15:02:34 (asr_fsq.sh:283:main) ./asr_fsq.sh --stage 11 --stop_stage 11 --lang en --ngpu 1 --nj 4 --gpu_inference true --inference_asr_model valid.cer_ctc.best.pth --inference_nj 1 --nbpe 5000 --max_wav_duration 30 --feats_normalize None --audio_format flac --feats_type raw --use_lm false --asr_config conf/train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4.yaml --inference_config conf/decode_asr_ctc_greedy.yaml --train_set train --valid_set dev --test_sets test --lm_train_text dump/raw/train/text --bpe_train_text dump/raw/train/text
2025-03-29T15:02:34 (asr_fsq.sh:564:main) Skipped stages:  6 7 8 9 14 15 
2025-03-29T15:02:34 (asr_fsq.sh:1308:main) Stage 11: ASR Training: train_set=dump/raw/train, valid_set=dump/raw/dev
2025-03-29T15:02:34 (asr_fsq.sh:1407:main) Generate 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/run.sh'. You can resume the process from stage 11 using this script
2025-03-29T15:02:34 (asr_fsq.sh:1411:main) ASR training started... log: 'exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log'
2025-03-29 15:02:35,059 (launch:94) INFO: /data/mohan/anaconda3/envs/espnet/bin/python3 /data/mohan/workdir/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log' --log exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,sound --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000 --config conf/train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --train_shape_file exp/asr_stats_raw_en_bpe5000/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe5000/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe5000/valid/text_shape.bpe
2025-03-29 15:02:35,070 (launch:348) INFO: log file: exp/asr_train_asr_wavlm_base_plus_myst_fsq_512_ebranchformer_onlyctc_lr1e-4_raw_en_bpe5000/train.log
2025-03-31T08:35:19 (asr_fsq.sh:1942:main) Successfully finished. [elapsed=149565s]
